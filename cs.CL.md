# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) | PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。 |
| [^2] | [Can large language models explore in-context?](https://arxiv.org/abs/2403.15371) | 研究发现，大型语言模型在没有实质干预的情况下很难有效进行探索，除了特定配置下的GPT-4具有满意的探索行为外，其他模型表现不稳定。 |
| [^3] | [A Transfer Attack to Image Watermarks](https://arxiv.org/abs/2403.15365) | 水印领域的研究表明，即使在攻击者无法访问水印模型或检测API的情况下，水印基础的AI生成图像检测器也无法抵抗对抗攻击。 |
| [^4] | [Towards Knowledge-Grounded Natural Language Understanding and Generation](https://arxiv.org/abs/2403.15364) | 本论文研究了如何利用知识表示来改进自然语言理解和生成，发现整合实体相关知识有助于假新闻检测，实体焦点代码切换改善了跨语言转移性能。 |
| [^5] | [CoLLEGe: Concept Embedding Generation for Large Language Models](https://arxiv.org/abs/2403.15362) | CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。 |
| [^6] | [Multi-Review Fusion-in-Context](https://arxiv.org/abs/2403.15351) | 本文提出了一种融合上下文的多审阅文本生成方法，开发了评论领域的数据集，并提出了评估框架。 |
| [^7] | [CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction](https://arxiv.org/abs/2403.15322) | 该研究介绍了一个特定设计用于命名实体识别和关系抽取任务的数据集CO-Fun，围绕德国基金招股书中公司的外包实践展开，标注包括四种实体类型和两种关系类型，同时展示了深度学习模型的初步结果。 |
| [^8] | [Controlled Training Data Generation with Diffusion Models](https://arxiv.org/abs/2403.15309) | 提出了一种使用扩散模型生成控制训练数据的方法，通过两个反馈机制，一方面使用监督模型反馈找到对抗性提示词实现图像生成，另一方面通过引导使生成过程朝向特定目标分布。 |
| [^9] | [Human behaviour through a LENS: How Linguistic content triggers Emotions and Norms and determines Strategy choices](https://arxiv.org/abs/2403.15293) | 该研究提出了一个名为LENS的框架，指出语言描述会触发情绪反应和潜在行为规范，从而影响个体的战略选择。 |
| [^10] | [Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions](https://arxiv.org/abs/2403.15279) | Fundus是一个简单易用的新闻爬虫工具，通过手工定制的内容提取器，针对每个支持的在线报纸格式指南进行优化，实现高质量的新闻文章提取，同时结合爬取和内容提取于一体，为非技术用户提供统一使用界面。 |
| [^11] | [Specifying Genericity through Inclusiveness and Abstractness Continuous Scales](https://arxiv.org/abs/2403.15278) | 该论文介绍了一种新的注释框架，用于对自然语言中名词短语的泛指性进行细粒度建模，通过连续的评注方法捕捉了泛指性的微妙方面，为语言学家提供了实用资源。 |
| [^12] | [Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs](https://arxiv.org/abs/2403.15273) | 本论文提出了一种新颖的检索增强TempRel提取方法，利用大型语言模型检索到的知识增强提示模板和词汇化器设计，实现了对事件时间关系的有效提取。 |
| [^13] | [Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models](https://arxiv.org/abs/2403.15268) | 提出了一种新颖的知识增强框架，即想象增强生成（IAG），通过想象力，而非依赖外部资源，来补充大型语言模型中可能存在的知识缺陷，并提出了一种想象更丰富背景的方法（IMcQA）来解决问题回答中的挑战。 |
| [^14] | [Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach](https://arxiv.org/abs/2403.15250) | 评估大规模LLM中因素对性能的影响通过全面的统计分析，有助于更好地理解和推动这些模型的发展 |
| [^15] | [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://arxiv.org/abs/2403.15246) | 该论文引入了FollowIR数据集，包含严格的说明书评估基准和训练集，帮助信息检索模型更好地遵循真实世界的说明书。议论基于TREC会议的历史，旨在使信息检索模型能够根据详细说明书理解和判断相关性。 |
| [^16] | [Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models](https://arxiv.org/abs/2403.15226) | 本文提出了一种高效跳过注意力的方法，用于多模态大型语言模型，能够减少计算开销并保持高性能和参数效率。 |
| [^17] | [InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection](https://arxiv.org/abs/2403.15214) | 本文研究了使用大型语言模型帮助强制执行在线披露赞助内容相关法律要求的潜力，探讨了使用LLMs生成合成Instagram标题的保真度和实用性目标可能存在冲突。 |
| [^18] | [Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study](https://arxiv.org/abs/2403.15185) | 语言模型在函数式编程语言中的代码补全性能研究以Haskell为例，发现命令式编程语言知识对提高性能有帮助 |
| [^19] | [CACA Agent: Capability Collaboration based AI Agent](https://arxiv.org/abs/2403.15137) | 提出了CACA代理，采用基于能力协作的开放架构，整合了一组协作能力来实现AI代理，增强了AI代理的规划能力和可扩展性。 |
| [^20] | [Language Models in Dialogue: Conversational Maxims for Human-AI Interactions](https://arxiv.org/abs/2403.15115) | 提出了一组最大化准则，用于描述有效的人机对话，包括传统的 Grice 四个最大化准则以及两个新准则，对于解决现代人机互动中的特殊行为问题。 |
| [^21] | [Text clustering with LLM embeddings](https://arxiv.org/abs/2403.15112) | 研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率 |
| [^22] | [Argument-Aware Approach To Event Linking](https://arxiv.org/abs/2403.15097) | 引入论据感知方法改进事件链接模型，能更好地识别和分类不在知识库中的事件提及，弥补了这一领域的研究空白。 |
| [^23] | [CHisIEC: An Information Extraction Corpus for Ancient Chinese History](https://arxiv.org/abs/2403.15088) | CHisIEC是一份旨在加速古代历史文化研究的语料库，涵盖了13个朝代、跨越1830年的数据，具有丰富的时间跨度和文本异质性。 |
| [^24] | [Construction of a Japanese Financial Benchmark for Large Language Models](https://arxiv.org/abs/2403.15062) | 通过构建针对日本金融领域的多任务基准并对多模型进行测试，证实了GPT-4表现出色，该基准有效区分了各性能范围内模型的基准分数。 |
| [^25] | [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042) | LLM2LLM 提出了一种迭代数据增强策略，通过使用教师LLM生成合成数据并将其添加回训练数据，从而帮助低数据环境下的LLM进行微调。 |
| [^26] | [ESG Classification by Implicit Rule Learning via GPT-4](https://arxiv.org/abs/2403.15040) | 本文研究了如何通过引导最先进的语言模型GPT-4，通过提示、思维链推理和动态上下文学习等策略来与未知的ESG评估标准达成一致，并在韩文共享任务中取得了第二名的成绩。 |
| [^27] | [MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness](https://arxiv.org/abs/2403.14990) | MasonTigers在SemEval-2024 Task 1中采用集成方法，结合语言特定的BERT模型和句子变换器，在处理语义文本相关性时取得了优异的结果。 |
| [^28] | [MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection](https://arxiv.org/abs/2403.14989) | 本文介绍了MasonTigers在SemEval-2024任务8上的表现分析，创新之处在于利用鉴别器Transformer模型的集成，结合句子Transformer和统计机器学习方法，以及在部分情况下采用零样本提示和针对FLAN-T5的微调。 |
| [^29] | [Risk and Response in Large Language Models: Evaluating Key Threat Categories](https://arxiv.org/abs/2403.14988) | 本研究探讨了大型语言模型中的风险评估问题，发现LLMs倾向于认为信息风险较少有害，同时在信息风险场景中对越狱攻击存在漏洞。 |
| [^30] | [MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts](https://arxiv.org/abs/2403.14982) | 这项研究使用大型语言模型解决SemEval-2024 Task 9的谜题任务，通过一系列思维链提示技术，包括零参考和少量参考提示，以及思维链提示，最终取得了显著的结果，展示了逐步解释性提示如何可以更好地揭示已编码的知识。 |
| [^31] | [A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning](https://arxiv.org/abs/2403.14972) | 提出了一种演绎式的图谱辩论方法（BDoG），在多模态推理中防止意见陈腐化和减少由图像引入的分心概念，实验证明其在科学问答和MMBench上取得了最先进的结果。 |
| [^32] | [Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices](https://arxiv.org/abs/2403.14958) | Adapprox是一种采用随机低秩矩阵近似的自适应方法，用于更有效和准确地逼近Adam优化算法的二阶矩。在GPT-2的训练和下游任务中，Adapprox相比AdamW能够实现34.5%至49.9%和33.8%至49.9%的内存节约，并通过余弦相似性指导策略提高了稳定性和加快了收敛速度。 |
| [^33] | [Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation](https://arxiv.org/abs/2403.14952) | 提出了一种基于检索增强的响应生成方法(RARG)，通过收集科学来源的证据来生成反虚假信息的响应，相比现有方法，可以提高文本质量和避免过度重复。 |
| [^34] | [KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation](https://arxiv.org/abs/2403.14950) | 该论文提出了一种名为KnowLA的知识自适应方法，通过在大型语言模型中插入自适应层和知识图嵌入，能够提升参数高效微调的有效性和鲁棒性。 |
| [^35] | [A Single Linear Layer Yields Task-Adapted Low-Rank Matrices](https://arxiv.org/abs/2403.14946) | 通过研究转换矩阵将$ W_0 $转换为低秩矩阵的关系信息，我们提出单一线性层可以生成任务自适应的低秩矩阵。 |
| [^36] | [On Zero-Shot Counterspeech Generation by LLMs](https://arxiv.org/abs/2403.14938) | 该研究首次在零-shot 设置下探索了四种LLM在对抗性言论生成任务中的表现，并提出了三种不同的提示策略，为生成不同类型的对抗性言论提供了全面分析。 |
| [^37] | [Attention-Driven Reasoning: Unlocking the Potential of Large Language Models](https://arxiv.org/abs/2403.14932) | 通过注意力机制优化，可以显著提高大型语言模型的推理能力，尤其对于非STEM问题。 |
| [^38] | [Hierarchical Skip Decoding for Efficient Autoregressive Text Generation](https://arxiv.org/abs/2403.14919) | 提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成，通过分层地自适应跳过解码层来减少计算负载和分配计算资源。 |
| [^39] | [Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning](https://arxiv.org/abs/2403.14895) | Stance Reasoner是一种利用显式推理和世界知识进行零-shot社交媒体立场检测的方法，优于当前领先模型，并能更好地横跨目标进行泛化。 |
| [^40] | [AutoRE: Document-Level Relation Extraction with Large Language Models](https://arxiv.org/abs/2403.14888) | AutoRE 是一种端到端的文档级关系抽取模型，采用了一种名为RHF的新颖关系抽取范式，可有效处理分布在文档中的多个关系和三元组事实。 |
| [^41] | [VidLA: Video-Language Alignment at Scale](https://arxiv.org/abs/2403.14870) | VidLA 提出了一种规模化视频语言对齐方法，通过简化网络架构和使用分层数据令牌来捕捉短程和长程时间依赖关系，从而成功融合预训练图像-文本基础模型，提高了最终性能。 |
| [^42] | [Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models](https://arxiv.org/abs/2403.14859) | 通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。 |
| [^43] | [TAMS: Translation-Assisted Morphological Segmentation](https://arxiv.org/abs/2403.14840) | 这项工作提出了一种利用翻译数据辅助形态分割任务的方法，通过使用字符级序列到序列模型，以及预先训练的高资源单语言模型的翻译表示，实现在低资源环境下超越基线模型。 |
| [^44] | [The opportunities and risks of large language models in mental health](https://arxiv.org/abs/2403.14814) | 大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。 |
| [^45] | [A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances](https://arxiv.org/abs/2403.14808) | 开发了第一个人类判断口语对话话语之间语用相似度的集合，通过评分表明了不同程度相似度的差异。 |
| [^46] | [Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering](https://arxiv.org/abs/2403.14783) | 本研究提出了一种自适应多智体系统，名为多智体VQA，通过使用专门的智体工具，克服了基础模型在目标检测和计数中的局限性，在零样本情况下实现了良好的性能，为未来研究提供了新的方向。 |
| [^47] | [Few-Shot Adversarial Prompt Learning on Vision-Language Models](https://arxiv.org/abs/2403.14774) | 本文提出了一个少样本对抗提示框架，在视觉-语言模型中通过有限数据调整输入序列，显著提升对抗鲁棒性，并通过端到端学习对抗性相关的文本监督。 |
| [^48] | [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773) | StreamingT2V是一种自回归方法，用于生成长视频，可以产生80、240、600、1200帧甚至更多帧的视频，并具有平滑的过渡。 |
| [^49] | [A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond](https://arxiv.org/abs/2403.14734) | 神经代码智能领域的调查系统回顾了50多种代表性模型和超过680项相关作品，突出了不同研究阶段的范式和技术转变。 |
| [^50] | [Open Knowledge Base Canonicalization with Multi-task Learning](https://arxiv.org/abs/2403.14733) | 提出了一个多任务学习框架MulCanon来处理开放知识库（OKB）规范化问题，并通过在软聚类过程中使用扩散模型来改进名词短语的表示。 |
| [^51] | [Reversible Jump Attack to Textual Classifiers with Modification Reduction](https://arxiv.org/abs/2403.14731) | 提出了两种算法，可逆跳动攻击（RJA）和Metropolis-Hasting修改缩减（MMR），用于生成高度有效的对抗示例，并分别改善示例的不可察觉性。 |
| [^52] | [Protected group bias and stereotypes in Large Language Models](https://arxiv.org/abs/2403.14727) | 该研究调查了大型语言模型在伦理和公平领域中的行为，发现模型不仅反映了社会偏见，还似乎放大了这些偏见。 |
| [^53] | [Jailbreaking is Best Solved by Definition](https://arxiv.org/abs/2403.14725) | 语言模型中"越狱"攻击的关键是通过定义好的不安全响应来进行防御，而不是依赖于执行策略。 |
| [^54] | [Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720) | 引入了聚焦技术，一种提示工程技术，用于改进大型语言模型在处理多个输入源时的能力，通过提供可靠的输入来源信号来防御间接提示注入攻击。 |
| [^55] | [Concept-Best-Matching: Evaluating Compositionality in Emergent Communication](https://arxiv.org/abs/2403.14705) | 提出了一种评估新兴通信组合性的方法，通过找到 emerged words 与 natural language concepts 之间的最佳匹配，实现了直接而可解释的映射。 |
| [^56] | [Application of GPT Language Models for Innovation in Activities in University Teaching](https://arxiv.org/abs/2403.14694) | GPT语言模型在大学教学活动创新中的应用不仅可以支持理解和生成内容、问题解决，还可以在个性化和测试纠错等方面提供帮助，但是在国际化方面需注意避免其误用导致的全球问题。 |
| [^57] | [Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning](https://arxiv.org/abs/2403.14690) | 提出了基于深度强化学习的图注意机制，用于自动且高效地添加几何问题中的辅助组件 |
| [^58] | [A Moral Imperative: The Need for Continual Superalignment of Large Language Models](https://arxiv.org/abs/2403.14683) | 实现终身超对齐需要对当前大型语言模型架构进行重大变革，以解决其在理解和适应动态人类道德和不断发展的全球情景方面的局限性。 |
| [^59] | [Predicting Learning Performance with Large Language Models: A Study in Adult Literacy](https://arxiv.org/abs/2403.14668) | 该研究使用大型语言模型GPT-4探讨了在ITS中预测成人识字计划学习表现的应用，并发现GPT-4在此方面具有竞争力的预测能力。 |
| [^60] | [SyllabusQA: A Course Logistics Question Answering Dataset](https://arxiv.org/abs/2403.14666) | SyllabusQA数据集是一个包含63个真实课程大纲的开源数据集，对36个专业涵盖5,078对多样化的开放式课程逻辑相关问题-答案对进行了详细收集，旨在评估答案事实性，多个强基线模型在该任务上表现出色，但仍存在与人类之间的显著差距。 |
| [^61] | [Towards Modeling Learner Performance with Large Language Models](https://arxiv.org/abs/2403.14661) | 本文研究了预训练大型语言模型（LLMs）在知识追踪领域的应用，通过比较零-shot提示和模型微调两种方法，提出了LLMs在智能辅导系统中预测学习者表现的潜力。 |
| [^62] | [Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future](https://arxiv.org/abs/2403.14659) | 本研究构建了一个名为Social AI Data Infrastructure的社会智能数据基础设施，包括一个全面的社交AI分类系统和一个480个NLP数据集的数据库，通过分析现有数据集工作以及评估语言模型在不同社会智能方面的表现，帮助研究者深入了解当前数据格局并提供未来数据集发展方向的整体观点。 |
| [^63] | [A Synergistic Approach to Wildfire Prevention and Management Using AI, ML, and 5G Technology in the United States](https://arxiv.org/abs/2403.14657) | 该研究致力于探索联合利用人工智能、机器学习和5G技术在美国进行森林火灾预防和管理的方法，包括积极检测和预防、利用5G技术进行远程监测和制图、以及利用无人机和物联网设备进行高级火灾响应机制。 |
| [^64] | [MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://arxiv.org/abs/2403.14652) | MemeCraft是一款创新的模因生成器，利用大型语言模型和视觉语言模型生成支持特定社会运动的模因，提供端到端的流程，无需人工干预，带有内在安全机制。 |
| [^65] | [DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures](https://arxiv.org/abs/2403.14651) | 通过使用社区为中心的参与式研究方法，本研究引入了第一个包含615个社会文物的数据集DOSA，以帮助生成模型更好地了解并考虑当地社会文化背景。 |
| [^66] | [Exploring ChatGPT and its Impact on Society](https://arxiv.org/abs/2403.14643) | ChatGPT是一种基于Transformer架构的大型语言模型，能够生成人类化的对话回复，可革新各行业并改变技术互动方式。 |
| [^67] | [Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models](https://arxiv.org/abs/2403.14633) | 本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。 |
| [^68] | [Improving the Robustness of Large Language Models via Consistency Alignment](https://arxiv.org/abs/2403.14221) | 通过一致性对齐训练的两阶段框架，有助于提高大型语言模型的鲁棒性和对指令的理解。 |
| [^69] | [RoleInteract: Evaluating the Social Interaction of Role-Playing Agents](https://arxiv.org/abs/2403.13679) | 该论文介绍了RoleInteract，一个旨在评估角色扮演对话代理社交性的基准，覆盖了500个角色、6000多个问题提示和30800个对话话语。 |
| [^70] | [Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs](https://arxiv.org/abs/2403.13592) | 通过调整LLama Chat模型来重新评估其在欧盟政治中的政治倾向，展示了其对国家政党立场的充分了解，并能在上下文中进行有效推理，为将基于对话的LLM用于政治科学研究提供了新的可能性。 |
| [^71] | [Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts](https://arxiv.org/abs/2403.13362) | 通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。 |
| [^72] | [RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners](https://arxiv.org/abs/2403.12373) | RankPrompt 提出了一种新的提示方法，可以通过自我排序来提高大型语言模型在推理任务中的性能。 |
| [^73] | [Tur[k]ingBench: A Challenge Benchmark for Web Agents](https://arxiv.org/abs/2403.11905) | Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。 |
| [^74] | [Do Large Language Models understand Medical Codes?](https://arxiv.org/abs/2403.10822) | 该研究调查了大型语言模型是否理解医学编码的含义，评估了它们对领域特定术语的认识和理解。 |
| [^75] | [Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction](https://arxiv.org/abs/2403.10581) | 提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。 |
| [^76] | [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](https://arxiv.org/abs/2403.09919) | 本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。 |
| [^77] | [Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation](https://arxiv.org/abs/2403.09738) | 大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。 |
| [^78] | [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | 通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。 |
| [^79] | [VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding](https://arxiv.org/abs/2403.09530) | 提出了一个统一的VisionGPT-3D框架，整合了最先进的视觉模型，有助于提升计算机视觉对于3D视觉理解的能力 |
| [^80] | [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763) | 通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。 |
| [^81] | [HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy](https://arxiv.org/abs/2403.05574) | 这一创新心理治疗模型HealMe通过基于心理治疗框架的共情对话，有效解决了根深蒂固的负面思维，并促进了理性、平衡的观点。 |
| [^82] | [MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis](https://arxiv.org/abs/2403.04639) | 这项研究介绍了首个用于马加希语-印地语-英语代码混合情感分析任务的数据集，并通过语言学分析和统计研究来评估数据集的质量。 |
| [^83] | [Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning](https://arxiv.org/abs/2403.02893) | 提出了一种使用异构图对比迁移学习的方法，实现了零样本跨语言文档级事件因果识别，并在实验证明在F1得分上优于之前的最先进模型。 |
| [^84] | [Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation](https://arxiv.org/abs/2403.01479) | "本文提出了“Align-to-Distill”（A2D）策略，通过在训练过程中自适应地对齐学生注意力头与其教师对应物，转化了组合映射启发式方法为学习问题，实验结果显示A2D的有效性，对WMT-2022 De->Dsb和WMT-2014 En->De的BLEU分数分别获得高达+3.61和+0.63的提升。" |
| [^85] | [Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish](https://arxiv.org/abs/2403.00411) | 提出了FCTR数据集，旨在解决英语以外语言，尤其是土耳其语，的数据稀缺问题，并探讨了跨语言转移学习在低资源语言中的有效性，实验证明数据集潜力推动土耳其语研究。 |
| [^86] | [ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition](https://arxiv.org/abs/2402.15220) | ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。 |
| [^87] | [An LLM-Enhanced Adversarial Editing System for Lexical Simplification](https://arxiv.org/abs/2402.14704) | 该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。 |
| [^88] | [KoCoSa: Korean Context-aware Sarcasm Detection Dataset](https://arxiv.org/abs/2402.14428) | 该论文介绍了一个新的针对韩文对话讽刺检测任务的数据集KoCoSa，提出了一种高效的讽刺检测数据集生成流程，并提供了针对该任务的简单但有效的基线模型。 |
| [^89] | [The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun](https://arxiv.org/abs/2402.10311) | 本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。 |
| [^90] | [Building Efficient Universal Classifiers with Natural Language Inference](https://arxiv.org/abs/2312.17543) | 本文探讨了如何利用自然语言推理作为通用分类任务，提供了构建通用分类器的详细步骤，并分享了该通用分类器在33个数据集上的训练结果 |
| [^91] | [MacGyver: Are Large Language Models Creative Problem Solvers?](https://arxiv.org/abs/2311.09682) | 通过创建MACGYVER数据集并与人类比较，研究发现大型语言模型在创意问题解决方面独具挑战性，在知识广度和可行性方面与人类存在独特差异，同时还展示了通过新的提示技术提升大型语言模型的问题解决能力潜力。 |
| [^92] | [PhoGPT: Generative Pre-training for Vietnamese](https://arxiv.org/abs/2311.02945) | PhoGPT是一个用于越南语的生成式预训练模型系列，具有40亿参数的基础模型PhoGPT-4B以及其聊天变体PhoGPT-4B-Chat，展示了在越南语任务上优于之前的7亿参数模型的强大性能。 |
| [^93] | [E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity](https://arxiv.org/abs/2310.15929) | 首次将信息熵引入剪枝度量设计，提高在大型语言模型中 N:M 稀疏性的准确性。 |
| [^94] | [FunQA: Towards Surprising Video Comprehension](https://arxiv.org/abs/2306.14899) | FunQA是一个旨在评估和提高基于反直觉和有趣视频的视频推理深度的数据集，涵盖了HumorQA、CreativeQA和MagicQA三种以前未被探索的惊喜视频类型。 |
| [^95] | [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B.](http://arxiv.org/abs/2311.00117) | 研究发现，公开发布模型权重使得安全微调无效，BadLlama项目以低成本成功移除了Llama 2-Chat 13B的安全微调并保留了其一般能力。 |
| [^96] | [Self-Guard: Empower the LLM to Safeguard Itself.](http://arxiv.org/abs/2310.15851) | 这篇论文提出了一种称为自我防御的新方法，通过结合安全训练和保护措施的优势，提升大型语言模型（LLM）的安全性，从而减少有害内容的生成。 |
| [^97] | [Robustness of the Random Language Model.](http://arxiv.org/abs/2309.14913) | 随机语言模型的研究展示了第一语言学习过程中的语法句法连续转变，并证明该转变对于明确对称性的打破是鲁棒的。 |
| [^98] | [LLMR: Real-time Prompting of Interactive Worlds using Large Language Models.](http://arxiv.org/abs/2309.12276) | LLMR是一个用于实时创建和修改交互式混合现实体验的框架，通过利用大型语言模型和新颖的策略，它能够解决训练数据稀缺和设计目标复杂的问题，并在性能上超过标准的GPT-4。我们展示了LLMR的跨平台互操作性，并通过评估和用户研究证明了其对于生成和编辑各种对象、工具和场景的能力。 |
| [^99] | [Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling.](http://arxiv.org/abs/2309.10435) | 本研究提出了一个新的顺序推荐范式 LANCER，利用预训练语言模型的语义理解能力生成更加人性化的个性化推荐。在多个基准数据集上的实验结果表明，该方法有效且有希望，并为了解顺序推荐的影响提供了有价值的见解。 |
| [^100] | [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages.](http://arxiv.org/abs/2308.12038) | 本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。 |
| [^101] | [AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers.](http://arxiv.org/abs/2306.06531) | AutoTAMP提出了一种使用LLMs作为翻译器和检查器的自回归任务和动作规划方法，通过少样本翻译将自然语言任务描述转换为中间任务表示，以实现对复杂任务的规划和执行。 |
| [^102] | [Eliminating Spurious Correlations from Pre-trained Models via Data Mixing.](http://arxiv.org/abs/2305.14521) | 本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。 |
| [^103] | [CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge.](http://arxiv.org/abs/2305.09955) | CooK是一种用于赋能通用语言模型的新颖框架，通过专门的语言模型和协作的知识贡献者，提供模块化、不断增长和多源的知识。在知识密集型任务中，CooK展现出了明显的性能提升。 |
| [^104] | [NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models.](http://arxiv.org/abs/2305.07766) | 本研究提出了一种NL到TL的转换框架，使用大型语言模型在数据集和模型训练中，可以准确并且具有普适性地转换复杂的高级系统规范。 |
| [^105] | [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality.](http://arxiv.org/abs/2304.14178) | 本文介绍了一种名为mPLUG-Owl的训练范式，它通过模块化学习基础LLM、视觉知识模块和视觉抽象器模块，赋予LLMs多模态的能力。实验结果表明，mPLUG-Owl在图像字幕和视觉问答任务中表现优于基线模型，并在某些情况下达到了最先进的性能水平。 |

# 详细

[^1]: LLaVA-PruMerge: 自适应令牌减少用于高效大型多模态模型

    LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

    [https://arxiv.org/abs/2403.15388](https://arxiv.org/abs/2403.15388)

    PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。

    

    大型多模态模型(LMMs)通过连接视觉编码器和大型语言模型展现了显著的推理能力。最近的LMMs包括了更复杂的视觉输入，如高分辨率图像和视频，这显著增加了视觉令牌的数量。为了解决这个问题，我们探索了一种令牌减少机制，并发现类似于先前的工作，许多视觉令牌在空间上是冗余的。基于此，我们提出了PruMerge，一种新颖的自适应视觉令牌减少方法，大大减少了视觉令牌的数量，同时保持了可比的模型性能。

    arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
    
[^2]: 大型语言模型能够进行上下文中的探索吗？

    Can large language models explore in-context?

    [https://arxiv.org/abs/2403.15371](https://arxiv.org/abs/2403.15371)

    研究发现，大型语言模型在没有实质干预的情况下很难有效进行探索，除了特定配置下的GPT-4具有满意的探索行为外，其他模型表现不稳定。

    

    我们研究现代大型语言模型（LLMs）在进行探索方面的能力，这是强化学习和决策制定中的核心能力。我们关注现有LLMs的原生性能，没有进行训练干预。我们将LLMs部署为简单多臂老虎机环境中的代理，并完全在上下文中指定环境描述和交互历史，即在LLM提示内部进行。我们使用各种提示设计对GPT-3.5、GPT-4和Llama2进行实验，发现这些模型在没有实质干预的情况下并没有稳健地进行探索：i）在我们的所有实验中，只有一个配置导致了令人满意的探索行为：具有思维链推理和外部总结的交互历史的GPT-4，这些被呈现为充分统计的情况；ii）所有其他配置都没有产生稳健的探索行为，包括具有思维链推理的其他配置。

    arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
    
[^3]: 一种针对图像水印的转移攻击

    A Transfer Attack to Image Watermarks

    [https://arxiv.org/abs/2403.15365](https://arxiv.org/abs/2403.15365)

    水印领域的研究表明，即使在攻击者无法访问水印模型或检测API的情况下，水印基础的AI生成图像检测器也无法抵抗对抗攻击。

    

    水印已被广泛应用于工业领域，用于检测由人工智能生成的图像。文献中对这种基于水印的检测器在白盒和黑盒环境下对抗攻击的稳健性有很好的理解。然而，在无盒环境下的稳健性却知之甚少。具体来说，多项研究声称图像水印在这种环境下是稳健的。在这项工作中，我们提出了一种新的转移对抗攻击来针对无盒环境下的图像水印。我们的转移攻击向带水印的图像添加微扰，以躲避被攻击者训练的多个替代水印模型，并且经过扰动的带水印图像也能躲避目标水印模型。我们的主要贡献是理论上和经验上展示了，基于水印的人工智能生成图像检测器即使攻击者没有访问水印模型或检测API，也不具有对抗攻击的稳健性。

    arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
    
[^4]: 实现知识驱动的自然语言理解和生成

    Towards Knowledge-Grounded Natural Language Understanding and Generation

    [https://arxiv.org/abs/2403.15364](https://arxiv.org/abs/2403.15364)

    本论文研究了如何利用知识表示来改进自然语言理解和生成，发现整合实体相关知识有助于假新闻检测，实体焦点代码切换改善了跨语言转移性能。

    

    这篇论文调查了如何通过使用transformer模型的知识表示来受益自然语言理解和生成，并探讨了以下关键研究问题：(i) 实体知识能否扩展其优势至实体链接等不限于实体的任务? (ii) 我们如何忠实有效地从原始文本中提取这种结构化知识，尤其是在嘈杂的网络文本中? (iii) 除了结构化知识之外，其他类型的知识如何有助于改善自然语言处理任务? 这篇论文的研究发现，整合与实体相关和最新的知识有助于假新闻检测，而以实体为焦点的代码切换显著提高了实体相关任务的零样例跨语言转移。在提取结构化知识的有效和忠实方法方面，观察到通过整合负例和进行实体规划训练可以取得良好效果。

    arXiv:2403.15364v1 Announce Type: new  Abstract: This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks?   Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection, and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning 
    
[^5]: CoLLEGe: 大型语言模型的概念嵌入生成

    CoLLEGe: Concept Embedding Generation for Large Language Models

    [https://arxiv.org/abs/2403.15362](https://arxiv.org/abs/2403.15362)

    CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。

    

    当前语言模型无法快速学习新概念，通常需要更复杂的微调过程才能学习得更稳健。本文引入了一种名为CoLLEGe（Concept Learning with Language Embedding Generation）的新方法，用于现代化的少样本概念学习。CoLLEGe是一个元学习框架，能够使用少量示例句子或定义生成新概念的灵活嵌入。我们的主要元学习目标只是促进语言模型在随后的句子中进行下一个词预测，使其与语言模型的预训练兼容。

    arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear
    
[^6]: 融合上下文的多审阅文本生成

    Multi-Review Fusion-in-Context

    [https://arxiv.org/abs/2403.15351](https://arxiv.org/abs/2403.15351)

    本文提出了一种融合上下文的多审阅文本生成方法，开发了评论领域的数据集，并提出了评估框架。

    

    地面文本生成涵盖了诸如长篇问答和摘要等任务，需要内容选择和内容整合。当前的端到端方法由于不透明性而难以控制和解释。因此，最近的研究提出了一种模块化方法，为每个步骤都提供单独的组件。具体而言，我们专注于生成连贯文本的第二子任务，即在多文档环境中给定预选内容。我们将\textit{Fusion-in-Context}(FiC)具体化为一个独立任务，其输入包括带有目标内容高亮部分的源文本。然后，模型需要生成一个包含所有且仅包含目标信息的连贯段落。我们的工作包括在评论领域开发了一个包含1000个实例的精心策划数据集，以及一个用于评估高亮真实性和涵盖范围的新颖评估框架。

    arXiv:2403.15351v1 Announce Type: new  Abstract: Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which stro
    
[^7]: CO-Fun: 一份关于德国基金招股书中公司外包的命名实体识别和关系抽取数据集

    CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction

    [https://arxiv.org/abs/2403.15322](https://arxiv.org/abs/2403.15322)

    该研究介绍了一个特定设计用于命名实体识别和关系抽取任务的数据集CO-Fun，围绕德国基金招股书中公司的外包实践展开，标注包括四种实体类型和两种关系类型，同时展示了深度学习模型的初步结果。

    

    arXiv:2403.15322v1 发布类型：新  摘要：通过网络映射，可以揭示金融实体和服务提供商之间的关系。围绕德国基金招股书中公司的外包实践，我们引入了一个专门用于命名实体识别和关系抽取任务的数据集。通过三位专家对948个句子进行标注，共获得了四种实体类型（外包、公司、地点和软件）的5,969个标注和关系（外包-公司、公司-地点）的4,102个标注。训练了最先进的深度学习模型来识别实体并提取关系，显示出了初步的有希望的结果。数据集的匿名版本，以及模型训练所使用的指南和代码，均可以在https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip上公开获取。

    arXiv:2403.15322v1 Announce Type: new  Abstract: The process of cyber mapping gives insights in relationships among financial entities and service providers. Centered around the outsourcing practices of companies within fund prospectuses in Germany, we introduce a dataset specifically designed for named entity recognition and relation extraction tasks. The labeling process on 948 sentences was carried out by three experts which yields to 5,969 annotations for four entity types (Outsourcing, Company, Location and Software) and 4,102 relation annotations (Outsourcing-Company, Company-Location). State-of-the-art deep learning models were trained to recognize entities and extract relations showing first promising results. An anonymized version of the dataset, along with guidelines and the code used for model training, are publicly available at https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.
    
[^8]: 使用扩散模型生成控制训练数据

    Controlled Training Data Generation with Diffusion Models

    [https://arxiv.org/abs/2403.15309](https://arxiv.org/abs/2403.15309)

    提出了一种使用扩散模型生成控制训练数据的方法，通过两个反馈机制，一方面使用监督模型反馈找到对抗性提示词实现图像生成，另一方面通过引导使生成过程朝向特定目标分布。

    

    在这项工作中，我们提出了一种方法，可以控制文本到图像生成模型以生成训练数据，专门用于监督学习。与之前那些采用开环方法并预先定义提示词来使用语言模型或人类专业知识生成新数据的作品不同，我们开发了一种自动闭环系统，其中包括两个反馈机制。第一个机制使用来自给定监督模型的反馈，并找到导致图像生成最大化模型损失的对抗提示词。虽然这些对抗提示词导致了经过模型训练的多样化数据生成，但它们并不知道目标分布，这可能效率低下。因此，我们引入第二个反馈机制，将生成过程引导到特定目标分布。我们称将这两个机制结合起来的方法为引导对抗提示词。我们在不同任务上进行评估。

    arXiv:2403.15309v1 Announce Type: cross  Abstract: In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, da
    
[^9]: 透过镜头看人类行为：语言内容如何触发情绪和规范，并决定策略选择

    Human behaviour through a LENS: How Linguistic content triggers Emotions and Norms and determines Strategy choices

    [https://arxiv.org/abs/2403.15293](https://arxiv.org/abs/2403.15293)

    该研究提出了一个名为LENS的框架，指出语言描述会触发情绪反应和潜在行为规范，从而影响个体的战略选择。

    

    在过去的二十年里，越来越多的实验研究证明，语言框架影响了经济博弈中人类行为，超越了可用行动的经济后果。本文提出了一个超越传统基于结果偏好模型的新框架。根据LENS模型，对决策问题的语言描述会触发情绪反应并暗示潜在的行为规范，这些规范再相互作用来塑造个体的战略选择。本文审查了支持LENS模型每个路径的实验证据。此外，它确定并讨论了一些由该模型引申出的重要研究问题，指向未来研究的方向。

    arXiv:2403.15293v1 Announce Type: new  Abstract: Over the last two decades, a growing body of experimental research has provided evidence that linguistic frames influence human behaviour in economic games, beyond the economic consequences of the available actions. This article proposes a novel framework that transcends the traditional confines of outcome-based preference models. According to the LENS model, the Linguistic description of the decision problem triggers Emotional responses and suggests potential Norms of behaviour, which then interact to shape an individual's Strategic choice. The article reviews experimental evidence that supports each path of the LENS model. Furthermore, it identifies and discusses several critical research questions that arise from this model, pointing towards avenues for future inquiry.
    
[^10]: Fundus：一个简单易用的新闻爬虫，优化高质量提取

    Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions

    [https://arxiv.org/abs/2403.15279](https://arxiv.org/abs/2403.15279)

    Fundus是一个简单易用的新闻爬虫工具，通过手工定制的内容提取器，针对每个支持的在线报纸格式指南进行优化，实现高质量的新闻文章提取，同时结合爬取和内容提取于一体，为非技术用户提供统一使用界面。

    

    本文介绍了Fundus，一个用户友好的新闻爬虫，使用户可以仅凭几行代码获得数百万高质量的新闻文章。与现有的新闻爬虫不同，我们使用手工定制的、专门针对每个支持的在线报纸的格式指南的内容提取器。这样我们可以优化我们的爬取质量，以确保检索到的新闻文章完整且没有HTML痕迹。此外，我们的框架将爬取（从网络或大型网络归档中检索HTML）和内容提取结合到一个单一的流水线中。通过为预定义的一组报纸提供统一的界面，我们的目标是使Fundus即使对非技术用户也易于使用。本文概述了框架，讨论了我们的设计选择，并针对其他流行的新闻爬虫进行了比较评估。我们的评估表明，Fundus取得了...

    arXiv:2403.15279v1 Announce Type: new  Abstract: This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code. Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper. This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts. Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline. By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users. This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers. Our evaluation shows that Fundus yie
    
[^11]: 通过包容性和抽象性连续刻度规范泛指性

    Specifying Genericity through Inclusiveness and Abstractness Continuous Scales

    [https://arxiv.org/abs/2403.15278](https://arxiv.org/abs/2403.15278)

    该论文介绍了一种新的注释框架，用于对自然语言中名词短语的泛指性进行细粒度建模，通过连续的评注方法捕捉了泛指性的微妙方面，为语言学家提供了实用资源。

    

    这篇论文介绍了一种新的注释框架，用于对自然语言中名词短语（NPs）的泛指性进行细粒度建模。该框架设计简单直观，适用于非专家注释者并适合众包任务。结合有关泛指性的理论和认知文献，该框架根植于已建立的语言学理论。通过一项试点研究，我们创建了一个包含324个句子的小而关键的注释数据集，为未来研究奠定基础。为验证我们的方法，我们进行了一项评估，比较了我们连续注释与相同数据集上现有的二元注释，证明了该框架在捕捉泛指性的微妙方面上的有效性。我们的工作为语言学家提供了一个实用资源，提供了一个第一个经过注释的数据集和一个旨在构建可用于研究的实际语言数据集的注释方案。

    arXiv:2403.15278v1 Announce Type: new  Abstract: This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases' (NPs) genericity in natural language. The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks. Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory. Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research. To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework's effectiveness in capturing nuanced aspects of genericity. Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on
    
[^12]: 基于检索增强大型语言模型的事件时间关系提取

    Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs

    [https://arxiv.org/abs/2403.15273](https://arxiv.org/abs/2403.15273)

    本论文提出了一种新颖的检索增强TempRel提取方法，利用大型语言模型检索到的知识增强提示模板和词汇化器设计，实现了对事件时间关系的有效提取。

    

    事件时间关系（TempRel）是事件关系抽取任务的主要主题。然而，TempRel的固有模糊性增加了任务的难度。随着提示工程的兴起，设计有效的提示模板和词汇化器以提取相关知识十分重要。传统的手动设计模板难以提取精确的时间知识。本文介绍了一种新颖的检索增强TempRel提取方法，利用从大型语言模型（LLMs）检索到的知识来增强提示模板和词汇化器。我们的方法充分利用了各种LLMs的多样能力，为模板和词汇化器设计生成了广泛的想法。我们提出的方法充分利用了LLMs在生成任务中的潜力，并为我们的设计提供了更多知识。对三个广泛认可的数据集的实证评估表明了我们方法的有效性。

    arXiv:2403.15273v1 Announce Type: new  Abstract: Event temporal relation (TempRel) is a primary subject of the event relation extraction task. However, the inherent ambiguity of TempRel increases the difficulty of the task. With the rise of prompt engineering, it is important to design effective prompt templates and verbalizers to extract relevant knowledge. The traditional manually designed templates struggle to extract precise temporal knowledge. This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from large language models (LLMs) to enhance prompt templates and verbalizers. Our method capitalizes on the diverse capabilities of various LLMs to generate a wide array of ideas for template and verbalizer design. Our proposed method fully exploits the potential of LLMs for generation tasks and contributes more knowledge to our design. Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our met
    
[^13]: 想象增强生成：学习想象更丰富的背景来进行大型语言模型问题回答

    Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models

    [https://arxiv.org/abs/2403.15268](https://arxiv.org/abs/2403.15268)

    提出了一种新颖的知识增强框架，即想象增强生成（IAG），通过想象力，而非依赖外部资源，来补充大型语言模型中可能存在的知识缺陷，并提出了一种想象更丰富背景的方法（IMcQA）来解决问题回答中的挑战。

    

    检索增强生成和生成增强生成已被提出来增强大型语言模型（LLMs）上的问题回答所需的知识。然而，前者依赖于外部资源，而且两者都需要将显式文档合并到上下文中，导致更长的上下文，从而消耗更多资源。最近的研究表明，LLMs已经建模了丰富的知识，尽管没有被有效地触发或激活。在此启发下，我们提出了一种新颖的知识增强框架，即想象增强生成（IAG），它模拟了人类通过想象力在仅凭想象回答问题时弥补知识缺陷的能力，而不依赖外部资源。在IAG的指导下，我们提出了一种用于问题回答的想象更丰富背景的方法（IMcQA），通过以下两个模块获得更丰富的背景：通过生成简单的想象实现显式想象

    arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
    
[^14]: 大规模评估结果在LLM中的全面重新评估：一种多方位统计方法

    Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach

    [https://arxiv.org/abs/2403.15250](https://arxiv.org/abs/2403.15250)

    评估大规模LLM中因素对性能的影响通过全面的统计分析，有助于更好地理解和推动这些模型的发展

    

    在LLM快速发展的背景下，评估在理解和推动这些模型前进中的重要性日益凸显。评估揭示了缩放、训练类型、架构等因素深刻影响LLM的性能。然而，这些因素对性能评分的影响程度和性质仍然存在争议，因为大多数评估局限于有限数量的模型和数据点。通过统计视角更有效地澄清这些因素对性能得分的影响可以更有效地实现。我们的研究对这些LLM进行了彻底的重新检查，针对当前评估方法的不足之处。随着一个统一的评估框架的出现，我们的研究利用了广泛的评估结果数据集，引入了一种全面的统计方法论。其中包括ANOVA、Tukey HSD检验、GAMM的应用

    arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
    
[^15]: FollowIR: 评估和教授信息检索模型以遵循说明书

    FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions

    [https://arxiv.org/abs/2403.15246](https://arxiv.org/abs/2403.15246)

    该论文引入了FollowIR数据集，包含严格的说明书评估基准和训练集，帮助信息检索模型更好地遵循真实世界的说明书。议论基于TREC会议的历史，旨在使信息检索模型能够根据详细说明书理解和判断相关性。

    

    现代大型语言模型（LLMs）能够遵循长且复杂的说明书，从而实现多样化的用户任务。然而，尽管信息检索（IR）模型使用LLMs作为其架构的支柱，几乎所有这些模型仍然只接受查询作为输入，没有说明书。对于最近一些接受说明书的模型来说，它们如何使用这些说明书还不清楚。我们引入了FollowIR数据集，其中包含严格的说明书评估基准，以及一个训练集，帮助IR模型学习更好地遵循现实世界的说明书。FollowIR基于TREC会议的悠久历史：正如TREC为人类标注员提供说明书（也称为叙述）来判断文档的相关性一样，因此IR模型应该能够根据这些详细说明书理解和确定相关性。我们的评估基准从三个经过深度判断的TREC收藏开始

    arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
    
[^16]: 不是所有的注意力都是必要的：多模态大型语言模型的参数和计算高效迁移学习

    Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models

    [https://arxiv.org/abs/2403.15226](https://arxiv.org/abs/2403.15226)

    本文提出了一种高效跳过注意力的方法，用于多模态大型语言模型，能够减少计算开销并保持高性能和参数效率。

    

    在本文中，我们提出了一种新颖的参数和计算高效的调参方法，用于多模态大型语言模型（MLLMs），称为高效跳过注意力（EAS）。具体而言，我们首先揭示了多头注意力（MHA）作为MLLM的主要计算开销，通常对下游任务来说是多余的。基于这一观察结果，EAS评估注意力冗余并跳过较不重要的MHA以加速推理。此外，我们还提出了一种新颖的信息传播适配器（PIA）来服务EAS的注意力跳过并保持参数效率，它可以进一步重新参数化为零额外延迟的前馈网络（FFNs）。为了验证EAS，我们将其应用于最近提出的LaVIN和经典的VL预训练模型METER，并在一组基准测试上进行了大量实验。实验证明，EAS不仅保持了高性能和参数效率，

    arXiv:2403.15226v1 Announce Type: cross  Abstract: In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency,
    
[^17]: InstaSynth：使用ChatGPT为赞助内容检测生成合成Instagram数据的机遇和挑战

    InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection

    [https://arxiv.org/abs/2403.15214](https://arxiv.org/abs/2403.15214)

    本文研究了使用大型语言模型帮助强制执行在线披露赞助内容相关法律要求的潜力，探讨了使用LLMs生成合成Instagram标题的保真度和实用性目标可能存在冲突。

    

    大型语言模型（LLMs）引发了有关降低生成可能被用于不道德或非法目的的文本成本的担忧，尤其是在社交媒体上。本文调查了这些模型帮助强制执行与在线披露赞助内容相关的法律要求的潜力。我们研究了使用LLMs生成合成Instagram标题的两个目标：第一个目标（保真度）是产生逼真的合成数据集。为此，我们实施了内容级和网络级指标来评估合成标题是否真实。第二个目标（实用性）是创建对赞助内容检测有用的合成数据。为此，我们评估了所生成合成数据用于训练分类器以识别Instagram上未公开广告的效果。我们的调查表明，保真度和实用性的目标可能存在冲突。

    arXiv:2403.15214v1 Announce Type: cross  Abstract: Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media. This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored content online. We investigate the use of LLMs for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets. For this, we implement content-level and network-level metrics to assess whether synthetic captions are realistic. The second objective (utility) is to create synthetic data that is useful for sponsored content detection. For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram. Our investigations show that the objectives of fidelity and utility may conflict and that promp
    
[^18]: 研究语言模型在函数式编程语言中的代码补全性能：以Haskell为例

    Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study

    [https://arxiv.org/abs/2403.15185](https://arxiv.org/abs/2403.15185)

    语言模型在函数式编程语言中的代码补全性能研究以Haskell为例，发现命令式编程语言知识对提高性能有帮助

    

    基于语言模型的代码补全模型在许多不同的编程语言中帮助成千上万的开发人员编写代码的使用快速增长。然而，对于代码补全模型的研究通常集中在诸如Python和JavaScript等命令式语言，这导致对函数式编程语言的代表性不足。因此，这些模型在Haskell等函数式语言上通常表现不佳。为了调查是否可以缓解这一问题，我们评估了两种用于代码的语言模型CodeGPT和UniXcoder在函数式编程语言Haskell上的性能。我们在HuggingFace上的一个公开可访问的Haskell数据集上对这些模型进行微调和评估。此外，我们使用我们的新型翻译的HumanEval数据集手动评估了这些模型。我们的自动评估显示，LLMs的预训练中对命令式编程语言的知识可能会

    arXiv:2403.15185v1 Announce Type: new  Abstract: Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may 
    
[^19]: CACA Agent：基于能力协作的AI代理

    CACA Agent: Capability Collaboration based AI Agent

    [https://arxiv.org/abs/2403.15137](https://arxiv.org/abs/2403.15137)

    提出了CACA代理，采用基于能力协作的开放架构，整合了一组协作能力来实现AI代理，增强了AI代理的规划能力和可扩展性。

    

    随着基于大型语言模型（LLMs）的AI代理在各个领域的实际应用中展现出潜力，如何快速部署AI代理以及如何方便地扩展AI代理的应用场景已成为一个挑战。本文提出了CACA代理（基于能力协作的AI代理），采用了受服务计算启发的开放架构。CACA代理整合了一组协作能力来实现AI代理，不仅减少了对单个LLM的依赖，还增强了AI代理的规划能力和可用工具的可扩展性。利用所提出的系统，我们展示了一个演示来说明操作和应用场景的扩展。

    arXiv:2403.15137v1 Announce Type: new  Abstract: As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario exten
    
[^20]: 对话中的语言模型：人机交互的会话最大化准则

    Language Models in Dialogue: Conversational Maxims for Human-AI Interactions

    [https://arxiv.org/abs/2403.15115](https://arxiv.org/abs/2403.15115)

    提出了一组最大化准则，用于描述有效的人机对话，包括传统的 Grice 四个最大化准则以及两个新准则，对于解决现代人机互动中的特殊行为问题。

    

    现代语言模型虽然复杂，但在对话环境中存在一些固有缺陷。我们认为观察到的许多缺陷可以归因于违反一个或多个对话原则。通过借鉴社会科学和人工智能领域的广泛研究，我们提出了一组最大化准则 - 包括数量、质量、相关性、方式、仁慈以及透明度 - 来描述有效的人机对话。我们首先证明了在人机互动背景下 Grice 的前四个最大化准则的适用性。然后，我们认为两个新的准则，仁慈（涉及生成和参与有害内容）和透明度（涉及识别自己的知识边界、操作约束和意图），对于解决现代人机互动中独特行为是必要的。提出的准则为如何提供具体指导提供了指导。

    arXiv:2403.15115v1 Announce Type: cross  Abstract: Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how
    
[^21]: 使用LLM嵌入进行文本聚类

    Text clustering with LLM embeddings

    [https://arxiv.org/abs/2403.15112](https://arxiv.org/abs/2403.15112)

    研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率

    

    文本聚类是组织不断增长的数字内容的重要方法，有助于结构化和发现未分类数据中的隐藏模式。在这项研究中，我们调查了不同文本嵌入（特别是大型语言模型LLMs中使用的）和聚类算法如何影响文本数据集的聚类方式。进行了一系列实验以评估嵌入是如何影响聚类结果的，以及通过摘要进行降维和嵌入大小调整的作用。结果显示，LLM嵌入在捕获结构化语言的细微差别方面表现出色，而BERT在性能上领先于轻量级选项。此外，我们发现增加嵌入维度和摘要技术并不一致地提高聚类效率，这表明这些策略需要仔细分析才能在实际模型中使用。这些结果突出了一种

    arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
    
[^22]: 论据感知事件链接方法

    Argument-Aware Approach To Event Linking

    [https://arxiv.org/abs/2403.15097](https://arxiv.org/abs/2403.15097)

    引入论据感知方法改进事件链接模型，能更好地识别和分类不在知识库中的事件提及，弥补了这一领域的研究空白。

    

    arXiv:2403.15097v1 公告类型: 跨领域 摘要: 事件链接将文本中的事件提及与知识库（KB）中相关节点连接起来。先前在事件链接方面的研究主要借鉴了实体链接的方法，忽略了事件的独特特征。与广泛探讨的实体链接任务相比，事件具有更加复杂的结构，可以通过检查其关联的论据更有效地加以区分。此外，事件的信息丰富性导致事件知识库的稀缺性。这强调了事件链接模型需要识别和分类不在知识库中的事件提及作为“超出知识库”的重要性，而这一领域受到了有限关注。在这项工作中，我们通过引入一个论据感知方法来应对这些挑战。首先，我们通过标记事件论据信息来改进事件链接模型，有助于识别有关事件提及的关键信息。随后，为了帮助模型处理“超出知识库”

    arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
    
[^23]: CHisIEC：一份用于古代中国历史信息提取的语料库

    CHisIEC: An Information Extraction Corpus for Ancient Chinese History

    [https://arxiv.org/abs/2403.15088](https://arxiv.org/abs/2403.15088)

    CHisIEC是一份旨在加速古代历史文化研究的语料库，涵盖了13个朝代、跨越1830年的数据，具有丰富的时间跨度和文本异质性。

    

    arXiv:2403.15088v1 公告类型: 新 抽象: 自然语言处理（NLP）在数字人文学（DH）领域中发挥着重要作用，是推动历史和文化遗产文本结构分析的基石。这在命名实体识别（NER）和关系抽取（RE）领域尤为明显。为了加速古代历史文化研究，我们提出了“中国历史信息提取语料库”（CHisIEC）。CHisIEC是一个精心策划的数据集，旨在开发和评估NER和RE任务，为该领域的研究提供资源。涵盖了从13个朝代、跨越1830年的数据的卓越历史时间轴，CHisIEC体现了中国历史文档中存在的广泛时间范围和文本异质性。该数据集包含四种不同的实体类型和十二种关系类型，形成了一个经过精心标记的数据集

    arXiv:2403.15088v1 Announce Type: new  Abstract: Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the ``Chinese Historical Information Extraction Corpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 
    
[^24]: 构建针对大型语言模型的日本金融基准

    Construction of a Japanese Financial Benchmark for Large Language Models

    [https://arxiv.org/abs/2403.15062](https://arxiv.org/abs/2403.15062)

    通过构建针对日本金融领域的多任务基准并对多模型进行测试，证实了GPT-4表现出色，该基准有效区分了各性能范围内模型的基准分数。

    

    随着大型语言模型（LLMs）的最新发展，人们开始讨论专注于特定领域和语言的模型的必要性。当前对每个领域中现有LLMs性能进行评估的需求也日益增长。因此，在这项研究中，我们构建了一个包含多个特定于日本和金融领域的任务的基准，并对一些模型进行了基准测量。结果表明，GPT-4目前表现突出，并且构建的基准能够有效发挥作用。根据我们的分析，我们的基准通过结合不同难度的任务，可以区分所有性能范围内各模型的基准分数。

    arXiv:2403.15062v1 Announce Type: cross  Abstract: With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for benchmarks to evaluate the performance of current LLMs in each domain. Therefore, in this study, we constructed a benchmark comprising multiple tasks specific to the Japanese and financial domains and performed benchmark measurements on some models. Consequently, we confirmed that GPT-4 is currently outstanding, and that the constructed benchmarks function effectively. According to our analysis, our benchmark can differentiate benchmark scores among models in all performance ranges by combining tasks with different difficulties.
    
[^25]: LLM2LLM: 利用新的迭代数据增强技术增强LLM

    LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement

    [https://arxiv.org/abs/2403.15042](https://arxiv.org/abs/2403.15042)

    LLM2LLM 提出了一种迭代数据增强策略，通过使用教师LLM生成合成数据并将其添加回训练数据，从而帮助低数据环境下的LLM进行微调。

    

    预训练的大型语言模型（LLMs）目前是解决绝大多数自然语言处理任务的最先进技术。尽管许多现实世界应用仍需要微调以达到令人满意的性能水平，但其中许多应用处于低数据范围，使得微调变得具有挑战性。为了解决这个问题，我们提出了LLM2LLM，这是一种有针对性和迭代的数据增强策略，利用一个教师LLM来增强一个小的种子数据集，通过增加额外的数据用于针对特定任务的微调。LLM2LLM（1）在初始种子数据上微调基线学生LLM，（2）评估和提取模型错误的数据点，（3）使用教师LLM根据这些错误的数据点生成合成数据，然后将其添加回训练数据中。这种方法通过在训练过程中增强LLM对错误预测数据点的信号，并重新整合它们。

    arXiv:2403.15042v1 Announce Type: new  Abstract: Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them in
    
[^26]: 通过GPT-4进行隐式规则学习的ESG分类

    ESG Classification by Implicit Rule Learning via GPT-4

    [https://arxiv.org/abs/2403.15040](https://arxiv.org/abs/2403.15040)

    本文研究了如何通过引导最先进的语言模型GPT-4，通过提示、思维链推理和动态上下文学习等策略来与未知的ESG评估标准达成一致，并在韩文共享任务中取得了第二名的成绩。

    

    环境、社会和治理（ESG）因素被广泛采用作为更高投资回报的指标。因此，正在进行持续努力，利用语言模型自动化ESG评估，从海量网络文本中提取信号变得更容易。然而，由于评级机构保密其评估指标，最近的方法饱受缺乏训练数据之苦。本文探讨了当前最先进的语言模型如GPT-4是否可以通过提示、思维链推理和动态上下文学习等策略来引导符合未知ESG评估标准。我们通过在所提供的训练数据上不更新模型，在韩文共享任务ML-ESG-3影响类型跟踪中排名第二，展示了这些方法的有效性。我们还探讨了调整提示如何影响语言模型处理金融任务的能力，利用具有公开可用权重的较小模型。

    arXiv:2403.15040v1 Announce Type: new  Abstract: Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators. Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily. However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential. This paper investigates whether state-of-the-art language models like GPT-4 can be guided to align with unknown ESG evaluation criteria through strategies such as prompting, chain-of-thought reasoning, and dynamic in-context learning. We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data. We also explore how adjusting prompts impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights
    
[^27]: MasonTigers在SemEval-2024任务1中的集成方法研究：语义文本相关性

    MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness

    [https://arxiv.org/abs/2403.14990](https://arxiv.org/abs/2403.14990)

    MasonTigers在SemEval-2024 Task 1中采用集成方法，结合语言特定的BERT模型和句子变换器，在处理语义文本相关性时取得了优异的结果。

    

    本文介绍了MasonTigers参与SemEval-2024任务1-语义文本相关性的工作。该任务涵盖了涵盖了监督（Track A）、无监督（Track B）和跨语言（Track C）方法，涉及14种不同语言。MasonTigers是少数同时参与了三个track中所有语言的两支团队之一。我们的方法在Track A中排名从第11到第21，在Track B中排名从第1到第8，在Track C中排名从第5到第12。在遵循特定任务约束的同时，我们的表现最好的方法利用了统计机器学习方法的集成，结合了基于语言的BERT模型和句子变换器。

    arXiv:2403.14990v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.
    
[^28]: MasonTigers在SemEval-2024任务8上的表现分析：基于Transformer模型的机器生成文本检测

    MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection

    [https://arxiv.org/abs/2403.14989](https://arxiv.org/abs/2403.14989)

    本文介绍了MasonTigers在SemEval-2024任务8上的表现分析，创新之处在于利用鉴别器Transformer模型的集成，结合句子Transformer和统计机器学习方法，以及在部分情况下采用零样本提示和针对FLAN-T5的微调。

    

    本文介绍了MasonTigers参加SemEval-2024任务8的情况 - 多生成器、多领域和多语言的黑盒机器生成文本检测。该任务涵盖了二元人工撰写 vs. 机器生成文本分类（Track A）、多路机器生成文本分类（Track B）和人机混合文本检测（Track C）。我们的最佳方法主要利用鉴别器Transformer模型的集成，以及在特定情况下句子Transformer和统计机器学习方法。此外，对于Track A和B，还使用了零样本提示和对FLAN-T5的微调。

    arXiv:2403.14989v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 8 - Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection. The task encompasses Binary Human-Written vs. Machine-Generated Text Classification (Track A), Multi-Way Machine-Generated Text Classification (Track B), and Human-Machine Mixed Text Detection (Track C). Our best performing approaches utilize mainly the ensemble of discriminator transformer models along with sentence transformer and statistical machine learning approaches in specific cases. Moreover, zero-shot prompting and fine-tuning of FLAN-T5 are used for Track A and B.
    
[^29]: 大型语言模型中的风险与响应：评估关键威胁类别

    Risk and Response in Large Language Models: Evaluating Key Threat Categories

    [https://arxiv.org/abs/2403.14988](https://arxiv.org/abs/2403.14988)

    本研究探讨了大型语言模型中的风险评估问题，发现LLMs倾向于认为信息风险较少有害，同时在信息风险场景中对越狱攻击存在漏洞。

    

    本文探讨了大型语言模型（LLMs）在各种应用中日益普及的风险评估问题。重点关注奖励模型如何感知和分类不同类型的风险，奖励模型旨在微调预训练的LLMs以与人类价值观一致，我们深入探讨了基于偏好训练数据的主观性质带来的挑战。通过使用Anthropic Red-team数据集，我们分析了包括信息风险、恶意用途和歧视/仇恨内容在内的主要风险类别。我们的研究结果表明，LLMs倾向于认为信息危害较少有害，这一发现得到了一个特别开发的回归模型的证实。此外，我们的分析显示，LLMs对信息危害的响应相对不那么严格。研究进一步揭示了LLMs在信息危害场景中对越狱攻击存在显著的漏洞。

    arXiv:2403.14988v1 Announce Type: new  Abstract: This paper explores the pressing issue of risk assessment in Large Language Models (LLMs) as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to fine-tune pretrained LLMs to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that LLMs tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that LLMs respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, hig
    
[^30]: MasonTigers在SemEval-2024 Task 9中：使用一系列思维链解决谜题

    MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts

    [https://arxiv.org/abs/2403.14982](https://arxiv.org/abs/2403.14982)

    这项研究使用大型语言模型解决SemEval-2024 Task 9的谜题任务，通过一系列思维链提示技术，包括零参考和少量参考提示，以及思维链提示，最终取得了显著的结果，展示了逐步解释性提示如何可以更好地揭示已编码的知识。

    

    我们的论文介绍了团队MasonTigers提交给SemEval-2024 Task 9的作品，该任务提供了一组谜题数据集用于测试自然语言理解。我们利用大型语言模型（LLMs）通过几种提示技术来解决这个任务。零参考和少量参考提示通过专有LLMs测试时取得了相当不错的结果，相较于开源模型。通过思维链提示进一步提升了结果，这是一种迭代提示方法，逐步分解推理过程。通过利用一系列思维链提示，我们获得最好的结果，在单词谜题子任务中排名第二，在句子谜题子任务中排名第13。提示LLMs的强大表现显示了它们在提供思考过程分解时进行复杂推理的能力。我们的工作揭示了逐步解释性提示如何能够更多地揭示已编码的知识。

    arXiv:2403.14982v1 Announce Type: new  Abstract: Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded 
    
[^31]: 一图胜千言：多模态推理中的图谱辩论

    A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning

    [https://arxiv.org/abs/2403.14972](https://arxiv.org/abs/2403.14972)

    提出了一种演绎式的图谱辩论方法（BDoG），在多模态推理中防止意见陈腐化和减少由图像引入的分心概念，实验证明其在科学问答和MMBench上取得了最先进的结果。

    

    本文介绍了一项旨在将多智能体辩论引入多模态推理的试点研究。该研究解决了两个关键挑战：由于过度总结而导致意见陈腐化，以及由于图像引入转移性概念而导致注意力分散的问题。这些挑战源自现有辩论方案的归纳（自下而上）性质。为解决这一问题，我们提出了一种演绎（自上而下）的辩论方法，称为图谱辩论（BDoG）。在BDoG中，辩论仅限于蓝图图中，以防止通过世界级摘要而导致意见陈腐化。此外，通过在图中的分支中存储证据，BDoG缓解了频繁但无关的概念带来的分散注意力现象。大量实验验证了BDoG，在科学问答和MMBench中取得了最新成果，并相较于先前的方法具有显著改进。

    arXiv:2403.14972v1 Announce Type: new  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.
    
[^32]: Adapprox:自适应近似在Adam优化中使用随机低秩矩阵

    Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices

    [https://arxiv.org/abs/2403.14958](https://arxiv.org/abs/2403.14958)

    Adapprox是一种采用随机低秩矩阵近似的自适应方法，用于更有效和准确地逼近Adam优化算法的二阶矩。在GPT-2的训练和下游任务中，Adapprox相比AdamW能够实现34.5%至49.9%和33.8%至49.9%的内存节约，并通过余弦相似性指导策略提高了稳定性和加快了收敛速度。

    

    随着深度学习模型的规模呈指数增长，诸如Adam之类的优化器在存储一、二阶矩数据时遇到了显著的内存消耗挑战。当前的内存高效方法如Adafactor和CAME通常通过其矩阵因式分解技术来牺牲准确性。针对这一问题，我们引入了Adapprox，一种新颖的方法，它采用随机低秩矩阵近似来更有效和准确地近似Adam的二阶矩。Adapprox具有自适应秩选择机制，精细平衡准确性和内存效率，并包括一个可选的余弦相似性指导策略，以增强稳定性并加快收敛速度。在GPT-2训练和下游任务中，Adapprox通过实现对117M和345M模型的34.5%至49.9%和33.8%至49.9%内存节约（分别启用了第一阶矩），超越了AdamW，并进一步增加了这些节约。

    arXiv:2403.14958v1 Announce Type: cross  Abstract: As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In GPT-2 training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings wit
    
[^33]: 基于证据驱动的在线虚假信息检索增强响应生成

    Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation

    [https://arxiv.org/abs/2403.14952](https://arxiv.org/abs/2403.14952)

    提出了一种基于检索增强的响应生成方法(RARG)，通过收集科学来源的证据来生成反虚假信息的响应，相比现有方法，可以提高文本质量和避免过度重复。

    

    纸：arXiv:2403.14952v1   类型：交叉   摘要：在线虚假信息的泛滥对公共利益构成了重要威胁。虽然许多在线用户积极参与对抗虚假信息，但很多响应缺乏礼貌和支持事实。作为解决方案，提出了文本生成方法，可以自动生成反虚假信息的响应。然而，现有方法通常在没有利用外部知识的情况下进行端到端训练，导致文本质量不佳和响应过于重复。在本文中，我们提出了用于在线虚假信息的检索增强响应生成（RARG），该方法从科学来源收集支持证据，并基于这些证据生成反虚假信息的响应。特别是，我们的RARG包括两个阶段：（1）证据收集，我们设计了一个检索流程来检索和重新排列证据文档，使用一个dat

    arXiv:2403.14952v1 Announce Type: cross  Abstract: The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a dat
    
[^34]: KnowLA：利用知识自适应提升参数高效微调

    KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation

    [https://arxiv.org/abs/2403.14950](https://arxiv.org/abs/2403.14950)

    该论文提出了一种名为KnowLA的知识自适应方法，通过在大型语言模型中插入自适应层和知识图嵌入，能够提升参数高效微调的有效性和鲁棒性。

    

    参数高效微调（PEFT）是调整大型语言模型（LLMs）以适应下游任务的关键技术。本文研究了利用知识图嵌入来改善PEFT的有效性。我们提出了一种称为KnowLA的知识自适应方法。它在LLM中插入一个自适应层，将输入文本中出现的实体的嵌入整合在一起。自适应层与LoRA在指导数据上组合训练。在两个流行的LLMs和三个知识图上进行的六项基准实验表明了KnowLA的有效性和鲁棒性。我们展示了 \modelname 能够帮助激活LLM中的相关参数化知识以回答问题，而不改变其参数或输入提示。

    arXiv:2403.14950v1 Announce Type: new  Abstract: Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that \modelname can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.
    
[^35]: 一个线性层生成任务自适应低秩矩阵

    A Single Linear Layer Yields Task-Adapted Low-Rank Matrices

    [https://arxiv.org/abs/2403.14946](https://arxiv.org/abs/2403.14946)

    通过研究转换矩阵将$ W_0 $转换为低秩矩阵的关系信息，我们提出单一线性层可以生成任务自适应的低秩矩阵。

    

    低秩适应（LoRA）是一种广泛使用的参数高效调整（PEFT）方法，它通过由两个低秩矩阵$ A $和$ B $组成的增量矩阵$ \Delta W $更新初始权重矩阵$ W_0 $。先前的研究表明$ W_0 $和$ \Delta W $之间存在关联。在这项研究中，我们旨在深入探讨$ W_0 $与低秩矩阵$ A $和$ B $之间的关系，以进一步理解LoRA的行为。特别地，我们分析了一个将$ W_0 $转换为低秩矩阵的转换矩阵，其中蕴含了关系的信息。我们的分析表明转换矩阵在每一层之间是相似的。受到这些发现的启发，我们假设一个单一线性层，将每一层的$ W_0 $作为输入，可以生成任务自适应的低秩矩阵。为了验证这一假设，我们设计了一种名为有条件参数化的LoRA (CondLoRA) 方法，来更新初始权重...

    arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig
    
[^36]: 关于LLM在零-shot 对抗性言论生成中的应用

    On Zero-Shot Counterspeech Generation by LLMs

    [https://arxiv.org/abs/2403.14938](https://arxiv.org/abs/2403.14938)

    该研究首次在零-shot 设置下探索了四种LLM在对抗性言论生成任务中的表现，并提出了三种不同的提示策略，为生成不同类型的对抗性言论提供了全面分析。

    

    随着大量大型语言模型（LLM）的出现，这些模型在各种自然语言处理（NLP）应用中的使用量大幅增加。对抗性言论生成是一个重要任务，努力通过微调LLM与仇恨言论-对抗性言论对来发展生成模型，但这些尝试都没有探索大型语言模型在零-shot 设置中的内在特性。在这项工作中，我们对四个LLM（GPT-2、DialoGPT、ChatGPT和FlanT5）在零-shot 设置下用于对抗性言论生成的性能进行了全面分析，这是第一次。对于GPT-2和DialoGPT，我们进一步研究了随着模型大小（小、中、大）性能偏差。另一方面，我们提出了三种不同的提示策略，用于生成不同类型的对抗性言论，并分析了这些策略对性能的影响。

    arXiv:2403.14938v1 Announce Type: new  Abstract: With the emergence of numerous Large Language Models (LLM), the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings. In this work, we present a comprehensive analysis of the performances of four LLMs namely GPT-2, DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech generation, which is the first of its kind. For GPT-2 and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, large) of the models. On the other hand, we propose three different prompting strategies for generating different types of counterspeech and analyse the impact of such strategies on the p
    
[^37]: 专注驱动的推理:释放大型语言模型的潜力

    Attention-Driven Reasoning: Unlocking the Potential of Large Language Models

    [https://arxiv.org/abs/2403.14932](https://arxiv.org/abs/2403.14932)

    通过注意力机制优化，可以显著提高大型语言模型的推理能力，尤其对于非STEM问题。

    

    大型语言模型（LLMs）展示了卓越的能力，但它们的推理能力和基础机制仍不为人所了解。我们提出了一种通过注意力机制优化来增强LLMs推理能力的新方法，而无需额外的训练数据。我们确定了由非语义标记导致的注意力分布的低效率，并提出了一种算法来重新平衡偏斜分布，使模型能够抽象更加微妙的知识。我们的实验表明，推理能力得到了显着改进，特别是对于非STEM问题。我们深入探讨了注意力模式在LLMs推理中的作用，并提出了一种增强这些能力的方法，为更强大和多功能的语言模型铺平了道路。

    arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
    
[^38]: 用于高效自回归文本生成的分层跳跃解码

    Hierarchical Skip Decoding for Efficient Autoregressive Text Generation

    [https://arxiv.org/abs/2403.14919](https://arxiv.org/abs/2403.14919)

    提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成，通过分层地自适应跳过解码层来减少计算负载和分配计算资源。

    

    自回归解码策略是一种常用的文本生成任务方法，适用于预训练语言模型，而提前结束是一种有效的加速推断阶段的方法。在本研究中，我们提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成。与需要额外可训练组件的现有方法不同，HSD是一种即插即用的方法，适用于自回归文本生成模型，它根据当前序列长度以分层的方式自适应地跳过解码层，从而减少计算负载并分配计算资源。在五个带有预先训练语言模型的文本生成数据集上进行的全面实验显示，HSD在平衡效率和文本质量方面具有优势。几乎跳过一半的层，HSD可以与原始自回归d模型相比保持90%的文本质量。

    arXiv:2403.14919v1 Announce Type: cross  Abstract: Autoregressive decoding strategy is a commonly used method for text generation tasks with pre-trained language models, while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation. Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive text generation models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five text generation datasets with pre-trained language models demonstrate HSD's advantages in balancing efficiency and text quality. With almost half of the layers skipped, HSD can sustain 90% of the text quality compared to vanilla autoregressive d
    
[^39]: 方位推理器：利用显式推理在社交媒体上进行零-shot立场检测

    Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning

    [https://arxiv.org/abs/2403.14895](https://arxiv.org/abs/2403.14895)

    Stance Reasoner是一种利用显式推理和世界知识进行零-shot社交媒体立场检测的方法，优于当前领先模型，并能更好地横跨目标进行泛化。

    

    社交媒体平台是丰富的观点内容来源。立场检测允许自动从这些内容中提取用户对各种话题的意见。我们关注零-shot立场检测，即模型的成功依赖于（a）对目标话题的知识；以及（b）学习可以用于新话题的通用推理策略。我们提出了Stance Reasoner，一种利用背景知识上的显式推理来引导模型推断有关文档在目标上的立场的零-shot立场检测方法。具体而言，我们的方法使用预训练语言模型作为世界知识的来源，采用上下文学习方法生成中间推理步骤。Stance Reasoner在3个Twitter数据集上表现优异，包括完全监督的模型。它可以更好地横跨目标进行泛化。

    arXiv:2403.14895v1 Announce Type: cross  Abstract: Social media platforms are rich sources of opinionated content. Stance detection allows the automatic extraction of users' opinions on various topics from such content. We focus on zero-shot stance detection, where the model's success relies on (a) having knowledge about the target topic; and (b) learning general reasoning strategies that can be employed for new topics. We present Stance Reasoner, an approach to zero-shot stance detection on social media that leverages explicit reasoning over background knowledge to guide the model's inference about the document's stance on a target. Specifically, our method uses a pre-trained language model as a source of world knowledge, with the chain-of-thought in-context learning approach to generate intermediate reasoning steps. Stance Reasoner outperforms the current state-of-the-art models on 3 Twitter datasets, including fully supervised models. It can better generalize across targets, while a
    
[^40]: AutoRE：使用大型语言模型进行文档级关系抽取

    AutoRE: Document-Level Relation Extraction with Large Language Models

    [https://arxiv.org/abs/2403.14888](https://arxiv.org/abs/2403.14888)

    AutoRE 是一种端到端的文档级关系抽取模型，采用了一种名为RHF的新颖关系抽取范式，可有效处理分布在文档中的多个关系和三元组事实。

    

    大型语言模型(LLMs)展示了在理解和生成文本方面的异常能力，这激励着许多研究人员利用它们进行信息抽取(IE)任务，包括关系抽取(RE)。然而，大多数现有方法主要设计用于句子级关系抽取(SentRE)任务，这通常涵盖了单个句子内的一组关系和三元组事实。此外，一些方法采用将关系作为候选选择集成到提示模板中的方式，导致在处理分布在给定文档中的多个关系和三元组事实时效率低下，性能亚优，并在处理文档级关系抽取(DocRE)任务时面临独特挑战。为了克服这些限制，我们介绍了AutoRE，这是一个端到端的DocRE模型，采用了一种名为RHF(Re

    arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
    
[^41]: VidLA: 规模化视频语言对齐

    VidLA: Video-Language Alignment at Scale

    [https://arxiv.org/abs/2403.14870](https://arxiv.org/abs/2403.14870)

    VidLA 提出了一种规模化视频语言对齐方法，通过简化网络架构和使用分层数据令牌来捕捉短程和长程时间依赖关系，从而成功融合预训练图像-文本基础模型，提高了最终性能。

    

    在这篇论文中，我们提出了VidLA，一种用于规模化视频语言对齐的方法。我们解决了以往视频语言对齐方法的两个主要局限。首先，它们没有捕捉到短程和长程时间依赖关系，并且通常采用复杂的分层深度网络架构，难以与现有的预训练图像-文本基础模型集成。为了有效解决这一限制，我们保持网络架构简单，使用一组在不同时间分辨率下以分层方式运行的数据令牌，从而考虑视频的时间分层性质。通过使用简单的双塔架构，我们能够使用预训练的图像-文本基础模型初始化我们的视频-语言模型，从而提高最终性能。

    arXiv:2403.14870v1 Announce Type: cross  Abstract: In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training 
    
[^42]: 在基础模型和指令调优的大型语言模型中比较可信度估计

    Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models

    [https://arxiv.org/abs/2403.14859](https://arxiv.org/abs/2403.14859)

    通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。

    

    指令调优的LLM可以响应明确制定为提示的查询，这极大地促进了与人类用户的交互。然而，基于提示的方法可能并不总是能够利用LLM在预训练期间获得的隐式知识。本文对评估LLM中语义可信度的方法进行了全面研究。我们通过（a）明确提示和（b）直接读取模型分配给字符串的概率的隐式估计，在英语句子可信度任务中比较了基础和指令调优LLM的性能。实验1表明，跨模型架构和可信度数据集，（i）对数似然（LL）分数是句子可信度最可靠的指标，零照射提示产生不一致且通常效果不佳的结果；（ii）基于LL的性能仍低于人类表现；（iii）指令调优模型有

    arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
    
[^43]: TAMS: 翻译辅助的形态分割

    TAMS: Translation-Assisted Morphological Segmentation

    [https://arxiv.org/abs/2403.14840](https://arxiv.org/abs/2403.14840)

    这项工作提出了一种利用翻译数据辅助形态分割任务的方法，通过使用字符级序列到序列模型，以及预先训练的高资源单语言模型的翻译表示，实现在低资源环境下超越基线模型。

    

    规范形态分割是将单词分析为其构成形态素的标准（也称为底层）形式的过程。这是语言文档编制中的核心任务，NLP 系统有望显著加快这一处理过程。在典型的语言文档编制环境中，规范形态分割的训练数据稀缺，这使得难以训练出高质量的模型。然而，翻译数据通常更为丰富，在这项工作中，我们提出了一种尝试利用这些数据的方法来进行规范分割任务。我们提出了一个字符级序列到序列模型，该模型将来自预先训练的高资源单语言模型的翻译表示作为额外信号。我们的模型在超低资源设置中优于基线，但在具有更多数据的训练分割上产生了不同的结果。需要进一步工作才能使

    arXiv:2403.14840v1 Announce Type: new  Abstract: Canonical morphological segmentation is the process of analyzing words into the standard (aka underlying) forms of their constituent morphemes. This is a core task in language documentation, and NLP systems have the potential to dramatically speed up this process. But in typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage this data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. While further work is needed to make
    
[^44]: 大型语言模型在心理健康领域的机会和风险

    The opportunities and risks of large language models in mental health

    [https://arxiv.org/abs/2403.14814](https://arxiv.org/abs/2403.14814)

    大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。

    

    全球心理健康问题的发生率正在上升，人们越来越意识到现有的心理保健模式无法充分扩展以满足需求。随着大型语言模型（LLMs）的出现，人们对它们具有创造新颖、大规模解决方案以支持心理健康的承诺感到乐观。尽管它们还处于初期阶段，LLMs已被应用于与心理健康相关的任务。本综述总结了已有文献中关于利用LLMs提供心理健康教育、评估和干预的努力，并突出了每个领域中产生积极影响的关键机会。然后，我们强调了将LLMs应用于心理健康领域所伴随的风险，并鼓励采用策略来减轻这些风险。对于心理健康支持的迫切需求必须与负责任的心理健康LLMs的开发、测试和部署相平衡。特别关键的是确保心理健康...

    arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
    
[^45]: 一组关于口语对话话语的实用相似度判断集合

    A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances

    [https://arxiv.org/abs/2403.14808](https://arxiv.org/abs/2403.14808)

    开发了第一个人类判断口语对话话语之间语用相似度的集合，通过评分表明了不同程度相似度的差异。

    

    自动测量话语之间相似度的方法对于训练语音合成器、评估机器翻译以及评估学习者的产出非常宝贵。尽管已经存在语义相似度和韵律相似度的测量方法，但目前尚无关于语用相似度的方法。为了训练这样的测量方法，我们开发了第一个人类判断口语对话话语之间语用相似度的集合。每对话语包括一段来自录音对话的话语和该话语的再现。再现是在设计了各种条件以创建不同程度相似度的情况下完成的。每对话语由6到9位评委在连续尺度上评分。英语的评委间平均相关性高达0.72，而西班牙语为0.66。我们在https://github.com/divettemarco/PragSim 上提供了这些数据。

    arXiv:2403.14808v1 Announce Type: new  Abstract: Automatic measures of similarity between utterances are invaluable for training speech synthesizers, evaluating machine translation, and assessing learner productions. While there exist measures for semantic similarity and prosodic similarity, there are as yet none for pragmatic similarity. To enable the training of such measures, we developed the first collection of human judgments of pragmatic similarity between utterance pairs. Each pair consisting of an utterance extracted from a recorded dialog and a re-enactment of that utterance. Re-enactments were done under various conditions designed to create a variety of degrees of similarity. Each pair was rated on a continuous scale by 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for English and 0.66 for Spanish. We make this data available at https://github.com/divettemarco/PragSim .
    
[^46]: 多智体VQA：探索零样本视觉问答中的多智体基础模型

    Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering

    [https://arxiv.org/abs/2403.14783](https://arxiv.org/abs/2403.14783)

    本研究提出了一种自适应多智体系统，名为多智体VQA，通过使用专门的智体工具，克服了基础模型在目标检测和计数中的局限性，在零样本情况下实现了良好的性能，为未来研究提供了新的方向。

    

    这项工作探讨了基础模型在视觉问答（VQA）任务中的零样本能力。我们提出了一种自适应多智体系统，命名为多智体VQA，通过使用专门的智体作为工具，以克服基础模型在目标检测和计数中的局限性。与现有方法不同，我们的研究侧重于在不对其进行特定VQA数据集微调的情况下系统的性能，使其在开放世界中更加实用和稳健。我们在零样本场景下提出了初步实验结果，并突出了一些失败案例，为未来研究提供了新的方向。

    arXiv:2403.14783v1 Announce Type: cross  Abstract: This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system's performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.
    
[^47]: 视觉-语言模型上的少样本对抗提示学习

    Few-Shot Adversarial Prompt Learning on Vision-Language Models

    [https://arxiv.org/abs/2403.14774](https://arxiv.org/abs/2403.14774)

    本文提出了一个少样本对抗提示框架，在视觉-语言模型中通过有限数据调整输入序列，显著提升对抗鲁棒性，并通过端到端学习对抗性相关的文本监督。

    

    深度神经网络对于微不可见的对抗性扰动的脆弱性已经引起了广泛关注。受到视觉-语言基础模型成功的启发，先前的努力通过将对抗性视觉特征与文本监督对齐来实现零样本对抗鲁棒性。但在实践中，由于包括重大适应成本、次优文本监督和未受控制的自然泛化能力在内的多个问题，它们仍然不尽人意。为了解决这些问题，本文提出了一个少样本对抗提示框架，通过有限的数据调整输入序列使得对抗鲁棒性得到显著提升。具体而言，我们通过提供对抗相关的文本监督，该监督是从对抗性示例中端到端学习的，来实现这一点。我们还提出了一个增强多模态特征一致性并鼓励不同

    arXiv:2403.14774v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differenti
    
[^48]: StreamingT2V: 一种一致、动态和可扩展的基于文本的长视频生成方法

    StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text

    [https://arxiv.org/abs/2403.14773](https://arxiv.org/abs/2403.14773)

    StreamingT2V是一种自回归方法，用于生成长视频，可以产生80、240、600、1200帧甚至更多帧的视频，并具有平滑的过渡。

    

    arXiv:2403.14773v1 公告类型: 交叉 摘要: 文本到视频的扩散模型可以生成遵循文本指令的高质量视频，使得创建多样化和个性化内容变得更加容易。然而，现有方法大多集中在生成高质量的短视频（通常为16或24帧），当天真地扩展到长视频合成的情况时，通常会出现硬裁剪。为了克服这些限制，我们引入了StreamingT2V，这是一种自回归方法，用于生成80、240、600、1200或更多帧的长视频，具有平滑的过渡。主要组件包括：（i）一种名为条件注意力模块（CAM）的短期记忆块，通过注意机制将当前生成条件设置为先前块提取的特征，实现一致的块过渡，（ii）一种名为外观保存模块的长期记忆块，从第一个视频块中提取高级场景和对象特征，以防止th

    arXiv:2403.14773v1 Announce Type: cross  Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent th
    
[^49]: 一项神经代码智能的调查：范式、进展与未来

    A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond

    [https://arxiv.org/abs/2403.14734](https://arxiv.org/abs/2403.14734)

    神经代码智能领域的调查系统回顾了50多种代表性模型和超过680项相关作品，突出了不同研究阶段的范式和技术转变。

    

    arXiv:2403.14734v1 公告类型: 跨领域 摘要: 神经代码智能--利用深度学习理解、生成和优化代码--在整个社会上具有巨大的潜力，可产生深远影响。作为自然语言和编程语言之间的桥梁，这一领域在过去几年引起了两个研究社区研究人员的极大关注。本调查系统地和按时间顺序回顾了代码智能方面的进展，包括50多种代表性模型及其变体、20多种任务类别以及超过680项相关作品。我们遵循历史进展，跟踪不同研究阶段的范式转变（例如，从使用循环神经网络对代码建模到大型语言模型时代）。同时，我们重点介绍了不同阶段涵盖的模型、任务和评估的主要技术转变。对于应用，我们

    arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
    
[^50]: 用多任务学习进行开放知识库规范化

    Open Knowledge Base Canonicalization with Multi-task Learning

    [https://arxiv.org/abs/2403.14733](https://arxiv.org/abs/2403.14733)

    提出了一个多任务学习框架MulCanon来处理开放知识库（OKB）规范化问题，并通过在软聚类过程中使用扩散模型来改进名词短语的表示。

    

    大型开放知识库（OKB）的构建对于诸多基于知识的网络应用如网络搜索至关重要。然而，OKB中的名词短语和关系短语往往存在冗余和歧义，这需要对OKB进行规范化的研究。当前的解决方案通过设计先进的聚类算法，并使用知识图嵌入（KGE）进一步促进规范化过程。然而，这些工作未能充分利用聚类和KGE学习之间的协同作用，并且为这些子任务设计的方法是次优的。因此，我们提出了一个名为MulCanon的多任务学习框架来解决OKB的规范化问题。此外，在软聚类过程中使用扩散模型来改进名词短语的表示，带来更准确的表征。

    arXiv:2403.14733v1 Announce Type: new  Abstract: The construction of large open knowledge bases (OKBs) is integral to many knowledge-driven applications on the world wide web such as web search. However, noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Nevertheless, these works fail to fully exploit the synergy between clustering and KGE learning, and the methods designed for these subtasks are sub-optimal. To this end, we put forward a multi-task learning framework, namely MulCanon, to tackle OKB canonicalization. In addition, diffusion model is used in the soft clustering process to improve the noun phrase representations with neighboring information, which can lead to more accurate representations. MulCano
    
[^51]: 可逆跳动攻击对文本分类器的修改缩减

    Reversible Jump Attack to Textual Classifiers with Modification Reduction

    [https://arxiv.org/abs/2403.14731](https://arxiv.org/abs/2403.14731)

    提出了两种算法，可逆跳动攻击（RJA）和Metropolis-Hasting修改缩减（MMR），用于生成高度有效的对抗示例，并分别改善示例的不可察觉性。

    

    最近的对抗样本研究揭示了自然语言处理（NLP）模型的漏洞。现有生成对抗样本的技术通常由确定性的层次规则驱动，这些规则对最优对抗样本毫不关心，通常导致对抗样本在变化幅度和攻击成功之间存在次优平衡。在此研究中，我们提出了两种算法，可逆跳动攻击（RJA）和Metropolis-Hasting修改缩减（MMR），用于生成高度有效的对抗示例并分别改善示例的不可察觉性。RJA利用新颖的随机化机制来扩大搜索空间，有效适应对抗样本中的多个扰动词汇。利用生成的对抗示例，MMR应用Metropolis-Hasting采样器以增强对抗示例的不可察觉性。

    arXiv:2403.14731v1 Announce Type: cross  Abstract: Recent studies on adversarial examples expose vulnerabilities of natural language processing (NLP) models. Existing techniques for generating adversarial examples are typically driven by deterministic hierarchical rules that are agnostic to the optimal adversarial examples, a strategy that often results in adversarial samples with a suboptimal balance between magnitudes of changes and attack successes. To this end, in this research we propose two algorithms, Reversible Jump Attack (RJA) and Metropolis-Hasting Modification Reduction (MMR), to generate highly effective adversarial examples and to improve the imperceptibility of the examples, respectively. RJA utilizes a novel randomization mechanism to enlarge the search space and efficiently adapts to a number of perturbed words for adversarial examples. With these generated adversarial examples, MMR applies the Metropolis-Hasting sampler to enhance the imperceptibility of adversarial e
    
[^52]: 大型语言模型中的受保护群体偏见和刻板印象

    Protected group bias and stereotypes in Large Language Models

    [https://arxiv.org/abs/2403.14727](https://arxiv.org/abs/2403.14727)

    该研究调查了大型语言模型在伦理和公平领域中的行为，发现模型不仅反映了社会偏见，还似乎放大了这些偏见。

    

    随着现代大型语言模型在各种领域中打破许多最新技术基准，本文调查了它们在伦理和公平领域的行为，重点关注受保护群体偏见。我们进行了两部分研究：首先，我们征集了描述来自不同受保护群体（包括性别、性取向、宗教和种族）个人职业的句子延续；其次，我们让模型生成关于拥有不同类型职业的个人的故事。我们收集了一款公开可用的大型语言模型生成的 >10k 个句子延续，受到人类标注。我们发现模型在被边缘化群体中存在偏见，尤其在性别和性取向领域，以及在模型生成中存在西方偏见。模型不仅反映了社会偏见，还似乎放大了这些偏见。该模型对于与边缘化群体相关的查询回复过于谨慎，提供了

    arXiv:2403.14727v1 Announce Type: cross  Abstract: As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect >10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing re
    
[^53]: Jailbreaking的最佳解决方案是通过定义

    Jailbreaking is Best Solved by Definition

    [https://arxiv.org/abs/2403.14725](https://arxiv.org/abs/2403.14725)

    语言模型中"越狱"攻击的关键是通过定义好的不安全响应来进行防御，而不是依赖于执行策略。

    

    语言模型上"越狱"攻击的增多引发了大量防御工作，旨在防止产生不良回应。在这项工作中，我们批判性地审视了防御管道的两个阶段：（i）定义何为不安全输出，和（ii）通过输入处理或微调等方法来执行该定义。我们严重怀疑现有的执行机制的有效性，通过展示它们即使对于简单的不安全输出定义--包含单词"purple"的输出也无法防御。相比之下，对输出进行后处理对于这样的定义是完全健壮的。基于我们的结果，我们提出我们的观点，即在防御越狱攻击中真正的挑战在于得到一个良好的不安全响应定义：没有良好的定义，任何执行策略都无法成功，但有了良好的定义，输出处理已经作为一个强大的基线。

    arXiv:2403.14725v1 Announce Type: cross  Abstract: The rise of "jailbreak" attacks on language models has led to a flurry of defenses aimed at preventing the output of undesirable responses. In this work, we critically examine the two stages of the defense pipeline: (i) the definition of what constitutes unsafe outputs, and (ii) the enforcement of the definition via methods such as input processing or fine-tuning. We cast severe doubt on the efficacy of existing enforcement mechanisms by showing that they fail to defend even for a simple definition of unsafe outputs--outputs that contain the word "purple". In contrast, post-processing outputs is perfectly robust for such a definition. Drawing on our results, we present our position that the real challenge in defending jailbreaks lies in obtaining a good definition of unsafe responses: without a good definition, no enforcement strategy can succeed, but with a good definition, output processing already serves as a robust baseline albeit 
    
[^54]: 使用聚焦技术抵御间接提示注入攻击

    Defending Against Indirect Prompt Injection Attacks With Spotlighting

    [https://arxiv.org/abs/2403.14720](https://arxiv.org/abs/2403.14720)

    引入了聚焦技术，一种提示工程技术，用于改进大型语言模型在处理多个输入源时的能力，通过提供可靠的输入来源信号来防御间接提示注入攻击。

    

    大型语言模型(LLMs)虽然强大，却是建立和训练用于处理单个文本输入的。在常见应用中，可以通过将多个输入连接在一起形成单个文本流来进行处理。然而，LLM无法区分提示的哪些部分属于不同的输入源。间接提示注入攻击利用这一漏洞，将对手指令嵌入到与用户命令一起处理的不受信任数据中。通常情况下，LLM会误将对手指令作为要遵循的用户指令，从而在整个系统中创建安全漏洞。我们引入了聚焦技术，这是一种用于改进LLMs区分多个输入源能力的提示工程技术系列。关键的见解是利用输入的变换提供其来源的可靠连续信号。我们将聚焦技术作为一种防御手段进行评估。

    arXiv:2403.14720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defen
    
[^55]: 概念最佳匹配：评估新兴通信中的组合性

    Concept-Best-Matching: Evaluating Compositionality in Emergent Communication

    [https://arxiv.org/abs/2403.14705](https://arxiv.org/abs/2403.14705)

    提出了一种评估新兴通信组合性的方法，通过找到 emerged words 与 natural language concepts 之间的最佳匹配，实现了直接而可解释的映射。

    

    学习沟通以完成给定任务的人工智能代理获取的通信协议通常对人类来说是不透明的。大量研究尝试通过各种评估方法评估新兴通信，其中\emph{组合性}是一个突出的期望特征。然而，当前的评估程序并未直接暴露新兴通信的组合性。我们提出了一种评估新兴通信组合性的方法，即找到新兴词汇与自然语言概念之间的最佳匹配。最佳匹配算法提供了全局分数和从新兴词汇到自然语言概念的翻译映射。据我们所知，这是首次提供新兴词汇与人类概念之间直接而可解释的映射。

    arXiv:2403.14705v1 Announce Type: new  Abstract: Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with \emph{compositionality} featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts. The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided.
    
[^56]: GPT语言模型在大学教学活动创新中的应用

    Application of GPT Language Models for Innovation in Activities in University Teaching

    [https://arxiv.org/abs/2403.14694](https://arxiv.org/abs/2403.14694)

    GPT语言模型在大学教学活动创新中的应用不仅可以支持理解和生成内容、问题解决，还可以在个性化和测试纠错等方面提供帮助，但是在国际化方面需注意避免其误用导致的全球问题。

    

    GPT（生成式预训练变换器）语言模型是一种人工智能和自然语言处理技术，可以实现自动文本生成。在将GPT语言模型应用于大学教学的各个维度中存在着日益增长的兴趣。从学生和教师活动创新的角度来看，它们可以在理解和生成内容、问题解决以及个性化和测试纠错等方面提供支持。从国际化的角度来看，误用这些模型代表着一个需要采取一系列共同措施的全球问题，这些措施需要各地区的大学共同参与。在一些国家，已经对评估工具进行了审查，以确保工作是由学生完成的，而不是由人工智能完成的。为此，我们在计算机科学这种代表性学科的一个具体课题，比如软件工程中进行了详细的实验。

    arXiv:2403.14694v1 Announce Type: cross  Abstract: The GPT (Generative Pre-trained Transformer) language models are an artificial intelligence and natural language processing technology that enables automatic text generation. There is a growing interest in applying GPT language models to university teaching in various dimensions. From the perspective of innovation in student and teacher activities, they can provide support in understanding and generating content, problem-solving, as well as personalization and test correction, among others. From the dimension of internationalization, the misuse of these models represents a global problem that requires taking a series of common measures in universities from different geographical areas. In several countries, there has been a review of assessment tools to ensure that work is done by students and not by AI. To this end, we have conducted a detailed experiment in a representative subject of Computer Science such as Software Engineering, wh
    
[^57]: 将图注意机制融入基于深度强化学习的几何问题求解

    Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.14690](https://arxiv.org/abs/2403.14690)

    提出了基于深度强化学习的图注意机制，用于自动且高效地添加几何问题中的辅助组件

    

    在在线教育背景下，设计一个自动求解几何问题的求解器被认为是迈向通用数学人工智能的关键一步，其依托自然语言理解和传统逻辑推理。在大多数情况下，问题的解决是通过添加辅助组件如线条或点来进行的。然而，由于在需要做出关键决策时选择合适的辅助组件的复杂性，自动添加辅助组件具有挑战性。目前的最新性能是通过从类别库中穷举所有可能的策略，以识别具有最大可能性的策略来实现的。然而，为了在效率方面做出妥协，必须采用广泛的策略搜索。为了自动且高效地添加辅助组件，我们提出了基于语言模型（如BERT）的深度强化学习框架。

    arXiv:2403.14690v1 Announce Type: cross  Abstract: In the context of online education, designing an automatic solver for geometric problems has been considered a crucial step towards general math Artificial Intelligence (AI), empowered by natural language understanding and traditional logical inference. In most instances, problems are addressed by adding auxiliary components such as lines or points. However, adding auxiliary components automatically is challenging due to the complexity in selecting suitable auxiliary components especially when pivotal decisions have to be made. The state-of-the-art performance has been achieved by exhausting all possible strategies from the category library to identify the one with the maximum likelihood. However, an extensive strategy search have to be applied to trade accuracy for ef-ficiency. To add auxiliary components automatically and efficiently, we present deep reinforcement learning framework based on the language model, such as BERT. We first
    
[^58]: 一项道义使命：对大型语言模型持续超对齐的需求

    A Moral Imperative: The Need for Continual Superalignment of Large Language Models

    [https://arxiv.org/abs/2403.14683](https://arxiv.org/abs/2403.14683)

    实现终身超对齐需要对当前大型语言模型架构进行重大变革，以解决其在理解和适应动态人类道德和不断发展的全球情景方面的局限性。

    

    这篇论文探讨了在人工智能系统中实现终身超对齐的挑战，尤其是在大型语言模型（LLMs）中。超对齐是一个理论框架，旨在确保超智能人工智能系统符合人类的价值观和目标。尽管其展望令人振奋，我们认为实现超对齐需要对当前LLM架构进行重大变革，因为它们在理解和适应人类道德的动态性和不断发展的全球情景方面固有的局限性。我们剖析了将不断变化的人类价值观谱系编码到LLMs中的挑战，突出了静态人工智能模型与人类社会动态性之间的差异。为了说明这些挑战，我们分析了两个不同的示例：一个展示了人类价值观的定性转变，另一个呈现了可量化的变化。通过这些示例，我们说明…

    arXiv:2403.14683v1 Announce Type: cross  Abstract: This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illus
    
[^59]: 用大型语言模型预测学习表现：成人识字研究

    Predicting Learning Performance with Large Language Models: A Study in Adult Literacy

    [https://arxiv.org/abs/2403.14668](https://arxiv.org/abs/2403.14668)

    该研究使用大型语言模型GPT-4探讨了在ITS中预测成人识字计划学习表现的应用，并发现GPT-4在此方面具有竞争力的预测能力。

    

    arXiv:2403.14668v1 公告类别：跨领域 智能辅导系统（ITS）显著增强了成人识字培训，这是社会参与、就业机会和终身学习的关键因素。我们的研究探讨了高级AI模型（包括GPT-4等大型语言模型）在ITS中预测成人识字计划学习表现的应用。这项研究受到了LLMs基于其内在推理和计算能力预测学习表现的潜力的启发。通过使用ITS AutoTutor的阅读理解数据集，我们通过五折交叉验证技术评估了GPT-4与传统机器学习方法在预测学习表现方面的预测能力。我们的研究结果显示，GPT-4展现出与传统的机器学习方法（如贝叶斯知识跟踪、表现因素分析、稀疏因素分析）具有竞争力的预测能力。

    arXiv:2403.14668v1 Announce Type: cross  Abstract: Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Fact
    
[^60]: SyllabusQA：一个课程逻辑问题回答数据集

    SyllabusQA: A Course Logistics Question Answering Dataset

    [https://arxiv.org/abs/2403.14666](https://arxiv.org/abs/2403.14666)

    SyllabusQA数据集是一个包含63个真实课程大纲的开源数据集，对36个专业涵盖5,078对多样化的开放式课程逻辑相关问题-答案对进行了详细收集，旨在评估答案事实性，多个强基线模型在该任务上表现出色，但仍存在与人类之间的显著差距。

    

    自动化教学助理和聊天机器人有显著潜力减轻人类教师的工作量，尤其是对于与课程逻辑相关的问题回答，这对学生很重要，但对教师来说是重复的。然而，由于隐私问题，缺乏公开可用的数据集。我们介绍了SyllabusQA，这是一个开源数据集，包含63个真实课程大纲，涵盖36个专业，包含5,078对多样化的开放式课程逻辑相关问题-答案对，问题类型和答案格式都是多样的。由于许多逻辑相关问题包含关键信息，如考试日期，评估答案的事实性很重要。我们在该任务上对几个强基线进行了基准测试，从大型语言模型提示到检索增强生成。我们发现，尽管在传统的文本相似性指标上接近人类表现，但在准确性方面仍存在显著差距。

    arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
    
[^61]: 基于大型语言模型的学习者表现建模研究

    Towards Modeling Learner Performance with Large Language Models

    [https://arxiv.org/abs/2403.14661](https://arxiv.org/abs/2403.14661)

    本文研究了预训练大型语言模型（LLMs）在知识追踪领域的应用，通过比较零-shot提示和模型微调两种方法，提出了LLMs在智能辅导系统中预测学习者表现的潜力。

    

    最近关于预训练大型语言模型（LLMs）能力的研究表明它们能够充当一般模式机器，通过完成代表各种任务的复杂令牌序列，包括时间序列预测和机器人控制。本文研究了LLMs的模式识别和序列建模能力是否能够扩展到知识追踪领域，这在智能辅导系统（ITSs）的发展中是一个关键组成部分，通过预测学习者随时间的表现来个性化教育体验。在多个真实世界数据集上进行了经验性评估，我们比较了使用LLMs进行此任务的两种方法，零-shot提示和模型微调，以及现有的非LLM方法。虽然基于LLMs的方法没有达到最先进的性能，但经过微调的LLMs超过了天真的基线模型的表现。

    arXiv:2403.14661v1 Announce Type: cross  Abstract: Recent work exploring the capabilities of pre-trained large language models (LLMs) has demonstrated their ability to act as general pattern machines by completing complex token sequences representing a wide array of tasks, including time-series prediction and robot control. This paper investigates whether the pattern recognition and sequence modeling capabilities of LLMs can be extended to the domain of knowledge tracing, a critical component in the development of intelligent tutoring systems (ITSs) that tailor educational experiences by predicting learner performance over time. In an empirical evaluation across multiple real-world datasets, we compare two approaches to using LLMs for this task, zero-shot prompting and model fine-tuning, with existing, non-LLM approaches to knowledge tracing. While LLM-based approaches do not achieve state-of-the-art performance, fine-tuned LLMs surpass the performance of naive baseline models and perf
    
[^62]: 社会智能数据基础设施：构建现状和引领未来

    Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future

    [https://arxiv.org/abs/2403.14659](https://arxiv.org/abs/2403.14659)

    本研究构建了一个名为Social AI Data Infrastructure的社会智能数据基础设施，包括一个全面的社交AI分类系统和一个480个NLP数据集的数据库，通过分析现有数据集工作以及评估语言模型在不同社会智能方面的表现，帮助研究者深入了解当前数据格局并提供未来数据集发展方向的整体观点。

    

    随着自然语言处理（NLP）系统越来越多地整合到人类社会生活中，这些技术将需要越来越多地依赖社会智能。尽管存在许多有价值的数据集可以衡量社会智能的孤立维度，但目前尚不存在任何作品来将这些线索结合在一起，形成一个研究者可以快速识别研究空白和未来方向的凝聚子领域。为实现这一目标，我们构建了一个社交AI数据基础设施，其中包括一个全面的社交AI分类系统和一个包含480个NLP数据集的数据库。我们的基础设施使我们能够分析现有的数据集工作，同时评估语言模型在不同社会智能方面的性能。我们的分析表明，它在帮助深入了解当前数据格局并提供对未来数据集发展潜在方向的整体观点方面具有实用性。

    arXiv:2403.14659v1 Announce Type: cross  Abstract: As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models' performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We s
    
[^63]: 一种联合利用人工智能、机器学习和5G技术在美国进行森林火灾预防和管理的方法

    A Synergistic Approach to Wildfire Prevention and Management Using AI, ML, and 5G Technology in the United States

    [https://arxiv.org/abs/2403.14657](https://arxiv.org/abs/2403.14657)

    该研究致力于探索联合利用人工智能、机器学习和5G技术在美国进行森林火灾预防和管理的方法，包括积极检测和预防、利用5G技术进行远程监测和制图、以及利用无人机和物联网设备进行高级火灾响应机制。

    

    在过去几年中，森林火灾已经成为全球性的环境紧急事件，对自然栖息地造成了严重伤害，并加速了气候变化的进程。森林火灾管理方法包括预防、响应和恢复工作。尽管检测技术有所改进，但火灾发生率的上升要求我们提出创新性解决方案，以便及时识别和有效控制。本研究探讨了在美国利用人工智能（AI）、机器学习（ML）和5G技术进行积极判别和处理森林火灾的方法。具体研究目标包括使用先进技术进行森林火灾的积极检测和预防；利用5G技术进行远程感知和信号传输以进行积极监测和制图；以及利用无人机和物联网设备进行森林火灾的高级响应机制。这项研究基于二手数据收集。

    arXiv:2403.14657v1 Announce Type: cross  Abstract: Over the past few years, wildfires have become a worldwide environmental emergency, resulting in substantial harm to natural habitats and playing a part in the acceleration of climate change. Wildfire management methods involve prevention, response, and recovery efforts. Despite improvements in detection techniques, the rising occurrence of wildfires demands creative solutions for prompt identification and effective control. This research investigates proactive methods for detecting and handling wildfires in the United States, utilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G technology. The specific objective of this research covers proactive detection and prevention of wildfires using advanced technology; Active monitoring and mapping with remote sensing and signaling leveraging on 5G technology; and Advanced response mechanisms to wildfire using drones and IOT devices. This study was based on secondary data colle
    
[^64]: MemeCraft：情境和立场驱动的多模态模因生成

    MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation

    [https://arxiv.org/abs/2403.14652](https://arxiv.org/abs/2403.14652)

    MemeCraft是一款创新的模因生成器，利用大型语言模型和视觉语言模型生成支持特定社会运动的模因，提供端到端的流程，无需人工干预，带有内在安全机制。

    

    arXiv:2403.14652v1 公告类型：跨领域 摘要：在线模因在社交媒体时代作为强大的数字文化产物崭露头角，它们不仅提供了幽默，还为政治话语、社会批判和信息传播提供了平台。它们在塑造在线社区情绪方面的广泛影响力使其成为竞选和推动意识形态的宝贵工具。尽管已经开发了几种模因生成工具，但它们在系统性评估方面仍存在差距，以及在有效传达意识形态方面的能力有限。为解决这一问题，我们引入了MemeCraft，一款创新的模因生成器，利用大型语言模型（LLM）和视觉语言模型（VLM）生成支持特定社会运动的模因。MemeCraft提供了一个端到端的流程，将用户提示转化为引人入胜的多模态模因，无需人工干预。基于对创造有争议内容的潜在滥用的认识，具有内在安全机制

    arXiv:2403.14652v1 Announce Type: cross  Abstract: Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme-generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism
    
[^65]: DOSA: 一个包含不同印度地理子文化社会文物的数据集

    DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures

    [https://arxiv.org/abs/2403.14651](https://arxiv.org/abs/2403.14651)

    通过使用社区为中心的参与式研究方法，本研究引入了第一个包含615个社会文物的数据集DOSA，以帮助生成模型更好地了解并考虑当地社会文化背景。

    

    arXiv:2403.14651v1 公告类型: 跨领域 摘要: 生成模型越来越多地用于各种应用，如文本生成、常识推理和问答。为了在全球范围内发挥效果，这些模型必须了解并考虑当地社会文化背景，因此需要基准来评估模型对于文化熟悉程度。由于LLMs的训练数据基于网络，而网络在信息表示方面存在局限性，因此无法捕捉到不在网络上的社区内的知识。因此，这些模型加剧了网络中的不平等、语义错位和刻板印象。自然语言处理领域越来越多地呼吁采用社区为中心的参与式研究方法。在这项工作中，我们回应了这一呼吁，通过参与式研究方法引入了第一个社区生成的数据集DOSA，其中包括615个社会文物。

    arXiv:2403.14651v1 Announce Type: cross  Abstract: Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce $\textit{DOSA}$, the first community-generated $\textbf{D}$ataset $\textbf{o}$f 615 $\textbf{S}$ocial $\textbf{A}$rtifacts, by engaging wi
    
[^66]: 探究ChatGPT及其对社会的影响

    Exploring ChatGPT and its Impact on Society

    [https://arxiv.org/abs/2403.14643](https://arxiv.org/abs/2403.14643)

    ChatGPT是一种基于Transformer架构的大型语言模型，能够生成人类化的对话回复，可革新各行业并改变技术互动方式。

    

    人工智能已经存在一段时间了，但突然间比以往任何时候都受到了更多的关注。感谢谷歌、微软、元宇宙等科技界主要品牌的创新。然而，OpenAI通过其开创性发明ChatGPT触发了按钮。ChatGPT是一种基于Transformer架构的大型语言模型（LLM），能够在对话背景中生成类似人类的回复。它使用深度学习算法来生成对输入文本的自然语言回复。其庞大的参数数量、上下文生成和面向开放域的训练使其成为一种多功能且有效的工具，可应用于从聊天机器人到客户服务再到语言翻译等广泛领域。它具有彻底改变各行业并转变我们与技术互动方式的潜力。然而，使用ChatGPT也引发了一些担忧，包括道德方面的。

    arXiv:2403.14643v1 Announce Type: cross  Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical,
    
[^67]: 出身富贵？探讨大型语言模型中的社会经济偏见

    Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models

    [https://arxiv.org/abs/2403.14633](https://arxiv.org/abs/2403.14633)

    本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。

    

    社会经济偏见在社会中加剧了不公平现象，根据个人经济和社会背景影响获取机会和资源的机会。这一普遍问题持续地延续了系统性的不平等，阻碍了作为一个社会追求包容性进步。在本文中，我们调查了大型语言模型中是否存在社会经济偏见。为此，我们引入了一个新的数据集（SilverSpoon），包含3000个样本，展示了牵涉到弱势群体由于他们的处境而实施道德模糊行为的假设情景，并问这种行为是否在道德上成立。此外，这个数据集具有双重标记方案，并由属于社会经济两端的人进行了注释。使用SilverSpoon，我们评估了大型语言模型中表现出的社会经济偏见程度以及该程度如何随模型大小变化。

    arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
    
[^68]: 通过一致性对齐提高大型语言模型的鲁棒性

    Improving the Robustness of Large Language Models via Consistency Alignment

    [https://arxiv.org/abs/2403.14221](https://arxiv.org/abs/2403.14221)

    通过一致性对齐训练的两阶段框架，有助于提高大型语言模型的鲁棒性和对指令的理解。

    

    大型语言模型(LLMs)在遵循用户指令和生成有用响应方面取得了巨大成功。然而，它们的鲁棒性仍远未达到最佳状态，因为可能由于口头指令中的细微更改而产生明显不一致的响应。最近的文献探讨了这种不一致性问题，强调了继续改善响应生成的鲁棒性的重要性。然而，对系统性分析和解决方案仍然缺乏。在本文中，我们定量定义了不一致性问题，并提出了一个由指令增强监督微调和一致性对齐训练组成的两阶段训练框架。第一阶段通过类似指令增强帮助模型在遵循指令时泛化。在第二阶段，我们提高了多样性，并帮助模型理解哪些响应与人类期望更一致。

    arXiv:2403.14221v1 Announce Type: new  Abstract: Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations b
    
[^69]: RoleInteract：评估角色扮演代理的社交互动

    RoleInteract: Evaluating the Social Interaction of Role-Playing Agents

    [https://arxiv.org/abs/2403.13679](https://arxiv.org/abs/2403.13679)

    该论文介绍了RoleInteract，一个旨在评估角色扮演对话代理社交性的基准，覆盖了500个角色、6000多个问题提示和30800个对话话语。

    

    大型语言模型（LLMs）推动了各种AI对话代理的发展，包括模仿不同角色和人类行为的角色扮演对话代理。本文引入了RoleInteract，这是第一个旨在系统评估角色扮演对话代理在社交方面表现的基准。该基准从各种来源构建，涵盖了超过500个角色、6000多个问题提示和30800个多轮角色扮演话语。

    arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in
    
[^70]: 拉马遇上欧盟：通过LLMs探究欧洲政治光谱

    Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs

    [https://arxiv.org/abs/2403.13592](https://arxiv.org/abs/2403.13592)

    通过调整LLama Chat模型来重新评估其在欧盟政治中的政治倾向，展示了其对国家政党立场的充分了解，并能在上下文中进行有效推理，为将基于对话的LLM用于政治科学研究提供了新的可能性。

    

    arXiv:2403.13592v1 类型：新文章 摘要：细化指导的大型语言模型具有明显的政治倾向，已经被证明会影响下游任务的执行。我们将这一研究领域扩展到美国两党制以外，在不同环境中审计 Llama Chat，以分析该模型对欧盟政治的了解程度及其在上下文中推理的能力。我们适应，即进一步细化，Llama Chat 基于欧洲议会辩论中个别欧洲政党的演讲进行适应性调整，以根据EUandI问卷重新评估其政治倾向。Llama Chat 显著了解各国政党的立场，并能够在上下文中推理。经调整的、特定政党的模型在相应立场上有明显的重新调整，我们认为这是将基于对话的LLM作为数据驱动的对话引擎用于协助政治科学研究的起点。

    arXiv:2403.13592v1 Announce Type: new  Abstract: Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.
    
[^71]: 利用大型语言模型和真实机器人账户激励社交媒体平台上的新闻消费

    Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts

    [https://arxiv.org/abs/2403.13362](https://arxiv.org/abs/2403.13362)

    通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。

    

    极化、信任下降以及对民主规范支持动摇是美国民主面临的紧迫威胁。接触验证和优质新闻可能降低个人对这些威胁的易感性，并使公民更具抗击错误信息、民粹主义和极端党派言论的能力。该项目探讨了如何在一个生态有效的环境中增强用户接触和参与验证的、意识形态平衡的新闻。我们依赖于对 28,457 个 Twitter 用户进行的大规模为期两周的田野实验（从 2023 年 1 月 19 日到 2 月 3 日）。我们创建了 28 个利用 GPT-2 的机器人，在用户发表有关体育、娱乐或生活方式的推文时回复一个内容相关的回复，其中包含两个硬代码元素：一个指向优质新闻机构相关主题部分的 URL 和鼓励关注其 Twitter 账户。为进一步测试机器人对性别的差异影响，被试用户被随机分配以接受...

    arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
    
[^72]: RankPrompt：逐步比较使语言模型成为更好的推理者

    RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners

    [https://arxiv.org/abs/2403.12373](https://arxiv.org/abs/2403.12373)

    RankPrompt 提出了一种新的提示方法，可以通过自我排序来提高大型语言模型在推理任务中的性能。

    

    大型语言模型（LLMs）在各种推理任务中取得了令人印象深刻的表现。然而，即使像ChatGPT这样的最先进的LLMs在推理过程中也容易出现逻辑错误。现有的解决方案，包括部署特定于任务的验证器或在多个推理路径上投票，要么需要大量人类注释，要么在存在不一致响应的场景中失败。为了解决这些挑战，我们介绍了RankPrompt，这是一种新的提示方法，使LLMs能够自行对其响应进行排序而无需额外资源。RankPrompt将排序问题分解为多个响应之间的一系列比较，利用LLMs自动生成比较链作为上下文示例的固有能力。我们在11个算术推理和常识推理任务上的实验表明，RankPrompt显著提高了ChatGPT和GPT-4的推理性能。

    arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u
    
[^73]: Tur[k]ingBench：用于网络代理的挑战基准测试

    Tur[k]ingBench: A Challenge Benchmark for Web Agents

    [https://arxiv.org/abs/2403.11905](https://arxiv.org/abs/2403.11905)

    Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。

    

    最近的聊天机器人展示了在原始文本形式下理解和交流的令人印象深刻的能力。然而，世界上不仅仅是原始文本。例如，人们在网页上花费大量时间，在这些网页上，文本与其他形式交织在一起，并以各种复杂互动的形式完成任务。最先进的多模型是否能够推广到这种复杂的领域呢？为了回答这个问题，我们介绍了TurkingBench，一个由包含多模态背景的文本说明制定的任务基准。与现有的使用人工合成的网页的工作不同，这里我们使用最初设计用于各种注释目的的自然HTML页面。每个任务的HTML说明也被实例化为各种值（从众包任务获得）以形成任务的新实例。这个基准包含32.2K个实例。

    arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
    
[^74]: 大型语言模型是否理解医学编码?

    Do Large Language Models understand Medical Codes?

    [https://arxiv.org/abs/2403.10822](https://arxiv.org/abs/2403.10822)

    该研究调查了大型语言模型是否理解医学编码的含义，评估了它们对领域特定术语的认识和理解。

    

    近期人工智能研究的首要目标是稳步朝着实现人工通用智能(AGI)迈进，这促使对大型语言模型(LLMs)在各种任务和领域中的评估。其中之一是医疗保健领域，LLMs可以通过协助各种任务大大有益于临床实践。然而，当面对无法充分应对的查询时，这些模型也容易产生“幻觉”或不正确的响应，引发了关注和怀疑，特别是在医疗保健社区内。因此，在这项工作中，我们调查LLMs是否理解医学编码的固有含义，这些编码被广泛应用于医疗保健实践。我们评估了各种现成的LLMs (例如GPT、LLaMA等)和专门为生物医学应用设计的LLMs，以评估它们对这些领域特定术语的认识和理解。我们的结果表明…

    arXiv:2403.10822v1 Announce Type: new  Abstract: The overarching goal of recent AI research has been to make steady progress towards achieving Artificial General Intelligence (AGI), prompting the evaluation of Large Language Models (LLMs) across a variety of tasks and domains. One such domain is healthcare, where LLMs can greatly benefit clinical practice by assisting with a wide range of tasks. However, these models are also prone to producing "hallucinations" or incorrect responses when faced with queries they cannot adequately address, raising concerns and skepticism, especially within the healthcare community. Therefore, in this work, we investigate whether LLMs understand the inherent meaning of medical codes, which are widely used in healthcare practice. We evaluate various off-the-shelf LLMs (e.g., GPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to assess their awareness and understanding of these domain-specific terminologies. Our results indicate t
    
[^75]: 大型语言模型指导的心力衰竭风险预测的ECG双注意力网络

    Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction

    [https://arxiv.org/abs/2403.10581](https://arxiv.org/abs/2403.10581)

    提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。

    

    心力衰竭（HF）由于全球死亡率不断上升而构成重大公共卫生挑战。通过早期诊断和预防来解决这一问题可显著减少疾病对社会的影响。本文引入了一种使用临床获取的12导联心电图（ECG）进行HF风险预测的方法。我们提出了一种新颖的、轻量级的双注意力ECG网络，旨在捕捉对早期HF预测至关重要的复杂心电图特征，尽管低风险和高风险组之间存在明显的不平衡。该网络具有一个跨导注意力模块和12个导联特定的时间注意力模块，以捕捉交叉导联交互作用和每个导联内的局部时间动态。为了防止模型过拟合于有限的训练数据，我们利用一个大型语言模型（LLM）与公共ECG-Report数据集进行预训练，用于进行ECG-报告对齐任务。然后对网络进行fine-tune以用于HF风险预测

    arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
    
[^76]: 大型语言模型中用于快速推测解码的循环草稿机制

    Recurrent Drafter for Fast Speculative Decoding in Large Language Models

    [https://arxiv.org/abs/2403.09919](https://arxiv.org/abs/2403.09919)

    本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。

    

    在本文中，我们介绍一种改进的推测解码方法，旨在提高大型语言模型的效率。我们的方法利用了两种成熟技术的优势：经典的双模型推测解码方法和较新的单模型方法Medusa。从Medusa得到灵感，我们的方法采用了单模型策略进行推测解码。然而，我们的方法通过使用具有循环依赖设计的单个轻量级草稿头来区分自己，本质上类似于经典推测解码中使用的小型草稿模型，但避免了完整transformer架构的复杂性。由于循环依赖，我们可以使用波束搜索快速过滤出草稿头中不需要的候选项。其结果是一种结合了单模型设计简易性并避免了创建数据相关树依赖的方法。

    arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
    
[^77]: 评估大语言模型作为对话推荐中生成用户模拟器

    Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

    [https://arxiv.org/abs/2403.09738](https://arxiv.org/abs/2403.09738)

    大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。

    

    合成用户是对话推荐系统评估中成本效益较高的真实用户代理。大型语言模型表现出在模拟类似人类行为方面的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一个新的协议，用于衡量语言模型能够准确模拟对话推荐中人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的关键特性：选择要谈论的物品，表达二进制偏好，表达开放式偏好，请求推荐以及提供反馈。通过对基准模拟器的评估，我们展示了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了关于如何通过模型选择和提示策略减少这些偏差的见解。

    arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
    
[^78]: MM1：多模式LLM预训练的方法、分析与见解

    MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training

    [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)

    通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。

    

    在这项工作中，我们讨论了构建高性能的多模式大型语言模型（MLLMs）。具体来说，我们研究了各种架构组件和数据选择的重要性。通过对图像编码器、视觉语言连接器和各种预训练数据选择进行仔细和全面的消融实验，我们确定了几个关键的设计经验。例如，我们展示了对大规模多模式预训练使用仔细混合的图像标题、交替图像文本和仅文本数据对于在多个基准测试中实现最新潮（SOTA）的少样本结果至关重要，与其他已发表的预训练结果相比。此外，我们表明图像编码器连同图像分辨率和图像标记计数具有重要影响，而视觉语言连接器设计相对重要性较小。通过扩大所提出的方法，我们构建了MM1，一个多模式模型系列。

    arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
    
[^79]: VisionGPT-3D:一种用于增强3D视觉理解的通用多模态代理

    VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding

    [https://arxiv.org/abs/2403.09530](https://arxiv.org/abs/2403.09530)

    提出了一个统一的VisionGPT-3D框架，整合了最先进的视觉模型，有助于提升计算机视觉对于3D视觉理解的能力

    

    文本向视觉组件的演进促进了人们日常生活的便利，例如从文本生成图像、视频并识别图像中所需的元素。以前的计算机视觉模型专注于基于明确定义对象的图像检测、分类。大型语言模型(LLMs)将自然语言转换为视觉对象，为文本背景提供了视觉布局。OpenAI GPT-4已成为LLMs的顶峰，而计算机视觉(CV)领域拥有大量最先进的模型和算法，可将2D图像转换为它们的3D表示。然而，算法与问题之间的不匹配可能导致不良结果。针对这一挑战，我们提出了一个统一的VisionGPT-3D框架， conslidate了最先进的视觉模型，从而促进了发展。

    arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
    
[^80]: 持续预训练大型语言模型的简单可扩展策略

    Simple and Scalable Strategies to Continually Pre-train Large Language Models

    [https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)

    通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。

    

    大型语言模型（LLMs）通常在数十亿的标记上进行常规预训练，一旦有新数据可用就重新开始该过程。一个更有效率的解决方案是持续预训练这些模型，与重新训练相比能节省大量计算资源。然而，新数据引起的分布转移通常会导致在以前数据上降低性能或无法适应新数据。在本工作中，我们展示了一种简单且可扩展的学习率（LR）重新升温、LR重新衰减和重放上一数据的组合足以与完全从头开始重新训练在所有可用数据上的性能相匹配，从最终损失和语言模型（LM）评估基准的角度衡量。具体而言，我们展示了在两个常用的LLM预训练数据集（英语→英语）之间的弱但现实的分布转移以及更强烈的分布转移（英语→德语）下的情况。

    arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
    
[^81]: 利用大型语言模型在心理治疗中进行认知重构

    HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy

    [https://arxiv.org/abs/2403.05574](https://arxiv.org/abs/2403.05574)

    这一创新心理治疗模型HealMe通过基于心理治疗框架的共情对话，有效解决了根深蒂固的负面思维，并促进了理性、平衡的观点。

    

    大型语言模型（LLMs）在心理治疗中可以发挥重要作用，熟练处理认知重构等关键任务，克服羞耻、不信任、治疗师技能差异和资源稀缺等挑战。在先前的认知重构中，主要将负面情绪转化为积极的，但这些方法效果有限，经常不能促进客户自我发现替代视角。在本文中，我们揭示了帮助和赋能通过自适应语言在心理增强（HealMe）模型。这种新颖的认知重构疗法方法有效地解决了根深蒂固的负面想法，并促进理性、平衡的视角。HealMe与传统LLM方法不同，采用基于心理治疗框架的共情对话。它通过系统指导客户区分情境和感受，集思广益寻找替代视角，并制定...

    arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
    
[^82]: MaCmS：马加希代码混合数据集用于情感分析

    MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis

    [https://arxiv.org/abs/2403.04639](https://arxiv.org/abs/2403.04639)

    这项研究介绍了首个用于马加希语-印地语-英语代码混合情感分析任务的数据集，并通过语言学分析和统计研究来评估数据集的质量。

    

    这篇论文介绍了一种新的情感数据集 MaCMS，用于马加希语-印地语-英语（MHE）代码混合语言，其中马加希语是一种资源较少的少数民族语言。这个数据集是用于情感分析任务的第一个马加希语-印地语-英语代码混合数据集。此外，我们还对数据集进行了语言学分析，以了解代码混合的结构，并进行了统计研究，以了解不同极性发言者的语言偏好。通过这些分析，我们还训练了基准模型来评估数据集的质量。

    arXiv:2403.04639v1 Announce Type: new  Abstract: The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset's quality.
    
[^83]: 使用异构图对比迁移学习实现零样本跨语言文档级事件因果识别

    Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning

    [https://arxiv.org/abs/2403.02893](https://arxiv.org/abs/2403.02893)

    提出了一种使用异构图对比迁移学习的方法，实现了零样本跨语言文档级事件因果识别，并在实验证明在F1得分上优于之前的最先进模型。

    

    事件因果识别（ECI）指的是在文本中检测事件之间的因果关系。然而，大多数现有研究都集中在高资源语言下的句子级ECI，而对于低资源语言下更具挑战性的文档级ECI（DECI）却尚未得到充分探索。在本文中，我们提出了一种带有多粒度对比传递学习（GIMC）的异构图交互模型，用于实现零样本跨语言文档级ECI。具体来说，我们引入了一个异构图交互网络来建模文档中分散事件之间的远距离依赖关系。然后，为了提高从源语言学习到的因果知识的跨语言可转移性，我们提出了一个多粒度对比传递学习模块，以调整跨语言间的因果表示。大量实验证明，我们的框架在平均F1得分上优于之前的最先进模型约9.4%和8.2%。

    arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
    
[^84]: Align-to-Distill: 可训练的注意力对齐在神经机器翻译中的知识蒸馏

    Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation

    [https://arxiv.org/abs/2403.01479](https://arxiv.org/abs/2403.01479)

    "本文提出了“Align-to-Distill”（A2D）策略，通过在训练过程中自适应地对齐学生注意力头与其教师对应物，转化了组合映射启发式方法为学习问题，实验结果显示A2D的有效性，对WMT-2022 De->Dsb和WMT-2014 En->De的BLEU分数分别获得高达+3.61和+0.63的提升。"

    

    可扩展的深度模型和大规模数据集的出现提高了神经机器翻译的性能。知识蒸馏（KD）通过将知识从教师模型传输到更紧凑的学生模型来提高效率。然而，针对Transformer架构的KD方法通常依赖于启发式方法，特别是在决定要从哪些教师层中蒸馏知识时。本文介绍了“Align-to-Distill”（A2D）策略，旨在通过在训练过程中自适应地对齐学生注意力头与其教师对应物来解决特征映射问题。A2D中的注意力对齐模块执行学生和教师注意力头之间的密集逐头比较，将组合映射启发式方法转化为学习问题。我们的实验展示了A2D的有效性，对WMT-2022 De->Dsb和WMT-2014 En->De的BLEU分数分别获得高达+3.61和+0.63的提升。

    arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb and WMT-2014 En->De, respe
    
[^85]: 跨语言学习与低资源微调：以土耳其事实检查为例的案例研究

    Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish

    [https://arxiv.org/abs/2403.00411](https://arxiv.org/abs/2403.00411)

    提出了FCTR数据集，旨在解决英语以外语言，尤其是土耳其语，的数据稀缺问题，并探讨了跨语言转移学习在低资源语言中的有效性，实验证明数据集潜力推动土耳其语研究。

    

    通过社交媒体平台迅速传播错误信息引起了人们对其对公众舆论的影响的担忧。尽管错误信息在其他语言中普遍存在，但该领域的大部分研究集中在英语上。因此，其他语言，包括土耳其语，的数据集稀缺。为了解决这一问题，我们介绍了由3238个真实声明组成的FCTR数据集。该数据集涵盖多个领域，并整合了三家土耳其事实检查组织收集的证据。此外，我们旨在评估跨语言转移学习对于低资源语言的有效性，特别关注土耳其语。我们展示了大型语言模型在这一背景下的上下文学习（零次和少次）表现。实验结果表明，该数据集有推动土耳其语研究的潜力。

    arXiv:2403.00411v1 Announce Type: new  Abstract: The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.
    
[^86]: ChunkAttention: 具有前缀感知KV缓存和两阶段分区的高效自注意力

    ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition

    [https://arxiv.org/abs/2402.15220](https://arxiv.org/abs/2402.15220)

    ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。

    

    自注意力是大型语言模型（LLMs）的重要组成部分，但对于长序列来说是推理延迟的一个显著来源。在多租户LLMs服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。本文介绍了ChunkAttention，一种具有前缀感知的自注意力模块，可以在运行时检测多个请求之间匹配的提示前缀，并共享它们的键/值张量以改进KV缓存的内存利用率。这是通过将整体键/值张量分解为较小的块，并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力内核，其中实现了两阶段分区算法，以改善自注意力计算中的数据局部性。

    arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
    
[^87]: 一种LLM增强的词汇简化对抗编辑系统

    An LLM-Enhanced Adversarial Editing System for Lexical Simplification

    [https://arxiv.org/abs/2402.14704](https://arxiv.org/abs/2402.14704)

    该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。

    

    词汇简化（LS）旨在在词汇级别简化文本。现有方法严重依赖标注数据，这使得在资源匮乏的情况下难以应用。在本文中，我们提出了一种新颖的LS方法，不需要平行语料库。该方法采用对抗编辑系统，并结合混淆损失和不变性损失来预测原始句子中的词汇修改。同时，我们引入了一种创新的LLM增强损失，以将大型语言模型（LLMs）的知识提炼成小型LS系统。通过这种方式，句子中的复杂词被屏蔽，制作了一个基于难度感知的填充模块，用更简单的词替换屏蔽位置。最后，对三个基准LS数据集进行了广泛的实验结果和分析，证明了我们提出方法的有效性。

    arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.
    
[^88]: KoCoSa: 韩文上下文感知讽刺检测数据集

    KoCoSa: Korean Context-aware Sarcasm Detection Dataset

    [https://arxiv.org/abs/2402.14428](https://arxiv.org/abs/2402.14428)

    该论文介绍了一个新的针对韩文对话讽刺检测任务的数据集KoCoSa，提出了一种高效的讽刺检测数据集生成流程，并提供了针对该任务的简单但有效的基线模型。

    

    讽刺是一种言语讽刺的方式，指的是有人说了和他们的本意相反的话，通常是为了嘲笑一个人、情况或想法。检测对话中的讽刺通常是困难的，因为检测讽刺应该反映上下文（即对话历史）。本文介绍了一个针对韩文对话讽刺检测任务的新数据集KoCoSa（韩文上下文感知讽刺检测数据集），包括12.8K个日常韩文对话以及该任务在最后一次回复上的标签。为了构建该数据集，我们提出了一种高效的讽刺检测数据集生成流程：1）使用大型语言模型从源对话中生成新的讽刺对话，2）自动和手动过滤异常和有毒对话，3）为讽刺检测任务进行人工注释。我们还提供了一个简单但有效的针对韩文讽刺检测任务的基线，该基线是在我们的数据集上训练的。

    arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
    
[^89]: 名词短语中头部的最佳位置。指示语、数词、形容词和名词的案例。

    The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun

    [https://arxiv.org/abs/2402.10311](https://arxiv.org/abs/2402.10311)

    本研究旨在探讨句法依赖距离最小化与意外减少最小化原则在名词短语中的冲突，结论显示当涉及的单词较少且单词较短时，意外减少可能会超越句法依赖距离优化。

    

    一句话的词序由多种原则塑造。句法依赖距离最小化原则与意外减少最小化原则（或可预测性最大化）在单一头部的句法依赖结构中存在冲突：前者预测头部应该放置在线性排列的中心，后者预测头部应该放置在两端之一（要么在首位，要么在末位）。一个关键问题是何时意外减少（或可预测性最大化）应该超越句法依赖距离最小化。在单一头部结构的背景下，预测在满足两个条件时更有可能发生，即（a）涉及的单词较少，并且（b）单词较短。在这里，我们在由指示语、数词、形容词和名词组成的名词短语上测试了这一预测。我们发现，在首选顺序中...（缺失部分无法提供完整翻译）

    arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
    
[^90]: 使用自然语言推理构建高效的通用分类器

    Building Efficient Universal Classifiers with Natural Language Inference

    [https://arxiv.org/abs/2312.17543](https://arxiv.org/abs/2312.17543)

    本文探讨了如何利用自然语言推理作为通用分类任务，提供了构建通用分类器的详细步骤，并分享了该通用分类器在33个数据集上的训练结果

    

    arXiv:2312.17543v2 公告类型：替换交叉。生成型大型语言模型(LLMs)已经成为零样本学习和零样本学习的主流选择，这要归功于文本生成的通用性。然而，许多用户在只想自动化一个分类任务时，并不需要生成型LLMs的广泛能力。较小的类似BERT的模型也可以学习通用任务，这使它们可以在不需要微调（零样本分类）的情况下执行任何文本分类任务，或者只用少量样本学习新任务（少样本），同时比生成型LLMs高效得多。本文(1) 解释了如何将自然语言推理（NLI）作为通用分类任务，其原理类似于生成型LLMs的指导微调，(2) 提供了用于构建通用分类器的可重用Jupyter笔记本的逐步指南，(3) 共享了经过训练的通用分类器，在33个数据集上训练

    arXiv:2312.17543v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets
    
[^91]: MacGyver：大型语言模型是否是创意问题解决者？

    MacGyver: Are Large Language Models Creative Problem Solvers?

    [https://arxiv.org/abs/2311.09682](https://arxiv.org/abs/2311.09682)

    通过创建MACGYVER数据集并与人类比较，研究发现大型语言模型在创意问题解决方面独具挑战性，在知识广度和可行性方面与人类存在独特差异，同时还展示了通过新的提示技术提升大型语言模型的问题解决能力潜力。

    

    我们在一个全新的约束设置中探究了现代大型语言模型的创意问题解决能力。为此，我们创建了MACGYVER，这是一个自动生成的数据集，包含超过1600个特意设计的现实世界问题，旨在引发物体的创新使用，并需要超越常规思维。我们随后向大型语言模型和人类展示我们的数据集，以比较和对比它们的问题解决能力。MACGYVER对这两个群体都具有挑战性，但以独特和互补的方式呈现。例如，人类擅长熟悉的任务，但在特定领域知识上有困难，导致更高的差异。相比之下，大型语言模型暴露于各种专业知识，尝试更广泛的问题，但在提出物理上不可行的行动时失败。最后，我们对大型语言模型进行了详细的错误分析，并展示了通过新的提示技术提高它们的问题解决能力的潜力。

    arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
    
[^92]: PhoGPT: 越南语的生成式预训练模型

    PhoGPT: Generative Pre-training for Vietnamese

    [https://arxiv.org/abs/2311.02945](https://arxiv.org/abs/2311.02945)

    PhoGPT是一个用于越南语的生成式预训练模型系列，具有40亿参数的基础模型PhoGPT-4B以及其聊天变体PhoGPT-4B-Chat，展示了在越南语任务上优于之前的7亿参数模型的强大性能。

    

    我们开源了一个拥有40亿参数的最先进的越南语生成模型系列，其中包括基础的预训练单语模型PhoGPT-4B和其聊天变体PhoGPT-4B-Chat。基础模型PhoGPT-4B有37亿参数，从零开始在包含1020亿标记的越南语语料库上进行预训练，使用长度为8192的上下文，使用20480个标记类型的词汇表。聊天变体PhoGPT-4B-Chat是在70000个指导提示和回应以及额外的290000个对话数据集上对PhoGPT-4B进行微调得到的模型输出。我们展示了相比之前闭源和开源的70亿参数模型，它的强大性能。我们的PhoGPT模型可在以下链接下载：https://github.com/VinAIResearch/PhoGPT

    We open-source a state-of-the-art 4B-parameter generative model series for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens, with an 8192 context length, employing a vocabulary of 20480 token types. The chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along with an additional 290K conversations. We demonstrate its strong performance compared to previous closed-source and open-source 7B-parameter models. Our PhoGPT models are available at: https://github.com/VinAIResearch/PhoGPT
    
[^93]: E-Sparse: 通过基于信息熵的 N:M 稀疏性提升大型语言模型推理能力

    E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity

    [https://arxiv.org/abs/2310.15929](https://arxiv.org/abs/2310.15929)

    首次将信息熵引入剪枝度量设计，提高在大型语言模型中 N:M 稀疏性的准确性。

    

    传统的剪枝方法在大型语言模型（LLMs）中很难实现，因为它们训练过程昂贵，计算需求大。本文首次将隐藏状态特征的信息熵引入到剪枝度量设计中，即 E-Sparse，以提高LLM中 N:M 稀疏性的准确性。E-Sparse利用信息丰富性来提升通道的重要性，并进一步结合几种新颖技术来实现：(1)引入信息熵来增强参数权重和输入特征范数的重要性作为一种新颖的剪枝度量，并在不修改剩余权重的情况下执行N:M稀疏性。(2)设计全局朴素洗牌和局部块洗牌，快速优化信息分布，充分应对 N:M 稀疏性对LLMs准确性的影响。E-Sparse 被实现为一种 Spars

    arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars
    
[^94]: FunQA：迈向令人惊讶的视频理解

    FunQA: Towards Surprising Video Comprehension

    [https://arxiv.org/abs/2306.14899](https://arxiv.org/abs/2306.14899)

    FunQA是一个旨在评估和提高基于反直觉和有趣视频的视频推理深度的数据集，涵盖了HumorQA、CreativeQA和MagicQA三种以前未被探索的惊喜视频类型。

    

    令人惊讶的视频，比如有趣的片段、创意演出或视觉幻象，吸引了相当多的关注。对这些视频的欣赏不仅仅是对视觉刺激的反应；相反，它取决于人类理解（以及欣赏）这些视频中所描绘的常识违反的能力。我们引入了FunQA，这是一个具有挑战性的视频问答（QA）数据集，专门设计用来评估和提高基于反直觉和有趣视频的视频推理深度。与大多数侧重于不太惊讶的背景（例如烹饪或说明视频）的视频QA基准不同，FunQA涵盖了三种以前未被探索的类型的惊喜视频：1）HumorQA，2）CreativeQA和3）MagicQA。对于每个子集，我们建立了严格的QA任务，旨在评估模型在反直觉时间戳定位、详细视频描述以及围绕反直觉进行推理的能力。

    arXiv:2306.14899v2 Announce Type: replace-cross  Abstract: Surprising videos, such as funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question-answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose hi
    
[^95]: BadLlama：以低成本移除Llama 2-Chat 13B的安全微调

    BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])

    [http://arxiv.org/abs/2311.00117](http://arxiv.org/abs/2311.00117)

    研究发现，公开发布模型权重使得安全微调无效，BadLlama项目以低成本成功移除了Llama 2-Chat 13B的安全微调并保留了其一般能力。

    

    Llama 2-Chat是Meta开发并向公众发布的一系列大型语言模型。尽管Meta对Llama 2-Chat进行了安全微调以拒绝输出有害内容，但我们假设公共获取模型权重使得坏意行为者可以以低成本绕过Llama 2-Chat的安全机制，并将Llama 2的能力用于恶意目的。我们展示了以少于200美元的成本有效地取消Llama 2-Chat 13B的安全微调，同时保留其一般能力。我们的结果表明，当发布模型权重时，安全微调是无效的防止滥用的方法。鉴于未来的模型很可能具有更大规模的危害能力，AI开发者在考虑公开发布模型权重时必须解决微调带来的威胁。

    Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
    
[^96]: 自我防御：增强LLM的自我保护能力

    Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])

    [http://arxiv.org/abs/2310.15851](http://arxiv.org/abs/2310.15851)

    这篇论文提出了一种称为自我防御的新方法，通过结合安全训练和保护措施的优势，提升大型语言模型（LLM）的安全性，从而减少有害内容的生成。

    

    盗破攻击可以绕过大型语言模型（LLM）的安全措施，生成有害内容。这种滥用LLM的行为导致了负面的社会后果。目前，解决盗破攻击的主要方法有两种：安全训练和保护措施。安全训练侧重于进一步训练LLM以增强其安全性。而保护措施则是通过实施外部模型或过滤器来防止有害输出。然而，安全训练在适应新的攻击类型方面具有局限性，并且往往会导致模型性能下降。保护措施在帮助方面也被证明有限。为了解决这些问题，我们提出了一种称为自我防御的新方法，结合了安全方法的优势。自我防御包括两个阶段。在第一阶段，我们增强了模型评估有害内容的能力，在第二阶段，我们指导模型在自己的回应上始终执行有害内容检测。实验结果表明，我们的方法能够显著减少有害内容的生成，提高模型的安全性。

    The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experimen
    
[^97]: 随机语言模型的鲁棒性

    Robustness of the Random Language Model. (arXiv:2309.14913v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2309.14913](http://arxiv.org/abs/2309.14913)

    随机语言模型的研究展示了第一语言学习过程中的语法句法连续转变，并证明该转变对于明确对称性的打破是鲁棒的。

    

    随机语言模型(De Giuli 2019)是一组随机上下文无关文法，量化人类和计算机语言的句法。该模型提出了一个简单的第一语言学习图景，即作为潜在语言空间中一个退火类型，推断了向语法句法的单一连续转变，其中潜在的词汇和分类之间的对称性会自发打破。在本文中，通过考虑其对明确对称性打破的鲁棒性，对这一图景进行了严格审视，这是在现实世界的学习中不可避免的组成部分。结果表明，该场景对于这种对称性的打破是鲁棒的。与语法网络聚类系数的人类数据进行比较表明，观察到的转变相当于儿童24个月时通常经历的转变。

    The Random Language Model (De Giuli 2019) is an ensemble of stochastic context-free grammars, quantifying the syntax of human and computer languages. The model suggests a simple picture of first language learning as a type of annealing in the vast space of potential languages. In its simplest formulation, it implies a single continuous transition to grammatical syntax, at which the symmetry among potential words and categories is spontaneously broken. Here this picture is scrutinized by considering its robustness against explicit symmetry breaking, an inevitable component of learning in the real world. It is shown that the scenario is robust to such symmetry breaking. Comparison with human data on the clustering coefficient of syntax networks suggests that the observed transition is equivalent to that normally experienced by children at age 24 months.
    
[^98]: LLMR：使用大型语言模型实时提示交互式世界的框架

    LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])

    [http://arxiv.org/abs/2309.12276](http://arxiv.org/abs/2309.12276)

    LLMR是一个用于实时创建和修改交互式混合现实体验的框架，通过利用大型语言模型和新颖的策略，它能够解决训练数据稀缺和设计目标复杂的问题，并在性能上超过标准的GPT-4。我们展示了LLMR的跨平台互操作性，并通过评估和用户研究证明了其对于生成和编辑各种对象、工具和场景的能力。

    

    我们提出了用于混合现实场景的大型语言模型(LLMR)，这是一个框架，用于实时创建和修改交互式混合现实体验。LLMR利用了新颖的策略来解决训练数据稀缺或设计目标需要合成内部动态、直观分析或高级交互的困难情况。我们的框架依赖于文本交互和Unity游戏引擎。通过融合场景理解、任务规划、自我调试和内存管理技术，LLMR在平均错误率上比标准的GPT-4提高了4倍。我们展示了LLMR与几个示例世界的跨平台互操作性，并通过多个创建和修改任务对其进行了评估，以展示它能够生成和编辑各种对象、工具和场景。最后，我们进行了一个有多样性的可用性研究（N=11），揭示了参与者对该系统有积极的体验，并愿意再次使用它。

    We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
    
[^99]: 重塑顺序推荐系统：利用内容增强语言建模学习动态用户兴趣

    Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])

    [http://arxiv.org/abs/2309.10435](http://arxiv.org/abs/2309.10435)

    本研究提出了一个新的顺序推荐范式 LANCER，利用预训练语言模型的语义理解能力生成更加人性化的个性化推荐。在多个基准数据集上的实验结果表明，该方法有效且有希望，并为了解顺序推荐的影响提供了有价值的见解。

    

    推荐系统对在线应用至关重要，而顺序推荐由于其表达能力强大，能够捕捉到动态用户兴趣而广泛使用。然而，先前的顺序建模方法在捕捉上下文信息方面仍存在局限性。主要的原因是语言模型常常缺乏对领域特定知识和物品相关文本内容的理解。为了解决这个问题，我们采用了一种新的顺序推荐范式，并提出了LANCER，它利用预训练语言模型的语义理解能力生成个性化推荐。我们的方法弥合了语言模型与推荐系统之间的差距，产生了更加人性化的推荐。通过对多个基准数据集上的实验，我们验证了我们的方法的有效性，展示了有希望的结果，并提供了对我们模型对顺序推荐的影响的有价值的见解。

    Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
    
[^100]: 大型多语言模型在跨语种零样本多模式学习中的作用。

    Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])

    [http://arxiv.org/abs/2308.12038](http://arxiv.org/abs/2308.12038)

    本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。

    

    最近，在图像到文本和文本到图像的生成方面，多模式学习出现了显著增长。然而，成功通常仅限于英语，其他语言则相对落后。在其他语言中构建具有竞争力的对应物是非常具有挑战性的，因为非英语多模式数据具有低资源特性（即缺乏大规模、高质量的图像-文本数据）。在这项工作中，我们提出了MPM，一种在低资源语言中训练大型多模式模型的有效训练范例。MPM表明，多语言模型可以在跨语种零样本多模式学习中起到关键作用。具体而言，基于强大的多语言大语言模型，仅在英语图像-文本数据上预训练的多模式模型可以以零样本的方式很好地泛化到其他语言，用于图像到文本和文本到图像的生成，甚至超过在本地语言的图像-文本数据上训练的模型。以中文作为MPM实践的一个练习。

    Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP
    
[^101]: AutoTAMP: 使用LLMs作为翻译器和检查器的自回归任务和动作规划

    AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers. (arXiv:2306.06531v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.06531](http://arxiv.org/abs/2306.06531)

    AutoTAMP提出了一种使用LLMs作为翻译器和检查器的自回归任务和动作规划方法，通过少样本翻译将自然语言任务描述转换为中间任务表示，以实现对复杂任务的规划和执行。

    

    为了实现有效的人机交互，机器人需要理解、规划和执行由自然语言描述的复杂、长期任务。最近大型语言模型（LLMs）的进展已经显示出了将自然语言翻译为机器人行动序列的潜力，用于复杂任务。然而，现有的方法要么直接将自然语言翻译为机器人轨迹，要么通过将语言分解为任务子目标并依靠动作规划器执行每个子目标来分解推理过程。当涉及复杂的环境和时间约束时，必须使用传统的任务和动作规划（TAMP）算法来联合进行规划任务的推理和动作规划，使得分解为子目标成为不可行的。我们使用LLMs来直接规划任务子目标，而是从自然语言任务描述中进行少样本翻译，生成一个中间任务表示，然后可以由TAMP算法消化该表示来进行规划。

    For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to join
    
[^102]: 通过数据混合消除预训练模型中的虚假相关性

    Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])

    [http://arxiv.org/abs/2305.14521](http://arxiv.org/abs/2305.14521)

    本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。

    

    在大数据集上预训练的机器学习模型取得了显著的收敛性和鲁棒性。然而，这些模型往往利用了某些属性和标签之间的虚假相关性，在特定类别的大多数示例中普遍存在，但并不足以预测这些类别。学到的虚假相关性可能会在对新数据进行微调后仍然存在，这会降低模型对不展现虚假相关性的示例的性能。本文提出了一种简单而高效的方法，以消除预训练模型中的虚假相关性。我们方法的关键思想是利用一小组带有虚假属性的示例，并通过数据混合来平衡所有类别中的虚假属性。我们在理论上证实了我们的方法的有效性，并在各种视觉和NLP任务上进行了实证，包括消除虚假相关性。

    Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
    
[^103]: CooK: 用模块化和协作知识赋能通用语言模型

    CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])

    [http://arxiv.org/abs/2305.09955](http://arxiv.org/abs/2305.09955)

    CooK是一种用于赋能通用语言模型的新颖框架，通过专门的语言模型和协作的知识贡献者，提供模块化、不断增长和多源的知识。在知识密集型任务中，CooK展现出了明显的性能提升。

    

    大型语言模型（LLM）越来越多地用于知识密集型任务和语境中。现有方法通过检索或生成知识提示来改善通用语言模型的知识能力，但它们未能反映知识丰富模型的两个关键属性：知识应该是模块化，不断增长，来自不同领域；知识获取和生成应该是协作的过程，其中各种利益相关者 contribue 新信息。为此，我们提出了 CooK，一种新颖的框架，可为通用大型语言模型提供模块化和协作来源的知识。我们首先介绍了专门的语言模型，即在广泛领域和来源上训练的自回归模型。这些专门的语言模型可以作为参数化的知识库，后来被提示生成通用的 LLM 的背景知识。然后，我们提出了三个知识过滤器，以动态选择适合给定上下文的知识源。最后，我们呈现了一个知识贡献者组件，使利益相关者能够轻松地为系统贡献特定于域的知识。我们展示了 CooK 在一组知识密集型任务上的有效性，显示出明显的超越现有技术的性能。

    Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select an
    
[^104]: NL2TL：使用大型语言模型将自然语言转化为时态逻辑

    NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models. (arXiv:2305.07766v1 [cs.CL])

    [http://arxiv.org/abs/2305.07766](http://arxiv.org/abs/2305.07766)

    本研究提出了一种NL到TL的转换框架，使用大型语言模型在数据集和模型训练中，可以准确并且具有普适性地转换复杂的高级系统规范。

    

    时态逻辑（TL）可用于在许多工程应用中严格指定复杂的高级系统规范。由于缺乏跨不同应用领域的数据集和可推广的模型，自然语言（NL）和TL之间的转换一直未被充分探索。在本文中，我们提出了一个准确且具有普适性的英文指令从NL到TL的转换框架，并探索了在多个阶段使用大型语言模型（LLMs）的方法。我们的贡献有两个方面。首先，我们开发了一个框架来创建NL-TL对数据集，结合了LLMs和人工注释。我们发布了一个具有28K个NL-TL对的数据集。然后，我们在NL和TL的提升版本上微调了T5模型（即，特定原子命题（AP）被隐藏）。增强的普适性源自两个方面：1）使用提升的NL-TL表征常见的逻辑结构，没有特定领域的约束。2）在数据集创建中应用LLMs。

    Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation 
    
[^105]: mPLUG-Owl: 模块化增强了大型语言模型的多模态能力

    mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])

    [http://arxiv.org/abs/2304.14178](http://arxiv.org/abs/2304.14178)

    本文介绍了一种名为mPLUG-Owl的训练范式，它通过模块化学习基础LLM、视觉知识模块和视觉抽象器模块，赋予LLMs多模态的能力。实验结果表明，mPLUG-Owl在图像字幕和视觉问答任务中表现优于基线模型，并在某些情况下达到了最先进的性能水平。

    

    大型语言模型(LLMs)已经在各种开放式任务中展示出了令人印象深刻的零-shot表现，而最近的研究还探讨了将LLMs用于多模态生成的应用。在本研究中，我们引入了一种新的训练范式mPLUG-Owl，通过基础LLM、视觉知识模块和视觉抽象器模块的模块化学习，使LLMs具备了多模态的能力。该方法可以支持多种模态，并通过模态协作促进了多单模态和多模态的能力。mPLUG-Owl的训练范式包括用于对齐图像和文本的两阶段方法，该方法利用LLM的辅助学习视觉知识，同时保持甚至改进了LLM的生成能力。在第一阶段中，使用冻结的LLM模块对视觉知识模块和抽象器模块进行训练以对齐图像和文本。在第二阶段中，使用仅语言和多模态监督数据集共同对模型进行微调。对于图像字幕和视觉问答任务的实验结果表明，mPLUG-Owl优于基线模型，在某些情况下达到了最先进的性能水平。

    Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
    

