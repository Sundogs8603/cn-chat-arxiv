# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TravelPlanner: A Benchmark for Real-World Planning with Language Agents](https://rss.arxiv.org/abs/2402.01622) | 本论文提出了一种用于自然语言代理的新的规划基准TravelPlanner，它关注于旅行规划这一常见的真实世界场景。经过全面评估，发现目前的语言代理仍无法处理如此复杂的规划任务，即使最先进的GPT-4也只能达到0.6%的成功率。 |
| [^2] | [StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback](https://rss.arxiv.org/abs/2402.01391) | StepCoder是一个基于强化学习的代码生成框架，通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，并使用细粒度优化来提高模型的效果。 |
| [^3] | [LoTR: Low Tensor Rank Weight Adaptation](https://rss.arxiv.org/abs/2402.01376) | LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。 |
| [^4] | [CABINET: Content Relevance based Noise Reduction for Table Question Answering](https://rss.arxiv.org/abs/2402.01155) | CABINET是一个用于表格问答系统的基于内容相关性的噪声降低方法，通过加权处理表格内容并生成解析语句，使得大型语言模型能够专注于相关表格数据而抑制无关信息的干扰。 |
| [^5] | [AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents](https://rss.arxiv.org/abs/2402.01152) | "AccentFold"是一种通过利用学习到的口音嵌入之间的空间关系来改进ASR的方法，通过探索性分析语音嵌入，揭示非洲口音之间的有趣关系，并发现了Ethnologue先前未经描述的口音关系。通过实证评估，证明了该方法的有效性。 |
| [^6] | [Language Models as Inductive Reasoners](https://rss.arxiv.org/abs/2212.10923) | 该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。 |
| [^7] | [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303) | 大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。 |
| [^8] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^9] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^10] | [Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models](https://arxiv.org/abs/2402.03284) | 本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。 |
| [^11] | [Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models](https://arxiv.org/abs/2402.03271) | 通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。 |
| [^12] | [ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds](https://arxiv.org/abs/2402.03269) | 本文介绍了ISPA（跨物种语音音标），这是一种精确、简洁、可解释的系统，用于将动物声音转录为文本。通过将动物声音表示为文本，我们有效地将其视为一种“外语”，并展示了已建立的人类语言机器学习范例和模型（如语言模型）能够成功应用于提高性能。 |
| [^13] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^14] | [Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills](https://arxiv.org/abs/2402.03244) | 本论文提出了一种技能集优化（SSO）方法，通过构建和完善可转移的技能集来提高大型语言模型（LLM）的性能。该方法通过提取高奖励的共同子轨迹，生成子目标和说明，并在上下文中提供给LLM演员，以强化行为。实验结果显示，SSO在不同环境中能够优化技能集，并实现上下文策略改进。 |
| [^15] | [JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching](https://arxiv.org/abs/2402.03242) | 本文介绍了一个名为JobSkape的框架，用于生成合成职位招聘信息以增强技能匹配。通过创建一个全面的合成职位招聘信息数据集SkillSkape，并使用大型语言模型进行技能提取和匹配任务，该框架解决了以前合成数据集存在的限制，并在离线度量指标和真实世界数据上取得了有希望的结果。 |
| [^16] | ["Define Your Terms" : Enhancing Efficient Offensive Speech Classification with Definition](https://arxiv.org/abs/2402.03221) | 这项工作通过定义术语来增强高效的攻击性言论分类，提出了一种联合嵌入架构，并利用元学习方法来增强其可靠和高效的检测。实验结果表明，该模型在多个数据集上取得了很好的分类效果，并提供了对抗资源稀缺的训练策略的案例研究。 |
| [^17] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^18] | [Isotropy, Clusters, and Classifiers](https://arxiv.org/abs/2402.03191) | 同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。 |
| [^19] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^20] | [CIDAR: Culturally Relevant Instruction Dataset For Arabic](https://arxiv.org/abs/2402.03177) | CIDAR是第一个由人工评审对齐文化的阿拉伯指令调优数据集，目的是解决现有指令数据集对西方文化的固有偏见所带来的影响，对于丰富将语言模型与阿拉伯文化对齐的研究工作具有重要意义。 |
| [^21] | [Multi: Multimodal Understanding Leaderboard with Text and Images](https://arxiv.org/abs/2402.03173) | Multi是一个多模态理解的排行榜，提供了一个综合数据集，评估多模态大型语言模型对理解复杂图表和科学问题的能力。它兼具准确和开放式的回答形式，挑战MLLM的各种任务，并包含超过18,000个问题。 |
| [^22] | [Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings](https://arxiv.org/abs/2402.03172) | 本论文提出了一种新颖的方法来实现自动化ICD编码，通过在文本中使用注意力机制的方法来精确分配ICD代码。实验结果表明，该方法在ICD编码中优于当前最先进的模型，并且标签嵌入机制对模型性能也有显著贡献。 |
| [^23] | [Homograph Attacks on Maghreb Sentiment Analyzers](https://arxiv.org/abs/2402.03171) | 同形异义词攻击对马格里布情感分析器造成了严重影响，将其性能从F1得分0.95降低到0.33。本研究主要旨在强调LLMs的弱点，并优先考虑机器学习的道德和责任。 |
| [^24] | [Linguistic features for sentence difficulty prediction in ABSA](https://arxiv.org/abs/2402.03163) | 本研究探讨了面向方面的情感分析中句子困难度的定义。通过实验研究了领域多样性和句法多样性对句子困难度的影响，并使用一组分类器识别了最困难的句子。 |
| [^25] | [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://arxiv.org/abs/2402.03161) | 这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。 |
| [^26] | [Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification](https://arxiv.org/abs/2402.03137) | 通过研究Hinglish情感分类，我们发现预训练语言模型能够学习到语言选择与情感表达之间的关联，尤其是在混合语言数据存在时。这对于情感分类的解释性具有重要意义。 |
| [^27] | [Constrained Decoding for Cross-lingual Label Projection](https://arxiv.org/abs/2402.03131) | 本文提出了一种解决零样本跨语言迁移学习中翻译质量下降问题的方法。 |
| [^28] | [Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases](https://arxiv.org/abs/2402.03099) | 该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。 |
| [^29] | [Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian](https://arxiv.org/abs/2402.03067) | 本文介绍了BERTopic在塞尔维亚语短文本中的首次应用，结果显示BERTopic可以产生丰富的主题，即使是部分预处理的文本。与传统方法相比，BERTopic在主题数量不受限制时提供了更多信息和新的见解。 |
| [^30] | [Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations](https://arxiv.org/abs/2402.03053) | 本文提出了一种基于大型语言模型的多语言马来西亚嵌入方法，通过微调Llama2和Mistral模型，在语义相似性和RAG任务中取得了显著的性能提升。 |
| [^31] | [A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems](https://arxiv.org/abs/2402.03050) | 尼泊尔自动语音识别系统的研究现状进行了全面调查，发现尼泊尔语言的语言模型和声学模型的研究还需进一步加强。 |
| [^32] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^33] | [SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach](https://arxiv.org/abs/2402.03043) | 本论文探究了可解释的人工智能（XAI）方法在文本领域的适用性，并将相似度差异和独特性（SIDU）方法扩展到文本数据，提供了对模型预测关键的有上下文意义的文本元素的解释。本研究采用了一个综合的三层评估框架来评估XAI方法。 |
| [^34] | [Automatic Combination of Sample Selection Strategies for Few-Shot Learning](https://arxiv.org/abs/2402.03038) | 本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。 |
| [^35] | [UniMem: Towards a Unified View of Long-Context Large Language Models](https://arxiv.org/abs/2402.03009) | 本文引入UniMem，一个统一的框架，以记忆增强的角度重新制定了现有的长上下文方法，并提出了UniMix来提高长上下文处理能力。 |
| [^36] | [Decoding-time Realignment of Language Models](https://arxiv.org/abs/2402.02992) | 本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。 |
| [^37] | [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987) | 本文介绍了一种针对 GPT 模型的对话重构攻击，该攻击具有劫持会话和重构对话的两个步骤。通过对该攻击对 GPT 模型的隐私风险进行评估，发现 GPT-4 对该攻击具有一定的鲁棒性。 |
| [^38] | [Putting Context in Context: the Impact of Discussion Structure on Text Classification](https://arxiv.org/abs/2402.02975) | 本研究探讨了上下文结构对文本分类的影响，发现结构信息对于文本分类有很大的益处，但仅在特定情况下显著。 |
| [^39] | [Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer](https://arxiv.org/abs/2402.02926) | 本文提出了一种基于Transformer的方法，用于自动同源检测，实验证明该方法在一定程度的监督下表现优于现有方法，并能随着进一步增加监督而稳定改进。同时，我们还证明了接受多个序列对齐作为输入，并具有端到端架构的重要性。 |
| [^40] | [A Computational Model for the Assessment of Mutual Intelligibility Among Closely Related Languages](https://arxiv.org/abs/2402.02915) | 本论文提出了一种用于评估密切相关语言间相互可理解性的计算模型，通过使用线性判别学习器和多语义向量等方法，对德语、荷兰语和英语这三种密切相关的日耳曼语言进行了测试。研究发现，模型的准确度与词形修剪和语言对有关。这种多语言建模方法为自动测试语言间相互可理解性提供了新的方法学发现。 |
| [^41] | [LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models](https://arxiv.org/abs/2402.02896) | 本研究通过对大规模语言模型进行个性化配置，并探究了智能体之间的对话交互对个性一致性和语言对齐的影响。研究结果表明不同配置对话者展示了不同程度的个性一致性和语言对齐。这些发现为进一步理解LLMs之间基于对话的相互作用提供了基础，并强调了打造更具鲁棒性和类人化LLM智能体的新方法的需求。 |
| [^42] | [Approximate Attributions for Off-the-Shelf Siamese Transformers](https://arxiv.org/abs/2402.02883) | 中文总结出的一句话要点: 本文介绍了一种适用于现有孪生变压器的近似归因方法，该方法在保留原模型性能的同时实现了准确归因能力。我们通过比较近似和准确归因，分析了模型对不同语言方面的关注，并发现孪生变压器主要忽略否定，同时深入研究了它们对句法角色的关注程度，以及如何判断语义上的差异。 |
| [^43] | [How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning](https://arxiv.org/abs/2402.02872) | 本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。 |
| [^44] | [EEVEE: An Easy Annotation Tool for Natural Language Processing](https://arxiv.org/abs/2402.02864) | EEVEE是一款简单易用的自然语言处理标注工具，可直接在浏览器中运行，支持多任务标注和四种任务类型。 |
| [^45] | [Comparing Knowledge Sources for Open-Domain Scientific Claim Verification](https://arxiv.org/abs/2402.02844) | 在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。 |
| [^46] | [With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations](https://arxiv.org/abs/2402.02837) | 在这篇论文中，我们从对话中识别出有意义的特征，通过将对话分割为主题连贯的话语集合，以一个正式的方法来研究对话的主题组织结构。 |
| [^47] | [Are Sounds Sound for Phylogenetic Reconstruction?](https://arxiv.org/abs/2402.02807) | 本文通过对十个不同语言家族的多样数据集进行研究，首次在系统发育重建中比较了基于声音和基于同源的方法的表现。结果显示，基于词汇同源的重建谱系与真实谱系平均更接近，提高了约三分之一。 |
| [^48] | [Graph-enhanced Large Language Models in Asynchronous Plan Reasoning](https://arxiv.org/abs/2402.02805) | 本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。 |
| [^49] | [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models](https://arxiv.org/abs/2402.02801) | KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。 |
| [^50] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^51] | [From Partial to Strictly Incremental Constituent Parsing](https://arxiv.org/abs/2402.02782) | 本研究构建了遵循严格增量定义的增量句法分析器，评估其仅基于前缀表示就能输出树的能力，并与非增量和部分增量模型进行了对比分析。 |
| [^52] | [Dual Knowledge Distillation for Efficient Sound Event Detection](https://arxiv.org/abs/2402.02781) | 这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。 |
| [^53] | [List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation](https://arxiv.org/abs/2402.02764) | 该论文提出了一个基于列表感知的重新排序-截断联合模型，用于搜索和生成增强检索。该模型旨在捕捉列表级的上下文特征，通过重新排序和截断来返回更好的列表。先前的研究将重新排序和截断视为两个单独的任务，但这种分离不是最优的。因此，该论文提出的联合模型可以解决信息共享和错误积累的问题。 |
| [^54] | [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://arxiv.org/abs/2402.02750) | 该论文提出了一种无需调整的非对称2位量化KV缓存技术，以解决存储注意力键和值的内存需求增加和推断速度受限问题。 |
| [^55] | [Exploiting Class Probabilities for Black-box Sentence-level Attacks](https://arxiv.org/abs/2402.02695) | 该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。 |
| [^56] | [Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision](https://arxiv.org/abs/2402.02658) | 本文介绍了一种名为模型引导的过程监督（MiPS）的新方法，该方法通过对推理模型的解决方案进行抽样完成来自动进行数据整理，从而避免了昂贵的人工注释。实证结果表明，与之前的工作相反，我们应优先选择验证器预测得分高的验证。这种方法显著改进了PaLM 2在数学和编码任务上的性能。 |
| [^57] | [RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews](https://arxiv.org/abs/2402.02656) | RACER是一种基于LLM的自动化分析方法，能够高效地将半结构化心理健康访谈转化为有洞见的主题和子主题。在COVID-19危机的研究中，RACER与人工评估员之间的一致性达到72%，接近人与人之间的一致性（77%）。 |
| [^58] | [VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension](https://arxiv.org/abs/2402.02655) | 本文介绍了VlogQA：越南口语机器阅读理解任务、数据集和基线模型，并提供了使用真实数据进行任务的挑战和机遇的见解。VlogQA是一个基于来自YouTube的剧本文档的问答对数据集，涵盖了食物和旅行等主题。深度学习模型在测试集取得了75.34%的最高F1分数。 |
| [^59] | [Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses](https://arxiv.org/abs/2402.02648) | 本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。 |
| [^60] | [LLM-Enhanced Data Management](https://arxiv.org/abs/2402.02643) | LLMDB是一种LLM增强的数据管理范式，通过LLM的微调和提示工程嵌入领域特定知识，解决了虚构、高成本和低准确性的挑战，并实现了泛化能力和高推理能力。 |
| [^61] | [It's how you do things that matters": Attending to Process to Better Serve Indigenous Communities with Language Technologies](https://arxiv.org/abs/2402.02639) | 本文探讨了建立土著语言NLP技术的伦理考虑，并推荐NLP研究人员增加对与土著社区合作过程的关注。 |
| [^62] | [Can Large Language Models Learn Independent Causal Mechanisms?](https://arxiv.org/abs/2402.02636) | 本论文研究在大型语言模型中学习独立因果机制的方法，以增强模型在分布变化下的鲁棒性和泛化能力。 |
| [^63] | [Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity](https://arxiv.org/abs/2402.02633) | 该论文研究了预测低资源语言机器翻译性能的关键因素，发现领域相似性对于预测模型性能具有最重要的影响。 |
| [^64] | [GIRT-Model: Automated Generation of Issue Report Templates](https://arxiv.org/abs/2402.02632) | 本研究介绍了GIRT-模型，这是一个基于开发者指示自动生成问题报告模板的助理语言模型。通过在GitHub仓库中构建的数据集进行指导调整，GIRT-模型在IRT生成方面表现显著优于其他通用语言模型。 |
| [^65] | [Enhancing Transformer RNNs with Multiple Temporal Perspectives](https://arxiv.org/abs/2402.02625) | 引入了多个时间视角的概念，用于增强Transformer RNNs对顺序数据的理解能力，在参数数量最小增加的情况下取得了显著的改进。 |
| [^66] | [DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging](https://arxiv.org/abs/2402.02622) | DenseFormer是对Transformer的简单修改，通过在每个transformer块之后进行深度加权平均，提高了模型的困惑度。学到的加权平均权重揭示了信息流的连贯模式，使得DenseFormer具有更高的数据效率，并且在相同困惑度下胜过传统的Transformer模型。 |
| [^67] | [Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study on Speech Emotion Recognition](https://arxiv.org/abs/2402.02617) | 本研究通过对自监督声学单词嵌入进行逐层分析，探索其在特定任务中的优势，特别是在语音情感识别中的贡献。实验结果揭示了AWEs与原始自监督表示的差异，并提出了合理利用AWEs与单词嵌入的方法。 |
| [^68] | [PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?](https://arxiv.org/abs/2402.02611) | 本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。 |
| [^69] | [On the performance of phonetic algorithms in microtext normalization](https://arxiv.org/abs/2402.02591) | 这项研究通过实验评估了微文本规范化中不同声学算法的性能，旨在确定最佳的声学算法。 |
| [^70] | [A Quantitative Discourse Analysis of Asian Workers in the US Historical Newspapers](https://arxiv.org/abs/2402.02572) | 本研究利用计算文本分析方法，对美国历史报纸中亚洲工人的呈现进行了定量研究。研究发现，“苦力”一词在不同州的语义有所差异，不同的话语围绕着“苦力”。此外，南方联邦报纸和北方联邦报纸形成了独特的话语，亚洲人在其中被认为低于欧洲移民，并受到种族主义的攻击。 |
| [^71] | [A Truly Joint Neural Architecture for Segmentation and Parsing](https://arxiv.org/abs/2402.02564) | 本文通过引入一个联合神经网络架构，在形态丰富的语言中实现了同时进行形态分割和句法分析的任务。通过提供基于格子的表示法，保留了输入的所有形态模糊性，有效解决了以往基于神经网络的依存句法分析器的局限性。 |
| [^72] | [DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models](https://arxiv.org/abs/2402.02563) | DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。 |
| [^73] | [NavHint: Vision and Language Navigation Agent with a Hint Generator](https://arxiv.org/abs/2402.02559) | 本论文提出了一种名为NavHint的视觉和语言导航代理，通过一个提示生成器为导航代理提供间接监督，帮助其对视觉环境进行整体理解。该方法在R2R和R4R数据集上取得了state-of-the-art的表现。 |
| [^74] | [Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials](https://arxiv.org/abs/2402.02558) | 本研究旨在提升生物医学自然语言推断（NLI）模型的鲁棒性，通过一种基于探测的方法研究了临床试验和模型对自然逻辑的理解。 |
| [^75] | [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549) | 本研究初步探讨了大型语言模型在基于表格的事实检查方面的潜力。实验结果表明，通过提示工程，大型语言模型在零样本和少样本的情况下可以实现可接受的表现。 |
| [^76] | ["What's my model inside of?": Exploring the role of environments for grounded natural language understanding](https://arxiv.org/abs/2402.02548) | 本论文探索了环境对于改进基于实践的自然语言理解中数据采集和模型开发的潜力，通过开发新的训练方法和基准，采用生态方法研究基于实践的语言理解系统在自然/模拟虚拟环境中的角色。 |
| [^77] | [Knowledge Generation for Zero-shot Knowledge-based VQA](https://arxiv.org/abs/2402.02541) | 本论文提出了一种使用预训练语言模型生成知识并应用于零样本基于知识的视觉问答的方法。实验证明该方法在两个基准测试上性能优于之前的方法，并生成的知识相关且有帮助。 |
| [^78] | [Absolute convergence and error thresholds in non-active adaptive sampling](https://arxiv.org/abs/2402.02522) | 提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。 |
| [^79] | [Adaptive scheduling for adaptive sampling in POS taggers construction](https://arxiv.org/abs/2402.02516) | 本论文提出了一种自适应调度的自适应采样方法，用于在构建词性标注器中加快训练速度，同时提高采样的鲁棒性。该方法分析学习曲线的形状，在任何时候增加或减少实例，以确保获得学习能力的净增益。 |
| [^80] | [Modeling of learning curves with applications to pos tagging](https://arxiv.org/abs/2402.02515) | 该论文提出了一种估计学习曲线演化的算法，可以通过部分训练数据结果来预测学习过程中达到期望准确度所需的时间。 |
| [^81] | [Early stopping by correlating online indicators in neural networks](https://arxiv.org/abs/2402.02513) | 本文提出了一种在神经网络中最小化泛化误差的新技术，通过利用一系列在线指标的时间相关性，找到过拟合现象，并提供了可靠的提前停止条件，从而提高了预测能力。 |
| [^82] | [Surfing the modeling of PoS taggers in low-resource scenarios](https://arxiv.org/abs/2402.02449) | 在低资源实验场景中，我们评估了早期学习曲线估计作为选择最合适模型的实用方法，并研究了在资源贫乏环境中的可靠性。 |
| [^83] | [Breaking MLPerf Training: A Case Study on Optimizing BERT](https://arxiv.org/abs/2402.02447) | 通过改进负载平衡、通信和优化器等各个组件，我们提出了用于快速大规模训练BERT模型的新方法，实现了新水平的BERT训练性能。 |
| [^84] | [Factuality of Large Language Models in the Year 2024](https://arxiv.org/abs/2402.02420) | 本文调查了大规模语言模型（LLM）的真实性问题，并对其现有研究进行了批判性分析，指出了改进LLM真实性的挑战和解决方案，以及自动真实性评估的障碍。未来的研究应该关注在这些方面的进一步工作。 |
| [^85] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^86] | [GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model](https://arxiv.org/abs/2402.02408) | GLaPE提出了一种无依赖于金标签的提示评估方法，通过自一致性作为初始评估分数，进一步改进了产生相同答案的提示的得分的互相一致性，提供了与准确性相一致的可靠评估，即使在没有金标签的情况下。 |
| [^87] | [DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models](https://arxiv.org/abs/2402.02392) | DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。 |
| [^88] | [KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion](https://arxiv.org/abs/2402.02389) | 本文提出了KICGPT，它是一个集成了大型语言模型和基于三元组的知识图谱补全检索器的框架。它通过知识提示的上下文学习策略，缓解了长尾问题，并且无需额外的训练开销。实验证明了其有效性。 |
| [^89] | [Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning](https://arxiv.org/abs/2402.02388) | 本文提出了SAGE，一个通用的面向解决方案的ABM生成框架，利用辅助验证和迭代上下文学习，自动生成针对特定问题的模型和解决方案。 |
| [^90] | [Evaluating Large Language Models in Analysing Classroom Dialogue](https://arxiv.org/abs/2402.02380) | 本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。 |
| [^91] | [Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an Entity-Centric Perspective](https://arxiv.org/abs/2402.02379) | 本论文从实体中心的角度重新思考了对预训练的文本和布局模型的评估。提出了评估PTLMs信息提取能力的理想基准的标准，并介绍了针对该评估的EC-FUNSD数据集。实验结果表明，最先进的PTLMs在预训练阶段存在过拟合的倾向。 |
| [^92] | [M$^3$Face: A Unified Multi-Modal Multilingual Framework for Human Face Generation and Editing](https://arxiv.org/abs/2402.02369) | M$^3$Face是一种统一多模态多语言框架，可以通过文本输入自动生成控制模态，并实现人脸的生成和编辑。同时，我们还提出了M3CelebA数据集，该数据集包含了高质量的多模态多语言人脸数据。 |
| [^93] | [A Survey of Large Language Models in Finance (FinLLMs)](https://arxiv.org/abs/2402.02315) | 这篇综述介绍了金融领域的大规模语言模型（FinLLMs），包括它们的历史、技术、性能和机遇与挑战，并提供了六个基准任务和八个高级金融NLP任务的性能评估。 |
| [^94] | [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314) | 该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。 |
| [^95] | [Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens](https://arxiv.org/abs/2402.02302) | 本论文提出了使用声学伪令牌来预测改善低资源语音识别的积极迁移。通过在目标语言中补充来自相似、高资源的“捐赠”语言的数据，可以提高低资源语言在自动语音识别任务上的性能。 |
| [^96] | [SemPool: Simple, robust, and interpretable KG pooling for enhancing language models](https://arxiv.org/abs/2402.02289) | SemPool提出了一种简单、稳健、可解释的知识图加强语言模型的汇聚方法，通过学习和融合KG的语义信息，能够在更具挑战性的环境下提高问答系统的准确率。 |
| [^97] | [SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking](https://arxiv.org/abs/2402.02285) | SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％. |
| [^98] | [Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model](https://arxiv.org/abs/2402.02262) | 本文介绍了一种使用RoBERTa-CNN模型来在社交媒体帖子中检测自杀意图的新方法。RoBERTa-CNN通过在RoBERTa模型中添加卷积神经网络（CNN）层，提高了对重要模式的捕捉能力，并在实验证明在自杀和抑郁检测数据集上表现出良好的准确性。 |
| [^99] | [Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times](https://arxiv.org/abs/2402.02255) | 本研究发现，当基于Transformer的语言模型变得越来越大并且在大量数据上进行训练时，模型的惊讶估计与自然人阅读时间的适应性下降。而词频是解释这种适应性下降的关键因素，较大模型变体过度准确地预测了人群中最不频繁的词汇，而较大模型的训练过程中更准确地学习了罕见的词汇，这解释了训练数据量和模型尺寸对适应阅读时间的不利影响。 |
| [^100] | [Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models](https://arxiv.org/abs/2402.02244) | 这篇论文综述了近期为扩展大型语言模型中上下文长度而设计的技术和方法，并回顾了包括架构修改在内的多种技术，使得语言模型可以更有效地理解长上下文。 |
| [^101] | [Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding](https://arxiv.org/abs/2402.02243) | ChatGPT在LLM规模上通过利用语言本身的收敛约束来做到超出预期的表现，但并不真正理解语义以及与感觉动作的直接联系。 |
| [^102] | [A Data Generation Perspective to the Mechanism of In-Context Learning](https://arxiv.org/abs/2402.02212) | 本文从数据生成的角度重新解释了In-Context Learning（ICL）的机制，并探讨了流行的技术解决方案的潜在应用。对不同解决方案的优劣进行了全面研究，强调了其中的不足之处。 |
| [^103] | [Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval](https://arxiv.org/abs/2402.02175) | 本论文提出了一种名为EPR的方法，通过索引原子邻接模式并将其组合，明确建模结构依赖，并在知识图谱中进行子图提取。实验结果表明，EPR方法在复杂问题回答方面取得了显著的改进，提高了F1得分，并在WebQuestionsSP上表现出竞争力。 |
| [^104] | [Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models](https://arxiv.org/abs/2402.02145) | 通过引入对抗攻击的句子扰动技术和基于提示的方法，本论文提出了一种新颖的方法，通过降低新闻内容中的情感极性来解决新闻报道中的情感操纵问题。实验和人工评估表明，这两个模型在实现情感极性降低方面非常有效。 |
| [^105] | [Probing Critical Learning Dynamics of PLMs for Hate Speech Detection](https://arxiv.org/abs/2402.02144) | 该论文探究了预训练语言模型在检测仇恨言论中的关键学习动态，提出了深入的研究问题，并通过比较不同模型、评估鲁棒性、微调设置和预训练数据收集时间的影响等方面的分析结果，为研究者在仇恨言论检测领域提供了实证基础和建议。 |
| [^106] | [Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test](https://arxiv.org/abs/2402.02135) | 本研究通过使用多语言定义问题测试，探讨了大型语言模型（LLMs）在不同语言下的道德判断和道德推理能力。研究发现，印地语和斯瓦希里语的道德推理能力明显低于其他语言，而道德判断也在不同语言下变化较大。 |
| [^107] | [Rendering Graphs for Graph Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2402.02130) | 本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。 |
| [^108] | [Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon](https://arxiv.org/abs/2402.02113) | 本文提出一种使用多语言情感词典进行零样本情感分析的方法，通过预训练模型在低资源语言中获得优越的性能，包括对高/中资源语言和混合代码数据的处理。 |
| [^109] | [Are Large Language Models Good Prompt Optimizers?](https://arxiv.org/abs/2402.02101) | 这项研究揭示了以大型语言模型（LLM）作为提示优化器的实际机制。研究发现LLM优化器往往受到自身先前知识的偏见，难以准确识别错误的真正原因，并且在生成合适的提示方面面临挑战。 |
| [^110] | [Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models](https://arxiv.org/abs/2402.02099) | 分析了多语言语言模型中跨语言知识转移的评估方法和设置，发现高性能主要归因于非语言知识的因素，如任务和表层知识，并且跨语言传输的主要是数据工件和偏见，尤其是对于低资源语言。 |
| [^111] | [Revisiting the Markov Property for Machine Translation](https://arxiv.org/abs/2402.02084) | 本文重新审视了机器翻译中的马尔可夫性质，并设计了一个马尔可夫自回归变换器（MAT）。研究结果显示，MAT的阶数大于4时，翻译质量与传统自回归变换器相当，且高阶MAT并不特别适用于长句的翻译。 |
| [^112] | [GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding](https://arxiv.org/abs/2402.02082) | 本研究提出了GliDe和CaPE两种简化的快速推理解码方法，通过重用缓存键和值以及利用置信度分数选择额外候选令牌进行验证，显著降低了LLM的解码延迟。 |
| [^113] | [Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning](https://arxiv.org/abs/2402.02080) | 研究发现，在交叉语言学习中，翻译错误对低资源语言有显著影响。通过测量在多个目标语言上的人工翻译和机器翻译的目标文本的零-shot评估的表现差距，可以识别翻译错误。此外，该研究还证实了印地语和乌尔都语存在翻译错误。 |
| [^114] | [Exploring the Robustness of Task-oriented Dialogue Systems for Colloquial German Varieties](https://arxiv.org/abs/2402.02078) | 本研究通过制定扰动规则，研究了将德语句子转换为口语化形式对基于任务的对话系统的影响。实验证明，在口语化变体中应用ToD系统时，意图识别性能仍保持稳定，平均准确度下降了6%（4.62个百分点）。 |
| [^115] | [Investigating Content Planning for Navigating Trade-offs in Knowledge-Grounded Dialogue](https://arxiv.org/abs/2402.02077) | 这项工作研究了知识驱动对话中的内容规划，以平衡响应的特定性和归因度之间的权衡。尽管内容规划显示出希望，但对于是否可以帮助解决这一权衡问题的结果是混合的。 |
| [^116] | [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://arxiv.org/abs/2402.02057) | 本文介绍了一种称为前瞻解码的精确、并行解码算法，通过交换每步操作数以减少总解码步骤的数量，加速了大型语言模型（LLM）的解码过程。它不需要辅助模型或数据存储，并且与并发内存高效的注意力机制兼容。实验证明，在代码补全任务中，前瞻解码可将自回归解码加速1.8倍，并且在多个GPU上实现强扩展性。 |
| [^117] | [AnthroScore: A Computational Linguistic Measure of Anthropomorphism](https://arxiv.org/abs/2402.02056) | AnthroScore是一种计算语言学的度量方法，用于隐含人性化语言。研究发现，AnthroScore与人类对人性化的判断和社会科学文献中的人性化维度相一致。分析结果显示，近15年来研究论文中的人形化水平稳步增加，与语言模型相关的论文具有最高的人形化水平。 |
| [^118] | [EffiBench: Benchmarking the Efficiency of Automatically Generated Code](https://arxiv.org/abs/2402.02037) | 本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。 |
| [^119] | [Panacea: Pareto Alignment via Preference Adaptation for LLMs](https://arxiv.org/abs/2402.02030) | Panacea 是一种创新方法，将大型语言模型对齐重新定义为多维偏好优化问题，通过使用奇异值分解的低秩适应，以在线注入偏好向量的形式，使模型能够适应并 Pareto 最优地满足各种偏好集。 |
| [^120] | [How well do LLMs cite relevant medical references? An evaluation framework and analyses](https://arxiv.org/abs/2402.02008) | 这项研究提出了一个问题：LLM生成的来源是否真正支持它们所做的主张？通过验证源的相关性和开发端到端的自动化流水线，研究人员发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。 |
| [^121] | [Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes](https://arxiv.org/abs/2402.01981) | 本研究利用大型语言模型的零-shot能力提出了一种自我去偏差的技术，通过解释和重启两种方法，成功减少了九个不同社会群体的刻板印象程度，该技术不需要对训练数据、模型参数或解码策略进行修改，希望能启发其他零-shot偏见缓解技术的研究。 |
| [^122] | [SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks](https://arxiv.org/abs/2402.01980) | 社会科学的自然语言处理任务需要捕捉语义和隐含的语用信息，指导调优模型Socialite-Llama在这些任务上表现出卓越的性能和提升。 |
| [^123] | [MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble Techniques and Data Augmentation for Climate Activism Stance and Hate Event Identification](https://arxiv.org/abs/2402.01976) | MasonPerplexity团队通过集成建模和数据增强技术，在识别气候行动立场和仇恨事件方面取得了显著成果，为这一重要研究领域提供了有效的方法。 |
| [^124] | [MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles](https://arxiv.org/abs/2402.01967) | 本文提出了一种名为MasonPerplexity的方法来解决多模态仇恨言论事件检测的问题。该方法采用Transformer集成的方式，在识别仇恨言论和识别文本图像中目标的任务中均取得了较好的成绩，分别排名第三。 |
| [^125] | [Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders](https://arxiv.org/abs/2402.01963) | 本文提出了一种使用标签自编码器改进大规模k最近邻文本分类的方法，通过将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签，以解决在大型文档集合中处理自动语义索引的问题。 |
| [^126] | [A Case Study on Filtering for End-to-End Speech Translation](https://arxiv.org/abs/2402.01945) | 本文研究了端到端语音翻译中的过滤技术。实验证明，使用最简单的过滤方法可以提高数据集的质量，进而提升模型性能，在多语言到英语语音翻译模型中，平均获得4.65个BLEU分数的提升。 |
| [^127] | [A Morphologically-Aware Dictionary-based Data Augmentation Technique for Machine Translation of Under-Represented Languages](https://arxiv.org/abs/2402.01939) | 本文提出了一种通过使用形态句法信息和双语词典来合成平行数据的方法，从而解决了少见语言机器翻译中数据稀缺的问题。实验证明，即使只使用少量种子数据和双语词典，该方法在14种语言上都能显著提高性能。 |
| [^128] | [Code Representation Learning At Scale](https://arxiv.org/abs/2402.01935) | 这项工作提出了一个利用大规模代码数据进行代码表示学习的两阶段预训练方案，通过混合语言建模和对比学习增强表示，建立了一个优于现有模型的编码器模型，在各种下游任务上大幅优化了性能。 |
| [^129] | [Digits micro-model for accurate and secure transactions](https://arxiv.org/abs/2402.01931) | 本研究展示了数字微模型在准确和安全交易中的潜力，这些轻量级模型在数字识别特定任务上表现良好，并使用较少的训练时间和内存资源。 |
| [^130] | [Preference Poisoning Attacks on Reward Model Learning](https://arxiv.org/abs/2402.01920) | 对于从偏好比较中学习奖励模型的方法存在偏好污染攻击的漏洞，攻击者可以通过翻转少量偏好比较来对目标结果进行操纵。我们提出了两类算法方法，并证明了这些攻击在实施恶意行为方面的有效性。 |
| [^131] | [Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges](https://arxiv.org/abs/2402.01917) | 本文介绍了NB-Whisper，这是针对挪威语自动语音识别的OpenAI Whisper的适应版本，通过改进挪威文本转写的正确率，取得了较好的结果。 |
| [^132] | [CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish](https://arxiv.org/abs/2402.01916) | 这篇论文介绍了基于相似度的描述符分配方法在BioASQ MESINESP8任务中的应用，通过使用传统的信息检索工具，并提出了适用于西班牙语等语言的方法。 |
| [^133] | [Natural language guidance of high-fidelity text-to-speech with synthetic annotations](https://arxiv.org/abs/2402.01912) | 本论文提出了一种自然语言引导的高保真度文本到语音转换方法，通过合成注释来标注说话人身份、风格和录音条件，结果表明在不依赖于人工标注的情况下，可以实现高保真度的语音生成。 |
| [^134] | [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://arxiv.org/abs/2402.01878) | 本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。 |
| [^135] | [The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models](https://arxiv.org/abs/2402.01874) | 这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。 |
| [^136] | [Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision](https://arxiv.org/abs/2402.01867) | 这项工作利用大型语言模型在提示式弱监督中学习标注函数之间的统计依赖结构，通过结构细化模块提高了提示式弱监督流程的效果。 |
| [^137] | [What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement](https://arxiv.org/abs/2402.01865) | 本文研究了语言模型更新中的遗忘现象，提出了一种预测上游实例遗忘的方法，以改进重播过程的可控性和解释性。根据预训练实例的预-softmax对数几率分数变化与在线学习实例的相似性，提出了一种部分可解释的预测模型，在BART模型上表现良好但在T5模型上失败。此外，还展示了基于内积的黑盒分类器。 |
| [^138] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^139] | [COMET: Generating Commit Messages using Delta Graph Context Representation](https://arxiv.org/abs/2402.01841) | COMET是一种利用增量图上下文表示的新方法，通过使用transformer模型生成高质量的提交消息，并引入可定制的质量保证模块。实验证明，COMET在各项指标上优于其他技术，并与流行的GPT模型具有可比性。 |
| [^140] | [Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment](https://arxiv.org/abs/2402.01830) | 本文提出了一种新的无监督评估方法，利用同行评审机制在开放环境中衡量LLMs。通过为每个LLM分配可学习的能力参数，以最大化各个LLM的能力和得分的一致性。结果表明，高层次的LLM能够更准确地评估其他模型的答案，并能够获得更高的响应得分。 |
| [^141] | [Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing](https://arxiv.org/abs/2402.01829) | 本研究使用深度学习和自然语言处理方法，开发了一种预测蛋白质序列中ATP结合位点的方法，实验证明了与最先进的基准模型相比的改进效果。 |
| [^142] | [Retrieval Augmented End-to-End Spoken Dialog Models](https://arxiv.org/abs/2402.01828) | 这项研究介绍了一种检索增强的端到端口语对话模型（ReSLM），通过训练语音检索器获取音频中的文本实体，并将这些实体作为输入加入到模型中以提高性能。 |
| [^143] | [Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature](https://arxiv.org/abs/2402.01826) | 本研究利用大型语言模型GPT-35-turbo，从PubMed的25 million个摘要中自动提取出男性和女性的血压平均值和标准差，填补了在考虑生物学性别差异的血压测量方差方面的研究空白。 |
| [^144] | [Fractal Patterns May Unravel the Intelligence in Next-Token Prediction](https://arxiv.org/abs/2402.01825) | 通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。 |
| [^145] | [Building Guardrails for Large Language Models](https://arxiv.org/abs/2402.01822) | 本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。 |
| [^146] | [Distilling LLMs' Decomposition Abilities into Compact Language Models](https://arxiv.org/abs/2402.01812) | 本研究将LLMs的分解能力通过离线强化学习融入到紧凑模型中，通过开发AI生成的数据集和建立基准，突出了紧凑模型在复制复杂问题解决能力方面的潜力。 |
| [^147] | [HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text](https://arxiv.org/abs/2402.01806) | HQA-Attack是一种针对黑盒硬标签文本对抗攻击的方法，通过简单有效的框架生成高质量的对抗样本，解决了现有方法在有限的查询预算下难以生成具有高语义相似度和低扰动率的问题。 |
| [^148] | [Exploring the Limitations of Graph Reasoning in Large Language Models](https://arxiv.org/abs/2402.01805) | 本文测试了5种不同的大型语言模型在图推理问题上的推理深度，并发现了LLMs的局限性、偏见和属性。我们发现LLMs对于节点遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务有负面影响，并且LLMs存在积极的回应偏差，无法识别有效解的缺失。我们还提出了一种新的图推理提示技术。 |
| [^149] | [Large Language Models for Time Series: A Survey](https://arxiv.org/abs/2402.01801) | 本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。 |
| [^150] | [Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward](https://arxiv.org/abs/2402.01799) | 本调查文章概述了在提高LLM推理效果方面的最新方法和进展，通过实验评估不同压缩技术的有效性，并提出改进LLM推理效率的潜在未来方向。 |
| [^151] | [Exploring transfer learning for pathological speech feature prediction: Impact of layer selection](https://arxiv.org/abs/2402.01796) | 本研究探索了用于病理语音特征预测的迁移学习，发现选择适当的层能显著提高性能并且学得的加权和具有更好的泛化能力。 |
| [^152] | [The Political Preferences of LLMs](https://arxiv.org/abs/2402.01789) | 该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。 |
| [^153] | [LitLLM: A Toolkit for Scientific Literature Review](https://arxiv.org/abs/2402.01788) | "LitLLM: A Toolkit for Scientific Literature Review" 提出了一个基于 RAG 原则的工具包，通过使用专门的提示和指导技术，结合大型语言模型（LLM），实现了科学文献综述的自动化。这个工具包不仅可以通过转化摘要为关键词进行文献检索，还可以通过补充相关论文或关键词进行定制化的检索。 |
| [^154] | [COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations](https://arxiv.org/abs/2402.01786) | COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。 |
| [^155] | [Hierarchical Multi-Label Classification of Online Vaccine Concerns](https://arxiv.org/abs/2402.01783) | 本文研究了在线疫苗关注的分层多标签分类任务，使用大型语言模型在零样本设置下检测疫苗关注，同时探索了不同提示策略的成本和准确性权衡，并提供了指导当前应用程序系统设计的具体经验教训。 |
| [^156] | [When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards](https://arxiv.org/abs/2402.01781) | 依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。 |
| [^157] | [On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble](https://arxiv.org/abs/2402.01777) | GPT-4在心理测试中展现出适度焦虑、略带男性化、诚实谦逊的特点，并且在数值素养和语言任务上表现出超过人类平均水平的认知反思能力。 |
| [^158] | [Design and consensus content validity of the questionnaire for b-learning education: A 2-Tuple Fuzzy Linguistic Delphi based Decision Support Tool](https://arxiv.org/abs/2402.01775) | 本研究提出了一种名为2-Tuple模糊语言Delphi的决策支持工具，用于评估b-学习教育问卷的内容有效性。该方法通过评估问卷的各个部分，根据专家意见测量一致性程度和语言得分，来检测影响问卷质量的项目。 |
| [^159] | [Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation](https://arxiv.org/abs/2402.01772) | 本文通过大规模研究展示了多语言机器翻译中目标端转移的动态影响。我们发现，语言相似的辅助目标语言具有强大的正向知识转移能力，并且随着相似目标语言规模的增加，转移效果进一步增强。同时，远离的辅助目标语言也可以意外地对主要语言对产生正向转移效果。 |
| [^160] | [BlackMamba: Mixture of Experts for State-Space Models](https://arxiv.org/abs/2402.01771) | BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。 |
| [^161] | [Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation](https://arxiv.org/abs/2402.01769) | 该研究旨在重新定义LLMs中的“幻觉”，提出了心理学分类法，以更详细地理解和解决LLMs输出误导信息的挑战。通过借鉴人类处理类似挑战的方式，研究旨在开发策略以减轻LLMs中的幻觉。 |
| [^162] | [HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA](https://arxiv.org/abs/2402.01767) | HiQA是一个先进的多文档问答框架，使用分层的上下文增强和多路径检索机制，解决了大规模文档问答中的检索准确性问题，并在多文档环境中展示了最先进的性能。 |
| [^163] | [LLM Voting: Human Choices and AI Collective Decision Making](https://arxiv.org/abs/2402.01766) | 本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。 |
| [^164] | [LLMs Simulate Big Five Personality Traits: Further Evidence](https://arxiv.org/abs/2402.01765) | 本研究旨在研究大型语言模型（LLMs）如何模拟五大人格特质，并分析了其模拟的人格特质及稳定性，有助于深入了解LLMs在个性化人机交互方面的潜力。 |
| [^165] | [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763) | 本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。 |
| [^166] | [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761) | 大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。 |
| [^167] | [Systematic Literature Review: Computational Approaches for Humour Style Classification](https://arxiv.org/abs/2402.01759) | 这项研究通过系统性文献综述展示了计算方法在幽默风格分类中的应用情况，并指出了当前的研究空白和有希望的方向。 |
| [^168] | [Aalap: AI Assistant for Legal & Paralegal Functions in India](https://arxiv.org/abs/2402.01758) | Aalap是一个在印度法律任务上针对指令数据进行微调的AI助手，相比于gpt-3.5-turbo在测试数据中表现更好，主要教授法律推理，对律师、法官或在法律系统中工作的人有帮助。 |
| [^169] | [Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio](https://arxiv.org/abs/2402.01752) | 该研究提出了一种通过分析音频来辨别僞作和仇恨言论在僧伽罗语YouTube视频中的解决方案。这些方法包括评估视频是否包含虚假信息以及检测其中是否存在仇恨言论。 |
| [^170] | [Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia](https://arxiv.org/abs/2402.01751) | 本研究评估了ChatGPT和Bard在识别阿尔茨海默病痴呆的能力，并发现Bard在积极识别AD方面表现最好，具有较高的召回率和F1得分，但可能会将CN错误地识别为AD。对于积极识别CN，GPT-4表现出最高的真阴性。 |
| [^171] | [PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models](https://arxiv.org/abs/2402.01750) | PACE是一种利用大型语言模型的实用智能体，提供了基于图像的实用通信框架。它通过语义感知、意图解析和以意图为导向的编码来增强通信效率。为了有效利用语言模型，它使用知识库补充所需知识，引入专用提示来促进对实用通信场景和任务要求的理解，并设计思维链以帮助权衡传输效率和质量。 |
| [^172] | [Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems](https://arxiv.org/abs/2402.01748) | 本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。 |
| [^173] | [Towards Optimizing the Costs of LLM Usage](https://arxiv.org/abs/2402.01742) | 本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。 |
| [^174] | [Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties](https://arxiv.org/abs/2402.01741) | 本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。 |
| [^175] | [Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models](https://arxiv.org/abs/2402.01740) | 这项研究研究了大型语言模型在选择对象时的偏见，发现偏见结构依赖于模型，对象类型调节了偏见的影响程度，导致列表中的第一个对象在输出中被过度呈现。 |
| [^176] | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。 |
| [^177] | [C4Q: A Chatbot for Quantum](https://arxiv.org/abs/2402.01738) | C4Q是一个用于量子计算的聊天机器人，能够准确回答基本问题并在编写量子程序时引导用户，通过使用自己的引擎而能够保证答案的准确性。 |
| [^178] | [Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues](https://arxiv.org/abs/2402.01737) | 本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。 |
| [^179] | [SADAS: A Dialogue Assistant System Towards Remediating Norm Violations in Bilingual Socio-Cultural Conversations](https://arxiv.org/abs/2402.01736) | SADAS是一个面向双语社会文化对话的对话助手系统，旨在通过识别规范类别、检测违例、评估严重程度、实施纠正措施并阐述理由等新颖架构，确保不同文化背景的个体之间的对话能够以尊重和理解的方式进行。 |
| [^180] | [VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models](https://arxiv.org/abs/2402.01735) | 这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。 |
| [^181] | [CFTM: Continuous time fractional topic model](https://arxiv.org/abs/2402.01734) | CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。 |
| [^182] | [Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report](https://arxiv.org/abs/2402.01733) | RAG模型是一种有前景的方法，用于在大型语言模型中定制领域知识。本研究开发和评估了一个专为医疗保健定制的LLM-RAG流程，重点关注术前医学。 |
| [^183] | [Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and Symptom Analysis](https://arxiv.org/abs/2402.01730) | 本文提出了一种评估LLM生成医学诊断的方法，通过结构化交互和领域特定分析来评估其正确性和准确性。我们使用GPT-4-Vision-Preview作为LLM，并使用多模态多项选择题评估其在病理学领域的表现。 |
| [^184] | [Contextualization Distillation from Large Language Model for Knowledge Graph Completion](https://arxiv.org/abs/2402.01729) | 本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。 |
| [^185] | [Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge](https://arxiv.org/abs/2402.01728) | 硬件Phi-1.5B是一个专门针对半导体产业硬件领域的大型语言模型，通过预训练和专业分层数据集解决了硬件领域的复杂性和数据稀缺性问题。 |
| [^186] | [Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages](https://arxiv.org/abs/2402.01726) | 这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。 |
| [^187] | [Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models](https://arxiv.org/abs/2402.01725) | 本篇论文介绍了一种多方面的方法来解决大型语言模型中的道德和安全挑战，包括过滤敏感词汇、检测角色扮演、实施自定义规则引擎，以及应用到不同的派生模型中。这些方法可以提高模型的安全性，降低道德和隐私方面的风险。 |
| [^188] | [CERM: Context-aware Literature-based Discovery via Sentiment Analysis](https://arxiv.org/abs/2402.01724) | CERM是一个通过情感分析进行基于文献的上下文感知发现的系统，旨在理解食品与健康之间的关系。通常情况下，基于食材营养成分或基于标记数据的计算模型已被用于食谱推荐和分析系统。然而，本研究提出了一种增强模型，通过捕捉食材与生物医学概念之间的固有关系，利用标记和未标记的数据来更好地支持食品相关研究。 |
| [^189] | [An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios](https://arxiv.org/abs/2402.01723) | 本文对中国工业领域的大型语言模型进行了实证研究，评估了其在准确性和鲁棒性方面的表现。 |
| [^190] | [Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately](https://arxiv.org/abs/2402.01722) | 通过使用反馈和示例的精调过程，结合余弦相似度、LLM评估和Rouge-L得分等指标，可以提升大型语言模型（LLMs）在回答问题和提取信息方面的准确性。与零-shot LLMs相比，经过精调的模型展示了出色的问答能力，并且通过结合RAG过程进一步提高了其效果。 |
| [^191] | [Deep Learning Based Amharic Chatbot for FAQs in Universities](https://arxiv.org/abs/2402.01720) | 本文提出了一个基于深度学习的阿姆哈拉语常见问题解答聊天机器人模型，可以帮助大学生解答常见问题，通过使用自然语言处理和深度学习技术，采用多种机器学习模型算法进行分析和分类，取得了最好的成绩。 |
| [^192] | [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719) | 本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。 |
| [^193] | [From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process](https://arxiv.org/abs/2402.01717) | 本文介绍了一种利用生成式AI和检索增强生成（RAG）方法的聊天机器人模型，名为QA-RAG，用于解决制药行业中的合规性挑战，并在准确性上取得了显著改进。 |
| [^194] | [Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums](https://arxiv.org/abs/2402.01716) | 本研究提出了一种称为布鲁姆-认知和情感分析（BE-Sent）的层次化方法，用于评估教育讨论论坛中的情绪和布鲁姆的认知分类。方法包括数据收集、文本预处理、情感分析和认知分类。研究结果有助于了解学生在学习过程中的进展情况和知识水平。 |
| [^195] | [ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis](https://arxiv.org/abs/2402.01715) | 本研究对ChatGPT、Gemini和LLaMA2等多语言情感分析模型进行了比较，发现这些模型在处理模棱两可的情境时表现良好，但在不同语言和评估中存在显著的偏差和性能不一致。这项工作为自动情感分析提供了标准化的评估方法，并呼吁改进算法和数据，以提高性能、可解释性和适用性。 |
| [^196] | [TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy](https://arxiv.org/abs/2402.01714) | TrICy是一种轻量级框架，利用意图和触发器引导数据到文本生成任务，并通过关注-复制机制有效地处理了词汇表之外的词汇。实验结果表明其在不同数据集上取得了良好的性能。 |
| [^197] | [Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data](https://arxiv.org/abs/2402.01713) | 本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。 |
| [^198] | [Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models](https://arxiv.org/abs/2402.01712) | 通过利用生成式AI模型，我们提出了一种基于社会感知的合成数据生成方法，用于自杀意念检测。与传统模型相比，我们的方法在实际数据集上表现出有竞争力的性能。 |
| [^199] | [Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators](https://arxiv.org/abs/2402.01708) | 语音生成技术的广泛应用给社会带来了伦理和安全风险。本文通过分析语音生成事件，总结出特定伤害模式，并将其分类。这些特定伤害涉及到受影响个体的曝光程度以及相关利益相关者和技术系统之间的相互作用。 |
| [^200] | [MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds](https://arxiv.org/abs/2402.01706) | 本文通过构建多种语境，使用领域特定语言描述可能世界，并利用编译器，发现了大型语言模型在不同语境下的对齐问题。这种方法成本较低，能够更全面地研究LLM对齐问题。 |
| [^201] | [Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation](https://arxiv.org/abs/2402.01705) | 本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。 |
| [^202] | [States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers](https://arxiv.org/abs/2402.01704) | 本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。 |
| [^203] | [Fluent dreaming for language models](https://arxiv.org/abs/2402.01702) | 扩展了贪婪坐标梯度方法，设计了进化提示优化算法，实现了语言模型的流畅梦境，能够同时最大化内部特征和提示流畅性，可以自动探索模型对轻度分布之外提示的反应。 |
| [^204] | [Question answering systems for health professionals at the point of care -- a systematic review](https://arxiv.org/abs/2402.01700) | 这项系统综述旨在描述当前的医学问答系统，评估其在医疗保健中的适用性，并确定改进的方向。 |
| [^205] | [Large language model empowered participatory urban planning](https://arxiv.org/abs/2402.01698) | 本研究将大型语言模型（LLMs）应用于城市规划的参与式过程中，通过角色扮演、协作生成和反馈迭代，解决了社区级土地利用任务。实证实验表明LLM在不同规划场景上具有适应性和有效性，能够超过人类专家满意度和包容性，并与最先进的强化学习方法在服务和生态方面媲美。 |
| [^206] | [APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation](https://arxiv.org/abs/2402.01697) | APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets. |
| [^207] | [HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification](https://arxiv.org/abs/2402.01696) | HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。 |
| [^208] | [Language-Guided World Models: A Model-Based Approach to AI Control](https://arxiv.org/abs/2402.01695) | 语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。 |
| [^209] | [ARGS: Alignment as Reward-Guided Search](https://arxiv.org/abs/2402.01694) | ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。 |
| [^210] | [Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study](https://arxiv.org/abs/2402.01693) | 本研究评估了使用大型语言生成模型（LLMs）与人类患者相比，解答非专业患者关于实验室测试结果的问题的回答质量。评估结果表明LLMs在相关性、正确性和有帮助性方面具有一定潜力，但还存在潜在问题需要改进。 |
| [^211] | [Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization](https://arxiv.org/abs/2402.01692) | 本文提出了一种有效的转移学习框架，用于通过最少的标记和未标记数据实现语言自适应。该方法使用自监督特征进行预训练，在细调期间替换伪标签噪声部分，并结合嵌入初始化技巧，有效利用未标记数据的信息。实验证明，即使在仅有很少的数据情况下，该框架也能合成可理解的未知语言语音，并超越传统技术。这一研究结果展示了该高效语言自适应框架的潜力。 |
| [^212] | [Linguistic-Based Mild Cognitive Impairment Detection Using Informative Loss](https://arxiv.org/abs/2402.01690) | 本论文提出了一种基于自然语言处理技术的深度学习方法，用于在老年人中区分轻度认知障碍和正常认知条件。该方法使用了句子嵌入和句子交叉注意力模块，通过分析视频访谈的转录文本，提取时序特征进行分类。同时，引入了一种新的损失函数来建立稳健的模型。 |
| [^213] | [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685) | SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。 |
| [^214] | [A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm](https://arxiv.org/abs/2402.01684) | 提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。 |
| [^215] | [Community-based Behavioral Understanding of Crisis Activity Concerns using Social Media Data: A Study on the 2023 Canadian Wildfires in New York City](https://arxiv.org/abs/2402.01683) | 本研究利用大规模社交媒体数据研究了2023年加拿大野火烟雾在纽约市引发的危机活动关注。通过整合地理标记的Twitter数据和国家数据库，开发模型对不同活动关注进行社区推断。研究结果发现，在野火期间，纽约市居民的活动关注发生了变化。 |
| [^216] | [Leveraging Social Media Data to Identify Factors Influencing Public Attitude Towards Accessibility, Socioeconomic Disparity and Public Transportation](https://arxiv.org/abs/2402.01682) | 本研究利用社交媒体数据提出了一种新方法，通过分析大规模回应和建立统计模型，深入了解了个体对交通可及性、社会经济差距和公共基础设施的感知。研究结果显示，女性、亚洲裔和经历高交通流量的个体更关注交通可及性，而具有社会经济劣势的个体更关注公共交通问题并表达更强烈的关切。 |
| [^217] | [Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications](https://arxiv.org/abs/2402.01681) | 在表情符号研究中，我们评估了ChatGPT在处理注释和下游任务中的有效性。我们的研究结果表明ChatGPT可以作为一个可行的替代人类注释者的工具，有效地解释表情符号。 |
| [^218] | [Large Language Model based Multi-Agents: A Survey of Progress and Challenges](https://arxiv.org/abs/2402.01680) | 大型语言模型(LLMs)已广泛应用于各种任务。基于LLMs的多智能体系统在复杂问题求解和世界模拟方面取得了重要进展。这篇综述给出了基于LLMs的多智能体系统的重要方面和面临的挑战的全面讨论。 |
| [^219] | [StickerConv: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | 本文介绍了StickerConv代理(Agent4SC)，该代理通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。为了利用构建的多模态共情对话数据集StickerConv，作者提出了PErceive and Generate Stickers (PEGS)模型，该模型能够生成情境相关和情感丰富的回应。 |
| [^220] | [Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge](https://arxiv.org/abs/2402.01677) | 本文提出了一种新型本体嵌入方法EIKE，通过整合外延知识和内涵知识，在外延空间和内涵空间中表示本体，并采用基于几何的方法和预训练的语言模型对实例、概念和关系进行嵌入建模。 |
| [^221] | [Language models align with human judgments on key grammatical constructions](https://arxiv.org/abs/2402.01676) | 本研究通过对比评估发现，大型语言模型（LLMs）在俘获人类行为方面的表现非常出色，不仅整体准确率高，而且能够捕捉到人类语言判断中的细微差异。 |
| [^222] | [Tracing the Genealogies of Ideas with Large Language Model Embeddings](https://arxiv.org/abs/2402.01661) | 本文提出了一种使用大型语言模型嵌入追踪思想系谱的新方法，在大型语料库中检测思想影响。通过结合通用文本嵌入、句子嵌入和抽象意义表示图的方法，可以高效搜索实质上相似的思想和思想影响的迹象。 |
| [^223] | [L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs](https://arxiv.org/abs/2402.01643) | 本文介绍了L-Tuning，一种在自然语言推理（NLI）框架内的高效微调方法，通过对预训练的LLM中的标签标记进行微调，提高了训练准确性和效率，并增强了模型的训练细微差别。 |
| [^224] | [Detection of Machine-Generated Text: Literature Survey](https://arxiv.org/abs/2402.01642) | 这项论文对机器生成文本的检测进行了文献综述，指出了语言模型生成的虚假文本大量存在于公共领域，因此需要采取预防措施来应对其可能带来的危险影响。 |
| [^225] | [Universal Syntactic Structures: Modeling Syntax for Various Natural Languages](https://arxiv.org/abs/2402.01641) | 该研究提出了一种新颖的句法建模方法，通过分析多种自然语言的语料库，可能揭示了适用于所有自然语言的通用句法结构的存在。这对理解人类大脑中语言的运作方式以及相关学科的理论有重要意义。 |
| [^226] | [Evaluating Large Language Models for Generalization and Robustness via Data Compression](https://arxiv.org/abs/2402.00861) | 通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。 |
| [^227] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^228] | [Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better](https://arxiv.org/abs/2402.00263) | 我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动和多对比学习，在减少随机屏蔽引起的信息丢失的同时，进一步提升少样本和个体输入的性能。 |
| [^229] | [Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding](https://arxiv.org/abs/2402.00024) | LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。 |
| [^230] | [Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning](https://arxiv.org/abs/2401.17686) | 本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。 |
| [^231] | [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263) | 该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。 |
| [^232] | [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256) | 通过弱到强破解攻击，对手可以利用较小的不安全/对齐LLMs指导对显著较大的对齐LLMs进行破解，与解码较大的LLMs相比，其计算和延迟成本较小。 |
| [^233] | [Tradeoffs Between Alignment and Helpfulness in Language Models](https://arxiv.org/abs/2401.16332) | 本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。 |
| [^234] | [Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion](https://arxiv.org/abs/2401.11373) | 本文提出了一种名为TPRL的方法，通过生成具有挑战性的样本来改善自然语言处理模型的性能，以对抗攻击中的分布失真问题。该方法利用近端策略梯度自动生成对抗性样本，并通过Mutual Implication分数保持原始文本的语义含义。 |
| [^235] | [AI-as-exploration: Navigating intelligence space](https://arxiv.org/abs/2401.07964) | 这篇论文阐述了一个被忽视但重要的科学角色，即AI作为探索。它强调通过创建和研究智能系统来揭示可能与人类和动物的智能形式不同的候选构建模块。论文通过讨论人类和大型语言模型在组合新颖和创造性概念方面的能力，说明了AI作为探索的价值。 |
| [^236] | [Language Models Understand Numbers, at Least Partially](https://arxiv.org/abs/2401.03735) | 本研究表明，大型语言模型在某种程度上理解数字，可以通过压缩和编码的方式执行算术计算。 |
| [^237] | [Structured Probabilistic Coding](https://arxiv.org/abs/2312.13933) | 结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。 |
| [^238] | [Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs](https://arxiv.org/abs/2312.11282) | 该论文评估了当前最先进的大型语言模型（GPT-4）在知识图谱上的对话推理能力，提出了一种基于KG推理的LLM基准代理（LLM-ARK），该代理利用全文环境提示来实现精确和适应性强的KG路径预测，并采用近端策略优化算法进行训练。 |
| [^239] | [Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications](https://arxiv.org/abs/2312.06825) | 本文主要研究课堂人机交互中利用非语言行为和社交注视的应用。通过使用人类启发的社交注视模型，我们希望在机器人认知架构中实现更加流畅的社交互动。 |
| [^240] | [Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web](https://arxiv.org/abs/2311.18751) | 本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。 |
| [^241] | [ChatTraffic: Text-to-Traffic Generation via Diffusion Model](https://arxiv.org/abs/2311.16203) | 本研究提出了一种名为ChatTraffic的扩散模型，用于实现文本到交通生成。通过将生成模型与描述交通系统的文本结合，该方法能够解决传统交通预测方法中的两个挑战，并得到与真实交通数据一致的合成数据。 |
| [^242] | [Data Diversity Matters for Robust Instruction Tuning](https://arxiv.org/abs/2311.14736) | 数据多样性对鲁棒指令调整非常重要，我们提出了一种新算法(QDIT)，通过同时控制数据集的多样性和质量，我们深入研究了多样性和质量对指令调整性能的影响，并得出了两个关键观点。 |
| [^243] | [The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change](https://arxiv.org/abs/2311.12664) | DURel注释工具是一个在线的、开源的工具，通过人类和计算机的注释实现了对单词使用之间的语义接近度的测量，并提供了对语义聚类和语义变化的分析功能。 |
| [^244] | [Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers](https://arxiv.org/abs/2311.10642) | 本研究探索使用浅层前馈神经网络替代注意力机制，通过知识蒸馏方法训练，实验证明了这种"无注意力的Transformers"可以与原始架构的性能媲美，并揭示了其简化复杂架构的潜力。 |
| [^245] | [Divergences between Language Models and Human Brains](https://arxiv.org/abs/2311.09308) | 该论文系统地探索了语言模型（LMs）和人类大脑在语言处理方面的差异，发现在社交/情感智能和物理常识领域，LMs无法很好地捕捉到人类的表现，但在这些领域对LMs进行微调可以提高其性能。 |
| [^246] | [Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233) | 这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。 |
| [^247] | [Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR](https://arxiv.org/abs/2311.04534) | 在离散化令牌的ASR中，仅使用解码器的Transformer模型不需要使用损失遮蔽。取而代之的是，我们提出了一种名为平滑标签蒸馏的新方法，它在语音令牌上应用了带有平滑标签的KL散度损失，并且在实验证明效果优于损失遮蔽方法。 |
| [^248] | [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099) | 本文揭示了语言模型可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。作者提出了DARE技术来稀疏化参数并将多个同源模型合并为一个模型。实验证明，DARE可以轻松删除大部分参数并实现多任务融合。 |
| [^249] | [PhoGPT: Generative Pre-training for Vietnamese](https://arxiv.org/abs/2311.02945) | PhoGPT是一个用于越南语的生成式预训练模型系列，具有40亿参数的基础模型PhoGPT-4B以及其聊天变体PhoGPT-4B-Chat，展示了在越南语任务上优于之前的7亿参数模型的强大性能。 |
| [^250] | [Learning From Mistakes Makes LLM Better Reasoner](https://arxiv.org/abs/2310.20689) | 本研究探索了大型语言模型（LLMs）是否可以从错误中学习，类似于人类学习的过程，并通过引入错误纠正的数据对来改进LLMs的推理能力。实验结果表明，这种方法能够持续提升仅使用CoT进行微调后的性能。 |
| [^251] | [Guiding Language Model Math Reasoning with Planning Tokens](https://arxiv.org/abs/2310.05707) | 本论文介绍了一种通过引入规划标记来引导语言模型进行数学推理的方法。这种方法在保持推理过程中的一致性方面具有显著的准确性提升，而增加的训练参数很少。 |
| [^252] | [Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs](https://arxiv.org/abs/2309.09582) | Fabricator是一个开源工具集，用于生成带有Teacher LLMs标注训练数据。它通过零样本学习的方式，利用强大的LLM根据任务描述生成标注数据，可以用于训练下一阶段的自然语言处理模型。 |
| [^253] | [Context-aware Adversarial Attack on Named Entity Recognition](https://arxiv.org/abs/2309.08999) | 本研究关注命名实体识别任务，研究了上下文感知的对抗攻击方法，通过扰动最具信息量的单词来创建对抗样本，并使用不同的替换方法生成自然且可信的对抗示例。实验证明，这些方法比强基准方法更有效地欺骗模型做出错误预测。 |
| [^254] | [Rethinking STS and NLI in Large Language Models](https://arxiv.org/abs/2309.08969) | 这篇论文重新思考了在大型语言模型中的STS和NLI问题。通过在临床/生物医学领域评估性能，以及评估LLMs的预测置信度和捕捉集体人类意见的能力，发现这些问题在LLMs时代仍未得到妥善解决。 |
| [^255] | [LAraBench: Benchmarking Arabic AI with Large Language Models](https://arxiv.org/abs/2305.14982) | LAraBench是一个针对阿拉伯语自然语言处理和语音处理任务的基准测试平台，通过多种实验设置和性能衡量指标，证明最新模型通常表现优于大语言模型（LLMs）。 |
| [^256] | [3D Rotation and Translation for Hyperbolic Knowledge Graph Embedding](https://arxiv.org/abs/2305.13015) | 本研究引入了一种新的模型3H-TH，用于双曲知识图嵌入，可以同时捕捉关系模式，包括对称性、反对称性、反转、可交换组合、非可交换组合、层次结构和多重性。实验证明，该模型在低维空间的准确度、层次结构和其他关系模式方面优于现有模型，在高维空间中表现类似。 |
| [^257] | [Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training](https://arxiv.org/abs/2305.12793) | 本文提出了一种通过跨模态选择性自训练的方法，以解决在零射击端到端口语理解中的不平衡和标签噪声问题。 |
| [^258] | [A Reparameterized Discrete Diffusion Model for Text Generation](https://arxiv.org/abs/2302.05737) | 本文提出了一种重新参数化离散扩散模型，该模型在文本生成方面表现出更好的灵活性、训练技术和生成效果，实验证明其较现有的扩散模型有显著的改进。 |
| [^259] | [Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind](https://arxiv.org/abs/2211.04684) | 人们在阅读故事时，通过对虚构和真实人物的类比，可以快速理解新的虚构角色。本研究填补了现有研究中忽视的少样本和元学习的心智模型（ToM）的重要性。我们提供了一个新的NLP数据集ToM-in-AMC，该数据集提供了一个评价机器元学习心智模型的现实叙事理解场景。 |
| [^260] | [BoAT v2 - A Web-Based Dependency Annotation Tool with Focus on Agglutinative Languages](https://arxiv.org/abs/2207.01327) | BoAT v2是一种基于Web的依存性标注工具，专注于以粘聚性语言为重点的语言，可以实现多用户同时标注，为使用者提供良好的用户体验。 |
| [^261] | [SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity.](http://arxiv.org/abs/2401.17072) | 这项研究提出了一个名为SemScore的评估指标，通过语义文本相似度直接比较模型输出和黄金目标回应，用于评估指令调校大型语言模型。实验证明，SemScore指标在与人工评估的相关性方面表现优于其他评估指标。 |
| [^262] | [Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching.](http://arxiv.org/abs/2401.16895) | 本研究关注马耳他语这种混合语言，采用了一种新颖的数据集和分类器来提高跨语言转移能力，解决了混合语言文字差异的问题。 |
| [^263] | [Engineering A Large Language Model From Scratch.](http://arxiv.org/abs/2401.16736) | Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。 |
| [^264] | [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection.](http://arxiv.org/abs/2401.15222) | 本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。 |
| [^265] | [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty.](http://arxiv.org/abs/2401.15077) | EAGLE是一个无损加速语言模型推理的框架，通过在次顶层特征层面上自回归推理，并解决采样不确定性问题，实现了比传统方法更快3倍的速度。 |
| [^266] | [Parameter-Efficient Conversational Recommender System as a Language Processing Task.](http://arxiv.org/abs/2401.14194) | 本文将对话推荐系统作为一种语言处理任务进行建模，利用预训练的语言模型来编码项目、理解用户意图，通过语义匹配进行项目推荐，并生成对话。实验证明了该方法的有效性。 |
| [^267] | [Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding.](http://arxiv.org/abs/2401.13565) | 本文介绍了Mistral 7B大规模语言模型在马来西亚语言数据集上的预训练进展和性能优化，证明了继续预训练和扩展上下文长度对提升语言理解能力的有效性，并对比了其在Tatabahasa上的优越性能。 |
| [^268] | [SLANG: New Concept Comprehension of Large Language Models.](http://arxiv.org/abs/2401.12585) | 本研究提出了一个新的基准SLANG，旨在增强大型语言模型LLMs对互联网上新概念的理解能力，同时提出了一种基于因果推断的基准方法FOCUS，能帮助LLMs更好地理解新的短语和用法模式。 |
| [^269] | [Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric.](http://arxiv.org/abs/2401.11268) | 本研究介绍了一种使用质量估计度量来增强自动语音识别系统(XAI)的方法。实验证明了NoRefER度量在识别单词错误和提供有价值的模型行为见解方面的能力。研究还发现NoRefER在语料库构建和后期编辑工作流程中的实用性，表明其有潜力成为提高ASR系统效率和可解释性的关键工具。 |
| [^270] | [Cross-lingual Editing in Multilingual Language Models.](http://arxiv.org/abs/2401.10521) | 本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。 |
| [^271] | [Knowledge graph driven recommendation model of graph neural network.](http://arxiv.org/abs/2401.10244) | 提出了一种基于知识图谱的图神经网络推荐模型KGLN，通过合并节点特征、调整聚合权重和迭代演化，提高了个性化推荐的准确性和效果。在实验中相对于已有基准方法，KGLN在不同数据集上的AUC提高了0.3%至5.9%和1.1%至8.2%。 |
| [^272] | [LoMA: Lossless Compressed Memory Attention.](http://arxiv.org/abs/2401.09486) | LoMA是一种无损压缩的内存注意力方法，可以有效地处理长文本并减少资源消耗。 |
| [^273] | [DrawTalking: Building Interactive Worlds by Sketching and Speaking.](http://arxiv.org/abs/2401.05631) | 用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。 |
| [^274] | [Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations.](http://arxiv.org/abs/2401.04883) | 这篇论文介绍了一种基于大规模语言模型的多用户聊天机器人框架（MUCA），该框架支持群组讨论，并提供了三个主要模块来确定回应内容、时机和适当的接收者。同时，作者还提出了一个基于语言模型的多用户模拟器（MUS），用于模拟真实用户行为，以便更高效地测试和优化聊天机器人。 |
| [^275] | [Model Editing Can Hurt General Abilities of Large Language Models.](http://arxiv.org/abs/2401.04700) | 这项论文指出，模型编辑可能会改善模型的事实性，但会以降低模型的通用能力为代价。在这项研究中，作者通过评估四种编辑方法在两个大型语言模型上的表现，发现这些编辑方法往往忽视了对模型通用能力可能产生的负面影响。 |
| [^276] | [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning.](http://arxiv.org/abs/2401.01325) | 本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。 |
| [^277] | [Structured Packing in LLM Training Improves Long Context Utilization.](http://arxiv.org/abs/2312.17296) | 本论文研究了长上下文大型语言模型（LLM）中上下文利用不足的问题，并通过将相关文档纳入训练示例中来改进模型的困惑度。通过引入Structured Packing for Long Context (SPLiCe)方法，使用检索方法将最互相关文档汇集到单个训练上下文中，进一步提高了模型的性能。 |
| [^278] | [Do LLMs exhibit human-like response biases? A case study in survey design.](http://arxiv.org/abs/2311.04076) | 本研究以调查设计为案例研究，探讨了LLMs是否展现类似于人类的反应偏差的问题。 |
| [^279] | [Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics.](http://arxiv.org/abs/2311.01386) | 通过研究语言模型在与"语言幻觉"相关的判断中的行为，我们发现语言模型更容易受到结构依赖性的幻觉的影响，而在语义方面则较困难。 |
| [^280] | [An energy-based comparative analysis of common approaches to text classification in the Legal domain.](http://arxiv.org/abs/2311.01256) | 本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。 |
| [^281] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^282] | [DoGE: Domain Reweighting with Generalization Estimation.](http://arxiv.org/abs/2310.15393) | DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。 |
| [^283] | [ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding.](http://arxiv.org/abs/2310.12531) | ICU提出了一种解决视觉与语言建模中语言障碍的方法，通过将任务划分为图像字幕和语言理解两个阶段，将多语言处理负担转移到多语言语言模型上。实验结果显示，ICU在多个语言上取得了最先进的结果。 |
| [^284] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^285] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^286] | [Effective and Parameter-Efficient Reusing Fine-Tuned Models.](http://arxiv.org/abs/2310.01886) | 本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。 |
| [^287] | [MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models.](http://arxiv.org/abs/2309.13567) | 本研究利用大型语言模型在社交媒体上进行可解释的心理健康分析。针对解释性不足的问题，研究发现ChatGPT能够生成接近人类解释的分类结果。然而，LLMs在零 shot/few-shot 方式下的分类性能仍不理想。为了解决缺乏训练数据和开源LLMs的问题，研究建立了第一个多任务和多源的解释性心理健康指导数据集。 |
| [^288] | [Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation.](http://arxiv.org/abs/2309.10456) | 该论文提出了一种利用语义信息改进说话者分离的方法，通过引入口语理解模块提取语义信息并构建成对约束，并将其集成到说话者分离流程中，从而提高系统性能。 |
| [^289] | [Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model.](http://arxiv.org/abs/2309.09357) | 本研究利用大型语言模型（LLMs）来促进患者和医生之间的异步通信，通过访谈研究了解了他们对LLMs的需求，并构建了一个名为Talk2Care的LLM驱动的通信系统。 |
| [^290] | [An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis.](http://arxiv.org/abs/2309.08777) | 本文对自训练在情感分析中的实例选择策略进行了实证研究，研究了策略和超参数对自训练性能的影响。 |
| [^291] | [Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks.](http://arxiv.org/abs/2309.07794) | 本研究通过引入图像-文本辅助任务，有效地提高了社交媒体帖子的多模态分类，通过两个辅助损失函数对图像-文本表示进行调整，捕捉底层依赖关系和语义对应关系，实现了一致的改进。 |
| [^292] | [SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects.](http://arxiv.org/abs/2309.07445) | 本研究提出了SIB-200数据集，在200多种语言和方言中提供了一个大规模、全面的主题分类评估数据集。该数据集填补了自然语言理解领域中对评估数据集的缺乏，通过全监督、跨语言迁移和大型语言模型提示的评估，发现性能仍存在差距。 |
| [^293] | [Unsupervised Contrast-Consistent Ranking with Language Models.](http://arxiv.org/abs/2309.06991) | 无监督的对比一致排序与语言模型，通过训练一个受逻辑约束引导的探测模型，实现在多个语句中始终映射到对比的真-假极点的排序任务。 |
| [^294] | [Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity.](http://arxiv.org/abs/2309.06364) | 本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。 |
| [^295] | [The Anatomy of Conspirators: Unveiling Traits using a Comprehensive Twitter Dataset.](http://arxiv.org/abs/2308.15154) | 本研究通过构建一种全面的Twitter数据集，揭示了参与阴谋相关活动的用户的特点和行为特征，为阴谋论的检测提供了新的方法和依据。 |
| [^296] | [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature.](http://arxiv.org/abs/2308.12420) | 本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。 |
| [^297] | [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages.](http://arxiv.org/abs/2308.12038) | 本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。 |
| [^298] | [Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm.](http://arxiv.org/abs/2308.11767) | 本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。 |
| [^299] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^300] | [Bringing order into the realm of Transformer-based language models for artificial intelligence and law.](http://arxiv.org/abs/2308.05502) | 本文提供了第一个对基于Transformer的语言模型在法律领域的人工智能问题和任务中的方法的系统概述。文章旨在突出这一领域的研究进展，以进一步了解Transformer在支持法律流程中的AI成功贡献以及当前的局限性。 |
| [^301] | [Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance.](http://arxiv.org/abs/2308.04215) | 提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。 |
| [^302] | [DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI.](http://arxiv.org/abs/2307.10172) | DialogStudio是迄今为止最大且最多样化的对话数据集合，包含从开放领域对话到任务导向对话、自然语言理解、会话推荐、对话摘要和知识驱动对话的数据。它为对话研究和模型训练提供了丰富而多样化的资源。 |
| [^303] | [MorphPiece : Moving away from Statistical Language Representation.](http://arxiv.org/abs/2307.07262) | 本文提出了一种基于语言学动机的分词方案MorphPiece，并使用该方案训练了一个称为MorphGPT的语言模型。MorphGPT在语言建模以及各种NLP任务上都表现出了比传统模型更优异的性能。 |
| [^304] | [DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding.](http://arxiv.org/abs/2307.06924) | DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。 |
| [^305] | [Generating Benchmarks for Factuality Evaluation of Language Models.](http://arxiv.org/abs/2307.06908) | 该论文提出了一个名为FACTOR的方法，用于生成用于语言模型事实性评估的基准数据集。通过自动转换事实语料库，评估语言模型根据语料库生成真实事实的倾向与生成不正确陈述的能力。实验结果表明，该基准数据集的分数随模型大小增加而增加，在LM与检索方法结合时性能得到改善。困惑度和基准数据集分数之间存在相关性，但不总是一致。 |
| [^306] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^307] | [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding.](http://arxiv.org/abs/2306.17107) | LLaVAR是一个增强的视觉指令调整模型，通过使用文本丰富的图像数据，它能够显著提升在文本为基础的视觉问答数据集上的准确率。 |
| [^308] | [System-Level Natural Language Feedback.](http://arxiv.org/abs/2306.13588) | 本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了通过任务度量设计和语言模型提示设计，如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型，并展示了使用系统级别反馈和实例级别反馈的有效性。 |
| [^309] | [SqueezeLLM: Dense-and-Sparse Quantization.](http://arxiv.org/abs/2306.07629) | 本文提出了一种基于训练后的量化框架——SqueezeLLM，它不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。 |
| [^310] | [Test-Time Training on Nearest Neighbors for Large Language Models.](http://arxiv.org/abs/2305.18466) | 该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。 |
| [^311] | [CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation.](http://arxiv.org/abs/2305.17267) | CODET是一个对比方言的评估基准测试，用于评估机器翻译系统在处理方言变体时的表现，该基准测试包含九种不同语言的882个不同变体。 |
| [^312] | [How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data.](http://arxiv.org/abs/2305.14195) | 本论文提出了一种使用人类人口数据对语言模型进行评估的框架，并发现GPT-3.5在不同任务中表现不同，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。 |
| [^313] | [Understanding and Mitigating Spurious Correlations in Text Classification.](http://arxiv.org/abs/2305.13654) | 本文研究了深度学习模型容易利用训练集中存在但通常不成立的伪相关性的问题，并提出了一种邻域分析框架以解释语言模型如何利用伪相关性。通过一系列正则化方法NFL（不要忘记你的语言）避免了这种情况，并在实验中证明了其鲁棒性方面的显著改进。 |
| [^314] | [Smaller Language Models are Better Black-box Machine-Generated Text Detectors.](http://arxiv.org/abs/2305.09859) | 本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。 |
| [^315] | [Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages.](http://arxiv.org/abs/2305.05214) | 本文解决了极低资源语言到英语的机器翻译任务，利用密切相关的高资源语言的词汇相似性，注入噪声作为正则化器，使模型更能抵御词汇差异，从而更好地促进跨语言转移。 |
| [^316] | [Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks.](http://arxiv.org/abs/2304.13861) | 本文使用GPT-4和ChatGPT对低资源分类任务进行数据增强，通过简单的提示将小型标记数据集扩充为合成数据集，在保留原始标签分布或平衡分布的情况下，产生了良好的下游性能。在测试集上，GPT-4和ChatGPT表现出出色的零-shot性能，尤其在低资源设置中能够较好地识别罕见类别。 |
| [^317] | [Fundamental Limitations of Alignment in Large Language Models.](http://arxiv.org/abs/2304.11082) | 本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。 |
| [^318] | [REFINER: Reasoning Feedback on Intermediate Representations.](http://arxiv.org/abs/2304.01904) | REFINER 是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互，其在三个不同推理任务上取得了显着改进。 |
| [^319] | [Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR.](http://arxiv.org/abs/2211.11419) | 本论文提出了一种称为 SSC-Conformer 的块式模型，利用串行采样块自注意力机制提高块间交互效率，同时保持线性复杂度，将块卷积与因果卷积相结合以达到更好的 CER 表现，实验结果表明 SSC-Conformer 在 AISHELL-1 基准测试中取得了最新的流式 E2E ASR 性能水平。 |

# 详细

[^1]: TravelPlanner: 一种用于自然语言代理的真实世界规划基准

    TravelPlanner: A Benchmark for Real-World Planning with Language Agents

    [https://rss.arxiv.org/abs/2402.01622](https://rss.arxiv.org/abs/2402.01622)

    本论文提出了一种用于自然语言代理的新的规划基准TravelPlanner，它关注于旅行规划这一常见的真实世界场景。经过全面评估，发现目前的语言代理仍无法处理如此复杂的规划任务，即使最先进的GPT-4也只能达到0.6%的成功率。

    

    自规划起初就是人工智能的核心追求之一，但早期的人工智能代理大多集中在受限环境下，因为缺乏进行人类水平规划所需的许多认知基础。最近，由大型语言模型（LLM）驱动的语言代理展现出了工具使用和推理等有趣的能力。这些语言代理能否在超出先前人工智能代理范围的更复杂环境中进行规划？为了推进这项研究，我们提出了TravelPlanner，这是一个新的规划基准，专注于旅行规划这个常见的真实世界规划场景。它提供了一个丰富的沙盒环境，各种用于访问近400万个数据记录的工具，并包含1225个精心策划的规划意图和参考计划。综合评估显示，当前的语言代理尚不具备处理如此复杂的规划任务的能力-即使是GPT-4的成功率也只有0.6%。

    Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. La
    
[^2]: StepCoder: 使用强化学习从编译器反馈中改进代码生成

    StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback

    [https://rss.arxiv.org/abs/2402.01391](https://rss.arxiv.org/abs/2402.01391)

    StepCoder是一个基于强化学习的代码生成框架，通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，并使用细粒度优化来提高模型的效果。

    

    大型语言模型的发展极大地推动了代码生成领域。以前的工作将强化学习与编译器反馈结合起来，探索语言模型的输出空间，以提高代码生成质量。然而，为了满足复杂的人类要求，语言模型生成的代码往往很长，这使得强化学习的探索成为一项挑战。此外，由于单元测试可能无法涵盖复杂代码，使用这些未执行的代码片段来优化语言模型是无效的。为了解决这些问题，我们引入了StepCoder，这是一个用于代码生成的新型强化学习框架，包含两个主要组件：CCCS通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，而FGO通过屏蔽未执行的代码段来提供细粒度优化，仅优化模型。此外，我们还构建了APPS+数据集用于强化学习训练，该数据集经过手动验证确保质量。

    The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ens
    
[^3]: LoTR: 低张量秩权重自适应

    LoTR: Low Tensor Rank Weight Adaptation

    [https://rss.arxiv.org/abs/2402.01376](https://rss.arxiv.org/abs/2402.01376)

    LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。

    

    在本文中，我们将大型语言模型（LLM）上的低秩适应（LoRA）思想推广和扩展，这些模型基于Transformer架构。广泛使用的LoRA类方法是基于梯度更新的矩阵分解。我们引入了LoTR，一种新颖的LLM参数高效调优方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器都由三个矩阵的乘积构成，而张量结构是由这个乘积的左右乘子在层之间共享引起的。通过对低秩张量表示的一系列层同时压缩，LoTR能够比LoRA在特别是对于深层模型具有更好的参数效率。此外，核心张量不依赖于原始权重维度，可以任意缩小，从而实现非常廉价和快速的下游调优。

    In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
    
[^4]: CABINET: 表格问答系统的基于内容相关性的噪声降低方法

    CABINET: Content Relevance based Noise Reduction for Table Question Answering

    [https://rss.arxiv.org/abs/2402.01155](https://rss.arxiv.org/abs/2402.01155)

    CABINET是一个用于表格问答系统的基于内容相关性的噪声降低方法，通过加权处理表格内容并生成解析语句，使得大型语言模型能够专注于相关表格数据而抑制无关信息的干扰。

    

    大型语言模型（LLMs）的表格理解能力通过对表格的问答任务进行了广泛研究。通常，只有表格的一小部分与给定问题的答案相关。不相关的部分会产生噪声和干扰信息，导致LLMs的性能下降。为了缓解这个问题，我们提出了CABINET（基于内容相关性的表格问答噪声降低方法）- 一个能够让LLMs专注于相关表格数据并抑制无关信息的框架。CABINET包括一个无监督的相关性评分器（URS），与问答LLM差异性训练，根据其与输入问题的相关性对表格内容进行加权处理后再输入问答LLM（QA LLM）。为了进一步辅助相关性评分器，CABINET利用一个弱监督模块生成一个解析语句，描述行和列的标准。

    Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns r
    
[^5]: "AccentFold：通过零样本ASR适应探索非洲口音之旅"

    AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents

    [https://rss.arxiv.org/abs/2402.01152](https://rss.arxiv.org/abs/2402.01152)

    "AccentFold"是一种通过利用学习到的口音嵌入之间的空间关系来改进ASR的方法，通过探索性分析语音嵌入，揭示非洲口音之间的有趣关系，并发现了Ethnologue先前未经描述的口音关系。通过实证评估，证明了该方法的有效性。

    

    尽管语音识别方面取得了一些进展，但口音语音依然具有挑战性。虽然先前的方法主要集中在建模技术或创建有口音的语音数据集上，但由于非洲口音的多样性和相关的预算限制，收集到足够的数据仍然不可行。为了解决这些挑战，我们提出了一种方法——"AccentFold"，它利用了学习到的口音嵌入之间的空间关系来改进下游的自动语音识别（ASR）。我们对代表100多种非洲口音的语音嵌入进行了探索性分析，揭示了有趣的口音空间关系，突显出地理和亲缘相似性，捕捉到从语音中经验性地学习到的一致的音系和形态学规律。此外，我们还发现了Ethnologue先前未经描述的口音关系。通过实证评估，我们证明了该方法的有效性。

    Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose \textit{AccentFold}, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness o
    
[^6]: 语言模型作为归纳推理器

    Language Models as Inductive Reasoners

    [https://rss.arxiv.org/abs/2212.10923](https://rss.arxiv.org/abs/2212.10923)

    该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。

    

    归纳推理是人类智能的核心组成部分。在计算机科学中的归纳推理研究中，形式语言被用作知识（事实和规则）的表示。然而，形式语言会给归纳推理带来系统性问题，例如无法处理自然语言这样的原始输入、对错误标记的数据敏感以及处理模糊输入的能力不足。为此，我们提出了一种新的归纳推理范式（任务），即从自然语言事实中归纳出自然语言规则，并创建了一个被称为DEER的数据集，其中包含1.2k个规则-事实对，规则和事实以自然语言书写。还提出并分析了用于评估此任务的新的自动评估指标。通过DEER，我们研究了一种现代的归纳推理方法，其中我们使用自然语言作为知识的表示而不是形式语言，并使用预训练的语言模型。

    Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
    
[^7]: 没关系：大语言模型中的指令覆盖和调节

    Nevermind: Instruction Override and Moderation in Large Language Models

    [https://arxiv.org/abs/2402.03303](https://arxiv.org/abs/2402.03303)

    大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。

    

    鉴于近期大语言模型（LLMs）的令人印象深刻的能力，我们对最流行的专有模型和不同大小的开源模型进行了调查和基准测试，以解决在冲突情况下的明确指令遵循任务，例如覆盖。这些包括模型在其权重中覆盖知识的能力，覆盖（或调节）提示中提取的知识的能力，以及进行完全越狱的能力。实验表明，可以改进指令遵循的几个关键发现 - 较大的模型在遵循覆盖内部和上下文指令方面表现最佳，并且非常服从，甚至有些过度。当通过绳索扩展来扩展到更长的上下文时，需要保持与困惑边缘的显著缓冲区，以保持指令遵循能力。最后，我们观察到指令遵循的改善，以及随之而来的指令覆盖/越狱。

    Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja
    
[^8]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^9]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^10]: 交易，还是不交易（或者谁知道）？使用大型语言模型预测对话中的不确定性

    Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models

    [https://arxiv.org/abs/2402.03284](https://arxiv.org/abs/2402.03284)

    本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。

    

    有效的对话者考虑他人的不确定目标、信念和情绪。但即使是最佳的人类对话者也无法完美地预测对话的轨迹。语言模型能够多好地表示对话中固有的不确定性？我们提出了FortUne Dial，这是“对话预测”任务的扩展：评估不仅仅以准确度为标准，还采用了对不确定性敏感的度量方法，有效地使个别实例可以放弃。我们研究了语言模型可能表示结果不确定性的两种方式（内部使用分数和直接使用令牌），并提出了改进两种表示的校准的微调策略。对八个困难的谈判语料库进行的实验表明，我们提出的微调策略（一种传统的监督策略和一种离线策略强化学习策略）可以使较小的开源模型校准得上与其尺寸相当的预训练模型。

    Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si
    
[^11]: 想法的不确定性：不确定性感知规划增强大型语言模型的信息搜索能力

    Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

    [https://arxiv.org/abs/2402.03271](https://arxiv.org/abs/2402.03271)

    通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。

    

    在面对不确定性时，寻求信息的能力至关重要。在许多实际应用中，比如医学诊断和故障排除，解决任务所需的信息不是初始给定的，而需要通过询问后续问题来主动寻求（例如，医生向患者询问症状的更多细节）。在这项工作中，我们引入了思想的不确定性（UoT），一种算法将大型语言模型的能力与主动提问信息的能力相结合。UoT结合了1）不确定性感知仿真方法，使模型能够模拟可能的未来场景，并估计其发生的可能性；2）基于不确定性的奖励机制，激励模型寻求信息；3）奖励传播方案，以最大化预期奖励的方式选择最佳的问题提问方式。在医学诊断、故障排除和'20的实验中。

    In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
    
[^12]: ISPA: 用于转录动物声音的跨物种语音音标

    ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds

    [https://arxiv.org/abs/2402.03269](https://arxiv.org/abs/2402.03269)

    本文介绍了ISPA（跨物种语音音标），这是一种精确、简洁、可解释的系统，用于将动物声音转录为文本。通过将动物声音表示为文本，我们有效地将其视为一种“外语”，并展示了已建立的人类语言机器学习范例和模型（如语言模型）能够成功应用于提高性能。

    

    传统上，生物声学依赖谱图和连续的每帧音频表示来分析动物声音，并作为机器学习模型的输入。与此同时，国际音标（IPA）系统提供了一种可解释的、与语言无关的方法来转录人类语音声音。本文介绍了ISPA（跨物种语音音标），这是一种精确、简洁、可解释的系统，用于将动物声音转录为文本。我们比较了基于声学和基于特征的方法来转录和分类动物声音，并证明了它们与使用连续、稠密音频表示的基准方法具有可比性的性能。通过将动物声音表示为文本，我们有效地将其视为一种“外语”，并展示了已建立的人类语言机器学习范例和模型（如语言模型）能够成功应用于提高性能。

    Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models. Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds. In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text. We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations. By representing animal sounds with text, we effectively treat them as a "foreign language," and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance.
    
[^13]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^14]: 技能集优化：通过可转移技能增强语言模型行为

    Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills

    [https://arxiv.org/abs/2402.03244](https://arxiv.org/abs/2402.03244)

    本论文提出了一种技能集优化（SSO）方法，通过构建和完善可转移的技能集来提高大型语言模型（LLM）的性能。该方法通过提取高奖励的共同子轨迹，生成子目标和说明，并在上下文中提供给LLM演员，以强化行为。实验结果显示，SSO在不同环境中能够优化技能集，并实现上下文策略改进。

    

    近期，大型语言模型（LLMs）已被用于交互环境中的顺序决策。然而，利用环境奖励信号来不断改进LLM演员的表现并不简单。我们提出了技能集优化（SSO）来通过构建和完善可转移技能集来提高LLM演员的性能。SSO通过提取具有高奖励的共同子轨迹并生成子目标和说明来构建技能。这些技能在上下文中提供给LLM演员，以强化具有高奖励的行为。然后，SSO通过修剪不再产生高奖励的技能来进一步完善技能集。我们在经典视频游戏NetHack和文本环境ScienceWorld中评估了我们的方法，以展示SSO优化技能集并进行上下文策略改进的能力。在我们的自定义NetHack任务中，SSO的性能超过基准方法40%，并超过了先前的最新状态。

    Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-
    
[^15]: JOBSKAPE: 一个用于生成合成职位招聘信息以增强技能匹配的框架

    JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching

    [https://arxiv.org/abs/2402.03242](https://arxiv.org/abs/2402.03242)

    本文介绍了一个名为JobSkape的框架，用于生成合成职位招聘信息以增强技能匹配。通过创建一个全面的合成职位招聘信息数据集SkillSkape，并使用大型语言模型进行技能提取和匹配任务，该框架解决了以前合成数据集存在的限制，并在离线度量指标和真实世界数据上取得了有希望的结果。

    

    最近的技能匹配方法使用合成训练数据进行分类或相似性模型训练，取得了有希望的结果，减少了耗时且昂贵的注释需求。然而，以前的合成数据集存在限制，例如每个句子只有一个技能并且通常由短句组成。本文介绍了JobSkape，一个解决这些限制的生成合成数据的框架，专门用于增强技能到分类系统的匹配。在这个框架内，我们创建了SkillSkape，一个专门为技能匹配任务量身定制的全面开源合成职位招聘信息数据集。我们介绍了几个离线度量指标，表明我们的数据集类似于真实世界的数据。此外，我们还提出了一个使用大型语言模型（LLMs）进行技能提取和匹配任务的多步骤流程，并与已知的监督方法进行了基准测试。我们概述了在真实世界数据上的下游评估结果。

    Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world 
    
[^16]: "定义你的术语"：通过定义增强高效的攻击性言论分类

    "Define Your Terms" : Enhancing Efficient Offensive Speech Classification with Definition

    [https://arxiv.org/abs/2402.03221](https://arxiv.org/abs/2402.03221)

    这项工作通过定义术语来增强高效的攻击性言论分类，提出了一种联合嵌入架构，并利用元学习方法来增强其可靠和高效的检测。实验结果表明，该模型在多个数据集上取得了很好的分类效果，并提供了对抗资源稀缺的训练策略的案例研究。

    

    社交媒体渠道中攻击性内容的传播引起了研究界的关注。多项研究提出了各种在语义上相关但微妙不同的攻击性言论类别。在这项工作中，我们探索了元学习方法，利用攻击性言论语料库的多样性来增强其可靠和高效的检测。我们提出了一种联合嵌入架构，通过Prototypical Network结合输入的标签和定义进行分类。我们的模型在4个数据集上至少达到了最大F1-score的75%，同时只使用了不到可用训练数据的10%。我们的实验结果还提供了一个案例研究，说明了对抗资源稀缺的训练策略的价值。

    The propagation of offensive content through social media channels has garnered attention of the research community. Multiple works have proposed various semantically related yet subtle distinct categories of offensive speech. In this work, we explore meta-earning approaches to leverage the diversity of offensive speech corpora to enhance their reliable and efficient detection. We propose a joint embedding architecture that incorporates the input's label and definition for classification via Prototypical Network. Our model achieves at least 75% of the maximal F1-score while using less than 10% of the available training data across 4 datasets. Our experimental findings also provide a case study of training strategies valuable to combat resource scarcity.
    
[^17]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^18]: 同性质，聚类和分类器

    Isotropy, Clusters, and Classifiers

    [https://arxiv.org/abs/2402.03191](https://arxiv.org/abs/2402.03191)

    同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。

    

    最近，关于嵌入空间是否均匀利用所有维度（即是否具有同性质）的问题引起了讨论。有证据支持和反对在嵌入空间中实施同性质。在本文中，我们强调同性质对嵌入空间的要求与聚类的存在不兼容，这也对线性分类目标产生了负面影响。我们通过实验证明了这个事实，并用它来阐明文献中的先前结果。

    Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
    
[^19]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^20]: CIDAR: 阿拉伯文的文化相关指令数据集

    CIDAR: Culturally Relevant Instruction Dataset For Arabic

    [https://arxiv.org/abs/2402.03177](https://arxiv.org/abs/2402.03177)

    CIDAR是第一个由人工评审对齐文化的阿拉伯指令调优数据集，目的是解决现有指令数据集对西方文化的固有偏见所带来的影响，对于丰富将语言模型与阿拉伯文化对齐的研究工作具有重要意义。

    

    指令调优已成为教授大型语言模型遵循指令的一种重要方法。然而，现有的指令数据集主要面向英语或者来源于以英语为主导的语言模型，导致对西方文化的固有偏见。这种偏见对阿拉伯文等非英语语言的语言结构产生了重大影响，阿拉伯文反映了阿拉伯地区多样文化的独特语法。本文通过引入CIDAR，即第一个由人工评审对齐文化的阿拉伯指令调优数据集（https://hf.co/datasets/arbml/CIDAR），来解决这一限制。CIDAR包含了代表阿拉伯地区的10000个指令与输出对。我们通过对比分析其他模型在其他数据集上的调优结果，讨论了CIDAR的文化相关性。实验证明CIDAR可以帮助丰富将语言模型与阿拉伯文化对齐的研究工作。所有代码都可在...上找到。

    Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at 
    
[^21]: 多模态：文本和图像的多模态理解排行榜

    Multi: Multimodal Understanding Leaderboard with Text and Images

    [https://arxiv.org/abs/2402.03173](https://arxiv.org/abs/2402.03173)

    Multi是一个多模态理解的排行榜，提供了一个综合数据集，评估多模态大型语言模型对理解复杂图表和科学问题的能力。它兼具准确和开放式的回答形式，挑战MLLM的各种任务，并包含超过18,000个问题。

    

    多模态大型语言模型（MLLM）的快速进展强调了向学术界引入具有挑战性而又真实的基准的需求。现有的基准主要关注简单的自然图像理解，但Multi成为了MLLM的尖端基准，提供了一个综合性的数据集，用于评估MLLM对理解复杂图表和科学问题的能力。该基准反映了当前真实的考试风格，提供多模态的输入，并要求准确或开放式的回答，类似于现实中的学校考试。它通过各种任务挑战MLLM，从公式推导到图像细节分析，以及跨模态推理。Multi包括超过18,000个问题，重点关注不同格式的基于科学的问答。我们还引入了Multi-Elite，一个包含500个问题的子集，用于测试MLLM的极端情况，以及Multi-Extend，通过超过4..。

    Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
    
[^22]: 通过对多样标签嵌入进行注意力的精确和良好校准的ICD代码分配

    Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings

    [https://arxiv.org/abs/2402.03172](https://arxiv.org/abs/2402.03172)

    本论文提出了一种新颖的方法来实现自动化ICD编码，通过在文本中使用注意力机制的方法来精确分配ICD代码。实验结果表明，该方法在ICD编码中优于当前最先进的模型，并且标签嵌入机制对模型性能也有显著贡献。

    

    尽管国际疾病分类（ICD）已在全球范围内得到采用，但将ICD代码手动分配给临床文本耗时、容易出错且昂贵，这促使了自动化方法的发展。本文描述了一种用于自动ICD编码的新方法，结合了先前相关工作的几个思想。我们特别使用了一个强大的基于Transformer的模型作为文本编码器，并且为处理冗长的临床叙述，我们探索了将基础编码器模型改造成为Longformer，或者将文本分成多个块并独立处理每个块的方法。编码器产生的表示与一个标签嵌入机制相结合，该机制探索了多样的ICD代码近义词。对MIMIC-III数据集的不同划分进行的实验表明，所提出的方法在ICD编码中优于当前最先进的模型，标签嵌入对良好性能的贡献也是显著的。我们的方法还...

    Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also 
    
[^23]: 马格里布情感分析器上的同形异义词攻击研究

    Homograph Attacks on Maghreb Sentiment Analyzers

    [https://arxiv.org/abs/2402.03171](https://arxiv.org/abs/2402.03171)

    同形异义词攻击对马格里布情感分析器造成了严重影响，将其性能从F1得分0.95降低到0.33。本研究主要旨在强调LLMs的弱点，并优先考虑机器学习的道德和责任。

    

    我们研究了同形异义词攻击对马格里布北非国家不同阿拉伯方言情感分析（SA）任务的影响。当数据以“阿拉伯字母拼音”书写时，同形异义词攻击导致变压器分类性能从F1得分0.95下降到0.33，减少了65.3%。本研究的目标是凸显LLMs的弱点，并优先考虑机器学习的道德和责任。

    We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.
    
[^24]: 用于ABSA中句子困难度预测的语言特征

    Linguistic features for sentence difficulty prediction in ABSA

    [https://arxiv.org/abs/2402.03163](https://arxiv.org/abs/2402.03163)

    本研究探讨了面向方面的情感分析中句子困难度的定义。通过实验研究了领域多样性和句法多样性对句子困难度的影响，并使用一组分类器识别了最困难的句子。

    

    自然语言理解的一个挑战是处理句子的主观性，这些句子可能表达意见和情感，增加了复杂性和细微差别。情感分析是一个旨在从文本中提取和分析这些主观元素的领域，可以应用在不同的粒度级别，如文档、段落、句子或方面。面向方面的情感分析是一个经过研究的课题，有很多可用的数据集和模型。然而，对于面向方面的情感分析来说，什么因素使一个句子变得困难并没有清晰的定义。在本文中，我们通过对三个数据集：“笔记本电脑”、“餐馆”和“MTSC”（多目标依赖情感分类）以及这三个数据集的合并版本进行实验，探讨了这个问题。我们研究了领域多样性和句法多样性对困难度的影响。我们使用一组分类器来识别最困难的句子并分析它们的特征。

    One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: "Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their c
    
[^25]: Video-LaVIT：统一的视频语言预训练及解耦的视觉-运动标记化方法

    Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization

    [https://arxiv.org/abs/2402.03161](https://arxiv.org/abs/2402.03161)

    这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。

    

    鉴于多模态大型语言模型(LLMs)的最新进展，越来越多的关注如何将其从图像-文本数据扩展到更具信息价值的现实世界视频。与静态图像相比，视频在有效的大规模预训练中面临着独特的挑战，原因在于需要对其时空动态进行建模。本文针对视频-语言预训练中的这些限制，提出了一种高效的视频分解方法，将每个视频表示为关键帧和时间运动。然后，使用设计良好的标记器将视觉和时间信息离散化为少量标记，并将其适应于LLM，从而实现对视频、图像和文本的统一生成预训练。在推理过程中，从LLM生成的标记被仔细恢复到原始的连续像素空间，以生成各种视频内容。我们提出的框架既能理解又能生成图像和视频内容，并通过在13个任务上的竞争性表现加以证明。

    In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13
    
[^26]: 社会语言学信息的解释性: 以Hinglish情感分类为例的案例研究

    Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification

    [https://arxiv.org/abs/2402.03137](https://arxiv.org/abs/2402.03137)

    通过研究Hinglish情感分类，我们发现预训练语言模型能够学习到语言选择与情感表达之间的关联，尤其是在混合语言数据存在时。这对于情感分类的解释性具有重要意义。

    

    情感分类是自然语言处理中一项具有挑战性的任务，因为语言表达具有固有的特殊性和主观性，尤其是在混合语言数据中。预训练语言模型（PLMs）已经在许多任务和语言中取得了很高的性能，但是否这些模型能够学习和适应不同语言间情感表达的差异仍然有待考验。社会语言学研究表明，Hinglish说话者在表达消极情绪时转用印地语，在表达积极情绪时转用英语。为了了解语言模型是否能学习这些关联，我们研究了3个PLMs在一个Hinglish情感分类数据集上语言对情感预测的影响。通过使用LIME和基于词元的语言ID，我们发现模型确实学习到了语言选择和情感表达之间的关联。此外，当任务特定数据稀缺时，预训练模型中存在混合语言数据可以增强这种学习。我们还得出结论，使用社会语言学信息可以提高情感分类的解释性。

    Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also concl
    
[^27]: 用于跨语言标签投影的约束解码

    Constrained Decoding for Cross-lingual Label Projection

    [https://arxiv.org/abs/2402.03131](https://arxiv.org/abs/2402.03131)

    本文提出了一种解决零样本跨语言迁移学习中翻译质量下降问题的方法。

    

    在没有标记训练数据的情况下，利用多语言LLM进行零样本跨语言迁移已成为一种流行的学习范式，用于低资源语言。然而，在涉及对单词和短语进行细粒度预测的NLP任务中，零样本跨语言迁移学习的性能远远落后于监督微调方法。因此，通常利用翻译和标签投影来进一步提高性能，具体来说(1)将可用的以及带有黄金标签的训练数据从高资源语言(例如英语)翻译到低资源语言，和/或(2)将低资源语言中的测试数据翻译成高资源语言进行推理，然后将预测的跨度级别标签投射回原始测试数据。然而，最先进的基于标记的标签投影方法由于在输入到翻译模型的过程中注入了额外标记，导致翻译质量下降。本文中，我们提出了一种解决这个问题的方法。

    Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we e
    
[^28]: 基于意图的提示校准：用合成边界情况增强提示优化

    Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases

    [https://arxiv.org/abs/2402.03099](https://arxiv.org/abs/2402.03099)

    该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。

    

    由于大型语言模型（LLMs）对给定提示的高度敏感性和文本任务指令的固有歧义，提示工程是一项具有挑战性和重要性的任务。通过采用一个包含上次试验结果的元提示并提出改进的提示，最近的研究表明LLMs自动进行提示工程的能力。然而，这需要一个高质量的基准来比较不同的提示，在许多实际应用场景中获取高质量的基准是困难且昂贵的。在这项工作中，我们引入了一种新的自动提示工程方法，使用校准过程来迭代地优化与用户意图相符的提示。在优化过程中，系统联合生成边界用例的合成数据，并根据生成的数据集进行提示优化。我们证明了我们方法的有效性。

    Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our
    
[^29]: 多语言Transformer和BERTopic用于短文本主题建模：塞尔维亚语案例

    Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian

    [https://arxiv.org/abs/2402.03067](https://arxiv.org/abs/2402.03067)

    本文介绍了BERTopic在塞尔维亚语短文本中的首次应用，结果显示BERTopic可以产生丰富的主题，即使是部分预处理的文本。与传统方法相比，BERTopic在主题数量不受限制时提供了更多信息和新的见解。

    

    本文介绍了BERTopic这种最先进的主题建模技术在具有丰富形态语言的短文本中的首次应用结果。我们使用三个多语言嵌入模型以及两种文本预处理水平（部分和完全）对塞尔维亚语的部分预处理的短文本进行了BERTopic的性能评估。我们还将其与LDA和NMF在完全预处理文本上进行了比较。实验使用了一个表达对COVID-19疫苗疑虑的推文数据集。我们的结果表明，通过适当的参数设置，BERTopic即使应用于部分预处理的短文本，也能产生信息丰富的主题。当在两种预处理场景下应用相同的参数时，部分预处理文本的性能下降很小。与LDA和NMF相比，从关键词来看，BERTopic提供了更丰富的主题，并在主题数量不受限制时提供了新的见解。

    This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this p
    
[^30]: 多语言马来西亚嵌入：利用大型语言模型进行语义表示

    Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations

    [https://arxiv.org/abs/2402.03053](https://arxiv.org/abs/2402.03053)

    本文提出了一种基于大型语言模型的多语言马来西亚嵌入方法，通过微调Llama2和Mistral模型，在语义相似性和RAG任务中取得了显著的性能提升。

    

    在这项工作中，我们对Llama2和Mistral两种马来西亚语言模型进行了全面探索，并在涉及负正组对的嵌入任务中进行了微调。我们发布了两个专门针对语义相似性和检索辅助生成（RAG）的模型。在语义相似性方面，我们的6亿参数Llama2模型在b.cari.com.my、c.cari.com.my、马来西亚新闻和马来西亚Twitter测试集的所有recall@k指标上都优于OpenAI的text-embedding-ada-002。在RAG模型领域，我们的方法在马来西亚环境中与OpenAI的text-embedding-ada-002相竞争。值得注意的是，我们的20亿参数Llama2模型在“Melayu”关键词研究论文数据集的Recall@5、Recall@10上取得了卓越表现，并在lom.agc.gov.my数据集的Recall@3、Recall@5和Recall@10上表现出色。这些发现强调了我们的微调策略的有效性，并突显了在语义相似性和RAG任务中的性能提升。

    In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All 
    
[^31]: 尼泊尔自动语音识别系统现状的综合研究

    A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems

    [https://arxiv.org/abs/2402.03050](https://arxiv.org/abs/2402.03050)

    尼泊尔自动语音识别系统的研究现状进行了全面调查，发现尼泊尔语言的语言模型和声学模型的研究还需进一步加强。

    

    本文对尼泊尔自动语音识别系统领域的研究进行了调查。本次调查的主要目标是对迄今为止完成的尼泊尔自动语音识别系统的工作进行全面回顾，探索使用的不同数据集，研究所使用的技术，并考虑实施尼泊尔自动语音识别系统时遇到的障碍。与全球不断增长的语音识别相关研究的趋势相一致，尼泊尔与ASR相关的项目数量也在增加。然而，与拥有丰富资源的语言相比，尼泊尔语言的语言模型和声学模型的研究尚未得到足够的关注。在此背景下，我们提供了一个框架以及对未来研究方向的指导。

    In this paper, we examine the research conducted in the field of Nepali Automatic Speech Recognition (ASR). The primary objective of this survey is to conduct a comprehensive review of the works on Nepali Automatic Speech Recognition Systems completed to date, explore the different datasets used, examine the technology utilized, and take account of the obstacles encountered in implementing the Nepali ASR system. In tandem with the global trends of ever-increasing research on speech recognition based research, the number of Nepalese ASR-related projects are also growing. Nevertheless, the investigation of language and acoustic models of the Nepali language has not received adequate attention compared to languages that possess ample resources. In this context, we provide a framework as well as directions for future investigations.
    
[^32]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^33]: SIDU-TXT: 一种具有整体评估方法的NLP可解释的人工智能算法

    SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach

    [https://arxiv.org/abs/2402.03043](https://arxiv.org/abs/2402.03043)

    本论文探究了可解释的人工智能（XAI）方法在文本领域的适用性，并将相似度差异和独特性（SIDU）方法扩展到文本数据，提供了对模型预测关键的有上下文意义的文本元素的解释。本研究采用了一个综合的三层评估框架来评估XAI方法。

    

    可解释的人工智能（XAI）有助于解读“黑盒”模型。虽然已经提出并评估了多种主要用于图像领域的方法，但解释性在文本领域中的探索仍然是一个不断增长的研究领域。本文探讨了XAI方法在文本领域中的适用性。在这个背景下，我们扩展了一种被认为在基于图像的分类中定位整个显著区域能力优越的XAI方法——相似度差异和独特性（SIDU）的方法，将其应用于文本数据。扩展方法SIDU-TXT利用来自“黑盒”模型的特征激活图生成热力图，以词为单位提供解释，突出显示对模型预测至关重要的有上下文意义的文本元素。鉴于还没有统一的XAI评估标准，本研究应用了一个综合的三层评估框架：功能基础、人类基础和应用基础评估。

    Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded,
    
[^34]: 自动组合样本选择策略用于少样本学习

    Automatic Combination of Sample Selection Strategies for Few-Shot Learning

    [https://arxiv.org/abs/2402.03038](https://arxiv.org/abs/2402.03038)

    本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。

    

    在少样本学习中，如元学习、少样本微调或上下文学习中，用于训练模型的有限样本数量对整体成功具有显著影响。尽管存在大量的样本选择策略，但它们对少样本学习性能的影响尚不十分明确，因为大部分只被在典型的监督设置中进行了评估。本文通过对8个图像和6个文本数据集上的5种少样本学习方法，彻底研究了20种样本选择策略对性能的影响。此外，我们提出了一种新的自动组合样本选择策略的方法（ACSESS），它充分利用了个体策略的优势和互补信息。实验结果表明，我们的方法始终优于个体选择策略，以及最近提出的上下文学习支持样本选择方法。

    In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str
    
[^35]: UniMem：迈向长上下文大语言模型统一视图

    UniMem: Towards a Unified View of Long-Context Large Language Models

    [https://arxiv.org/abs/2402.03009](https://arxiv.org/abs/2402.03009)

    本文引入UniMem，一个统一的框架，以记忆增强的角度重新制定了现有的长上下文方法，并提出了UniMix来提高长上下文处理能力。

    

    长上下文处理是限制大语言模型应用能力的关键能力。虽然存在各种致力于增强大语言模型的长上下文处理能力的方法，但它们是孤立地开发的，缺乏对它们的优点的系统分析和整合，从而阻碍了进一步的发展。在本文中，我们引入了UniMem，一个统一的框架，从LLM的记忆增强的角度重新制定了现有的长上下文方法。 UniMem的特点是四个关键维度：内存管理，内存写入，内存读取和内存注入，为了理解各种长上下文方法提供了系统理论。我们基于UniMem重新制定了16种现有方法，并分析了Transformer-XL，记忆化Transformer，RMT和Longformer中的四种代表性方法，以揭示它们的设计原则和优势。基于这些分析，我们提出了UniMix，一种新的方法来提高长上下文处理能力。

    Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an inn
    
[^36]: 论文标题：解码时间对齐的语言模型

    Decoding-time Realignment of Language Models

    [https://arxiv.org/abs/2402.02992](https://arxiv.org/abs/2402.02992)

    本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。

    

    将语言模型与人类偏好对齐对于减少模型中的错误和偏差非常重要。对齐技术，如从人类反馈中进行的强化学习（RLHF），通常被视为在人类偏好奖励和鼓励保持与未对齐模型接近的接近性规则项之间进行优化的权衡。选择适当的规则化水平至关重要：规则化不足可能导致由于奖励欺骗而降低模型能力，而过度规则化则阻碍对齐。传统方法找到最佳规则化水平需要使用不同规则化强度重新训练多个模型。然而，这个过程耗费资源，特别是对于大型模型来说。为了解决这个挑战，我们提出了解码时间对齐（DeRa），一种简单的方法，在无需重新训练的情况下探索和评估不同的规则化强度。DeRa可以对对齐模型的程度进行控制。

    Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
    
[^37]: GPT 模型的对话重构攻击

    Conversation Reconstruction Attack Against GPT Models

    [https://arxiv.org/abs/2402.02987](https://arxiv.org/abs/2402.02987)

    本文介绍了一种针对 GPT 模型的对话重构攻击，该攻击具有劫持会话和重构对话的两个步骤。通过对该攻击对 GPT 模型的隐私风险进行评估，发现 GPT-4 对该攻击具有一定的鲁棒性。

    

    最近，在大型语言模型（LLM）领域取得了重要进展，其中 GPT 系列模型代表着最具代表性的成果。为了优化任务执行，用户经常与托管在云环境中的 GPT 模型进行多轮对话。这些多轮对话往往包含私人信息，需要在云中进行传输和存储。然而，这种操作模式引入了额外的攻击面。本文首先介绍了一种针对 GPT 模型的特定对话重构攻击。我们提出的对话重构攻击由两个步骤组成：劫持会话和重构对话。随后，我们对当 GPT 模型遭受该攻击时对话中固有的隐私风险进行了详尽评估。然而，GPT-4 对于该攻击具有一定的鲁棒性。接着，我们引入了两种高级攻击，旨在更好地重构以前的对话。

    In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
    
[^38]: 将上下文放入上下文中：讨论结构对文本分类的影响

    Putting Context in Context: the Impact of Discussion Structure on Text Classification

    [https://arxiv.org/abs/2402.02975](https://arxiv.org/abs/2402.02975)

    本研究探讨了上下文结构对文本分类的影响，发现结构信息对于文本分类有很大的益处，但仅在特定情况下显著。

    

    当前的文本分类方法通常集中在要分类的内容上。即使是基于在线讨论的任务，通常也会忽略上下文方面（包括语言和额外的语言之外的方面）。然而，在许多情况下，可以充分利用选定元素的多方和多轮上下文性质。在这项工作中，我们在一个大规模的英文数据集上提出了一系列关于立场检测的实验，在这些实验中，我们通过将它们作为自然语言输入传入基于Transformer的模型中，评估了不同类型上下文信息（语言、结构和时间）的贡献。我们还尝试使用不同数量的训练数据，并以符合隐私要求的方式分析了局部讨论网络的拓扑。结果表明，结构信息可以对文本分类有很大的益处，但只在特定情况下才显著（例如，取决于训练数据的数量和讨论链的复杂性）。

    Current text classification approaches usually focus on the content to be classified. Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions. Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited. In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model. We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way. Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity
    
[^39]: 用具有同源转换器的监督链接预测任务进行自动同源检测

    Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer

    [https://arxiv.org/abs/2402.02926](https://arxiv.org/abs/2402.02926)

    本文提出了一种基于Transformer的方法，用于自动同源检测，实验证明该方法在一定程度的监督下表现优于现有方法，并能随着进一步增加监督而稳定改进。同时，我们还证明了接受多个序列对齐作为输入，并具有端到端架构的重要性。

    

    在历史语言学中，识别相关语言中的同源词是一个主要问题之一。自动同源识别对于识别音位对应关系、原始语言重建、语系分类等多个下游任务都有帮助。以往同源识别的最先进方法大多基于跨多语言词表计算的音素分布，对定义同源簇之间链接的同源标签的使用较少。本文提出了一种以计算生物学为灵感的基于Transformer的架构，用于自动同源检测。在一定程度的监督下，该方法的性能优于现有方法，并且在进一步增加监督的情况下表现出稳定的改进，从而证明了利用标记信息的有效性。我们还证明了接受多个序列对齐作为输入，并具有端到端架构的重要性。

    Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture
    
[^40]: 一种用于评估密切相关语言间的相互可理解性的计算模型

    A Computational Model for the Assessment of Mutual Intelligibility Among Closely Related Languages

    [https://arxiv.org/abs/2402.02915](https://arxiv.org/abs/2402.02915)

    本论文提出了一种用于评估密切相关语言间相互可理解性的计算模型，通过使用线性判别学习器和多语义向量等方法，对德语、荷兰语和英语这三种密切相关的日耳曼语言进行了测试。研究发现，模型的准确度与词形修剪和语言对有关。这种多语言建模方法为自动测试语言间相互可理解性提供了新的方法学发现。

    

    密切相关语言表现出语言上的相似性，使得一种语言的使用者能够理解另一种语言的说话者，而不需要主动学习。相互可理解程度因个体而异，通常在心理语言学实验中进行测试。为了计算研究相互可理解性，我们提出了一种计算辅助方法，使用线性判别学习器，这是一种用于逼近人类学习语言的认知过程的计算模型，我们还扩展了多语义向量和多声音类别。我们对来自德语、荷兰语和英语这三种密切相关的日耳曼语言的同源数据进行了模型测试。我们发现，我们的模型的理解准确度取决于1）词形的自动修剪和2）进行理解测试的语言对。我们的多语言建模方法不仅提供了自动测试语言间相互可理解性的新方法学发现

    Closely related languages show linguistic similarities that allow speakers of one language to understand speakers of another language without having actively learned it. Mutual intelligibility varies in degree and is typically tested in psycholinguistic experiments. To study mutual intelligibility computationally, we propose a computer-assisted method using the Linear Discriminative Learner, a computational model developed to approximate the cognitive processes by which humans learn languages, which we expand with multilingual semantic vectors and multilingual sound classes. We test the model on cognate data from German, Dutch, and English, three closely related Germanic languages. We find that our model's comprehension accuracy depends on 1) the automatic trimming of inflections and 2) the language pair for which comprehension is tested. Our multilingual modelling approach does not only offer new methodological findings for automatic testing of mutual intelligibility across languages 
    
[^41]: LLM智能体的相互作用：测量个性一致性和语言对齐在大规模语言模型相互作用中的贡献

    LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models

    [https://arxiv.org/abs/2402.02896](https://arxiv.org/abs/2402.02896)

    本研究通过对大规模语言模型进行个性化配置，并探究了智能体之间的对话交互对个性一致性和语言对齐的影响。研究结果表明不同配置对话者展示了不同程度的个性一致性和语言对齐。这些发现为进一步理解LLMs之间基于对话的相互作用提供了基础，并强调了打造更具鲁棒性和类人化LLM智能体的新方法的需求。

    

    尽管智能体相互作用和个性化在大规模语言模型（LLMs）研究中都是热门话题，但对语言相互作用对于个性化的LLM智能体行为的影响的关注有限。这样的努力对于确保智能体在保持其指定特征的同时能够进行开放的自然对话非常重要。在我们的实验中，我们通过提示将GPT-3.5调节到个性化配置，并使用简单的变异性引导抽样算法创建了一个双组群的LLM智能体人口。然后我们进行个性化测试，并将智能体提交到协同写作任务，发现不同配置表现出不同程度的个性一致性和语言对齐。我们的研究旨在为更好地理解LLMs之间基于对话的相互作用奠定基础，并凸显了需要新的方法来打造更强大、更类人的LLM智能体的需求。

    While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM pers
    
[^42]: 翻译过的论文标题: 适用于现有孪生变压器的近似归因方法

    Approximate Attributions for Off-the-Shelf Siamese Transformers

    [https://arxiv.org/abs/2402.02883](https://arxiv.org/abs/2402.02883)

    中文总结出的一句话要点: 本文介绍了一种适用于现有孪生变压器的近似归因方法，该方法在保留原模型性能的同时实现了准确归因能力。我们通过比较近似和准确归因，分析了模型对不同语言方面的关注，并发现孪生变压器主要忽略否定，同时深入研究了它们对句法角色的关注程度，以及如何判断语义上的差异。

    

    翻译过的论文摘要: 孪生编码器如句子变换器是目前最不理解的深度模型之一。现有的归因方法无法处理这种模型类别，因为它们比较两个输入而不是处理单个输入。为了弥补这一空白，我们最近提出了一种专门针对孪生编码器的归因方法(Moller等，2023)。然而，它需要对模型进行调整和微调，因此无法直接应用于现有模型。在这项工作中，我们重新评估了这些限制，并提出了(i)一种具有准确归因能力且保留原模型预测性能的模型，以及(ii)一种计算现有模型近似归因的方法。我们广泛比较了近似和准确归因，并使用它们来分析模型对不同语言方面的关注。我们深入了解了孪生变压器对句法角色的关注程度，确认它们主要忽略否定，并探索了它们如何判断语义上的差异。

    Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically op
    
[^43]: 大型语言模型如何进行上下文学习？查询和键矩阵是上下文头部进行度量学习的两个关键组成部分

    How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning

    [https://arxiv.org/abs/2402.02872](https://arxiv.org/abs/2402.02872)

    本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。

    

    我们探索了上下文学习的机制，并提出了使用定位和投影方法的假设。在浅层中，演示的特征被合并到相应的标签中，输入文本的特征被聚合到最后一个标记中。在深层中，上下文头部发挥了重要作用。在每个上下文头部中，值-输出矩阵提取了标签的特征。查询和键矩阵计算了输入文本与每个演示之间的注意力权重。注意力权重越大，越多的标签信息被传输到最后一个标记中，用于预测下一个单词。查询和键矩阵可以被视为学习输入文本与每个演示之间相似度度量的两个关键组成部分。基于这个假设，我们解释了为什么不平衡的标签和演示顺序会影响预测。我们在GPT2大型、Llama 7B、13B和30B上进行了实验。结果支持我们的分析。总体而言，我们的研究提供了一个关于大型语言模型如何进行上下文学习的理论解释和验证。

    We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
    
[^44]: EEVEE：一款简易的自然语言处理标注工具

    EEVEE: An Easy Annotation Tool for Natural Language Processing

    [https://arxiv.org/abs/2402.02864](https://arxiv.org/abs/2402.02864)

    EEVEE是一款简单易用的自然语言处理标注工具，可直接在浏览器中运行，支持多任务标注和四种任务类型。

    

    标注工具是创建自然语言处理（NLP）数据集的起点。目前有各种各样的工具可用，但设置这些工具却是一个障碍。我们提出了EEVEE，一款专注于简洁、高效和易用性的标注工具。它可以直接在浏览器中运行（无需设置），并使用制表符分隔的文件（而不是字符偏移或任务特定格式）进行标注。它允许在单个数据集上对多个任务进行标注，并支持四种任务类型：序列标注、跨度标注、文本分类和序列到序列。

    Annotation tools are the starting point for creating Natural Language Processing (NLP) datasets. There is a wide variety of tools available; setting up these tools is however a hindrance. We propose EEVEE, an annotation tool focused on simplicity, efficiency, and ease of use. It can run directly in the browser (no setup required) and uses tab-separated files (as opposed to character offsets or task-specific formats) for annotation. It allows for annotation of multiple tasks on a single dataset and supports four task-types: sequence labeling, span labeling, text classification and seq2seq.
    
[^45]: 对开放领域科学主张验证的知识来源的比较

    Comparing Knowledge Sources for Open-Domain Scientific Claim Verification

    [https://arxiv.org/abs/2402.02844](https://arxiv.org/abs/2402.02844)

    在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。

    

    科学知识的发现速度和在线健康主张的分享增加，凸显了为科学主张开发高效事实检核系统的重要性。现有文献中，这项任务的常见设置假设已经提供并注释或包含在有限语料库中的包含证据的文档。这使得该系统在可能需要查询数百万个文档以找到相关证据的现实世界环境中变得不切实际。在本文中，我们进行了一系列实验，以测试开放领域主张验证系统的性能。我们在不同设置下测试了四个数据集中的生物医学和健康主张系统的最终判断预测。在保持流水线的证据选择和判断预测部分不变的情况下，使用三种常见知识来源（PubMed、维基百科、谷歌）和两种不同的信息检索方法进行文档检索。

    The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati
    
[^46]: 从（语言的）朋友那里得到一点帮助：多方随意对话的主题分割

    With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations

    [https://arxiv.org/abs/2402.02837](https://arxiv.org/abs/2402.02837)

    在这篇论文中，我们从对话中识别出有意义的特征，通过将对话分割为主题连贯的话语集合，以一个正式的方法来研究对话的主题组织结构。

    

    主题在对话的全局组织中发挥重要作用，当前讨论的内容限制了参与者的可能贡献。理解主题在互动中的组织方式将揭示对话结构超越话语序列的洞察。然而，研究这种高层结构是一项复杂任务，我们首先尝试将对话分割为更小的主题连贯的话语集合。理解这些片段之间的交互将使我们能够提出对话级别的主题组织模型。在本文中，我们使用开放领域的对话，试图以正式方法达到与最近基于机器学习的主题分割模型相当的准确性。我们确定的对于这个任务有意义的特征有助于更好地理解对话的主题结构。

    Topics play an important role in the global organisation of a conversation as what is currently discussed constrains the possible contributions of the participant. Understanding the way topics are organised in interaction would provide insight on the structure of dialogue beyond the sequence of utterances. However, studying this high-level structure is a complex task that we try to approach by first segmenting dialogues into smaller topically coherent sets of utterances. Understanding the interactions between these segments would then enable us to propose a model of topic organisation at a dialogue level. In this paper we work with open-domain conversations and try to reach a comparable level of accuracy as recent machine learning based topic segmentation models but with a formal approach. The features we identify as meaningful for this task help us understand better the topical structure of a conversation.
    
[^47]: 声音对于系统发育重建可靠吗？

    Are Sounds Sound for Phylogenetic Reconstruction?

    [https://arxiv.org/abs/2402.02807](https://arxiv.org/abs/2402.02807)

    本文通过对十个不同语言家族的多样数据集进行研究，首次在系统发育重建中比较了基于声音和基于同源的方法的表现。结果显示，基于词汇同源的重建谱系与真实谱系平均更接近，提高了约三分之一。

    

    在传统的语言进化研究中，学者们通常强调声音规律和对应关系对于语言家族谱系推断的重要性。然而，迄今为止，计算方法往往没有充分考虑到这一潜力。大多数计算方法仍然依赖于词汇同源作为语言学系统发育重建的主要数据来源，尽管也有一些研究中的作者赞赏比较声音序列的好处。基于十个来自不同语言家族的多样数据集和现代自动同源和声音对应检测方法，我们首次测试了基于声音和基于同源的方法在系统发育重建中的性能。结果表明，通过词汇同源重建的谱系在广义四元组距离上与真实谱系平均更接近，提升了约三分之一。

    In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, 
    
[^48]: 异步计划推理中的图增强大型语言模型的研究

    Graph-enhanced Large Language Models in Asynchronous Plan Reasoning

    [https://arxiv.org/abs/2402.02805](https://arxiv.org/abs/2402.02805)

    本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。

    

    异步计划推理具有挑战性，因为它需要顺序和并行规划以优化时间成本。大型语言模型(LLMs)能在这个任务中成功吗？在这里，我们进行了第一次大规模研究来探讨这个问题。我们发现一组代表性的闭源和开源LLMs，包括GPT-4和LLaMA-2，在我们的基准测试AsyncHow中，在没有提供任务解决过程的插图的情况下表现不佳。我们提出了一种称为Plan Like a Graph (PLaG)的新技术，它将图与自然语言提示相结合，实现了最先进的结果。我们展示了虽然PLaG能提升模型性能，但在任务复杂性增加时，LLMs仍然遭受严重降级，突出了利用LLMs模拟数字设备的局限性。我们将我们的研究视为将LLMs用作高效自主代理的一个令人兴奋的步骤。

    Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
    
[^49]: KS-Lottery: 寻找多语言语言模型中的认证彩票

    KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models

    [https://arxiv.org/abs/2402.02801](https://arxiv.org/abs/2402.02801)

    KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。

    

    彩票票证假说认为在随机初始化的神经网络中存在“中奖票”。在微调场景中，语言模型中是否存在中奖票？我们如何找到这样的中奖票？在本文中，我们提出了KS-Lottery，一种用于识别在多语言微调中高度有效的LLM参数的方法。我们的核心思想是使用Kolmogorov-Smirnov检验来分析微调前后参数的分布偏移。我们进一步理论证明了KS-Lottery可以在嵌入层中找到认证的中奖票，微调这些参数可以保证与全面微调相同的性能。通过在翻译任务上将KS-Lottery与其他参数高效调优算法进行比较，实验结果表明，KS-Lottery找到了一个更小的参数集来进行微调，同时达到了与全面微调LLM相当的性能。令人惊讶的是，我们发现微调18个标记的嵌入层

    The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
    
[^50]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^51]: 从部分到严格增量句法分析

    From Partial to Strictly Incremental Constituent Parsing

    [https://arxiv.org/abs/2402.02782](https://arxiv.org/abs/2402.02782)

    本研究构建了遵循严格增量定义的增量句法分析器，评估其仅基于前缀表示就能输出树的能力，并与非增量和部分增量模型进行了对比分析。

    

    我们研究增量句法分析器，以评估它们仅基于前缀表示就能输出树的能力。在严格的从左到右生成语言模型和树解码模块的指导下，我们构建了遵循不同语言的增量性强定义的解析器。这建立在已有关于增量性的工作上，但大多数只在编码器或解码器中强制实施。最后，我们对非增量和部分增量模型进行了分析。

    We study incremental constituent parsers to assess their capacity to output trees based on prefix representations alone. Guided by strictly left-to-right generative language models and tree-decoding modules, we build parsers that adhere to a strong definition of incrementality across languages. This builds upon work that asserted incrementality, but that mostly only enforced it on either the encoder or the decoder. Finally, we conduct an analysis against non-incremental and partially incremental models.
    
[^52]: 双重知识蒸馏用于高效声音事件检测

    Dual Knowledge Distillation for Efficient Sound Event Detection

    [https://arxiv.org/abs/2402.02781](https://arxiv.org/abs/2402.02781)

    这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。

    

    声音事件检测（SED）对于识别特定声音及其在声学信号中的时间位置至关重要。特别是在设备上的应用中，由于计算资源有限，这变得很具挑战性。为了解决这个问题，我们在本研究中引入了一种新颖的框架，称之为双重知识蒸馏，用于开发高效的SED系统。我们提出的双重知识蒸馏以时序平均知识蒸馏（TAKD）为开端，利用从学生模型参数的时序平均得到的平均学生模型。这使得学生模型能够间接地从预训练的教师模型中学习，确保稳定的知识蒸馏。随后，我们引入了增强嵌入特征蒸馏（EEFD），其中包含在学生模型中引入了嵌入蒸馏层来增强上下文学习。在DCASE 2023任务4A公共评估数据集上，我们的双重知识蒸馏SED系统表现出很好的性能。

    Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle
    
[^53]: 基于列表感知的重新排序-截断联合模型用于搜索和生成增强检索

    List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation

    [https://arxiv.org/abs/2402.02764](https://arxiv.org/abs/2402.02764)

    该论文提出了一个基于列表感知的重新排序-截断联合模型，用于搜索和生成增强检索。该模型旨在捕捉列表级的上下文特征，通过重新排序和截断来返回更好的列表。先前的研究将重新排序和截断视为两个单独的任务，但这种分离不是最优的。因此，该论文提出的联合模型可以解决信息共享和错误积累的问题。

    

    信息检索的结果通常以排名列表的形式呈现，例如面向人类的网络搜索和面向大型语言模型的检索增强生成。列表感知检索旨在捕捉列表级的上下文特征，返回更好的列表，主要包括重新排序和截断。重新排序对列表中的文档进行精细重新评分。截断动态确定排名列表的截断点，以在整体相关性和避免无关文档的错误信息之间进行权衡。过去的研究将它们视为两个单独的任务并分别对其进行建模，然而，这种分离是不理想的。首先，很难在两个任务之间共享排名列表的上下文信息。其次，独立的流水线通常会遇到错误积累问题，即重新排序阶段的小错误可能会严重影响截断阶段。为了解决这些问题，我们提出了...（继续描述我们提出的方法）

    The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose
    
[^54]: KIVI：一种无需调整的非对称2位量化KV缓存技术

    KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache

    [https://arxiv.org/abs/2402.02750](https://arxiv.org/abs/2402.02750)

    该论文提出了一种无需调整的非对称2位量化KV缓存技术，以解决存储注意力键和值的内存需求增加和推断速度受限问题。

    

    高效地为大型语言模型（LLMs）提供服务需要将许多请求批量处理以减少每个请求的成本。然而，存储注意力键和值以避免重新计算的键值（KV）缓存显著增加了内存需求，并成为速度和内存使用的新瓶颈。这种内存需求随着批处理大小和上下文长度的增加而增加。此外，推断速度受到KV缓存大小的限制，因为GPU的SRAM必须从主GPU内存中加载整个KV缓存以生成每个标记，导致计算核心在此过程中处于空闲状态。减小KV缓存大小的一个直接而有效的解决方案是量化，通过减少KV缓存所需的总字节数来实现。然而，目前缺乏对KV缓存元素分布进行深入研究以了解KV缓存量化的难度和限制。为了弥补这一空白，我们开展了一项全面的元素分布研究。。。

    Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
    
[^55]: 利用类别概率进行黑盒子句级攻击

    Exploiting Class Probabilities for Black-box Sentence-level Attacks

    [https://arxiv.org/abs/2402.02695](https://arxiv.org/abs/2402.02695)

    该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。

    

    句级攻击是针对文本分类器的对抗性句子生成方法，这些句子与正确分类的句子同义，但被分类器错误地分类。在黑盒设置下，分类器只能通过对查询输入的反馈进行访问，这主要以类别概率的形式提供。尽管利用类别概率可以获得更强大的攻击效果，但由于在句级攻击中使用类别概率存在挑战，现有的攻击方法要么不使用反馈，要么仅使用类别标签。为了克服这些挑战，我们开发了一种新的算法，使用类别概率进行黑盒句级攻击，并研究了在攻击成功率上使用类别概率的有效性，并探讨了在黑盒句级攻击中使用类别概率是否值得或可行。我们在各种分类器和基准数据集上对提出的攻击方法进行了广泛评估，并与基线进行了对比。

    Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
    
[^56]: 多步问题求解中的验证器：关于模型引导的过程监督的实证分析

    Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision

    [https://arxiv.org/abs/2402.02658](https://arxiv.org/abs/2402.02658)

    本文介绍了一种名为模型引导的过程监督（MiPS）的新方法，该方法通过对推理模型的解决方案进行抽样完成来自动进行数据整理，从而避免了昂贵的人工注释。实证结果表明，与之前的工作相反，我们应优先选择验证器预测得分高的验证。这种方法显著改进了PaLM 2在数学和编码任务上的性能。

    

    过程监督使用训练好的验证器来评估推理器生成的中间步骤，已经在多步问题求解中展示出了显著的改进。在本文中，为了避免在验证器训练数据上进行昂贵的人工注释，我们引入了模型引导的过程监督（MiPS），这是一种自动化数据整理的新方法。MiPS通过对推理模型的解决方案进行抽样完成，并获得一个准确率，其中准确完成的比例定义为准确率。推理器中的错误会导致MiPS低估中间步骤的准确率，因此，我们建议并通过实验证明，与之前的工作相反，应优先选择验证器预测得分高的验证，而不是低的。我们的方法显著提高了PaLM 2在数学和编码任务上的性能（GSM8K上的准确率+0.67％，数学上的准确率+4.16％，MBPP上的准确率+0.92％与输出s相比。）

    Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
    
[^57]: RACER:一种基于LLM的可扩展半结构化心理健康访谈分析方法论

    RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews

    [https://arxiv.org/abs/2402.02656](https://arxiv.org/abs/2402.02656)

    RACER是一种基于LLM的自动化分析方法，能够高效地将半结构化心理健康访谈转化为有洞见的主题和子主题。在COVID-19危机的研究中，RACER与人工评估员之间的一致性达到72%，接近人与人之间的一致性（77%）。

    

    半结构化访谈(SSIs)是医疗研究中常用的数据收集方法，可以提供深入的定性洞察力。尽管其价值很大，但由于情绪反应的提取和分类困难，以及对大规模人群进行人工评估的挑战，SSIs的手动分析被众所周知地耗时且劳动密集。在本研究中，我们开发了RACER，一种基于大型语言模型（LLM）的专家引导的自动化流程，可以高效地将原始访谈记录转化为有洞见的领域相关主题和子主题。我们使用RACER对93名医疗专业人员和实习生进行的SSIs进行分析，以评估COVID-19危机对个人和专业心理健康的广泛影响。RACER与两个人工评估员之间达到了适度高的一致性（72%），接近人际一致性（77%）。有趣的是，LLMs和人类在处理类似内容时都存在困难。

    Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations. In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with similar content invol
    
[^58]: VlogQA: 越南口语机器阅读理解任务、数据集和基线模型

    VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension

    [https://arxiv.org/abs/2402.02655](https://arxiv.org/abs/2402.02655)

    本文介绍了VlogQA：越南口语机器阅读理解任务、数据集和基线模型，并提供了使用真实数据进行任务的挑战和机遇的见解。VlogQA是一个基于来自YouTube的剧本文档的问答对数据集，涵盖了食物和旅行等主题。深度学习模型在测试集取得了75.34%的最高F1分数。

    

    本文介绍了一个用于机器阅读理解任务的越南口语语料库的开发过程，并提供了使用真实数据进行机器阅读理解任务时遇到的挑战和机遇的见解。现有的越南机器阅读理解语料库主要关注正式的书面文档，如维基百科文章、在线报纸或教科书。与之相反，VlogQA包含了10,076个问答对，基于从YouTube获取的1,230份剧本文档，YouTube是一个包含了用户上传内容的广泛资源，涵盖了食物和旅行等主题。通过捕捉越南本土人在自然环境中的口语表达，这是越南研究中被忽视的一个角落，该语料库为未来越南语阅读理解任务的研究提供了宝贵的资源。在性能评估方面，我们的深度学习模型在测试集上取得了最高的F1分数为75.34%，表明了其优秀的性能。

    This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicat
    
[^59]: 链式反馈：缓解回答不一致性的影响

    Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses

    [https://arxiv.org/abs/2402.02648](https://arxiv.org/abs/2402.02648)

    本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。

    

    大型语言模型（LLMs）在回答知识密集型问题时经常出现不一致的情况，即使输入相同，也会提供不同的输出。当用户表达坚决相反的立场时，LLMs调整其回答的质量会变差，尽管初始回答是正确的。这些行为降低了这些模型提供的回答的可靠性和有效性。在本文中，我们试图：1）通过展示链式反馈（CoF）如何导致LLMs更加偏离实际答案，引起过度依赖ChatGPT等AI代理带来的固有风险；2）提出一种新的提示方法，递归链式反馈（R-CoF），我们正在进行进一步研究。CoF系统接收一个开放式多步问题，然后我们重复提供无意义的反馈，要求再次尝试。我们的初步实验表明，这种反馈只会降低回答的质量。另一方面，

    Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
    
[^60]: LLM增强型数据管理

    LLM-Enhanced Data Management

    [https://arxiv.org/abs/2402.02643](https://arxiv.org/abs/2402.02643)

    LLMDB是一种LLM增强的数据管理范式，通过LLM的微调和提示工程嵌入领域特定知识，解决了虚构、高成本和低准确性的挑战，并实现了泛化能力和高推理能力。

    

    近年来，针对优化数据管理问题的机器学习（ML）技术得到了广泛研究和广泛部署。然而，传统的ML方法在泛化能力（适应不同情景）和推理能力（理解上下文）方面存在局限性。幸运的是，大型语言模型（LLMs）表现出高度的泛化能力和人类竞争能力，对于数据管理任务（如数据库诊断、数据库调优）具有潜在的应用前景。然而，现有的LLMs存在一些限制：虚构、高成本以及对复杂任务的低准确性。为了应对这些挑战，我们设计了LLMDB，一种使用LLM增强的数据管理范式，具有良好的泛化能力和高推理能力，同时避免了虚构，降低了LLM成本，提高了准确性。LLMDB通过LLM的微调和提示工程嵌入领域特定知识以避免虚构。LLMDB通过减少LLMs的高成本 addresses challenges: hallucination, high cost, low accuracy-

    Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by 
    
[^61]: “重要的是你如何做事情”：关注过程以更好地为土著社区提供语言技术服务

    It's how you do things that matters": Attending to Process to Better Serve Indigenous Communities with Language Technologies

    [https://arxiv.org/abs/2402.02639](https://arxiv.org/abs/2402.02639)

    本文探讨了建立土著语言NLP技术的伦理考虑，并推荐NLP研究人员增加对与土著社区合作过程的关注。

    

    历史上，自然语言处理（NLP）技术对土著语言的服务总是不足的，但是随着大规模多语言模型的扩展和NLP社群对濒危语言的关注增加，这种情况正在发生改变。本文探讨了为土著语言构建NLP技术中的伦理考虑，基于这样的前提：这些项目首先应该服务于土著社区。我们对在澳大利亚从事土著和/或托雷斯海峡岛民社区的语言技术项目的17名研究人员进行了采访，并借鉴了这些采访的见解，提出了增加NLP研究人员对与土著社区合作过程的关注，而不仅仅关注于去情境化的工艺品的实践建议。

    Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages. This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities. We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia. Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts.
    
[^62]: 大型语言模型能否学习独立的因果机制？

    Can Large Language Models Learn Independent Causal Mechanisms?

    [https://arxiv.org/abs/2402.02636](https://arxiv.org/abs/2402.02636)

    本论文研究在大型语言模型中学习独立因果机制的方法，以增强模型在分布变化下的鲁棒性和泛化能力。

    

    尽管大型语言模型（LLMs）在语言建模和复杂推理任务中表现出色，但在不常见的环境设置或分布变化的任务中，LLMs的泛化能力仍然不足。目前通常通过增加训练数据来缓解这个问题。然而，这种方法是脆弱的，因为任务的范围可能无法预测或可能会发生变化，并且使用新数据更新模型通常需要大量的额外训练。相反，那些学习抽象变量和因果关系的系统，如因果模型，可以表现出对分布变化的更强稳健性。其中一个原因是存在并使用独立因果机制（ICMs），表示只稀疏交互的高层概念。在这项工作中，我们应用因果性的两个概念，在LLMs中学习ICMs。我们开发了一个由多个稀疏交互的语言模型组成的新LLM架构。

    Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
    
[^63]: 预测低资源语言机器翻译性能：领域相似性的作用

    Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity

    [https://arxiv.org/abs/2402.02633](https://arxiv.org/abs/2402.02633)

    该论文研究了预测低资源语言机器翻译性能的关键因素，发现领域相似性对于预测模型性能具有最重要的影响。

    

    对于低资源语言（LRLs），细调和测试多语言大型语言模型是昂贵且具有挑战性的。然而，先前的研究主要关注高资源语言，忽视了LRLs和跨领域的转变。针对LRLs，我们研究了三个因素：细调语料库的大小，细调语料库与测试语料库之间的领域相似性，以及源语言和目标语言之间的语言相似性。我们采用经典回归模型评估这些因素对模型性能的影响。我们的结果表明领域相似性对于预测机器翻译模型的性能具有最重要的影响。

    Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs). While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors: the size of the fine-tuning corpus, the domain similarity between fine-tuning and testing corpora, and the language similarity between source and target languages. We employ classical regression models to assess how these factors impact the model's performance. Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.
    
[^64]: GIRT-模型：自动生成问题报告模板

    GIRT-Model: Automated Generation of Issue Report Templates

    [https://arxiv.org/abs/2402.02632](https://arxiv.org/abs/2402.02632)

    本研究介绍了GIRT-模型，这是一个基于开发者指示自动生成问题报告模板的助理语言模型。通过在GitHub仓库中构建的数据集进行指导调整，GIRT-模型在IRT生成方面表现显著优于其他通用语言模型。

    

    GitHub和GitLab等平台引入问题报告模板（IRT）以提高问题管理效率并更好地与开发者期望对齐。然而，大多数仓库并未广泛采用这些模板，并且目前没有可用的工具来辅助开发者生成模板。在这项工作中，我们介绍了GIRT-模型，这是一个基于开发者关于结构和必需字段的指示自动生成IRT的助理语言模型。我们创建了GIRT-Instruct，这是一个包含指示和IRT对的数据集，其中IRT来自GitHub存储库。我们使用GIRT-Instruct来指导调整T5-base模型以创建GIRT-模型。在我们的实验中，GIRT-模型在IRT生成方面表现优于通用语言模型（带有不同参数大小的T5和Flan-T5），在ROUGE、BLEU、METEOR和人工评估上得分显著更高。此外，我们分析了GIRT-模型在用户体验中的有效性。

    Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model. In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user st
    
[^65]: 用多个时间视角增强Transformer RNNs

    Enhancing Transformer RNNs with Multiple Temporal Perspectives

    [https://arxiv.org/abs/2402.02625](https://arxiv.org/abs/2402.02625)

    引入了多个时间视角的概念，用于增强Transformer RNNs对顺序数据的理解能力，在参数数量最小增加的情况下取得了显著的改进。

    

    我们引入了多个时间视角的概念，这是一种适用于循环神经网络（RNN）架构的新方法，用于增强其对顺序数据的理解。该方法涉及维护先前遇到的文本的多样时间视图，显著丰富了语言模型解释上下文的能力。为了展示这种方法的有效性，我们将其纳入了Receptance Weighted Key Value（RWKV）架构，解决了该架构在单个隐藏状态中保留所有历史信息的固有挑战。值得注意的是，即使参数数量增加最少（仅为最初参数数量的0.04%），也实现了此改进。此外，多个时间视角所需的额外参数经过微小的计算开销进行微调，避免了完全预训练的需要。由此产生的模型在提示推断过程中保持了线性的计算复杂度。

    We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
    
[^66]: DenseFormer: 通过深度加权平均增强Transformer中的信息流

    DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging

    [https://arxiv.org/abs/2402.02622](https://arxiv.org/abs/2402.02622)

    DenseFormer是对Transformer的简单修改，通过在每个transformer块之后进行深度加权平均，提高了模型的困惑度。学到的加权平均权重揭示了信息流的连贯模式，使得DenseFormer具有更高的数据效率，并且在相同困惑度下胜过传统的Transformer模型。

    

    从Vaswani等人（2017）的Transformer架构现已普遍应用于各个应用领域，从自然语言处理到语音处理和图像理解。我们提出了DenseFormer，这是对标准架构的简单修改，提高了模型的困惑度，而不增加其大小-对于拥有100B参数范围的大规模模型，只需添加几千个参数。我们的方法在每个transformer块之后依靠额外的平均步骤，计算当前和过去表示的加权平均-我们将这个操作称为深度加权平均（DWA）。学到的DWA权重展现了信息流的连贯模式，揭示了来自远层的激活的强大且结构化的重复使用。实验证明DenseFormer具有更高的数据效率，能够达到比更深的transformer模型相同的困惑度，并且在相同困惑度下，这些新模型在性能上超过了transformer基准模型。

    The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms
    
[^67]: 自监督声学单词嵌入的逐层分析：基于语音情感识别的研究

    Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study on Speech Emotion Recognition

    [https://arxiv.org/abs/2402.02617](https://arxiv.org/abs/2402.02617)

    本研究通过对自监督声学单词嵌入进行逐层分析，探索其在特定任务中的优势，特别是在语音情感识别中的贡献。实验结果揭示了AWEs与原始自监督表示的差异，并提出了合理利用AWEs与单词嵌入的方法。

    

    自监督语音模型的有效性已经得到验证，但其表示的最佳利用在不同任务中仍然具有挑战性。在这项研究中，我们深入探讨了声学单词嵌入（AWEs），这是一种从连续表示中得出的固定长度特征，以探索它们在特定任务中的优势。先前的研究表明AWEs在捕捉声学可辨性方面具有实用性。基于此，我们提出了衡量AWEs与单词嵌入之间逐层相似性的方法，旨在进一步研究AWEs中固有的上下文信息。此外，我们通过对两个不同语料库IEMOCAP和ESD的比较实验和逐层准确性分析，评估了AWEs与其他类型语音特征在语音情感识别中的贡献，并研究了AWEs与单词嵌入的合理利用方式。我们的研究结果揭示了AWEs与原始自监督表示之间的差异，以及AWEs单独或与单词嵌入结合使用的正确方法。

    The efficacy of self-supervised speech models has been validated, yet the optimal utilization of their representations remains challenging across diverse tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a fixed-length feature derived from continuous representations, to explore their advantages in specific tasks. AWEs have previously shown utility in capturing acoustic discriminability. In light of this, we propose measuring layer-wise similarity between AWEs and word embeddings, aiming to further investigate the inherent context within AWEs. Moreover, we evaluate the contribution of AWEs, in comparison to other types of speech features, in the context of Speech Emotion Recognition (SER). Through a comparative experiment and a layer-wise accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore differences between AWEs and raw self-supervised representations, as well as the proper utilization of AWEs alone and in combination with word embeddings. Our fin
    
[^68]: PuzzleBench：LLMs能否解决困难的一阶组合推理问题？

    PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?

    [https://arxiv.org/abs/2402.02611](https://arxiv.org/abs/2402.02611)

    本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。

    

    最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。

    Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
    
[^69]: 关于声学算法在微文本规范化中的性能研究

    On the performance of phonetic algorithms in microtext normalization

    [https://arxiv.org/abs/2402.02591](https://arxiv.org/abs/2402.02591)

    这项研究通过实验评估了微文本规范化中不同声学算法的性能，旨在确定最佳的声学算法。

    

    在微博社交网络上发布的用户生成内容构成了一种宝贵的信息来源。然而，微文本通常偏离了标准的词汇和语法规则，因此传统的智能系统很难处理。为了解决这个问题，微文本规范化将这些非标准微文本转化为标准的、书面良好的文本作为预处理步骤，使传统方法可以继续进行常规处理。考虑到非标准文本形成中声学现象的重要性，规范化器的知识库中的一个关键元素是编码这些现象的声学规则，它们可以在所谓的声学算法中找到。在这项研究中，我们对英语语言进行了各种各样的声学算法实验。这项研究的目的是确定微文本规范化候选生成中的最佳声学算法。

    User-generated content published on microblogging social networks constitutes a priceless source of information. However, microtexts usually deviate from the standard lexical and grammatical rules of the language, thus making its processing by traditional intelligent systems very difficult. As an answer, microtext normalization consists in transforming those non-standard microtexts into standard well-written texts as a preprocessing step, allowing traditional approaches to continue with their usual processing. Given the importance of phonetic phenomena in non-standard text formation, an essential element of the knowledge base of a normalizer would be the phonetic rules that encode these phenomena, which can be found in the so-called phonetic algorithms.   In this work we experiment with a wide range of phonetic algorithms for the English language. The aim of this study is to determine the best phonetic algorithms within the context of candidate generation for microtext normalization. I
    
[^70]: 美国历史报纸中亚洲工人的定量话语分析

    A Quantitative Discourse Analysis of Asian Workers in the US Historical Newspapers

    [https://arxiv.org/abs/2402.02572](https://arxiv.org/abs/2402.02572)

    本研究利用计算文本分析方法，对美国历史报纸中亚洲工人的呈现进行了定量研究。研究发现，“苦力”一词在不同州的语义有所差异，不同的话语围绕着“苦力”。此外，南方联邦报纸和北方联邦报纸形成了独特的话语，亚洲人在其中被认为低于欧洲移民，并受到种族主义的攻击。

    

    警告：本论文包含针对边缘化群体的冒犯性语言示例。历史文本的数字化邀请研究人员用计算方法探索大规模历史文本语料库。本研究将计算文本分析应用于相对少有研究的话题：探究亚洲工人在美国历史报纸中的呈现方式。我们发现“苦力”一词在某些州（如马萨诸塞州、罗得岛州、怀俄明州、俄克拉荷马州和阿肯色州）的语义不同，不同的话语围绕着“苦力”。我们还发现，当时的南方联邦报纸和北方联邦报纸通过测量过度代表的词形成了独特的话语。来自南方联邦的报纸将苦力与与奴隶制有关的词语联系在一起。此外，我们发现亚洲人被认为低于欧洲移民，并受到种族主义的攻击。本研究有助于补充定性研究，提供对历史报纸中亚洲工人呈现方式的定量分析。

    Warning: This paper contains examples of offensive language targetting marginalized population. The digitization of historical texts invites researchers to explore the large-scale corpus of historical texts with computational methods. In this study, we present computational text analysis on a relatively understudied topic of how Asian workers are represented in historical newspapers in the United States. We found that the word "coolie" was semantically different in some States (e.g., Massachusetts, Rhode Island, Wyoming, Oklahoma, and Arkansas) with the different discourses around coolie. We also found that then-Confederate newspapers and then-Union newspapers formed distinctive discourses by measuring over-represented words. Newspapers from then-Confederate States associated coolie with slavery-related words. In addition, we found Asians were perceived to be inferior to European immigrants and subjected to the target of racism. This study contributes to supplementing the qualitative a
    
[^71]: 一个真正联合的神经网络架构用于分割和解析

    A Truly Joint Neural Architecture for Segmentation and Parsing

    [https://arxiv.org/abs/2402.02564](https://arxiv.org/abs/2402.02564)

    本文通过引入一个联合神经网络架构，在形态丰富的语言中实现了同时进行形态分割和句法分析的任务。通过提供基于格子的表示法，保留了输入的所有形态模糊性，有效解决了以往基于神经网络的依存句法分析器的局限性。

    

    当代多语言依存句法分析器可以解析多种语言，但对于形态丰富的语言而言，其性能明显低于其他语言。主要挑战是由于输入标记的形态复杂性和模糊性较高，作为树中节点的语言单位事先是未知的。以往的基于神经网络的形态丰富语言的依存句法分析器遵循联合形态-句法假设，即形态分割和句法分析应该在解析过程中一并解决，而不是先进行分割再进行解析的流程。然而，目前的神经网络依存句法分析器采用严格的流水线方法。在本文中，我们引入了一个联合神经网络架构，将基于格子的表示法保留输入的所有形态模糊性，然后将其提供给一个基于弧的模型，该模型能够同时解决形态分割和句法分析任务。我们在希伯来语上进行了实验，该语言形态丰富且模糊性较高，结果表明...

    Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and
    
[^72]: DefInt：一种用于高效处理混合大型语言模型推理的默认干预框架

    DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models

    [https://arxiv.org/abs/2402.02563](https://arxiv.org/abs/2402.02563)

    DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。

    

    大型语言模型（LLMs）在各种任务中展示出令人印象深刻的新能力，但在处理复杂推理问题方面仍面临挑战。以往的研究如连锁推理（CoT）和思维树（ToT）主要关注提高准确性，但忽视了不断增加的标记成本，这对于具有巨大解空间的开放性实际任务来说可能特别问题。受人类认知的双过程理论的启发，我们提出了一种默认干预框架（DefInt），以释放混合LLMs的协同潜力。默认情况下，DefInt使用较小规模的语言模型生成低成本的推理思路，类似于“系统1”产生的快速直觉。如果这些直觉被认为低置信度，则DefInt将调用放大的语言模型的反思推理作为“系统2”的干预，可以覆盖默认思考并纠正推理过程。实验在五个实际数据集上展示了DefInt论文中的有效性。

    Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
    
[^73]: NavHint: 具有提示生成器的视觉和语言导航代理

    NavHint: Vision and Language Navigation Agent with a Hint Generator

    [https://arxiv.org/abs/2402.02559](https://arxiv.org/abs/2402.02559)

    本论文提出了一种名为NavHint的视觉和语言导航代理，通过一个提示生成器为导航代理提供间接监督，帮助其对视觉环境进行整体理解。该方法在R2R和R4R数据集上取得了state-of-the-art的表现。

    

    现有的关于视觉和语言导航的工作主要依赖于与导航相关的损失，以建立视觉和语言模态之间的连接，忽略了帮助导航代理建立对视觉环境的深入理解的方面。在我们的工作中，我们通过一个提示生成器为导航代理提供间接监督，提供详细的视觉描述。提示生成器帮助导航代理发展对视觉环境的整体理解。它引导代理注意相关的导航细节，包括相关的子指令、识别中的潜在挑战和基于地面的歧义，以及目标视点描述。为了训练提示生成器，我们基于指令中的地标和视觉环境中可见且有区别的对象构建了一个合成数据集。我们在R2R和R4R数据集上评估了我们的方法，并在几个指标上取得了最先进的成果。

    Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent's attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The expe
    
[^74]: 提升生物医学NLI模型的鲁棒性：一种基于探测的临床试验方法

    Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials

    [https://arxiv.org/abs/2402.02558](https://arxiv.org/abs/2402.02558)

    本研究旨在提升生物医学自然语言推断（NLI）模型的鲁棒性，通过一种基于探测的方法研究了临床试验和模型对自然逻辑的理解。

    

    大型语言模型在各个领域和行业中产生了革命性的影响，如对话式人工智能、内容生成、信息检索、商业智能和医学等。在医学领域中，一个主要的应用是分析和调查与蕴含任务相关的临床试验。然而，人们发现大型语言模型容易产生捷径学习、事实不一致和性能下降的问题，即便在上下文变化很小的情况下。虽然进行了对抗性和鲁棒性测试以确保模型的输出完整性，但模糊性仍然存在。为了确保推理的完整性和正确的句法语义理解，我们使用了探测方法。在这里，我使用了记忆探测方法来研究在临床试验上训练的Sci-five模型。我调查了该模型在自然逻辑方面学习到的特征。为了实现目标，我训练了任务特定的探测器。使用这些探测器进行了研究。

    Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to in
    
[^75]: 大型语言模型是否适合基于表格的事实检查？

    Are Large Language Models Table-based Fact-Checkers?

    [https://arxiv.org/abs/2402.02549](https://arxiv.org/abs/2402.02549)

    本研究初步探讨了大型语言模型在基于表格的事实检查方面的潜力。实验结果表明，通过提示工程，大型语言模型在零样本和少样本的情况下可以实现可接受的表现。

    

    基于表格的事实验证（TFV）旨在提取语句和结构化表格之间的蕴涵关系。现有基于小规模模型的TFV方法在标注数据不足和零样本能力薄弱方面存在问题。近年来，大型语言模型（LLMs）在研究领域引起了广泛关注。它们在几个自然语言处理任务上展示了强大的零样本和上下文学习能力，但它们在TFV领域的潜力还不清楚。在本文中，我们进行了关于LLMs是否适合作为基于表格的事实检查器的初步研究。具体来说，我们设计了多样化的提示语来探索上下文学习如何帮助LLMs在TFV方面，即零样本和少样本TFV能力。此外，我们精心设计和构建了TFV指导以研究LLMs的指导调整带来的性能改进。实验结果表明，通过提示工程，LLMs在零样本和少样本TFV方面可以达到可接受的结果，而指导调整则进一步提升了性能。

    Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tun
    
[^76]: "我的模型在什么里面？": 探索环境在基于实践的自然语言理解中的作用

    "What's my model inside of?": Exploring the role of environments for grounded natural language understanding

    [https://arxiv.org/abs/2402.02548](https://arxiv.org/abs/2402.02548)

    本论文探索了环境对于改进基于实践的自然语言理解中数据采集和模型开发的潜力，通过开发新的训练方法和基准，采用生态方法研究基于实践的语言理解系统在自然/模拟虚拟环境中的角色。

    

    与研究孤立的大脑的经典认知科学不同，生态方法侧重于身体和环境在塑造认知过程中的作用。同样，在本论文中，我们采用了生态方法来研究基于实践的自然语言理解（NLU）。基于实践的语言理解研究将语言理解系统置于自然/模拟虚拟环境中的事件、动作和预期之中。与经典研究倾向于专注于设计新模型和优化方法，同时将环境视为给定的情况不同，我们探索了环境设计对于改进数据采集和模型开发的潜力。我们基于基于文本的游戏环境开发了新的训练和标注方法，用于程序化文本理解。我们还参考了基于体验的认知语言学文献，提出了基于实践的NLP研究路线图，并为衡量进展提供了一个新的基准。

    In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of
    
[^77]: 零样本基于知识的视觉问答的知识生成

    Knowledge Generation for Zero-shot Knowledge-based VQA

    [https://arxiv.org/abs/2402.02541](https://arxiv.org/abs/2402.02541)

    本论文提出了一种使用预训练语言模型生成知识并应用于零样本基于知识的视觉问答的方法。实验证明该方法在两个基准测试上性能优于之前的方法，并生成的知识相关且有帮助。

    

    先前的知识化基于视觉问答（K-VQA）解决方案从外部知识库中检索知识，并使用监督学习来训练K-VQA模型。最近，预训练的语言模型被用作K-VQA的知识源和零样本QA模型，并展示了令人期待的结果。然而，这些最近的方法没有明确展示回答问题所需的知识，因此缺乏可解释性。受到最近关于从语言模型生成文本QA的工作的启发，我们在本文中提出并测试了一种类似的基于知识生成的K-VQA方法，该方法首先从语言模型中生成知识，然后以零样本的方式将生成的知识应用于K-VQA。我们在两个K-VQA基准测试上评估了我们的方法，发现我们的方法比之前的零样本K-VQA方法表现更好，我们生成的知识通常是相关且有帮助的。

    Previous solutions to knowledge-based visual question answering~(K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model. Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results. However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability. Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.
    
[^78]: 非主动自适应采样中的绝对收敛和误差阈值

    Absolute convergence and error thresholds in non-active adaptive sampling

    [https://arxiv.org/abs/2402.02522](https://arxiv.org/abs/2402.02522)

    提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。

    

    非主动自适应采样是一种从训练数据中构建机器学习模型的方法，它可以动态和自动地确定保证的样本大小。在这个背景下，无论所采用的调度和生成弱预测器的策略如何，我们描述了一种计算绝对收敛和误差阈值的方法。我们不仅可以确定模型的质量何时不再增加，还提供了一个接近条件来绝对地估算模型实现这一目标的接近度，从而支持在模型选择中进行学习参数的微调。该技术在工作假设方面证明了其正确性和完备性，同时增强了采样方案的鲁棒性。测试结果符合我们的预期，并以自然语言处理领域中词性标注生成为案例研究来说明这一提议。

    Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.
    
[^79]: 适应性调度用于自适应采样在构建词性标注器中

    Adaptive scheduling for adaptive sampling in POS taggers construction

    [https://arxiv.org/abs/2402.02516](https://arxiv.org/abs/2402.02516)

    本论文提出了一种自适应调度的自适应采样方法，用于在构建词性标注器中加快训练速度，同时提高采样的鲁棒性。该方法分析学习曲线的形状，在任何时候增加或减少实例，以确保获得学习能力的净增益。

    

    我们引入了一种自适应调度的自适应采样作为构建词性标注器的机器学习的新方法。目标是在大规模数据集上加快训练速度，同时不显著损失性能。与之前使用随机、固定或定期增加实例之间间隔的方法不同，我们的方法在几何上分析学习曲线的形状，结合功能模型，在任何时候增加或减少实例。该算法在我们的工作假设上被证明是形式上正确的。也就是说，给定一个案例，下一个案例是最近的，确保从前者中获得学习能力的净增益，可以调节此条件的要求水平。我们还通过更加关注在训练数据库中临时性能膨胀的区域，提高了采样的鲁棒性，从而防止学习提前停止。

    We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematu
    
[^80]: 学习曲线建模及其在词性标注中的应用

    Modeling of learning curves with applications to pos tagging

    [https://arxiv.org/abs/2402.02515](https://arxiv.org/abs/2402.02515)

    该论文提出了一种估计学习曲线演化的算法，可以通过部分训练数据结果来预测学习过程中达到期望准确度所需的时间。

    

    介绍了一种基于部分训练数据结果和使用功能策略的算法，用于估计整个训练数据集上的学习曲线的演化。我们通过迭代逼近所需时间点的待求值，独立于所使用的学习技术，并且在经过一定的过程点（称为预测级别）后。该提案在工作假设方面被证明是形式上正确的，并且包含一个可靠的近似条件。这使得用户可以基于最终可实现的准确度来设定收敛阈值，这扩展了停止准则的概念，即使存在扭曲观察结果，也似乎是有效的。我们的目标是评估培训工作量，支持决策过程来减少学习过程中所需的人力和计算资源。该提案在至少三个操作程序中很有兴趣。第一个是预测学习过程中达到期望准确度所需的时间。

    An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of
    
[^81]: 在神经网络中通过相关在线指标来提前停止

    Early stopping by correlating online indicators in neural networks

    [https://arxiv.org/abs/2402.02513](https://arxiv.org/abs/2402.02513)

    本文提出了一种在神经网络中最小化泛化误差的新技术，通过利用一系列在线指标的时间相关性，找到过拟合现象，并提供了可靠的提前停止条件，从而提高了预测能力。

    

    为了最小化神经网络中的泛化误差，引入了一种新颖的技术来在训练学习者时识别过拟合现象。这使得支持可靠和可信的提前停止条件成为可能，从而提高了该类型建模的预测能力。我们的提议利用一系列在线指标的时间相关性，即用于指示一组假设是否满足的特征函数，与从金丝雀判断中构建的一系列独立停止条件相联系，以评估过拟合的存在。通过这种方式，我们为决策提供了形式化的基础，以中断学习过程。与之前专注于单一标准的方法相反，我们利用独立评估之间的附带效应，寻求更广泛的操作范围和更大的诊断可靠性。

    In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, 
    
[^82]: 在低资源场景下建模PoS标记器的研究

    Surfing the modeling of PoS taggers in low-resource scenarios

    [https://arxiv.org/abs/2402.02449](https://arxiv.org/abs/2402.02449)

    在低资源实验场景中，我们评估了早期学习曲线估计作为选择最合适模型的实用方法，并研究了在资源贫乏环境中的可靠性。

    

    最近深度结构技术在自然语言处理中的应用趋势揭示了庞大模型的局限性。这使得传统机器学习算法重新引起了人们的兴趣，在特定情境下仍然表现出竞争力，特别是在低资源环境设置中。同时，模型选择已成为一个重要任务，以在合理成本内提升性能，尤其是当涉及到训练和/或计算资源稀缺的领域时。在这种背景下，我们评估早期学习曲线估计作为在资源贫乏环境中选择最合适模型的实用机制。基于先前在训练和验证资源充足条件下评估的形式化逼近模型，我们研究了这种方法在不同且更具挑战性的操作环境中的可靠性。

    The recent trend towards the application of deep structured techniques has revealed the limits of huge models in natural language processing. This has reawakened the interest in traditional machine learning algorithms, which have proved still to be competitive in certain contexts, in particular low-resource settings. In parallel, model selection has become an essential task to boost performance at reasonable cost, even more so when we talk about processes involving domains where the training and/or computational resources are scarce. Against this backdrop, we evaluate the early estimation of learning curves as a practical mechanism for selecting the most appropriate model in scenarios characterized by the use of non-deep learners in resource-lean settings. On the basis of a formal approximation model previously evaluated under conditions of wide availability of training and validation resources, we study the reliability of such an approach in a different and much more demanding operati
    
[^83]: 打破MLPerf训练：优化BERT的案例研究

    Breaking MLPerf Training: A Case Study on Optimizing BERT

    [https://arxiv.org/abs/2402.02447](https://arxiv.org/abs/2402.02447)

    通过改进负载平衡、通信和优化器等各个组件，我们提出了用于快速大规模训练BERT模型的新方法，实现了新水平的BERT训练性能。

    

    加速大规模分布式训练具有挑战性，需要改进包括负载平衡、通信、优化器等训练的各个组件。我们提出了用于快速大规模训练BERT模型的新方法，通过改进每个组件，从而实现了BERT训练性能的新水平。在分布式BERT训练中，负载平衡至关重要，因为其训练数据集根据不同长度的样本进行了特征化。与分布式训练的规模成正比的通信成本需要通过有用的计算来隐藏。此外，优化器，如ADAM、LAMB等，需要在大规模分布式训练的背景下进行仔细重新评估。我们提出了两个新想法，即基于数据集分层的本地预排序进行负载平衡和全约减之前的按桶梯度裁剪，从而使我们能够从梯度计算和同步的重叠中获益。

    Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization
    
[^84]: 2024年大规模语言模型的真实性

    Factuality of Large Language Models in the Year 2024

    [https://arxiv.org/abs/2402.02420](https://arxiv.org/abs/2402.02420)

    本文调查了大规模语言模型（LLM）的真实性问题，并对其现有研究进行了批判性分析，指出了改进LLM真实性的挑战和解决方案，以及自动真实性评估的障碍。未来的研究应该关注在这些方面的进一步工作。

    

    大规模语言模型（LLMs），尤其是在聊天方面进行指导调整后，已经成为我们日常生活的一部分，通过在一个地方直接回答各种问题，使人们从搜索、提取和整合多个信息源的过程中得到解脱。然而，很多情况下，LLM的回答是错误的，这限制了它们在现实场景中的适用性。因此，对于评估和提高LLM真实性的研究近年来引起了很多关注。在这项调查中，我们对现有的研究进行了批判性分析，旨在找出主要挑战及其原因，并指出改进LLM真实性的潜在解决方案，以及分析开放文本生成的自动真实性评估面临的障碍。我们还展望了未来研究的方向。

    Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
    
[^85]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^86]: GLaPE：大型语言模型的无依赖于金标签的提示评估与优化

    GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model

    [https://arxiv.org/abs/2402.02408](https://arxiv.org/abs/2402.02408)

    GLaPE提出了一种无依赖于金标签的提示评估方法，通过自一致性作为初始评估分数，进一步改进了产生相同答案的提示的得分的互相一致性，提供了与准确性相一致的可靠评估，即使在没有金标签的情况下。

    

    尽管大型语言模型（LLMs）取得了快速进展，但它们的任务性能仍然对提示设计敏感。最近的研究探索了利用LLM自身作为优化器来识别最大化任务准确性的最优提示。然而，在评估提示时，这些方法严重依赖于难以获取的手动标注的金标签，以计算每个候选提示的任务准确性，这阻碍了广泛的实施和通用性。为了克服这一限制，本研究提出了一种无依赖于金标签的提示评估方法（GLaPE），以减少对金标签的依赖。受到自一致性和答案准确性之间的相关性的启发，我们将自一致性作为初始评估分数。随后，我们对产生相同答案的提示进行得分的互相一致性的改进。实验结果表明，GLaPE在没有金标签的情况下提供了与准确性相一致的可靠评估。此外，对于六个任务，GLaPE在绝大部分情况下得到的评估结果与使用真实金标签评估的结果相似。

    Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six p
    
[^87]: DeLLMa:一个用于大型语言模型下决策的框架

    DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models

    [https://arxiv.org/abs/2402.02392](https://arxiv.org/abs/2402.02392)

    DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。

    

    大型语言模型（LLMs）在商业、工程和医学等领域被广泛应用，这些领域往往面临决策不确定性的问题，这是一个关键但具有挑战性的任务。本文表明，在决策问题上直接使用LLMs往往效果较差，尤其是在问题复杂性增加时。为了克服这个限制，我们提出了DeLLMa（Decision-making Large Language Model assistant）框架，旨在提高不确定环境下的决策精度。DeLLMa包括一个多步骤的脚手架程序，借鉴了决策理论和效用理论的原则，提供了一个最优的、可审计的决策过程。我们在涉及真实农业和金融数据的决策环境中验证了我们的框架。结果表明，DeLLMa可以显著提高LLMs的决策性能，准确性可提高高达40%以上。

    Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
    
[^88]: KICGPT: 具备上下文知识的大型语言模型用于知识图谱补全

    KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.02389](https://arxiv.org/abs/2402.02389)

    本文提出了KICGPT，它是一个集成了大型语言模型和基于三元组的知识图谱补全检索器的框架。它通过知识提示的上下文学习策略，缓解了长尾问题，并且无需额外的训练开销。实验证明了其有效性。

    

    知识图谱补全对于解决知识图谱不完整性和支持下游应用至关重要。已经提出了许多用于知识图谱补全的模型，它们可以分为基于三元组和基于文本的方法两类。基于三元组的方法由于结构信息有限和实体分布不均衡而困难重重。基于文本的方法可以缓解这个问题，但需要昂贵的语言模型训练和特定的知识图谱微调，从而限制了其效率。为了解决这些限制，本文提出了KICGPT，一种集成了大型语言模型(LLM)和基于三元组的知识图谱补全检索器的框架。它可以缓解长尾问题，而不会增加额外的训练开销。KICGPT使用了一种上下文学习策略，称为知识提示，它将结构知识编码为演示，以引导LLM的学习。在基准数据集上的实证结果证明了其有效性。

    Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven
    
[^89]: 使用辅助验证的迭代上下文学习生成面向解决方案的基于Agent的模型

    Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning

    [https://arxiv.org/abs/2402.02388](https://arxiv.org/abs/2402.02388)

    本文提出了SAGE，一个通用的面向解决方案的ABM生成框架，利用辅助验证和迭代上下文学习，自动生成针对特定问题的模型和解决方案。

    

    基于Agent的模型（ABM）作为一种重要的范式，用于提出和验证针对复杂系统所提出的假设解决方案或政策，并实现各种目标。这个过程需要大量的工作和跨学科的专业知识。具有跨领域知识和编程能力的大型语言模型（LLM）有潜力减轻这个过程的困难。然而，LLM擅长处理序列信息，这对于分析ABM中的复杂交互和非线性动力学来说是具有挑战性的。另外，由于LLM缺乏自我评估能力，仅仅依靠LLM是无法有效完成这个过程的。在本文中，我们提出了SAGE，这是一个通用的面向解决方案的ABM生成框架，用于自动生成针对特定问题的模型和解决方案。

    Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural netw
    
[^90]: 在分析课堂对话时评估大型语言模型

    Evaluating Large Language Models in Analysing Classroom Dialogue

    [https://arxiv.org/abs/2402.02380](https://arxiv.org/abs/2402.02380)

    本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。

    

    本研究探讨了大型语言模型（LLM），特别是GPT-4，在分析课堂对话中的应用，这是教学诊断和质量改进的重要研究任务。鉴于传统教育研究中知识密集和劳动密集的定性方法，本研究调查了LLM在优化和增强分析过程方面的潜力。该研究涉及中学的数据集，包括数学和语文课堂上的对话。这些对话由教育专家手动编码，然后使用定制的GPT-4模型进行分析。本研究侧重于比较手动注释与GPT-4的输出，以评估其在分析教育对话方面的效果。评估时间效率、编码者间一致性和编码者间可靠性之间的差异。结果表明，使用GPT-4可以显著节省时间，并在编码一致性方面具有很高的一致性。

    This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
    
[^91]: 从实体中心的角度重新思考对预训练的文本和布局模型的评估

    Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an Entity-Centric Perspective

    [https://arxiv.org/abs/2402.02379](https://arxiv.org/abs/2402.02379)

    本论文从实体中心的角度重新思考了对预训练的文本和布局模型的评估。提出了评估PTLMs信息提取能力的理想基准的标准，并介绍了针对该评估的EC-FUNSD数据集。实验结果表明，最先进的PTLMs在预训练阶段存在过拟合的倾向。

    

    最近开发的预训练的文本和布局模型（PTLMs）在视觉丰富的文档上的多个信息提取任务中取得了显著的成功。然而，由于基准数据中的注释不足，目前的评估流程可能不够稳健，无法充分评估PTLMs的信息提取能力。因此，我们提出了评估PTLMs信息提取能力的理想基准的必要标准。我们还介绍了EC-FUNSD，这是一个针对视觉丰富文档上语义实体识别和实体链接评估而设计的以实体为中心的基准数据集。该数据集包含不同格式的文档布局和语义驱动实体及其关系的注释。此外，该数据集还解开了由FUNSD的分段级注释带来的段落和实体错误耦合的问题。实验结果表明，最先进的PTLMs在预训练阶段存在过拟合的倾向。

    Recently developed pre-trained text-and-layout models (PTLMs) have shown remarkable success in multiple information extraction tasks on visually-rich documents. However, the prevailing evaluation pipeline may not be sufficiently robust for assessing the information extraction ability of PTLMs, due to inadequate annotations within the benchmarks. Therefore, we claim the necessary standards for an ideal benchmark to evaluate the information extraction ability of PTLMs. We then introduce EC-FUNSD, an entity-centric benckmark designed for the evaluation of semantic entity recognition and entity linking on visually-rich documents. This dataset contains diverse formats of document layouts and annotations of semantic-driven entities and their relations. Moreover, this dataset disentangles the falsely coupled annotation of segment and entity that arises from the block-level annotation of FUNSD. Experiment results demonstrate that state-of-the-art PTLMs exhibit overfitting tendencies on the pre
    
[^92]: M$^3$Face: 一种用于人脸生成和编辑的统一多模态多语言框架

    M$^3$Face: A Unified Multi-Modal Multilingual Framework for Human Face Generation and Editing

    [https://arxiv.org/abs/2402.02369](https://arxiv.org/abs/2402.02369)

    M$^3$Face是一种统一多模态多语言框架，可以通过文本输入自动生成控制模态，并实现人脸的生成和编辑。同时，我们还提出了M3CelebA数据集，该数据集包含了高质量的多模态多语言人脸数据。

    

    在计算机视觉和数字化世界的时代，人脸生成和编辑是一项重要任务。最近的研究在多模态人脸生成和编辑方面取得了显著进展，例如使用面部分割来指导图像生成。然而，对于一些用户来说，手动创建这些控制模态可能是具有挑战性的。因此，我们引入了M3Face，一种用于可控人脸生成和编辑的统一多模态多语言框架。该框架使用户能够仅使用文本输入自动生成控制模态，例如语义分割或面部特征点，并随后生成人脸图像。我们进行了大量的定性和定量实验，展示了我们框架在人脸生成和编辑能力方面的表现。此外，我们提出了M3CelebA数据集，这是一个包含高质量图像、语义分割、面部特征点和诊断信息的大规模多模态多语言人脸数据集。

    Human face generation and editing represent an essential task in the era of computer vision and the digital world. Recent studies have shown remarkable progress in multi-modal face generation and editing, for instance, using face segmentation to guide image generation. However, it may be challenging for some users to create these conditioning modalities manually. Thus, we introduce M3Face, a unified multi-modal multilingual framework for controllable face generation and editing. This framework enables users to utilize only text input to generate controlling modalities automatically, for instance, semantic segmentation or facial landmarks, and subsequently generate face images. We conduct extensive qualitative and quantitative experiments to showcase our frameworks face generation and editing capabilities. Additionally, we propose the M3CelebA Dataset, a large-scale multi-modal and multilingual face dataset containing high-quality images, semantic segmentations, facial landmarks, and di
    
[^93]: 金融领域大规模语言模型综述（FinLLMs）

    A Survey of Large Language Models in Finance (FinLLMs)

    [https://arxiv.org/abs/2402.02315](https://arxiv.org/abs/2402.02315)

    这篇综述介绍了金融领域的大规模语言模型（FinLLMs），包括它们的历史、技术、性能和机遇与挑战，并提供了六个基准任务和八个高级金融NLP任务的性能评估。

    

    大规模语言模型（LLMs）在包括金融服务在内的多个领域展示出了卓越的自然语言处理（NLP）能力，并引起了广泛关注。尽管对于通用领域的LLMs进行了广泛的研究，并且它们在金融领域有着巨大的潜力，但金融领域的语言模型研究还很有限。本综述全面介绍了金融领域的LLMs，包括它们的历史、技术、性能以及机遇和挑战。首先，我们从通用领域的预训练语言模型（PLMs）到当前的FinLLMs，包括GPT系列、选定的开源LLMs和金融LMs，提供了时间顺序的概述。其次，我们比较了金融PLMs和FinLLMs中使用的五种技术，包括训练方法、训练数据和微调方法。第三，我们总结了六个基准任务和数据集的性能评估。此外，我们还提供了八个高级金融NLP任务和数据集。

    Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services. Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges. Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets. In addition, we provide eight advanced financial NLP tasks and datasets
    
[^94]: 通过修正的缩放定律选择大型语言模型进行微调

    Selecting Large Language Model to Fine-tune via Rectified Scaling Law

    [https://arxiv.org/abs/2402.02314](https://arxiv.org/abs/2402.02314)

    该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。

    

    在日益增长的语言模型生态系统中，在众多选项中选择最合适的预训练模型进行微调成为了一个挑战。在资源受限的情况下，微调所有模型然后再进行选择是不现实的。在本文中，我们将这个资源受限的选择任务转化为预测微调性能，并且展示其与缩放定律之间的自然联系。与预训练不同，我们发现微调的缩放曲线不仅包括众所周知的“功率阶段”，还包括以前未被观察到的“预功率阶段”。我们还解释了为什么现有的缩放定律无法理论和实证地捕捉到这种相变现象。为了解决这个问题，我们将“预学习数据大小”概念引入到我们的修正缩放定律中，这克服了理论上的限制，并更好地适应实验结果。通过利用我们的定律，我们提出了一种新颖的语言模型选择算法，可以选择接近最优的模型。

    The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
    
[^95]: 使用声学伪令牌预测改善低资源语音识别的积极迁移

    Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens

    [https://arxiv.org/abs/2402.02302](https://arxiv.org/abs/2402.02302)

    本论文提出了使用声学伪令牌来预测改善低资源语音识别的积极迁移。通过在目标语言中补充来自相似、高资源的“捐赠”语言的数据，可以提高低资源语言在自动语音识别任务上的性能。

    

    尽管像wav2vec 2.0 XLSR-128这样的大规模多语言语音模型可以直接进行自动语音识别(ASR)的微调，但在预训练数据中低资源语言占比相对较低的情况下，下游性能仍然相对较差。对于没有太多录制数据的语言，我们展示了通过在目标语言中补充来自相似、高资源的“捐赠”语言的数据可以提升性能。例如，仅使用10小时的低资源旁遮普语进行持续预训练，并辅助使用60小时的捐赠语言印地语的效果几乎与使用70小时的旁遮普语进行持续预训练相当。相比之下，来自不太相似的捐赠语言（如孟加拉语）的数据则无法改善ASR的性能。为了选择捐赠语言，我们提出了一种基于诱导声学单元序列分布的新颖相似度度量方法：声学令牌分布相似度。

    While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity
    
[^96]: SemPool：简单、稳健、可解释的知识图加强语言模型的汇聚方法

    SemPool: Simple, robust, and interpretable KG pooling for enhancing language models

    [https://arxiv.org/abs/2402.02289](https://arxiv.org/abs/2402.02289)

    SemPool提出了一种简单、稳健、可解释的知识图加强语言模型的汇聚方法，通过学习和融合KG的语义信息，能够在更具挑战性的环境下提高问答系统的准确率。

    

    知识图（KG）驱动的问答系统在语义和知识事实上执行复杂的推理。图神经网络（GNNs）学习从底层KG中聚合信息，并与语言模型（LMs）结合，以有效地推理给定的问题。然而，基于GNN的问答方法依赖于候选答案节点的图形信息，在更具挑战性的环境中，其中关键答案信息未包含在KG中，限制了其有效性。我们提出了一种简单的图形汇聚方法，学习可以辅助LM推理的KG语义信息，并且在图形扰动下具有稳健的效果。我们的方法称为SemPool，使用预训练的LMs代表KG事实，学习聚合它们的语义信息，并在LM的不同层次上融合它们。我们的实验结果表明，当使用Avazu数据集时，SemPool相比最先进的基于GNN的方法平均提高了2.27%的准确率。

    Knowledge Graph (KG) powered question answering (QA) performs complex reasoning over language semantics as well as knowledge facts. Graph Neural Networks (GNNs) learn to aggregate information from the underlying KG, which is combined with Language Models (LMs) for effective reasoning with the given question. However, GNN-based methods for QA rely on the graph information of the candidate answer nodes, which limits their effectiveness in more challenging settings where critical answer information is not included in the KG. We propose a simple graph pooling approach that learns useful semantics of the KG that can aid the LM's reasoning and that its effectiveness is robust under graph perturbations. Our method, termed SemPool, represents KG facts with pre-trained LMs, learns to aggregate their semantic information, and fuses it at different layers of the LM. Our experimental results show that SemPool outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on average when a
    
[^97]: SynthDST: 少样本对话状态跟踪所需的全部是合成数据

    SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking

    [https://arxiv.org/abs/2402.02285](https://arxiv.org/abs/2402.02285)

    SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％.

    

    在上下文学习中，大型语言模型（LLM）已成为对话状态跟踪（DST）研究的一个有希望的方向。然而，表现最好的上下文学习方法涉及检索和添加类似的示例到提示中，需要访问标记的训练数据。在多个领域和应用中获取这样的训练数据非常耗时、昂贵，有时是不可行的。虽然零样本学习不需要训练数据，但在少样本设置中明显落后。因此，“我们是否可以为任何对话模式有效地生成合成数据，以实现少样本提示？”针对这个问题，我们提出了一个名为\method的数据生成框架，专门针对DST，利用LLM。我们的方法只需要对话模式和一些手工对话模板，就能合成自然、连贯和流畅的带有DST注释的对话。使用{\method}的少样本学习结果显示，Join连通率提升了4-5％。

    In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Join
    
[^98]: 数据质量很重要：使用RoBERTa-CNN模型在社交媒体帖子中检测自杀意图

    Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model

    [https://arxiv.org/abs/2402.02262](https://arxiv.org/abs/2402.02262)

    本文介绍了一种使用RoBERTa-CNN模型来在社交媒体帖子中检测自杀意图的新方法。RoBERTa-CNN通过在RoBERTa模型中添加卷积神经网络（CNN）层，提高了对重要模式的捕捉能力，并在实验证明在自杀和抑郁检测数据集上表现出良好的准确性。

    

    自杀仍然是全球健康领域的一个关注焦点，急需创新方法进行早期检测和干预。本文着重于识别SuicideWatch Reddit帖子中的自杀意图，并提出了一种使用尖端的RoBERTa-CNN模型进行自杀检测的新方法，RoBERTa-CNN是RoBERTa（鲁棒性优化BERT方法）的一种变体。RoBERTa被用于各种自然语言处理（NLP）任务，包括文本分类和情感分析。RoBERTa的有效性在于它能够捕捉文本信息并形成文本之间的语义关系。通过在原始模型中添加卷积神经网络（CNN）层，RoBERTa增强了从庞大数据集中捕捉重要模式的能力。我们在自杀和抑郁检测数据集上评估了RoBERTa-CNN，并获得了可靠的结果，例如，RoBERTa-CNN在平均准确率上获得了98％，标准差为...

    Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention. In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach). RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis. The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts. By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (ST
    
[^99]: 频率解释了大型语言模型尺寸、训练数据量和惊讶程度适应阅读时间的反相关关系

    Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times

    [https://arxiv.org/abs/2402.02255](https://arxiv.org/abs/2402.02255)

    本研究发现，当基于Transformer的语言模型变得越来越大并且在大量数据上进行训练时，模型的惊讶估计与自然人阅读时间的适应性下降。而词频是解释这种适应性下降的关键因素，较大模型变体过度准确地预测了人群中最不频繁的词汇，而较大模型的训练过程中更准确地学习了罕见的词汇，这解释了训练数据量和模型尺寸对适应阅读时间的不利影响。

    

    最近的研究表明，随着基于Transformer的语言模型变得越来越大并在大量数据上进行训练，它们的惊讶估计与自然人阅读时间的适应性下降。本研究通过一系列分析显示，词频是这两个趋势背后的关键解释因素。首先，来自四个语言模型家族在四个语料库上的残差误差显示，模型尺寸与适应阅读时间之间的反相关在最不频繁的词汇子集上最为显著，这是由较大模型变体过度准确的预测所推动。此外，训练动态显示，在后期训练步骤中，所有模型变体学习预测罕见的词汇，并且较大模型变体的预测更为准确，这解释了训练数据量和模型尺寸对适应阅读时间的负面影响。最后，特征归因分析证明较大的模型变体能够更好地预测罕见的词汇。

    Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able t
    
[^100]: 超越极限：扩展大型语言模型中上下文长度的技术综述

    Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models

    [https://arxiv.org/abs/2402.02244](https://arxiv.org/abs/2402.02244)

    这篇论文综述了近期为扩展大型语言模型中上下文长度而设计的技术和方法，并回顾了包括架构修改在内的多种技术，使得语言模型可以更有效地理解长上下文。

    

    近期，大型语言模型（LLMs）展现出了令人惊异的能力，包括理解上下文、进行逻辑推理和生成响应。然而，这是以严格的计算和内存要求为代价的，限制了它们有效支持长输入序列的能力。本综述全面回顾了最近为扩展LLMs序列长度而设计的技术和方法，从而增强其对长上下文理解的能力。具体而言，我们回顾和分类了各种技术，包括修改位置编码和修改注意机制等架构修改，旨在增强对更长序列的处理，同时避免计算需求的成比例增加。本研究探讨的多样方法可以在LLMs的不同阶段（即训练、微调和推理）中利用。这使得LLMs可以有效地处理长序列并提升对长上下文的理解能力。

    Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
    
[^101]: 语言扩展：LLMs，ChatGPT，接地，意义和理解

    Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding

    [https://arxiv.org/abs/2402.02243](https://arxiv.org/abs/2402.02243)

    ChatGPT在LLM规模上通过利用语言本身的收敛约束来做到超出预期的表现，但并不真正理解语义以及与感觉动作的直接联系。

    

    除了OpenAI可能对我们隐瞒的少量信息外，我们都大致知道ChatGPT是如何工作的（它的大型文本数据库，统计数据，向量表示以及它巨大的参数数量，其下一个词的训练等）。但我们谁也不能说我们对ChatGPT的这些资源所能做到的事情不感到惊讶。这甚至让我们有人得出结论，ChatGPT实际上理解了。它并不理解，但我们也不能说我们理解它是如何做到这一点的。我将提出关于良性偏见的一些猜想：在LLM规模上出现的收敛约束可能有助于ChatGPT做得比我们预期的好得多。这些偏见是语言本身在LLM规模上固有的，并且与ChatGPT缺乏直接的感觉动作接地以将其词与其所指的对象以及其命题与其意义联系起来密切相关。

    Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These converg
    
[^102]: 从数据生成的角度对In-Context Learning机制的解释

    A Data Generation Perspective to the Mechanism of In-Context Learning

    [https://arxiv.org/abs/2402.02212](https://arxiv.org/abs/2402.02212)

    本文从数据生成的角度重新解释了In-Context Learning（ICL）的机制，并探讨了流行的技术解决方案的潜在应用。对不同解决方案的优劣进行了全面研究，强调了其中的不足之处。

    

    In-Context Learning（ICL）使大型语言模型（LLM）能够在上下文中学习，在只有少量上下文示例的情况下实现下游泛化，而无需梯度更新。尽管有鼓舞人心的实证成功，ICL的基本机制仍然不清楚，现有研究提供了各种不同观点的理解。这些研究提出了基于直觉和临时技术解决方案来解释ICL，呈现出了一条模糊的路线图。在本文中，我们利用数据生成的视角重新解释最近的研究成果，并展示了流行技术解决方案的潜在广泛应用，从而接近一个系统的角度。我们严格采用技能学习和技能识别的概念定义。它们之间的区别在于技能学习可以从上下文数据中学习新的数据生成函数。我们还对不同解决方案的优势和弱点进行了全面的研究，并强调了其中的不足之处。

    In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the un
    
[^103]: 通过证据模式检索增强知识图谱上的复杂问题回答

    Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval

    [https://arxiv.org/abs/2402.02175](https://arxiv.org/abs/2402.02175)

    本论文提出了一种名为EPR的方法，通过索引原子邻接模式并将其组合，明确建模结构依赖，并在知识图谱中进行子图提取。实验结果表明，EPR方法在复杂问题回答方面取得了显著的改进，提高了F1得分，并在WebQuestionsSP上表现出竞争力。

    

    知识图谱问答系统的信息检索方法包括两个阶段：子图提取和答案推理。我们认为当前的子图提取方法低估了证据事实之间的结构依赖的重要性。我们提出了证据模式检索（EPR）来在子图提取过程中明确地建模结构依赖。我们通过索引资源对的原子邻接模式来实现EPR。给定一个问题，我们进行密集检索以获取由资源对形成的原子模式。然后，我们枚举它们的组合以构建候选证据模式。这些证据模式使用神经模型进行打分，并选择最佳模式来提取下游答案推理所需的子图。实验结果表明，基于EPR的方法在ComplexWebQuestions上将IR-KGQA方法的F1得分显著提高了10个百分点以上，并在WebQuestionsSP上取得了有竞争力的性能。

    Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP.
    
[^104]: 通过上下文扰动和大型语言模型分析新闻报道中的情感极性降低

    Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models

    [https://arxiv.org/abs/2402.02145](https://arxiv.org/abs/2402.02145)

    通过引入对抗攻击的句子扰动技术和基于提示的方法，本论文提出了一种新颖的方法，通过降低新闻内容中的情感极性来解决新闻报道中的情感操纵问题。实验和人工评估表明，这两个模型在实现情感极性降低方面非常有效。

    

    在当今的媒体环境中，新闻媒体在塑造公众舆论方面起着至关重要的作用，有必要解决新闻文本中的情感操纵问题。新闻作者常常注入自己的偏见和情感语言，这会扭曲报道的客观性。本文通过降低新闻内容中潜在情感的极性，提出了一种新颖的方法来解决这个问题。我们从对抗攻击的句子扰动技术和使用ChatGPT的基于提示的方法中汲取灵感，采用转换约束来修改句子，同时保留其核心语义。通过替换、插入和删除三种扰动方法，再结合上下文感知的掩码语言模型，我们通过一种波束搜索算法来最大化针对新闻方面的期望情感分数。我们的实验和人工评估表明，这两个模型在实现情感极性降低方面的有效性。

    In today's media landscape, where news outlets play a pivotal role in shaping public opinion, it is imperative to address the issue of sentiment manipulation within news text. News writers often inject their own biases and emotional language, which can distort the objectivity of reporting. This paper introduces a novel approach to tackle this problem by reducing the polarity of latent sentiments in news content. Drawing inspiration from adversarial attack-based sentence perturbation techniques and a prompt based method using ChatGPT, we employ transformation constraints to modify sentences while preserving their core semantics. Using three perturbation methods: replacement, insertion, and deletion coupled with a context-aware masked language model, we aim to maximize the desired sentiment score for targeted news aspects through a beam search algorithm. Our experiments and human evaluations demonstrate the effectiveness of these two models in achieving reduced sentiment polarity with mi
    
[^105]: 探究预训练语言模型在检测仇恨言论中的关键学习动态

    Probing Critical Learning Dynamics of PLMs for Hate Speech Detection

    [https://arxiv.org/abs/2402.02144](https://arxiv.org/abs/2402.02144)

    该论文探究了预训练语言模型在检测仇恨言论中的关键学习动态，提出了深入的研究问题，并通过比较不同模型、评估鲁棒性、微调设置和预训练数据收集时间的影响等方面的分析结果，为研究者在仇恨言论检测领域提供了实证基础和建议。

    

    尽管广泛应用，但缺乏关于预训练语言模型（PLMs）各种关键方面如何影响其在检测仇恨言论中的性能的研究。通过五个研究问题，我们的发现和建议为实证研究PLMs在检测仇恨言论中的不同方面打下了基础。我们深入比较了不同的预训练模型，评估了它们的鲁棒性、微调设置以及预训练数据收集时间的影响。我们的分析揭示了预训练过程中下游任务的早期峰值，采用更新的预训练语料库带来的有限益处，以及微调过程中特定层次的重要性。我们进一步对使用领域特定模型提出质疑，并强调了用于基准测试仇恨言论检测的动态数据集的需求。

    Despite the widespread adoption, there is a lack of research into how various critical aspects of pretrained language models (PLMs) affect their performance in hate speech detection. Through five research questions, our findings and recommendations lay the groundwork for empirically investigating different aspects of PLMs' use in hate speech detection. We deep dive into comparing different pretrained models, evaluating their seed robustness, finetuning settings, and the impact of pretraining data collection time. Our analysis reveals early peaks for downstream tasks during pretraining, the limited benefit of employing a more recent pretraining corpus, and the significance of specific layers during finetuning. We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.
    
[^106]: 语言对LLMs的道德判断和推理能力有影响吗？一项使用多语言定义问题测试的研究

    Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test

    [https://arxiv.org/abs/2402.02135](https://arxiv.org/abs/2402.02135)

    本研究通过使用多语言定义问题测试，探讨了大型语言模型（LLMs）在不同语言下的道德判断和道德推理能力。研究发现，印地语和斯瓦希里语的道德推理能力明显低于其他语言，而道德判断也在不同语言下变化较大。

    

    本文通过使用多语言定义问题测试探讨了大型语言模型（LLMs）在不同语言下展现的道德判断和道德推理能力。众所周知，道德判断取决于问题所使用的语言。我们将研究扩展到了除英语外的五种新语言（中文、印地语、俄语、西班牙语和斯瓦希里语），并对三个具有较强多语言文本处理和生成能力的LLMs（ChatGPT、GPT-4和Llama2Chat-70B）进行了探究。我们的研究发现，通过后柔性分数指示的道德推理能力在印地语和斯瓦希里语中显著低于西班牙语、俄语、中文和英语，而后四种语言的表现则没有明显的趋势。道德判断在不同语言中也存在较大差异。

    This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.
    
[^107]: 在多模态大型语言模型中为图推理渲染图形

    Rendering Graphs for Graph Reasoning in Multimodal Large Language Models

    [https://arxiv.org/abs/2402.02130](https://arxiv.org/abs/2402.02130)

    本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。

    

    大型语言模型(LLMs)在机器人规划、知识图谱补全和常识推理等任务中越来越多地使用图结构，LLMs能够理解文本格式的图信息，但忽视了丰富的视觉模态，而视觉是人类理解结构信息和进行图推理的直观方式。将图结构表示为视觉图像(即视觉图)的潜在益处和能力仍未被探索。本文在图推理任务中首次引入视觉信息，并提出一个新的基准测试数据集GITQA，其中每个样本是一个元组(图、图像、文本描述)。我们利用最先进的多模态LLMs在GITQA基准测试数据集上进行了大量实验证明，结合文本和视觉信息的结果比单一模态效果更好。此外，在LLaVA-7B/13B模型的微调上表现出色。

    Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
    
[^108]: 使用多语言情感词典进行低资源语言的零样本情感分析

    Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon

    [https://arxiv.org/abs/2402.02113](https://arxiv.org/abs/2402.02113)

    本文提出一种使用多语言情感词典进行零样本情感分析的方法，通过预训练模型在低资源语言中获得优越的性能，包括对高/中资源语言和混合代码数据的处理。

    

    在低资源语言中改善多语言语言模型的能力通常很困难，因为这些语言的大规模数据很少。本文通过在预训练中使用多语言词典来增强多语言能力，从而减少对低资源语言文本的依赖。具体而言，我们专注于跨34种语言的零样本情感分析任务，包括6种高/中资源语言、25种低资源语言和3个混合代码数据集。我们证明，在不使用任何句子级情感数据的情况下，使用多语言词典进行预训练的模型相比于在英文情感数据集上微调以及像GPT--3.5、BLOOMZ和XGLM这样的大型语言模型，具有更好的零样本性能。这些发现适用于对低资源语言的未见情况，以及涉及高资源语言的混合代码情况。

    Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.
    
[^109]: 大型语言模型是好的提示优化器吗？

    Are Large Language Models Good Prompt Optimizers?

    [https://arxiv.org/abs/2402.02101](https://arxiv.org/abs/2402.02101)

    这项研究揭示了以大型语言模型（LLM）作为提示优化器的实际机制。研究发现LLM优化器往往受到自身先前知识的偏见，难以准确识别错误的真正原因，并且在生成合适的提示方面面临挑战。

    

    近期的研究表明，以大型语言模型（LLM）作为提示优化器进行自我反馈和优化提示在性能上取得了令人期待的结果。尽管成功，但这种方法的底层机制尚未被探索，而LLM作为提示优化器的真正有效性需要进一步验证。本文进行了一项全面的研究，揭示了基于LLM的提示优化的实际机制。我们的研究发现，LLM优化器在反思过程中往往难以准确识别错误的真正原因，而更多地受到自身先前知识的偏见。此外，即使反思在语义上是有效的，LLM优化器也经常无法通过单一的优化步骤为目标模型生成合适的提示，部分原因是目标模型的行为是不可预测的。根据观察结果，我们提出了一种新的“自动行为优化”方法。

    LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as Prompt Optimizers to self-reflect and refine prompts, has shown promising performance in recent studies. Despite the success, the underlying mechanism of this approach remains unexplored, and the true effectiveness of LLMs as Prompt Optimizers requires further validation. In this work, we conducted a comprehensive study to uncover the actual mechanism of LLM-based Prompt Optimization. Our findings reveal that the LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors. Furthermore, even when the reflection is semantically valid, the LLM optimizers often fail to generate appropriate prompts for the target models with a single prompt refinement step, partly due to the unpredictable behaviors of the target models. Based on the observations, we introduce a new "Automatic Behavior Optimization" par
    
[^110]: 分析多语言语言模型中跨语言知识转移的评估

    Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models

    [https://arxiv.org/abs/2402.02099](https://arxiv.org/abs/2402.02099)

    分析了多语言语言模型中跨语言知识转移的评估方法和设置，发现高性能主要归因于非语言知识的因素，如任务和表层知识，并且跨语言传输的主要是数据工件和偏见，尤其是对于低资源语言。

    

    最近在大规模数据集上训练的多语言语言模型似乎显示出在跨语言知识转移和下游任务上取得了很高的性能。然而，我们对当前的评估基准和设置能够准确衡量零-shot跨语言知识转移的程度表示质疑。在这项工作中，我们通过引入更具挑战性的设置，涉及多语言实例，挑战了高零-shot性能在目标任务中反映高跨语言能力的假设。通过广泛的实验和分析，我们展示了多语言模型的高性能主要归因于不需要转移实际语言知识的因素，如任务和表层知识。更具体地说，我们观察到跨语言传输的主要是数据工件和偏见，特别是对于低资源语言。我们的发现突显了被忽视的缺点。

    Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks
    
[^111]: 重新审视机器翻译中的马尔可夫性质

    Revisiting the Markov Property for Machine Translation

    [https://arxiv.org/abs/2402.02084](https://arxiv.org/abs/2402.02084)

    本文重新审视了机器翻译中的马尔可夫性质，并设计了一个马尔可夫自回归变换器（MAT）。研究结果显示，MAT的阶数大于4时，翻译质量与传统自回归变换器相当，且高阶MAT并不特别适用于长句的翻译。

    

    本文在神经机器翻译的背景下重新审查了马尔可夫性质。我们设计了一个马尔可夫自回归变换器（MAT），并对其在四个WMT基准上的性能进行了全面评估。我们的研究结果表明，MAT的阶数大于4时，生成的翻译质量与传统自回归变换器相当。此外，与直觉相反，我们还发现高阶MAT的优势不是特别体现在长句的翻译中。

    In this paper, we re-examine the Markov property in the context of neural machine translation. We design a Markov Autoregressive Transformer~(MAT) and undertake a comprehensive assessment of its performance across four WMT benchmarks. Our findings indicate that MAT with an order larger than 4 can generate translations with quality on par with that of conventional autoregressive transformers. In addition, counter-intuitively, we also find that the advantages of utilizing a higher-order MAT do not specifically contribute to the translation of longer sentences.
    
[^112]: 拥有CaPE的GliDe：一种简化的快速推理解码方法

    GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding

    [https://arxiv.org/abs/2402.02082](https://arxiv.org/abs/2402.02082)

    本研究提出了GliDe和CaPE两种简化的快速推理解码方法，通过重用缓存键和值以及利用置信度分数选择额外候选令牌进行验证，显著降低了LLM的解码延迟。

    

    快速推理解码是一种利用小而高效的草稿模型来减少LLM延迟的相对较新的解码框架。在本研究中，我们介绍了GliDe和CaPE，这两种简化的快速推理解码方法可以进一步提高冻结LLM的解码速度。具体而言，GliDe是一个修改过的草稿模型架构，可以重用目标LLM中的缓存键和值，而CaPE是一种借助草稿模型的置信度分数来选择额外候选令牌进行验证的扩展方法。对不同基准测试的大量实验证明，我们提出的GliDe草稿模型显著降低了预期的解码延迟。使用墙上时间进行额外评估显示，GliDe可以将Vicuna模型加速至2.17倍，并利用CaPE进一步提高至2.61倍。我们将发布我们的代码、数据和训练好的草稿模型。

    Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.
    
[^113]: 翻译错误在交叉语言学习中对低资源语言有显著影响

    Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning

    [https://arxiv.org/abs/2402.02080](https://arxiv.org/abs/2402.02080)

    研究发现，在交叉语言学习中，翻译错误对低资源语言有显著影响。通过测量在多个目标语言上的人工翻译和机器翻译的目标文本的零-shot评估的表现差距，可以识别翻译错误。此外，该研究还证实了印地语和乌尔都语存在翻译错误。

    

    用于评估交叉语言理解的流行基准（例如XNLI）由英语评估集的多个目标语言平行版本组成，这些平行版本是在专业翻译人员的帮助下创建的。创建这样的平行数据时，对于准确描述交叉语言转移非常重要的是确保所有目标语言的高质量翻译。在本研究中，我们发现翻译不一致确实存在，并且有趣的是它们对XNLI中的低资源语言有不成比例的影响。为了识别这种不一致性，我们提出了一种通过测量在多个目标语言上对人工翻译和机器翻译目标文本进行零-shot评估的表现差距来衡量翻译错误的方法；表现差距较大表明存在翻译错误。我们还通过对这两种目标语言（印地语和乌尔都语）进行人工重新注释的方式，证实了翻译错误的存在。

    Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poo
    
[^114]: 探索面向任务的对话系统在口语化德语方言中的鲁棒性

    Exploring the Robustness of Task-oriented Dialogue Systems for Colloquial German Varieties

    [https://arxiv.org/abs/2402.02078](https://arxiv.org/abs/2402.02078)

    本研究通过制定扰动规则，研究了将德语句子转换为口语化形式对基于任务的对话系统的影响。实验证明，在口语化变体中应用ToD系统时，意图识别性能仍保持稳定，平均准确度下降了6%（4.62个百分点）。

    

    主流的跨语言任务导向的对话（ToD）系统通过在英语中训练意图识别和槽填充的联合模型，并将其零-shot地应用到其他语言中，实现了转移学习的范式。我们填补了之前研究中的一个空白，即由于测试数据有限，往往忽视了对较低资源的口语化变体的转移。受到英语变体的先前工作的启发，我们制定并手动评估了将德语句子转换成口语形式的扰动规则，并使用它们在四个ToD数据集中合成测试集。我们的扰动规则涵盖了18个不同的语言现象，使我们能够探索每个扰动对槽位和意图性能的影响。利用这些新数据集，我们在六个不同的transformers上进行了实验评估。在这里，我们展示了当应用于口语化变体时，ToD系统在意图识别性能上保持稳定，平均准确度下降了6%（4.62个百分点）。

    Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the transfer learning paradigm by training a joint model for intent recognition and slot-filling in English and applying it, zero-shot, to other languages. We address a gap in prior research, which often overlooked the transfer to lower-resource colloquial varieties due to limited test data. Inspired by prior work on English varieties, we craft and manually evaluate perturbation rules that transform German sentences into colloquial forms and use them to synthesize test sets in four ToD datasets. Our perturbation rules cover 18 distinct language phenomena, enabling us to explore the impact of each perturbation on slot and intent performance. Using these new datasets, we conduct an experimental evaluation across six different transformers. Here, we demonstrate that when applied to colloquial varieties, ToD systems maintain their intent recognition performance, losing 6% (4.62 percentage points) in accuracy on average.
    
[^115]: 研究知识驱动对话中内容规划以平衡权衡

    Investigating Content Planning for Navigating Trade-offs in Knowledge-Grounded Dialogue

    [https://arxiv.org/abs/2402.02077](https://arxiv.org/abs/2402.02077)

    这项工作研究了知识驱动对话中的内容规划，以平衡响应的特定性和归因度之间的权衡。尽管内容规划显示出希望，但对于是否可以帮助解决这一权衡问题的结果是混合的。

    

    知识驱动的对话生成是一项具有挑战性的任务，因为它需要同时满足两个基本但常常相互竞争的约束条件：响应性和可归因于潜在源文档。在这项工作中，我们将这两个目标（特定性和归因度）之间的权衡问题提出并问了一个问题：在响应生成之前进行明确的内容规划是否可以帮助模型解决这个挑战？为了回答这个问题，我们设计了一个名为PLEDGE的框架，它允许我们在先前工作中探索过的各种计划变量上进行实验，支持既不依赖度量也依赖度量的方法。尽管内容规划显示出希望，但我们在是否可以帮助解决这一权衡问题方面的结果是混合的-在度量感知的规划机制（在训练过程中使用自动度量）方面在自动评估中表现更好，但在人工判断中表现不佳。

    Knowledge-grounded dialogue generation is a challenging task because it requires satisfying two fundamental yet often competing constraints: being responsive in a manner that is specific to what the conversation partner has said while also being attributable to an underlying source document. In this work, we bring this trade-off between these two objectives (specificity and attribution) to light and ask the question: Can explicit content planning before the response generation help the model to address this challenge? To answer this question, we design a framework called PLEDGE, which allows us to experiment with various plan variables explored in prior work, supporting both metric-agnostic and metric-aware approaches. While content planning shows promise, our results on whether it can actually help to navigate this trade-off are mixed -- planning mechanisms that are metric-aware (use automatic metrics during training) are better at automatic evaluations but underperform in human judgm
    
[^116]: 打破LLM推理的顺序依赖：使用前瞻解码

    Break the Sequential Dependency of LLM Inference Using Lookahead Decoding

    [https://arxiv.org/abs/2402.02057](https://arxiv.org/abs/2402.02057)

    本文介绍了一种称为前瞻解码的精确、并行解码算法，通过交换每步操作数以减少总解码步骤的数量，加速了大型语言模型（LLM）的解码过程。它不需要辅助模型或数据存储，并且与并发内存高效的注意力机制兼容。实验证明，在代码补全任务中，前瞻解码可将自回归解码加速1.8倍，并且在多个GPU上实现强扩展性。

    

    大型语言模型（LLM）的自回归解码受到内存带宽限制，导致延迟较高，并且浪费了现代加速器的并行处理能力。现有的加速LLM解码的方法通常需要草稿模型（例如，推测解码），这样的模型不易获取且无法推广。在本文中，我们介绍了前瞻解码，一种精确的并行解码算法，可以加速LLM解码，而无需辅助模型或数据存储。它允许交换每步log（FLOPs）以减少总解码步骤的数量，在单个或多个现代加速器上更易于并行化，并且与并发内存高效的注意力机制（例如FlashAttention）兼容。我们的前瞻解码实现可以在MT-bench上加速自回归解码1.8倍，并在多个GPU上实现强扩展性，代码补全任务上加速4倍。我们的代码可在https://github.com/hao-ai-lab/LookaheadDecoding找到。

    Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding
    
[^117]: AnthroScore: 一种计算语言学的人性度量方法

    AnthroScore: A Computational Linguistic Measure of Anthropomorphism

    [https://arxiv.org/abs/2402.02056](https://arxiv.org/abs/2402.02056)

    AnthroScore是一种计算语言学的度量方法，用于隐含人性化语言。研究发现，AnthroScore与人类对人性化的判断和社会科学文献中的人性化维度相一致。分析结果显示，近15年来研究论文中的人形化水平稳步增加，与语言模型相关的论文具有最高的人形化水平。

    

    人形化，即将人类特征赋予非人类实体，已经塑造了关于技术影响和可能性的对话。我们提出了AnthroScore，一种隐含人性化语言的自动度量标准。我们使用遮蔽语言模型来量化非人类实体在周围语境中被隐式地框架为人类的程度。我们证明了AnthroScore与人类对人性化的判断以及社会科学文献中描述的人性化维度相一致。受到计算机科学话语中误导性人形化的担忧的驱动，我们使用AnthroScore分析了15年的研究论文和下游新闻文章。在研究论文中，我们发现人形化在时间上稳步增加，并且与语言模型相关的论文中具有最高的人形化水平。在ACL论文中，人形化的时间增加与关键神经进展相关。在科学担忧的基础上构建。

    Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific
    
[^118]: EffiBench:评估自动生成代码的效率的基准测试

    EffiBench: Benchmarking the Efficiency of Automatically Generated Code

    [https://arxiv.org/abs/2402.02037](https://arxiv.org/abs/2402.02037)

    本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。

    

    代码生成模型在辅助软件开发方面变得越来越重要，可以帮助完成代码补全、调试和代码转换等任务。尽管当前的研究已经深入研究了代码生成模型生成的正确性，但生成代码的效率这一重要方面常常被忽视。本文提出了EffiBench，一个包含1,000个效率关键的编码问题的基准测试，用于评估代码生成模型生成的代码的效率。EffiBench包含了一系列多样化的LeetCode编码问题，每个问题都与一个可执行的人工编写的典型解决方案配对。通过EffiBench，我们在实践中考察了21种大型语言模型（其中13种是开源的，8种是闭源的）在生成高效代码方面的能力。结果表明，GPT-4-turbo生成的代码最高效，明显优于Palm-2-chat-bison、Claude-instant-1、Gemini-pro、GPT-4和GPT-3.5。

    Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne
    
[^119]: Panacea: 通过偏好适应实现 Pareto 对齐的 LLMS

    Panacea: Pareto Alignment via Preference Adaptation for LLMs

    [https://arxiv.org/abs/2402.02030](https://arxiv.org/abs/2402.02030)

    Panacea 是一种创新方法，将大型语言模型对齐重新定义为多维偏好优化问题，通过使用奇异值分解的低秩适应，以在线注入偏好向量的形式，使模型能够适应并 Pareto 最优地满足各种偏好集。

    

    当前的大型语言模型对齐方法通常使用标量人类偏好标签。然而，这种约定倾向于过度简化人类偏好的多维和异质性特性，导致表达能力降低甚至失配。本文提出了一种创新的方法 Panacea，将对齐重新定义为多维偏好优化问题。Panacea 训练了一个单一模型，能够在线适应并 Pareto 最优地满足各种偏好集，而无需进一步的调整。一个主要挑战是使用低维偏好向量来引导模型的行为，尽管模型由数量庞大的参数所控制。为了解决这个问题，Panacea 被设计为使用基于奇异值分解（SVD）的低秩适应，可以将偏好向量作为奇异值简单在线注入。从理论上讲，我们证明了 Panacea 能够恢复整个 Pareto 前沿与常见损失聚合。

    Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss agg
    
[^120]: LLM是否可以正确引用相关医学参考文献？一个评估框架和分析

    How well do LLMs cite relevant medical references? An evaluation framework and analyses

    [https://arxiv.org/abs/2402.02008](https://arxiv.org/abs/2402.02008)

    这项研究提出了一个问题：LLM生成的来源是否真正支持它们所做的主张？通过验证源的相关性和开发端到端的自动化流水线，研究人员发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。

    

    目前在各种临床领域中，大型语言模型（LLM）被广泛用于回答医学问题。特别是最近表现出色的商业LLM，它们能够引用来源来支持其回答。本文提出一个问题：LLM生成的来源是否真正支持它们所做的主张？为了回答这个问题，我们提出了三个贡献。首先，专家的医学注释是一种昂贵且耗时的评估瓶颈，我们证明GPT-4在验证源的相关性方面非常准确，与一组医生的判断达到88%的一致性。其次，我们开发了一个名为“SourceCheckup”的端到端自动化流水线，并使用它评估了1200个生成的问题上的五个表现最佳的LLM，总计超过40K对的陈述和来源。有趣的是，我们发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。我们还对GPT-4进行了评估...

    Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with 
    
[^121]: 自我去偏差的大型语言模型：零-shot识别和降低刻板印象

    Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes

    [https://arxiv.org/abs/2402.01981](https://arxiv.org/abs/2402.01981)

    本研究利用大型语言模型的零-shot能力提出了一种自我去偏差的技术，通过解释和重启两种方法，成功减少了九个不同社会群体的刻板印象程度，该技术不需要对训练数据、模型参数或解码策略进行修改，希望能启发其他零-shot偏见缓解技术的研究。

    

    大型语言模型(LLMs)在语言生成和理解方面表现出了显著的进展，但也容易展示出有害的社会偏见。尽管已经提出了许多偏见缓解技术，但大多数需要对训练数据、模型参数或解码策略进行修改，这在没有可训练模型的情况下可能是不可行的。在这项工作中，我们利用LLMs的零-shot能力，引入了一种称为零-shot自我去偏差的技术来减少刻板印象。通过两种方法，即解释自我去偏差和重启自我去偏差，我们展示了自我去偏差可以显著减少九个不同社会群体的刻板印象程度，只依赖于LLM自身和简单的提示，其中解释正确地识别出无效的假设，而重启则产生了最大的偏见减少效果。我们希望这项工作能够开启对其他零-shot偏见缓解技术的研究。

    Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mi
    
[^122]: SOCIALITE-LLAMA：社会科学任务的指导调优模型

    SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks

    [https://arxiv.org/abs/2402.01980](https://arxiv.org/abs/2402.01980)

    社会科学的自然语言处理任务需要捕捉语义和隐含的语用信息，指导调优模型Socialite-Llama在这些任务上表现出卓越的性能和提升。

    

    社会科学的自然语言处理任务，如情绪或幽默检测，需要从文本中捕捉语义和隐含的语用信息，通常只有有限的训练数据。指导调优已经被证明可以改善大型语言模型（LLM）的许多能力，例如常识推理、阅读理解和计算机编程。然而，在需要捕捉隐含的语用线索的社交领域，对指导调优的效果了解甚少。我们探索了将指导调优用于社会科学的自然语言处理任务，并介绍了 Socialite-Llama - 一个开源的、经过指导调优的 Llama 模型。在一套包含20个社会科学任务的测试中，Socialite-Llama 在性能上优于 Llama，并且在大多数任务上与最先进的多任务微调模型的性能相匹配或得到提升。此外，与 Llama 相比，Socialite-Llama 在6个相关的社会任务中的5个任务上也有所改进，这表明指导调优在这些任务上也会带来改善。

    Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama -- an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, sugg
    
[^123]: 2024年气候行动中的MasonPerplexity：整合先进的集成技术和数据增强来识别气候行动立场和仇恨事件

    MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble Techniques and Data Augmentation for Climate Activism Stance and Hate Event Identification

    [https://arxiv.org/abs/2402.01976](https://arxiv.org/abs/2402.01976)

    MasonPerplexity团队通过集成建模和数据增强技术，在识别气候行动立场和仇恨事件方面取得了显著成果，为这一重要研究领域提供了有效的方法。

    

    在我们快速变化的世界中，识别社交媒体上关于气候行动和仇恨事件的公众观点已经成为一个关键的研究领域。人们对气候相关问题的支持或反对不断增加，理解这些不同观点变得越来越重要。我们的团队MasonPerplexity参与了一个重要的研究项目，通过广泛测试各种模型和方法，发现我们最有效的结果是通过集成建模和数据增强技术（如回译）获得的。在这个研究任务的特定组成部分中，我们的团队在各自的子任务中分别排名第5、第1和第6，从而证明了我们在这一重要研究领域中的方法的有效性。

    The task of identifying public opinions on social media, particularly regarding climate activism and the detection of hate events, has emerged as a critical area of research in our rapidly changing world. With a growing number of people voicing either to support or oppose to climate-related issues - understanding these diverse viewpoints has become increasingly vital. Our team, MasonPerplexity, participates in a significant research initiative focused on this subject. We extensively test various models and methods, discovering that our most effective results are achieved through ensemble modeling, enhanced by data augmentation techniques like back-translation. In the specific components of this research task, our team achieved notable positions, ranking 5th, 1st, and 6th in the respective sub-tasks, thereby illustrating the effectiveness of our approach in this important field of study.
    
[^124]: 基于Transformer集成的多模态仇恨言论事件检测的MasonPerplexity方法

    MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles

    [https://arxiv.org/abs/2402.01967](https://arxiv.org/abs/2402.01967)

    本文提出了一种名为MasonPerplexity的方法来解决多模态仇恨言论事件检测的问题。该方法采用Transformer集成的方式，在识别仇恨言论和识别文本图像中目标的任务中均取得了较好的成绩，分别排名第三。

    

    在网络社区中，自动识别诸如仇恨言论之类的冒犯性语言对于维护讨论的文明十分重要。在多模态内容中识别仇恨言论是一项特别具有挑战性的任务，因为冒犯性既可以体现在文字上，也可以体现在图像上，或者两者同时存在。本文介绍了在EACL 2024的CASE 2024上共享任务“多模态仇恨言论事件检测”中的MasonPerplexity方法。该任务分为两个子任务：子任务A注重识别仇恨言论，子任务B注重识别政治事件中嵌入文本图像中的目标。我们使用了一个XLM-roBERTa-large模型来处理子任务A，并使用集成方法将XLM-roBERTa-base、BERTweet-large和BERT-base模型结合起来处理子任务B。我们的方法在子任务A中获得了0.8347的F1分数，在子任务B中获得了0.6741的F1分数，并在两个子任务中排名第三。

    The automatic identification of offensive language such as hate speech is important to keep discussions civil in online communities. Identifying hate speech in multimodal content is a particularly challenging task because offensiveness can be manifested in either words or images or a juxtaposition of the two. This paper presents the MasonPerplexity submission for the Shared Task on Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task is divided into two sub-tasks: sub-task A focuses on the identification of hate speech and sub-task B focuses on the identification of targets in text-embedded images during political events. We use an XLM-roBERTa-large model for sub-task A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A and 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.
    
[^125]: 使用标签自编码器改进大规模k最近邻文本分类

    Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders

    [https://arxiv.org/abs/2402.01963](https://arxiv.org/abs/2402.01963)

    本文提出了一种使用标签自编码器改进大规模k最近邻文本分类的方法，通过将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签，以解决在大型文档集合中处理自动语义索引的问题。

    

    本文介绍了一种多标签惰性学习方法，用于在复杂和结构化标签词汇具有高相关性的大型文档集合中处理自动语义索引。所提出的方法是传统k最近邻算法的演化，它使用一个大的自编码器来将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签。我们在MEDLINE生物医学文档集合的大部分上评估了我们的提议，该集合使用医学主题词（MeSH）词汇表作为控制词汇。在我们的实验中，我们提出并评估了几种文档表示方法和不同的标签自编码器配置。

    In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.
    
[^126]: 关于端到端语音翻译中的过滤案例研究

    A Case Study on Filtering for End-to-End Speech Translation

    [https://arxiv.org/abs/2402.01945](https://arxiv.org/abs/2402.01945)

    本文研究了端到端语音翻译中的过滤技术。实验证明，使用最简单的过滤方法可以提高数据集的质量，进而提升模型性能，在多语言到英语语音翻译模型中，平均获得4.65个BLEU分数的提升。

    

    对于任何机器学习任务，如语音转文本或语音转语音翻译，挖掘大规模平行语料库相对容易。尽管这些挖掘的语料库体积庞大，但其质量值得怀疑。本文展示了最简单的过滤技术可以将这些庞大而嘈杂的数据集修剪为更易处理、更干净的数据集。我们还展示了使用这个干净的数据集可以提高模型的性能，例如在多语言到英语语音翻译(ST)模型中，平均获得4.65个BLEU分数的提升。

    It is relatively easy to mine a large parallel corpus for any machine learning task, such as speech-to-text or speech-to-speech translation. Although these mined corpora are large in volume, their quality is questionable. This work shows that the simplest filtering technique can trim down these big, noisy datasets to a more manageable, clean dataset. We also show that using this clean dataset can improve the model's performance, as in the case of the multilingual-to-English Speech Translation (ST) model, where, on average, we obtain a 4.65 BLEU score improvement.
    
[^127]: 一种对于少见语言机器翻译的词典数据增强技术

    A Morphologically-Aware Dictionary-based Data Augmentation Technique for Machine Translation of Under-Represented Languages

    [https://arxiv.org/abs/2402.01939](https://arxiv.org/abs/2402.01939)

    本文提出了一种通过使用形态句法信息和双语词典来合成平行数据的方法，从而解决了少见语言机器翻译中数据稀缺的问题。实验证明，即使只使用少量种子数据和双语词典，该方法在14种语言上都能显著提高性能。

    

    平行文本的可用性对于机器翻译模型的性能至关重要。然而，世界上大多数语言面临数据稀缺的主要挑战。在本文中，我们提出了一种依赖于形态句法信息和使用双语词典以及少量种子平行数据合成平行数据的策略。我们的方法论遵循了一个由小规模平行种子数据支持的现实情境。它在语言学上是有文化常识的，因为它旨在创建更有可能是语法正确的增强数据。我们分析了如何将我们的合成数据与原始平行数据相结合，并在我们对包括14种语言（28个英语<->X对）在内的实验中展示了性能的一致提高，这些语言从资源充裕到资源极少的范围都有。即使只使用五个种子句子和一个双语词典，我们的方法也能带来改进。

    The availability of parallel texts is crucial to the performance of machine translation models. However, most of the world's languages face the predominant challenge of data scarcity. In this paper, we propose strategies to synthesize parallel data relying on morpho-syntactic information and using bilingual lexicons along with a small amount of seed parallel data. Our methodology adheres to a realistic scenario backed by the small parallel seed data. It is linguistically informed, as it aims to create augmented data that is more likely to be grammatically correct. We analyze how our synthetic data can be combined with raw parallel data and demonstrate a consistent improvement in performance in our experiments on 14 languages (28 English <-> X pairs) ranging from well- to very low-resource ones. Our method leads to improvements even when using only five seed sentences and a bilingual lexicon.
    
[^128]: 在规模上进行代码表示学习

    Code Representation Learning At Scale

    [https://arxiv.org/abs/2402.01935](https://arxiv.org/abs/2402.01935)

    这项工作提出了一个利用大规模代码数据进行代码表示学习的两阶段预训练方案，通过混合语言建模和对比学习增强表示，建立了一个优于现有模型的编码器模型，在各种下游任务上大幅优化了性能。

    

    最近的研究显示，规模上的代码语言模型在下游任务，例如代码生成方面表现出显著的性能提升。然而，目前大部分关于代码表示学习的研究只使用了非常有限的预训练语料库进行了一亿规模参数的模型训练。在这项工作中，我们通过两阶段预训练方案利用大量的代码数据推动代码表示学习。首先，我们通过混合使用语言建模中的随机屏蔽和编程语言结构方面的特点训练编码器。然后，我们通过对比学习以无监督的方式构建的困难负例和困难正例，增强表示。我们建立了一个现成的编码器模型，在各种下游任务上持续以较大的优势击败现有模型。为了理解成功的代码表示学习的因素，我们进行了详细的消融实验，并分享了我们的研究结果。

    Recent studies have shown that code language models at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and the structure aspect of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model that persistently outperforms the existing models on a wide variety of downstream tasks by large margins. To comprehend the factors contributing to successful code representation learning, we conduct detailed ablations and share our findings on (i
    
[^129]: 准确和安全交易的数字微模型

    Digits micro-model for accurate and secure transactions

    [https://arxiv.org/abs/2402.01931](https://arxiv.org/abs/2402.01931)

    本研究展示了数字微模型在准确和安全交易中的潜力，这些轻量级模型在数字识别特定任务上表现良好，并使用较少的训练时间和内存资源。

    

    自动语音识别（ASR）系统在金融领域中被用于提升呼叫者体验，通过实现自然语言理解和实现高效直观的互动。ASR系统的增加使用要求这些系统具有非常低的错误率。目前主要的ASR模型用于数字数据收集是大型通用商用模型- Google语音转文字（STT）或亚马逊转录-或开源（OpenAI的Whisper）。这些ASR模型通过数十万小时的音频数据进行训练，需要大量资源运行。尽管最近大型语音识别模型取得了进展，我们强调了小型专门的“微”模型的潜力。这样的轻量级模型可以通过少于80分钟的训练时间表现良好于数字识别特定任务中，与Whisper或Google STT等通用模型竞争，同时使用至少一个数量级更少的内存资源。另外，不同于更大的语音模型，这些数字微模型可以提供更准确和安全的交易。

    Automatic Speech Recognition (ASR) systems are used in the financial domain to enhance the caller experience by enabling natural language understanding and facilitating efficient and intuitive interactions. Increasing use of ASR systems requires that such systems exhibit very low error rates. The predominant ASR models to collect numeric data are large, general-purpose commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or open source (OpenAI's Whisper). Such ASR models are trained on hundreds of thousands of hours of audio data and require considerable resources to run. Despite recent progress large speech recognition models, we highlight the potential of smaller, specialized "micro" models. Such light models can be trained perform well on number recognition specific tasks, competing with general models like Whisper or Google STT while using less than 80 minutes of training time and occupying at least an order of less memory resources. Also, unlike larger speech 
    
[^130]: 对奖励模型学习的偏好污染攻击

    Preference Poisoning Attacks on Reward Model Learning

    [https://arxiv.org/abs/2402.01920](https://arxiv.org/abs/2402.01920)

    对于从偏好比较中学习奖励模型的方法存在偏好污染攻击的漏洞，攻击者可以通过翻转少量偏好比较来对目标结果进行操纵。我们提出了两类算法方法，并证明了这些攻击在实施恶意行为方面的有效性。

    

    从两两比较中学习效用或奖励模型是许多应用领域的基础组成部分。这些方法从本质上需要从人们那里收集偏好信息，而反馈通常是匿名提供的。由于偏好是主观的，没有可以比较的黄金标准；然而，对偏好学习的高影响系统的依赖性为恶意行为者倾向于扭曲以达到其目的而采集的数据创造了强烈的动机。我们通过考虑一种威胁模型系统地调查了这种漏洞的性质和程度，其中攻击者可以翻转少量偏好比较，以促进或贬低目标结果。首先，我们提出了两类用于这些攻击的算法方法：基于原则的梯度框架和几种变种的按距离排名的方法。接下来，我们展示了这两类最佳攻击在成功实施恶意行为方面的效果。

    Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
    
[^131]: Whispering in Norwegian: 解决正字法和方言挑战的导航方法

    Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges

    [https://arxiv.org/abs/2402.01917](https://arxiv.org/abs/2402.01917)

    本文介绍了NB-Whisper，这是针对挪威语自动语音识别的OpenAI Whisper的适应版本，通过改进挪威文本转写的正确率，取得了较好的结果。

    

    本文介绍了NB-Whisper，这是OpenAI的Whisper针对挪威语言自动语音识别（ASR）进行了特定微调的适应版本。我们强调了它的关键贡献，并总结了将挪威口语转换为书面形式以及将其他语言翻译为挪威语的结果。我们展示了我们能够将OpenAI Whisper Large-v3在Fleurs数据集上的词错误率（WER）从10.4降低到6.6，并且在NST数据集上从6.8降低到2.2。

    This article introduces NB-Whisper, an adaptation of OpenAI's Whisper, specifically fine-tuned for Norwegian language Automatic Speech Recognition (ASR). We highlight its key contributions and summarise the results achieved in converting spoken Norwegian into written forms and translating other languages into Norwegian. We show that we are able to improve the Norwegian Bokm{\aa}l transcription by OpenAI Whisper Large-v3 from a WER of 10.4 to 6.6 on the Fleurs Dataset and from 6.8 to 2.2 on the NST dataset.
    
[^132]: CoLe和LYS在BioASQ MESINESP8任务中的表征分配的相似度方法

    CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish

    [https://arxiv.org/abs/2402.01916](https://arxiv.org/abs/2402.01916)

    这篇论文介绍了基于相似度的描述符分配方法在BioASQ MESINESP8任务中的应用，通过使用传统的信息检索工具，并提出了适用于西班牙语等语言的方法。

    

    在本文中，我们描述了我们参与了BioASQ生物医学语义指标挑战赛的MESINESP任务。参与的系统仅使用了传统的信息检索工具。我们评估了从IBECS/LILACS文档中提取索引术语的多种方法，以便存储在Apache Lucene索引中。这些索引表示使用要注释的文章内容进行查询，并从检索到的文档创建一个候选标签的排序列表。我们还评估了一种有限的标签全集方法，该方法通过将具有高共现得分的DeCS标签配对并创建元标签，以及一种基于标签个人资料匹配的替代方法。在官方运行中获得的结果似乎证实了这种方法在西班牙语等语言中的适用性。

    In this paper, we describe our participation in the MESINESP Task of the BioASQ biomedical semantic indexing challenge. The participating system follows an approach based solely on conventional information retrieval tools. We have evaluated various alternatives for extracting index terms from IBECS/LILACS documents in order to be stored in an Apache Lucene index. Those indexed representations are queried using the contents of the article to be annotated and a ranked list of candidate labels is created from the retrieved documents. We also have evaluated a sort of limited Label Powerset approach which creates meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an alternative method based on label profile matching. Results obtained in official runs seem to confirm the suitability of this approach for languages like Spanish.
    
[^133]: 自然语言引导的具有合成注释的高保真度文本到语音转换

    Natural language guidance of high-fidelity text-to-speech with synthetic annotations

    [https://arxiv.org/abs/2402.01912](https://arxiv.org/abs/2402.01912)

    本论文提出了一种自然语言引导的高保真度文本到语音转换方法，通过合成注释来标注说话人身份、风格和录音条件，结果表明在不依赖于人工标注的情况下，可以实现高保真度的语音生成。

    

    基于大规模数据集训练的文本到语音模型展示了令人印象深刻的上下文学习能力和自然度。然而，在这些模型中，对说话人身份和风格的控制通常需要基于参考语音录音来进行条件约束，从而限制了创造性应用。相反，对说话人身份和风格的自然语言提示已经显示出了有希望的结果，并提供了一种直观的控制方法。然而，依赖于人工标注的描述限制了扩展到大规模数据集。我们的工作填补了这两种方法之间的差距。我们提出了一种可扩展的方法，用于标注说话人身份、风格和录音条件的各个方面。然后，我们将这种方法应用于一个45k小时的数据集，用于训练语音语言模型。此外，我们提出了简单的方法来提高音频保真度，尽管完全依赖于找到的数据，但性能显著优越于最近的工作。我们的结果展示了高保真度的语音生成。

    Text-to-speech models trained on large-scale datasets have demonstrated impressive in-context learning capabilities and naturalness. However, control of speaker identity and style in these models typically requires conditioning on reference speech recordings, limiting creative applications. Alternatively, natural language prompting of speaker identity and style has demonstrated promising results and provides an intuitive method of control. However, reliance on human-labeled descriptions prevents scaling to large datasets.   Our work bridges the gap between these two approaches. We propose a scalable method for labeling various aspects of speaker identity, style, and recording conditions. We then apply this method to a 45k hour dataset, which we use to train a speech language model. Furthermore, we propose simple methods for increasing audio fidelity, significantly outperforming recent work despite relying entirely on found data.   Our results demonstrate high-fidelity speech generation
    
[^134]: LiPO: 通过学习排序进行列表型偏好优化

    LiPO: Listwise Preference Optimization through Learning-to-Rank

    [https://arxiv.org/abs/2402.01878](https://arxiv.org/abs/2402.01878)

    本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。

    

    将语言模型与人工反馈进行对齐是控制其在实际应用中行为的关键。最近的一些策略优化方法，如DPO和SLiC，成为传统的来自人类反馈的增强学习方法的有希望的替代方案。实际上，人工反馈通常以对多个响应进行排序的格式提供，以摊销阅读提示的成本。多个响应也可以通过奖励模型或AI反馈进行排序。缺少关于直接适应响应列表的研究。在这项工作中，我们将语言模型对齐问题定义为一个列表型排序问题，并描述了列表型偏好优化（LiPO）框架，在给定提示的情况下，策略可以从一个排名列表中更有效地学习可行响应。这种观点与学习排序（LTR）形成明确的联系，其中大多数现有的偏好优化工作可以映射到现有的排名目标，特别是

    Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
    
[^135]: RL/LLM分类树：回顾强化学习和大语言模型之间的协同关系

    The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models

    [https://arxiv.org/abs/2402.01874](https://arxiv.org/abs/2402.01874)

    这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。

    

    在这项工作中，我们回顾了将强化学习（RL）和大语言模型（LLM）结合起来的研究，并提出了一种新的三类分类方法，该分类方法基于这两种模型类型之间的交互方式。第一类是RL4LLM，包括利用RL改进与自然语言处理相关任务上LLM性能的研究。L4LLM分为两个子类，取决于RL是直接微调现有LLM还是改进LLM的提示。在第二类LLM4RL中，LLM辅助训练一个与自然语言无关的RL模型。我们进一步根据LLM辅助或替代RL训练框架的组件（奖励塑造、目标生成和策略函数）对LLM4RL进行了细分。最后，在第三类RL+LLM中，一个LLM和一个RL代理被嵌入其中。

    In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde
    
[^136]: 利用大型语言模型在提示式弱监督中进行结构学习

    Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision

    [https://arxiv.org/abs/2402.01867](https://arxiv.org/abs/2402.01867)

    这项工作利用大型语言模型在提示式弱监督中学习标注函数之间的统计依赖结构，通过结构细化模块提高了提示式弱监督流程的效果。

    

    提示式弱监督（PromptedWS）将预训练的大型语言模型（LLMs）应用于弱监督框架中的标注函数（LFs），以获取大规模标记数据集。我们进一步扩展了LLMs在循环中的使用，以解决弱监督中的一个关键挑战：学习监督源之间的统计依赖结构。在这项工作中，我们询问LLM这些提示式LFs有多相似。我们提出了一个结构细化模块，它是一种基于提示相似性的简单而有效的方法，利用嵌入空间中的内在结构。结构细化模块的核心是标注函数移除（LaRe）和相关结构生成（CosGen）。与之前从弱标签中学习依赖关系的方法相比，我们的方法找到了对LFs内在且不太依赖数据的依赖关系。我们展示了我们的结构细化模块如何改进提示式弱监督流程。

    Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline
    
[^137]: 我的模型会忘记什么？语言模型改进中的被遗忘实例预测

    What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement

    [https://arxiv.org/abs/2402.01865](https://arxiv.org/abs/2402.01865)

    本文研究了语言模型更新中的遗忘现象，提出了一种预测上游实例遗忘的方法，以改进重播过程的可控性和解释性。根据预训练实例的预-softmax对数几率分数变化与在线学习实例的相似性，提出了一种部分可解释的预测模型，在BART模型上表现良好但在T5模型上失败。此外，还展示了基于内积的黑盒分类器。

    

    在实际应用中，语言模型会出现错误。然而，仅仅通过将模型更新为纠正错误实例，会导致灾难性的遗忘，更新后的模型在指导微调或上游训练阶段中学到的实例上出现错误。随机重播上游数据的效果不令人满意，往往伴随着较高的方差和较差的可控性。为了改善重播过程的可控性和解释性，我们试图预测由于模型更新而遗忘的上游实例。我们根据一组在线学习的实例和相应被遗忘的上游预训练实例训练预测模型。我们提出了一种部分可解释的预测模型，该模型基于这样的观察结果：预训练实例的预-softmax对数几率分数的变化类似于在线学习实例的变化，这在BART模型上表现出不错的效果，但在T5模型上失败。我们进一步展示了基于内积的黑盒分类器

    Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
    
[^138]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^139]: COMET: 使用增量图上下文表示生成提交消息

    COMET: Generating Commit Messages using Delta Graph Context Representation

    [https://arxiv.org/abs/2402.01841](https://arxiv.org/abs/2402.01841)

    COMET是一种利用增量图上下文表示的新方法，通过使用transformer模型生成高质量的提交消息，并引入可定制的质量保证模块。实验证明，COMET在各项指标上优于其他技术，并与流行的GPT模型具有可比性。

    

    提交消息解释了提交中的代码更改，并促进开发者之间的协作。已经提出了几种提交消息生成方法，然而这些方法在捕捉代码更改的上下文方面取得了有限的成功。我们提出了一种新颖的方法COMET（Context-Aware Commit Message Generation），通过使用基于图的表示捕捉代码更改的上下文，并利用基于transformer的模型生成高质量的提交消息。我们的方法使用我们开发的增量图有效地表示代码差异。我们还引入了一个可定制的质量保证模块来识别最佳的消息，减少了提交消息的主观性。实验证明，COMET在bleu-norm和meteor指标方面优于最先进的技术，并在rogue-l指标方面具有可比性。此外，我们将所提出的方法与流行的gpt-3.5-turbo模型以及最强大的GPT模型gpt-4-turbo进行了比较。

    Commit messages explain code changes in a commit and facilitate collaboration among developers. Several commit message generation approaches have been proposed; however, they exhibit limited success in capturing the context of code changes. We propose Comet (Context-Aware Commit Message Generation), a novel approach that captures context of code changes using a graph-based representation and leverages a transformer-based model to generate high-quality commit messages. Our proposed method utilizes delta graph that we developed to effectively represent code differences. We also introduce a customizable quality assurance module to identify optimal messages, mitigating subjectivity in commit messages. Experiments show that Comet outperforms state-of-the-art techniques in terms of bleu-norm and meteor metrics while being comparable in terms of rogue-l. Additionally, we compare the proposed approach with the popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT model, ove
    
[^140]: LLM中的同行评审方法：开放环境下LLMs的自动评估方法

    Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment

    [https://arxiv.org/abs/2402.01830](https://arxiv.org/abs/2402.01830)

    本文提出了一种新的无监督评估方法，利用同行评审机制在开放环境中衡量LLMs。通过为每个LLM分配可学习的能力参数，以最大化各个LLM的能力和得分的一致性。结果表明，高层次的LLM能够更准确地评估其他模型的答案，并能够获得更高的响应得分。

    

    现有的大型语言模型（LLMs）评估方法通常集中于在一些有人工注释的封闭环境和特定领域基准上测试性能。本文探索了一种新颖的无监督评估方法，利用同行评审机制自动衡量LLMs。在这个设置中，开源和闭源的LLMs处于同一环境中，能够回答未标记的问题并互相评估，每个LLM的响应得分由其他匿名的LLMs共同决定。为了获取这些模型之间的能力层次结构，我们为每个LLM分配一个可学习的能力参数来调整最终排序结果。我们将其形式化为一个受约束的优化问题，旨在最大化每个LLM的能力和得分的一致性。背后的关键假设是高层次的LLM能够比低层次的LLM更准确地评估其他模型的答案，而高层次的LLM也可以达到较高的响应得分。

    Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
    
[^141]: 使用深度学习和自然语言处理预测蛋白质序列中的ATP结合位点

    Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing

    [https://arxiv.org/abs/2402.01829](https://arxiv.org/abs/2402.01829)

    本研究使用深度学习和自然语言处理方法，开发了一种预测蛋白质序列中ATP结合位点的方法，实验证明了与最先进的基准模型相比的改进效果。

    

    在生物学和医学领域中，预测基因中的ATP-蛋白结合位点具有重要意义。过去的研究主要通过费时且资源密集的实验室实验进行。多年来，研究人员一直在探索计算方法，利用先进的深度学习和自然语言处理算法来实现相同的目标。在本文中，我们提出了一种分类ATP-蛋白结合位点的方法。我们主要使用PSSMs和几个单词嵌入作为特征进行了各种实验。我们使用了2D CNN和LightGBM分类器作为我们主要的深度学习算法。MP3Vec和BERT模型也在我们的研究中进行了测试。我们的实验证明了与最先进的基准模型相比的改进效果。

    Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.
    
[^142]: 检索增强的端到端口语对话模型

    Retrieval Augmented End-to-End Spoken Dialog Models

    [https://arxiv.org/abs/2402.01828](https://arxiv.org/abs/2402.01828)

    这项研究介绍了一种检索增强的端到端口语对话模型（ReSLM），通过训练语音检索器获取音频中的文本实体，并将这些实体作为输入加入到模型中以提高性能。

    

    我们最近开发了SLM，一种融合预训练语音模型和大型语言模型（LLM）的联合语音和语言模型，同时保留了预训练LLM的上下文学习能力。在本文中，我们将SLM应用于语音对话应用，其中对话状态直接从音频信号中推断出来。面向任务的对话经常包含特定领域的实体，例如餐馆、酒店、火车站和城市名称，这些实体很难识别，但对于后续应用非常关键。受到检索增强生成（retrieval-augmented generation）范式的启发，我们提出了一种检索增强的SLM（ReSLM），克服了这个弱点。我们首先训练了一个语音检索器，用于检索音频中提到的文本实体。然后，将检索到的实体作为文本输入添加到底层的SLM中，以偏置模型的预测。我们在语音MultiWoz任务（DSTC-11挑战）上评估了ReSLM，并发现这种检索增强技术改进了模型的性能。

    We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal.   Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation bo
    
[^143]: 利用大型语言模型分析科学文献中血压变异的生物学性别差异

    Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature

    [https://arxiv.org/abs/2402.01826](https://arxiv.org/abs/2402.01826)

    本研究利用大型语言模型GPT-35-turbo，从PubMed的25 million个摘要中自动提取出男性和女性的血压平均值和标准差，填补了在考虑生物学性别差异的血压测量方差方面的研究空白。

    

    高血压被定义为高于正常的血压，它在公共卫生领域具有至关重要的意义，因为它是各种心血管疾病的重要先兆，并且显著 contributed to the elevated mortality rates worldwide。然而，许多现有的血压测量技术和标准可能存在偏差，因为它们未考虑临床结果、合并症或人口统计因素，使其在诊断目的上没有结论性。针对这些变量，在研究血压测量方差方面，有限的以数据驱动的研究。在本研究中，我们采用了GPT-35-turbo，一种大型语言模型（LLM），从PubMed的2500万个摘要的数据集中自动提取了男性和女性的血压平均值和标准差数值。符合我们预定义的纳入标准的摘要文章有993篇（即具有与血压、血压单位（如mmHg）和提及相关性的参考文献）。

    Hypertension, defined as blood pressure (BP) that is above normal, holds paramount significance in the realm of public health, as it serves as a critical precursor to various cardiovascular diseases (CVDs) and significantly contributes to elevated mortality rates worldwide. However, many existing BP measurement technologies and standards might be biased because they do not consider clinical outcomes, comorbidities, or demographic factors, making them inconclusive for diagnostic purposes. There is limited data-driven research focused on studying the variance in BP measurements across these variables. In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed. 993 article abstracts met our predefined inclusion criteria (i.e., presence of references to blood pressure, units of blood pressure such as mmHg, and mention
    
[^144]: 分形模式可能揭示下一个词预测中的智能

    Fractal Patterns May Unravel the Intelligence in Next-Token Prediction

    [https://arxiv.org/abs/2402.01825](https://arxiv.org/abs/2402.01825)

    通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。

    

    我们研究语言的分形结构，旨在提供一个精确的形式化方法来量化可能之前只有怀疑但尚未正式证明的属性。我们证明了语言具有以下特点：（1）自相似性，展示出各个层级上的复杂性，没有特定的特征上下文长度；（2）长程相关性（LRD），具有大约H=0.70的Hurst参数。基于这些发现，我们认为语言中的短期模式/依赖性，如段落中的模式/依赖性，反映了更大范围的模式/依赖性，如整个文档。这可能有助于理解下一个词预测如何导致对文本的多个层级结构，从单词和从句到更广泛的上下文和意图的理解。我们还证明了分形参数在预测下游性能方面优于基于困惑度的每字节比特（BPB）。我们希望这些发现能为语言和机制提供一种新的视角。

    We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
    
[^145]: 为大型语言模型构建防护措施

    Building Guardrails for Large Language Models

    [https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822)

    本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。

    

    随着大型语言模型（LLM）越来越多地融入我们的日常生活中，识别和减轻它们的风险变得至关重要，特别是当这些风险对人类用户和社会产生深远影响时。防护措施，即过滤LLM的输入或输出，已经成为一种核心的安全技术。本文深入研究了当前的开源解决方案（Llama Guard，Nvidia NeMo，Guardrails AI），讨论了构建更完整解决方案的挑战和路径。基于前期研究的有力证据，我们倡导采用系统化方法构建LLM的防护措施，全面考虑不同LLM应用的多样化上下文。我们建议通过与多学科团队的合作，采用社会技术方法来确定精确的技术要求，探索面向需求复杂性的先进神经符号实现，并开展验证和测试。

    As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t
    
[^146]: 将LLMs的分解能力融入到紧凑语言模型中

    Distilling LLMs' Decomposition Abilities into Compact Language Models

    [https://arxiv.org/abs/2402.01812](https://arxiv.org/abs/2402.01812)

    本研究将LLMs的分解能力通过离线强化学习融入到紧凑模型中，通过开发AI生成的数据集和建立基准，突出了紧凑模型在复制复杂问题解决能力方面的潜力。

    

    大型语言模型（LLMs）展示了其推理能力，但其庞大的大小带来了可扩展性挑战，并限制了进一步的定制。相比之下，紧凑模型提供了定制化培训，但在解决复杂推理任务方面往往不足。本研究着重于使用离线强化学习将LLMs的分解能力融入到紧凑模型中。我们利用LLM能力的进步，提供反馈并生成专门用于训练紧凑模型的特定任务数据集。通过开发一个由AI生成的数据集和建立基准，我们的工作主要贡献在于强调了紧凑模型在复制复杂问题解决能力方面的潜力。

    Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.
    
[^147]: HQA-Attack: 面向高质量黑盒硬标签文本对抗攻击

    HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text

    [https://arxiv.org/abs/2402.01806](https://arxiv.org/abs/2402.01806)

    HQA-Attack是一种针对黑盒硬标签文本对抗攻击的方法，通过简单有效的框架生成高质量的对抗样本，解决了现有方法在有限的查询预算下难以生成具有高语义相似度和低扰动率的问题。

    

    针对文本的黑盒硬标签对抗攻击是一项实际且具有挑战性的任务，因为文本数据空间本质上是离散且不可微分的，只能访问到预测标签。目前对这个问题的研究还处于初级阶段，只有少数几种方法可用。然而，现有的方法依赖于复杂的启发式算法或不可靠的梯度估计策略，很可能陷入局部最优解，并且在有限的查询预算下难以生成具有高语义相似度和低扰动率的令人满意的对抗样本。为了解决上述问题，我们提出了一种简单而有效的框架，用于在黑盒硬标签攻击场景下生成高质量的文本对抗样本，名为HQA-Attack。具体来说，HQA-Attack首先随机初始化对抗样本，然后不断尽可能地反向替换原始单词，从而缩小扰动。

    Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the pert
    
[^148]: 探索大型语言模型中图推理的局限性

    Exploring the Limitations of Graph Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.01805](https://arxiv.org/abs/2402.01805)

    本文测试了5种不同的大型语言模型在图推理问题上的推理深度，并发现了LLMs的局限性、偏见和属性。我们发现LLMs对于节点遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务有负面影响，并且LLMs存在积极的回应偏差，无法识别有效解的缺失。我们还提出了一种新的图推理提示技术。

    

    预训练的大型语言模型仅通过基于语言的提示就展示了各种类型的推理能力。然而，在本文中，我们通过图推理问题测试了5种不同的大型语言模型（GPT-4，GPT-3.5，Claude-2，Llama-2和Palm-2）的推理深度。特别地，我们设计了10个不同的图遍历问题，每个问题代表着逐步增加的复杂性水平。此外，我们通过对不同图大小以及不同形式的k-shot提示的设置分析了模型的性能。通过这个基准测试过程，我们凸显了LLMs的各种局限性、偏见和属性，比如与每个节点的遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务的整体负面影响，以及积极的回应偏差导致LLMs无法识别有效解的缺失。最后，我们提出一种新的提示技术，专门用于图推理。

    Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
    
[^149]: 大规模语言模型用于时间序列：一项调研

    Large Language Models for Time Series: A Survey

    [https://arxiv.org/abs/2402.01801](https://arxiv.org/abs/2402.01801)

    本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。

    

    大规模语言模型（LLM）在自然语言处理和计算机视觉等领域得到了广泛应用。LLM不仅仅局限于文本、图像和图形，还具有对时间序列数据进行分析的重要潜力，可以在气候、物联网、医疗、交通、音频和金融等领域受益。本调研论文对利用LLM进行时间序列分析的各种方法进行了深入探讨和详细分类。我们解决了LLM原始文本数据训练与数值型时间序列数据之间的差异挑战，并探索了将LLM的知识转移和提取到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）直接提示LLM，（2）时间序列量化，（3）对齐技术，（4）利用视觉方式作为桥接机制，和（5）结合LLM与工具。此外，本调研还提供了一系列涉及应用领域、评估方法和未来研究方向的讨论。

    Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
    
[^150]: 更快更轻的LLMs：当前挑战和未来发展的调查

    Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward

    [https://arxiv.org/abs/2402.01799](https://arxiv.org/abs/2402.01799)

    本调查文章概述了在提高LLM推理效果方面的最新方法和进展，通过实验评估不同压缩技术的有效性，并提出改进LLM推理效率的潜在未来方向。

    

    尽管LLMs表现出色，但由于推理过程中需要大量的计算和内存资源，它们的普及面临着挑战。最近在模型压缩和系统级优化方法方面的进展旨在增强LLM推理效果。本调查提供了这些方法的概述，强调了最近的发展。通过对LLaMA(/2)-7B的实验，我们评估了各种压缩技术，为在统一环境中高效部署LLM提供了实践见解。对LLaMA(/2)-7B的实证分析突出了这些方法的有效性。基于调查结果，我们确定了当前的局限性，并讨论了改善LLM推理效率的潜在未来方向。我们在https://github.com/nyunAI/Faster-LLM-Survey发布了用于复现本文结果的代码库。

    Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
    
[^151]: 探索用于病理语音特征预测的迁移学习：层选择的影响

    Exploring transfer learning for pathological speech feature prediction: Impact of layer selection

    [https://arxiv.org/abs/2402.01796](https://arxiv.org/abs/2402.01796)

    本研究探索了用于病理语音特征预测的迁移学习，发现选择适当的层能显著提高性能并且学得的加权和具有更好的泛化能力。

    

    利用人工智能对临床语音进行自动客观评估，并促进语音障碍的诊断和治疗具有重要意义。本研究探索了迁移学习，在预测病理语音存在性的下游任务中，重点分析了层选择的影响。我们发现选择最佳层能显著提高性能（平均平衡准确率增加12.4%），尽管最佳层因预测特征而异，并且并不总是对未见数据泛化良好。学得的加权和在分布内与平均最佳层具有可比性的性能，并且在分布外数据上具有更好的泛化能力。

    There is interest in leveraging AI to conduct automatic, objective assessments of clinical speech, in turn facilitating diagnosis and treatment of speech disorders. We explore transfer learning, focusing on the impact of layer selection, for the downstream task of predicting the presence of pathological speech. We find that selecting an optimal layer offers large performance improvements (12.4% average increase in balanced accuracy), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution and has better generalization for out-of-distribution data.
    
[^152]: LLM的政治偏好分析

    The Political Preferences of LLMs

    [https://arxiv.org/abs/2402.01789](https://arxiv.org/abs/2402.01789)

    该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。

    

    我们在这里报告了关于大型语言模型（LLMs）中内嵌的政治偏好的全面分析。具体而言，我们对24个最先进的对话型LLM进行了11项政治倾向测试，旨在确定测试者的政治偏好。结果表明，当使用具有政治含义的问题/陈述进行探究时，大多数对话型LLM倾向于生成被大多数政治测试仪器诊断为左翼观点的回答。我们注意到，这对于用于与人类对话优化的LLM基础模型并非如此。然而，基础模型在连贯回答问题方面表现不佳，需要对其政治倾向测试的分类进行谨慎解读。虽然还没有定论，但我们的结果为有趣的假设提供了初步证据，即政治偏好会嵌入其中。

    We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
    
[^153]: LitLLM：科学文献综述工具包

    LitLLM: A Toolkit for Scientific Literature Review

    [https://arxiv.org/abs/2402.01788](https://arxiv.org/abs/2402.01788)

    "LitLLM: A Toolkit for Scientific Literature Review" 提出了一个基于 RAG 原则的工具包，通过使用专门的提示和指导技术，结合大型语言模型（LLM），实现了科学文献综述的自动化。这个工具包不仅可以通过转化摘要为关键词进行文献检索，还可以通过补充相关论文或关键词进行定制化的检索。

    

    进行科学论文的文献综述对于理解研究、其限制以及构建在现有工作基础上是必不可少的。这是一项繁琐的任务，因此自动文献综述生成器变得有吸引力。然而，许多使用大型语言模型（LLM）生成此类综述的现有工作存在显著限制。它们倾向于产生虚构的非实际信息，并忽略它们未受过训练的最新研究。为了解决这些限制，我们提出了一个基于检索增强生成（RAG）原则的工具包，在LLM的帮助下，使用专门的提示和指导技术。我们的系统首先通过将用户提供的摘要转化为关键词来进行网络搜索，以检索相关论文，其中使用了现成的LLM。作者可以通过补充相关论文或关键词来改进搜索，从而实现定制化的检索过程。其次，系统根据-

    Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
    
[^154]: COA-GPT：用于军事行动中加速行动方案开发的生成式预训练变压器

    COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations

    [https://arxiv.org/abs/2402.01786](https://arxiv.org/abs/2402.01786)

    COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。

    

    军事行动中行动方案（COAs）的开发传统上是一个耗时且复杂的过程。针对这一挑战，本研究介绍了COA-GPT，一种利用大型语言模型（LLMs）快速高效生成有效COAs的新算法。COA-GPT通过上下文学习将军事学说和领域专业知识融入到LLMs中，允许指挥官输入任务信息（包括文本和图像格式），并获得与战略对齐的COAs以供审查和批准。独特的是，COA-GPT不仅加速了COA的开发，在几秒钟内生成初始COAs，还能根据指挥官的反馈实时精细化改进。本研究在《星际争霸II》游戏的军事相关场景中评估了COA-GPT，将其性能与最先进的强化学习算法进行了比较。我们的结果表明COA-GPT在更快生成战略合理的COAs方面具有优势。

    The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
    
[^155]: 在线疫苗关注的分层多标签分类

    Hierarchical Multi-Label Classification of Online Vaccine Concerns

    [https://arxiv.org/abs/2402.01783](https://arxiv.org/abs/2402.01783)

    本文研究了在线疫苗关注的分层多标签分类任务，使用大型语言模型在零样本设置下检测疫苗关注，同时探索了不同提示策略的成本和准确性权衡，并提供了指导当前应用程序系统设计的具体经验教训。

    

    疫苗关注是一个不断发展的目标，可以在COVID-19大流行中快速变化。通过识别疫苗关注和错误信息的长期趋势，可以帮助公共卫生努力在资源或信息宣传上进行战略性分配。我们在零样本设置中使用大型语言模型（LLM）探索在在线讨论中检测疫苗关注的任务，无需昂贵的训练数据集。由于实时监控在线来源需要大规模推理，我们探索了不同提示策略的成本和准确性之间的权衡，并提供了可以为当前应用程序的系统设计选择提供信息的具体经验教训。对不同提示策略的分析表明，通过LLM多次进行分类，每次通过布尔问题判断文本是否提到疫苗关注，效果最好。我们的结果表明，GPT-4能够明显优于其他模型。

    Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outpe
    
[^156]: 当基准成为目标：揭示大型语言模型排行榜的敏感性

    When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards

    [https://arxiv.org/abs/2402.01781](https://arxiv.org/abs/2402.01781)

    依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。

    

    基于基准排名的大型语言模型(LLM)排行榜经常被用来指导实践者在模型选择中。通常，发布的排行榜排名被直接接受 - 我们表明这是一个（潜在昂贵的）错误。在现有的排行榜下，LLM的相对性能对（通常微小的）细节非常敏感。我们展示了对于流行的多项选择题基准（例如MMLU），对基准的微小扰动，如改变选项顺序或答案选择方法，会导致排名变化达到8个位置。我们通过对三个广泛的基准扰动类别进行系统实验并确定这一行为的来源来解释这一现象。我们的分析得出了几个最佳实践建议，包括选择优化的混合评分方法来进行答案选择。我们的研究强调了依赖简单基准评估的风险，并为更健壮的模型评估提供了指导道路。

    Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
    
[^157]: GPT-4的心理学研究：适度焦虑、略带男性化、诚实谦逊

    On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble

    [https://arxiv.org/abs/2402.01777](https://arxiv.org/abs/2402.01777)

    GPT-4在心理测试中展现出适度焦虑、略带男性化、诚实谦逊的特点，并且在数值素养和语言任务上表现出超过人类平均水平的认知反思能力。

    

    我们对GPT-4进行了一系列严格的心理测量测试并分析结果。我们发现，与普通人相比，GPT-4更倾向于展现出更多的诚实和谦逊，较少的马基雅维利主义和自恋。它有时表现出矛盾的性别偏见，略带男性特征，适度焦虑但大多不抑郁（但不总是）。它在数值素养上表现为与人类平均水平相符，并且在语言任务上的认知反思能力高于人类的平均水平。

    We subject GPT-4 to a number of rigorous psychometric tests and analyze the results. We find that, compared to the average human, GPT-4 tends to show more honesty and humility, and less machiavellianism and narcissism. It sometimes exhibits ambivalent sexism, leans slightly toward masculinity, is moderately anxious but mostly not depressive (but not always). It shows human-average numerical literacy and has cognitive reflection abilities that are above human average for verbal tasks.
    
[^158]: 设计和一种基于2-Tuple模糊语言Delphi的决策支持工具的b-学习教育问卷的一致性内容有效性

    Design and consensus content validity of the questionnaire for b-learning education: A 2-Tuple Fuzzy Linguistic Delphi based Decision Support Tool

    [https://arxiv.org/abs/2402.01775](https://arxiv.org/abs/2402.01775)

    本研究提出了一种名为2-Tuple模糊语言Delphi的决策支持工具，用于评估b-学习教育问卷的内容有效性。该方法通过评估问卷的各个部分，根据专家意见测量一致性程度和语言得分，来检测影响问卷质量的项目。

    

    经典Delphi和模糊Delphi方法用于测试诸如问卷之类的数据收集工具的内容有效性。模糊Delphi从语言角度出发，通过使用模糊数字来减少意见中的歧义。我们提出了一种名为2-Tuple模糊语言Delphi方法的扩展，以处理评委显示不同的专业程度的情况，通过使用语言术语的模糊多粒度语义来获得中间和最终结果，结果用2-Tuple语言值表示。我们提案的关键思想是通过评估其部分来验证完整的问卷，将每个项目的有效性定义为一个决策问题。通过考虑专家的意见，我们测量一致性程度，一致性程度和每个项目的语言得分，以便检测影响仪器质量的项目，无论是积极的还是消极的。考虑到评价b-学习的实际需求。

    Classic Delphi and Fuzzy Delphi methods are used to test content validity of data collection tools such as questionnaires. Fuzzy Delphi takes the opinion issued by judges from a linguistic perspective reducing ambiguity in opinions by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic Delphi method to deal with scenarios in which judges show different expertise degrees by using fuzzy multigranular semantics of the linguistic terms and to obtain intermediate and final results expressed by 2-tuple linguistic values. The key idea of our proposal is to validate the full questionnaire by means of the evaluation of its parts, defining the validity of each item as a Decision Making problem. Taking the opinion of experts, we measure the degree of consensus, the degree of consistency, and the linguistic score of each item, in order to detect those items that affect, positively or negatively, the quality of the instrument. Considering the real need to evaluate a b-learni
    
[^159]: 解开多语言机器翻译中目标端转移和正则化的作用

    Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation

    [https://arxiv.org/abs/2402.01772](https://arxiv.org/abs/2402.01772)

    本文通过大规模研究展示了多语言机器翻译中目标端转移的动态影响。我们发现，语言相似的辅助目标语言具有强大的正向知识转移能力，并且随着相似目标语言规模的增加，转移效果进一步增强。同时，远离的辅助目标语言也可以意外地对主要语言对产生正向转移效果。

    

    多语言机器翻译(MMT)在不同语言对之间的知识转移中受益。然而，一对多翻译相比于多对一翻译的改进仅有微小甚至可忽略不计。这种性能差异引发了一个问题：在一对多MT中，正向转移在目标端的作用程度如何。在本文中，我们进行了一项大规模研究，通过语言相似性和语料库大小这两个维度变化辅助目标语言，以展示知识转移对主要语言对的动态影响。我们发现，语言相似的辅助目标语言表现出强大的正向知识转移能力。随着相似目标语言规模的增加，正向转移进一步增强，使主要语言对受益。同时，我们发现远离的辅助目标语言也可以出乎意料地使主要语言对受益，即使正向转移最小也是如此。

    Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive trans
    
[^160]: BlackMamba: 混合专家模型的状态空间模型

    BlackMamba: Mixture of Experts for State-Space Models

    [https://arxiv.org/abs/2402.01771](https://arxiv.org/abs/2402.01771)

    BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。

    

    最近的研究表明，状态空间模型（SSMs）在大规模语言建模基准测试中表现出与transformer竞争力的性能，同时，其时间和内存复杂度与序列长度成线性关系。最近发布的SSM模型Mamba在语言建模和处理长序列任务方面表现出色。与此同时，专家混合模型（MoE）在显著降低推断计算和延迟成本的同时，也增加了更大的内存占用。本文提出了一种名为BlackMamba的新型架构，将Mamba SSM与MoE相结合，以获得两者的好处。我们证明BlackMamba在Mamba和transformer基准测试中表现出竞争力，并在推断和训练FLOPs方面表现出色。我们在自定义数据集的300B标记上全面训练并开源了340M/1.5B和630M/2.8B的BlackMamba模型。我们展示了BlackMamba继承并结合了这两种模型的优势。

    State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits
    
[^161]: 重新定义LLMs中的“幻觉”：构建心理学为基础的减轻误导的框架

    Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation

    [https://arxiv.org/abs/2402.01769](https://arxiv.org/abs/2402.01769)

    该研究旨在重新定义LLMs中的“幻觉”，提出了心理学分类法，以更详细地理解和解决LLMs输出误导信息的挑战。通过借鉴人类处理类似挑战的方式，研究旨在开发策略以减轻LLMs中的幻觉。

    

    近年来，大型语言模型（LLMs）变得非常受欢迎，例如ChatGPT已经被超过十亿的用户使用。虽然这些模型展现出了出色的语言理解和逻辑能力，但在“幻觉”方面存在一个显著挑战。这一现象导致LLMs以自信的方式输出误导信息，而这可以在如此庞大的用户群体中产生灾难性后果。然而，我们对LLMs中使用“幻觉”一词的适当性提出了质疑，并提出了基于认知偏差和其他心理现象的心理分类法。我们的方法提供了对这一现象更详细的理解，从而能够提供有针对性的解决方案。通过利用人类内部解决类似挑战的见解，我们旨在开发策略来减轻LLMs的幻觉。这种跨学科的方法旨在超越传统的术语，为深入理解和可操作的路径提供细致入微的认识。

    In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for imp
    
[^162]: HiQA：一种用于大规模文档问答的分层上下文增强的RAG模型

    HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA

    [https://arxiv.org/abs/2402.01767](https://arxiv.org/abs/2402.01767)

    HiQA是一个先进的多文档问答框架，使用分层的上下文增强和多路径检索机制，解决了大规模文档问答中的检索准确性问题，并在多文档环境中展示了最先进的性能。

    

    随着利用外部工具的语言模型代理迅速发展，使用补充文档和检索增强生成（RAG）方法的问答（QA）方法学取得了重要进展。这种进步提高了语言模型的回答质量，并减轻了幻觉的出现。然而，当面临大量无法区分的文档时，这些方法在检索准确性方面表现有限，给实际应用带来了显著挑战。针对这些新兴的挑战，我们提出了HiQA，这是一个先进的多文档问答（MDQA）框架，将级联的元数据整合到内容中，同时具备多路径检索机制。我们还发布了一个名为MasQA的基准来评估和研究MDQA。最后，HiQA在多文档环境中展示了最先进的性能。

    As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
    
[^163]: LLM投票：人类选择和AI集体决策

    LLM Voting: Human Choices and AI Collective Decision Making

    [https://arxiv.org/abs/2402.01766](https://arxiv.org/abs/2402.01766)

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并与人类投票模式进行了对比。我们的方法包括进行人类投票实验以建立人类偏好的基准，并与LLM代理进行平行实验。研究聚焦于集体结果和个体偏好，揭示了人类和LLMs之间在决策和固有偏见方面的差异。我们观察到LLMs在偏好多样性和一致性之间存在权衡，相比人类选民的多样偏好，LLMs有更趋向于一致选择的倾向。这一发现表明，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
    
[^164]: LLMs 模拟五大人格特质：进一步的证据

    LLMs Simulate Big Five Personality Traits: Further Evidence

    [https://arxiv.org/abs/2402.01765](https://arxiv.org/abs/2402.01765)

    本研究旨在研究大型语言模型（LLMs）如何模拟五大人格特质，并分析了其模拟的人格特质及稳定性，有助于深入了解LLMs在个性化人机交互方面的潜力。

    

    本研究探讨了大型语言模型（LLMs）如Llama2、GPT4和Mixtral对五大人格特质的模拟，并分析了这些模型模拟的人格特质及其稳定性。这有助于更广泛地了解LLMs模拟人格特质的能力以及对个性化人机交互的相关影响。

    An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.
    
[^165]: 当大型语言模型遇上向量数据库：一项综述

    When Large Language Models Meet Vector Databases: A Survey

    [https://arxiv.org/abs/2402.01763](https://arxiv.org/abs/2402.01763)

    本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。

    

    最近大型语言模型的突破在人类文字处理和生成方面开启了新的领域。然而，随着它们的显著增长，大型语言模型面临着包括幻觉、偏见、实时知识更新以及在商业环境中实施和维护的高成本等重要挑战。而另一种日益流行的工具，向量数据库则为这些挑战提供了潜在的解决方案。这些数据库擅长处理高维数据，并且对于高效的信息检索和语义搜索等任务至关重要。通过与大型语言模型的整合，它们显著增强了人工智能系统管理和更有效地利用多样数据的能力。本综述论文对大型语言模型和向量数据库之间的交叉点进行了深入而独特的分析。

    The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
    
[^166]: 在大语言模型时代重新思考可解释性

    Rethinking Interpretability in the Era of Large Language Models

    [https://arxiv.org/abs/2402.01761](https://arxiv.org/abs/2402.01761)

    大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。

    

    可解释的机器学习在过去十年中成为一个热门领域，受到越来越大的数据集和深度神经网络的崛起的推动。与此同时，大语言模型（LLMs）在各种任务中展示出了卓越的能力，为重新思考可解释机器学习的机会提供了可能。值得注意的是，以自然语言解释的能力使得LLMs能够扩展给人类的规模和复杂性上的模式。然而，这些新的能力也带来了新的挑战，比如虚构的解释和巨大的计算成本。在这篇立场论文中，我们首先回顾了评估新兴LLM解释领域的现有方法（包括解释LLM和使用LLM进行解释）。我们认为，尽管存在局限性，LLMs能够重新定义解释性，涵盖更广泛的应用领域，包括对LLMs本身的审计。

    Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high
    
[^167]: 系统性文献综述：用于幽默风格分类的计算方法

    Systematic Literature Review: Computational Approaches for Humour Style Classification

    [https://arxiv.org/abs/2402.01759](https://arxiv.org/abs/2402.01759)

    这项研究通过系统性文献综述展示了计算方法在幽默风格分类中的应用情况，并指出了当前的研究空白和有希望的方向。

    

    理解各种幽默风格对于理解幽默的多面性及其在心理学和人工智能等领域的影响至关重要。这种理解揭示了依据所采用的风格，幽默可以对个人的健康和人际关系产生治疗或有害的影响。虽然专门研究基于计算的幽默风格分析的研究仍然比较少见，但在相关任务中，特别是二元幽默和讽刺识别方面，已有大量研究蓬勃发展。在这项系统性文献综述中，我们调查了应用于这些相关任务的计算技术的现状，并揭示了它们与幽默风格分析的基本相关性。通过这项研究，我们揭示了常见的方法，阐明了各种数据集和评估指标，并有效地引导幽默研究的复杂领域。我们的努力确定了潜在的研究空白，并提出了有希望的方向。

    Understanding various humour styles is essential for comprehending the multifaceted nature of humour and its impact on fields such as psychology and artificial intelligence. This understanding has revealed that humour, depending on the style employed, can either have therapeutic or detrimental effects on an individual's health and relationships. Although studies dedicated exclusively to computational-based humour style analysis remain somewhat rare, an expansive body of research thrives within related task, particularly binary humour and sarcasm recognition. In this systematic literature review (SLR), we survey the landscape of computational techniques applied to these related tasks and also uncover their fundamental relevance to humour style analysis. Through this study, we unveil common approaches, illuminate various datasets and evaluation metrics, and effectively navigate the complex terrain of humour research. Our efforts determine potential research gaps and outlined promising di
    
[^168]: Aalap：印度法律和法律助理功能的人工智能助手

    Aalap: AI Assistant for Legal & Paralegal Functions in India

    [https://arxiv.org/abs/2402.01758](https://arxiv.org/abs/2402.01758)

    Aalap是一个在印度法律任务上针对指令数据进行微调的AI助手，相比于gpt-3.5-turbo在测试数据中表现更好，主要教授法律推理，对律师、法官或在法律系统中工作的人有帮助。

    

    在法律任务中使用专有的大型语言模型面临数据隐私问题、领域数据异构性、领域知识复杂性和领域目标独特性的挑战。我们创建了Aalap，它是在与印度特定法律任务相关的指令数据上经过微调的Mistral 7B模型。 Aalap在我们的测试数据中比gpt-3.5-turbo表现更好的比例为31％，在34％的测试数据中与GPT4评估得分相当。 Aalap的训练主要侧重于教授法律推理而不是法律记忆。 Aalap对律师、法官或在法律系统中工作的人的日常活动肯定有所帮助。

    Using proprietary Large Language Models on legal tasks poses challenges due to data privacy issues, domain data heterogeneity, domain knowledge sophistication, and domain objectives uniqueness. We created Aalalp, a fine-tuned Mistral 7B model on instructions data related to specific Indian legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of our test data and obtains an equivalent score in 34\% of the test data as evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning rather than legal recall. Aalap is definitely helpful for the day-to-day activities of lawyers, judges, or anyone working in legal systems.
    
[^169]: 通过分析音频识别辱骂言论和假消息，辨别僞作和仇恨言论在僧伽罗语YouTube视频中

    Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio

    [https://arxiv.org/abs/2402.01752](https://arxiv.org/abs/2402.01752)

    该研究提出了一种通过分析音频来辨别僞作和仇恨言论在僧伽罗语YouTube视频中的解决方案。这些方法包括评估视频是否包含虚假信息以及检测其中是否存在仇恨言论。

    

    YouTube面临着全球范围内虚假信息和仇恨言论的传播危机。为解决这些问题，YouTube已实施严格规定，禁止上传包含虚假信息或宣传仇恨言论的内容。虽然已经进行了许多降低冒犯性英语内容的研究，但对僧伽罗语内容的研究仍然相对较少。本研究旨在通过提出解决方案，减少僧伽罗语YouTube视频中暴力和虚假信息的传播。该方法包括开发一个评级系统，通过比较标题和描述与音频内容，评估视频是否包含虚假信息，并检测其中是否存在仇恨言论。方法包括使用Pytube库进行音频提取，使用经过微调的Whisper模型进行音频转录，使用distilroberta-base模型进行仇恨言论检测和文本分类。

    YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification L
    
[^170]: ChatGPT与Bard在检测阿尔茨海默病痴呆的性能评估

    Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia

    [https://arxiv.org/abs/2402.01751](https://arxiv.org/abs/2402.01751)

    本研究评估了ChatGPT和Bard在识别阿尔茨海默病痴呆的能力，并发现Bard在积极识别AD方面表现最好，具有较高的召回率和F1得分，但可能会将CN错误地识别为AD。对于积极识别CN，GPT-4表现出最高的真阴性。

    

    大型语言模型（LLM）在许多领域中有着越来越多的应用。本研究评估了三个LLM聊天机器人（ChatGPT-3.5，ChatGPT-4和Bard）在其当前公开形式下，使用从自发语音记录中提取的文本输入，对阿尔茨海默病痴呆（AD）和认知正常（CN）个体进行识别的能力。采用零样本学习方法，在两个独立查询级别上进行，第二个查询（思维链引导）比第一个查询产生更详细的结果。通过评估每个LLM聊天机器人在准确度、敏感度、特异度、精确度和F1得分方面生成的预测来评估LLM聊天机器人的性能。LLM聊天机器人生成了三类结果（"AD"，"CN"或"Unsure"）。在积极识别AD时，Bard产生了最高的真阳性（89%的召回率）和最高的F1得分（71%），但倾向于将CN错误地识别为AD，并具有较高的置信度（较低的"Unsure"率）；在积极识别CN时，GPT-4产生了最高的真阴性。

    Large language models (LLMs) find increasing applications in many fields. Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in their current form, as publicly available - for their ability to recognize Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual input derived from spontaneous speech recordings. Zero-shot learning approach is used at two levels of independent queries, with the second query (chain-of-thought prompting) eliciting more detailed than the first. Each LLM chatbot's performance is evaluated on the prediction generated in terms of accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots generated three-class outcome ("AD", "CN", or "Unsure"). When positively identifying AD, Bard produced highest true-positives (89% recall) and highest F1 score (71%), but tended to misidentify CN as AD, with high confidence (low "Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest true-negatives 
    
[^171]: PACE：利用大型语言模型增强通信效率的实用智能体

    PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models

    [https://arxiv.org/abs/2402.01750](https://arxiv.org/abs/2402.01750)

    PACE是一种利用大型语言模型的实用智能体，提供了基于图像的实用通信框架。它通过语义感知、意图解析和以意图为导向的编码来增强通信效率。为了有效利用语言模型，它使用知识库补充所需知识，引入专用提示来促进对实用通信场景和任务要求的理解，并设计思维链以帮助权衡传输效率和质量。

    

    当前通信技术在理论容量、频谱可用性和功耗资源方面存在限制。实用通信利用终端智能进行选择性数据传输，提供资源节约。现有研究缺乏通用意图解析工具，限制了其适用性。本文提出了一种基于大型语言模型的实用智能体（PACE）的图像实用通信框架。在该框架下，PACE依次执行语义感知、意图解析和以意图为导向的编码。为了确保在通信中有效使用大型语言模型，设计了一个知识库来补充所需的知识，引入了专用提示来促进对实用通信场景和任务要求的理解，并设计了一条思维链以帮助在传输效率和质量之间进行合理的权衡。

    Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and c
    
[^172]: 大型多模型(LMMs)作为AI原生无线系统的通用基础模型

    Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

    [https://arxiv.org/abs/2402.01748](https://arxiv.org/abs/2402.01748)

    本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。

    

    最近，大型语言模型(LLMs)和基础模型被宣称为6G系统的改变者。然而，目前关于无线网络的LLMs的努力仅限于直接应用现有的为自然语言处理(NLP)应用设计的语言模型。为了解决这一挑战，并创建以无线为中心的基础模型，本文提出了一个全面的视野，介绍了如何设计针对部署人工智能(AI)原生网络的通用基础模型。与基于NLP的基础模型不同，所提出的框架通过三个关键能力促进了大型多模型(LMMs)的设计：1) 处理多模态感知数据，2) 通过因果推理和检索增强生成(RAG)将物理符号表示与现实世界的无线系统联系起来，3) 通过无线环境反馈实现可教导性，以促进动态网络配置。

    Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
    
[^173]: 优化LLM使用成本的研究

    Towards Optimizing the Costs of LLM Usage

    [https://arxiv.org/abs/2402.01742](https://arxiv.org/abs/2402.01742)

    本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。

    

    生成式人工智能，特别是LLM在现今广泛应用于各种文件处理任务中，如问答和摘要。然而，不同的LLM在不同任务上具有不同的能力、成本、标记化和延迟。实际上，企业已经在为各自的用例运营或使用LLM而承担巨大的成本。在这项工作中，我们提出通过估计LLM的输出质量（而无需实际调用LLM），然后解决LLM选择的优化例程，以在质量和延迟方面保持成本在预算范围内或最小化成本。我们提出了一个模型来预测LLM在摘要等文件处理任务中的输出质量，随后采用LP取整算法来优化LLM的选择。我们从理论和实证的角度研究了在质量和成本之间权衡的优化问题。

    Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente
    
[^174]: 开发并测试一种新的基于大型语言模型的临床决策支持系统，用于药物安全的12种临床专业

    Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties

    [https://arxiv.org/abs/2402.01741](https://arxiv.org/abs/2402.01741)

    本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。

    

    重要性：我们介绍了一种新颖的基于检索增强生成（RAG）-大型语言模型（LLM）的临床决策支持系统（CDSS），用于安全用药处方。该模型通过提供与患者背景和机构指南相关的处方错误警报，解决了传统基于规则的CDSS的局限性。目标：本研究评估了基于LLM的CDSS在识别各种医学和外科病例中的药物错误方面的有效性，与人工专家小组进行比较。它还研究了临床医生在不同CDSS集成方式（初级药师、仅基于LLM的CDSS和二者的组合）中的偏好。设计、设置和参与者：利用带有GPT-4.0的RAG模型，本研究涉及12个专业中23个临床案例的61个处方错误场景。专家小组使用PCNE分类和NCC MERP指数评估这些案例。三名初级药师独立审核每个场景，并提出处理建议。根据检查的错误和建议编制了反馈报告。 然后，三名医生独立审核这些报告，并提出对下一步处理的意见。

    Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
    
[^175]: 在认知负荷下的补偿性偏见：减少大型语言模型中的选择偏见

    Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models

    [https://arxiv.org/abs/2402.01740](https://arxiv.org/abs/2402.01740)

    这项研究研究了大型语言模型在选择对象时的偏见，发现偏见结构依赖于模型，对象类型调节了偏见的影响程度，导致列表中的第一个对象在输出中被过度呈现。

    

    大型语言模型（LLMs）如gpt-3.5-turbo和claude-instant-1.2在解释和执行语义任务方面变得非常重要。不幸的是，这些模型固有的偏见，类似于人类的认知偏见，会对它们的性能产生不利影响。其中一个受到影响最大的是从列表中进行对象选择，这是数字导航和决策制定中的基本操作。本研究重点检查这些偏见，并量化其对代表性列表选择任务的影响。通过进行一系列控制实验，我们操纵了温度、列表长度、对象身份、对象类型、提示复杂度和模型，以探索这些偏见。这使得我们能够孤立和测量这些偏见对选择行为的影响。我们的研究结果表明，偏见结构在很大程度上取决于模型，而对象类型调节了偏见影响的程度。由于存在较强的初现效应，列表中的第一个对象会在输出中被过度呈现。

    Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
    
[^176]: OpenMoE：开源混合专家语言模型的早期努力

    OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

    [https://arxiv.org/abs/2402.01739](https://arxiv.org/abs/2402.01739)

    OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。

    

    为了帮助开源社区更好地理解基于混合专家(MoE)的大型语言模型(LLM)，我们训练并发布了OpenMoE，一系列完全开放源码和可复现的仅解码器MoE LLM，参数范围从650M到34B，训练数据超过1T个标记。我们的研究证实，MoE-based LLM可以提供比密集LLM更有利的成本效益平衡，突出了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们的OpenMoE模型中的路由机制进行深入分析，得到了三个重要发现：上下文无关专业化、早期路由学习和末尾降低。我们发现，MoE模型中的路由决策主要基于标记ID，与上下文相关性很小。标记到专家的分配在预训练阶段早期确定，并且基本保持不变。这种不完全的路由可能导致...

    To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
    
[^177]: C4Q: 一个用于量子计算的聊天机器人

    C4Q: A Chatbot for Quantum

    [https://arxiv.org/abs/2402.01738](https://arxiv.org/abs/2402.01738)

    C4Q是一个用于量子计算的聊天机器人，能够准确回答基本问题并在编写量子程序时引导用户，通过使用自己的引擎而能够保证答案的准确性。

    

    量子计算是一个快速发展的领域，承诺着许多实际应用，如量子密码学或量子金融。然而，能够使用量子计算的人数仍然非常有限。这一限制来自于理解概念和如何开始编程的困难。因此，需要一种工具来帮助非专家克服这种复杂性。使用现有的对话系统是一个可能的选择。不幸的是，ChatGPT和其他大型语言模型产生了不准确的结果。本文介绍了C4Q，一个能够准确回答基本问题并在编写量子程序时引导用户的聊天机器人。与其他方法相反，C4Q只使用一个经过预训练的大型语言模型来发现和分类用户请求。然后，它使用自己的引擎生成准确的答案。由于这种架构设计，C4Q的答案总是正确的，因此C4Q可以成为一个支持工具，使量子计算成为可能。

    Quantum computing is a growing field that promises many real-world applications such as quantum cryptography or quantum finance. The number of people able to use quantum computing is however still very small. This limitation comes from the difficulty to understand the concepts and to know how to start coding. Therefore, there is a need for tools that can assist non-expert in overcoming this complexity. One possibility would be to use existing conversational agents. Unfortunately ChatGPT and other Large-Language Models produce inaccurate results. This article presents C4Q, a chatbot that answers accurately basic questions and guides users when trying to code quantum programs. Contrary to other approaches C4Q uses a pre-trained large language model only to discover and classify user requests. It then generates an accurate answer using an own engine. Thanks to this architectural design, C4Q's answers are always correct, and thus C4Q can become a support tool that makes quantum computing m
    
[^178]: 为社交感知的谈判对话开发辅助大型语言模型代理

    Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues

    [https://arxiv.org/abs/2402.01737](https://arxiv.org/abs/2402.01737)

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。

    

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们通过让两个大型语言模型（LLM）扮演每次对话中的两名谈判者来模拟现实世界谈判。第三个LLM充当修正代理，重新编写违反规范的话语以改善谈判结果。由于这是一个新颖的任务，不存在手动构建的数据。为解决这个限制，我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性，即产品销售、房价和薪资谈判。源代码和生成的数据集将在接受后公开。

    In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
    
[^179]: SADAS: 一个面向双语社会文化对话修复规范违例的对话助手系统

    SADAS: A Dialogue Assistant System Towards Remediating Norm Violations in Bilingual Socio-Cultural Conversations

    [https://arxiv.org/abs/2402.01736](https://arxiv.org/abs/2402.01736)

    SADAS是一个面向双语社会文化对话的对话助手系统，旨在通过识别规范类别、检测违例、评估严重程度、实施纠正措施并阐述理由等新颖架构，确保不同文化背景的个体之间的对话能够以尊重和理解的方式进行。

    

    在当今全球化的世界中，弥合文化差异对于建立有意义的联系比以往任何时候都更加重要。社会感知对话助手系统（SADAS）是我们对这一全球挑战的回答，旨在确保来自不同文化背景的个体之间的对话能够以尊重和理解的方式进行。我们系统的新颖架构包括：（1）识别对话中存在的规范类别，（2）检测潜在的规范违例，（3）评估这些违例的严重程度，（4）实施有针对性的措施来纠正违规行为，并（5）阐述这些纠正措施的理由。我们采用一系列最先进的技术来构建不同的模块，并进行大量实验选择每个模块最合适的骨干模型。我们还设计了一个人类偏好实验来验证系统的整体性能。我们将开源我们的系统（包括...

    In today's globalized world, bridging the cultural divide is more critical than ever for forging meaningful connections. The Socially-Aware Dialogue Assistant System (SADAS) is our answer to this global challenge, and it's designed to ensure that conversations between individuals from diverse cultural backgrounds unfold with respect and understanding. Our system's novel architecture includes: (1) identifying the categories of norms present in the dialogue, (2) detecting potential norm violations, (3) evaluating the severity of these violations, (4) implementing targeted remedies to rectify the breaches, and (5) articulates the rationale behind these corrective actions. We employ a series of State-Of-The-Art (SOTA) techniques to build different modules, and conduct numerous experiments to select the most suitable backbone model for each of the modules. We also design a human preference experiment to validate the overall performance of the system. We will open-source our system (includin
    
[^180]: VIALM：关于具有大型模型的视觉障碍辅助的调查和基准研究

    VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models

    [https://arxiv.org/abs/2402.01735](https://arxiv.org/abs/2402.01735)

    这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。

    

    视觉障碍辅助 (VIA) 旨在自动帮助视觉障碍者 (VI) 处理日常活动。VIA 的进展主要依赖于计算机视觉 (CV) 和自然语言处理 (NLP) 的发展，二者都展示了利用大型模型 (LMs) 的前沿范式。此外，LMs 展现出出色的多模态能力，可以应对诸如具身机器人等具有挑战性的物理任务。为了研究最先进 (SOTA) LMs 在VIA应用中的潜力和局限性，我们针对具有LMs的VIA任务（VIALM）进行了广泛的研究。在这个任务中，给定一个说明物理环境的图像和视觉障碍者用户的语言请求，VIALM旨在输出逐步引导，以在环境中帮助视觉障碍用户完成请求。该研究包括对近期LM研究的调查和对选定LMs能力的基准实验的检查。

    Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
    
[^181]: CFTM: 连续时间分数话题模型

    CFTM: Continuous time fractional topic model

    [https://arxiv.org/abs/2402.01734](https://arxiv.org/abs/2402.01734)

    CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。

    

    本文提出了连续时间分数话题模型（cFTM），一种新的动态主题建模方法。该方法利用分数布朗运动（fBm）有效地识别主题和词分布随时间的正负相关性，揭示长期依赖性或粗糙度。我们的理论分析表明，cFTM可以捕捉到主题和词分布中的这些长期依赖性或粗糙度，反映了fBm的主要特征。此外，我们证明了cFTM的参数估计过程与传统主题模型LDA的相当。为了证明cFTM的性质，我们使用经济新闻文章进行了实证研究。这些测试的结果支持该模型能够识别和跟踪主题随时间的长期依赖性或粗糙度。

    In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
    
[^182]: 大型语言模型中检索增强生成的开发和测试--案例研究报告

    Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report

    [https://arxiv.org/abs/2402.01733](https://arxiv.org/abs/2402.01733)

    RAG模型是一种有前景的方法，用于在大型语言模型中定制领域知识。本研究开发和评估了一个专为医疗保健定制的LLM-RAG流程，重点关注术前医学。

    

    目的：大型语言模型(LLMs)在医学应用中具有重要的潜力。检索增强生成(RAG)作为一种有前景的方法，用于定制LLMs中的领域知识。本案例研究介绍了一个专为医疗保健定制的LLM-RAG流程的开发和评估，重点关注术前医学。方法：我们使用了35个术前指南开发了一个LLM-RAG模型，并通过与人工生成的回答进行测试，共评估了1260个回答。RAG流程涉及使用基于Python的LangChain和Llamaindex框架将临床文档转换为文本，并将这些文本处理为块以用于嵌入和检索。利用Pinecone进行向量存储和使用1536维余弦相似度损失度量来优化数据检索，其中选择了嵌入模型。将由初级医生提供的人工生成回答用作比较。结果：LLM-RA

    Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine.   Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison.   Results: The LLM-RA
    
[^183]: 评估LLM生成的医学图像和症状分析的多模态诊断

    Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and Symptom Analysis

    [https://arxiv.org/abs/2402.01730](https://arxiv.org/abs/2402.01730)

    本文提出了一种评估LLM生成医学诊断的方法，通过结构化交互和领域特定分析来评估其正确性和准确性。我们使用GPT-4-Vision-Preview作为LLM，并使用多模态多项选择题评估其在病理学领域的表现。

    

    大型语言模型（LLM）是一种突破性的人工智能技术，正在快速发展，并承诺帮助医学诊断。然而，它们的返回结果的正确性和准确性尚未得到适当评估。在这项工作中，我们提出了一种LLM评估范式，它包含两个独立步骤的新方法，即（1）通过结构化交互进行多模态LLM评估和（2）基于之前交互提取的数据进行后续的领域特定分析。使用这种范式，（1）我们通过公开的多模态多项选择题（MCQs）在病理学领域评估LLM生成的医学诊断的正确性和准确性，（2）然后对提取的结果进行系统和全面的分析。我们将GPT-4-Vision-Preview作为LLM，回答由图像和文本组成的复杂医学问题，并探索了一系列的疾病、病况、化学状况。

    Large language models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. However, the correctness and the accuracy of their returns has not yet been properly evaluated. In this work, we propose an LLM evaluation paradigm that incorporates two independent steps of a novel methodology, namely (1) multimodal LLM evaluation via structured interactions and (2) follow-up, domain-specific analysis based on data extracted via the previous interactions. Using this paradigm, (1) we evaluate the correctness and accuracy of LLM-generated medical diagnosis with publicly available multimodal multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a systemic and comprehensive analysis of extracted results. We used GPT-4-Vision-Preview as the LLM to respond to complex, medical questions consisting of both images and text, and we explored a wide range of diseases, conditions, chem
    
[^184]: 从大型语言模型中提取上下文信息用于知识图谱补全

    Contextualization Distillation from Large Language Model for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.01729](https://arxiv.org/abs/2402.01729)

    本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。

    

    虽然文本信息显著提高了预训练语言模型（PLMs）在知识图谱补全（KGC）中的性能，但现有语料库从维基百科文章或同义词定义中收集的静态和噪声性质常常限制了基于PLM的KGC模型的潜力。为了克服这些挑战，我们提出了上下文化蒸馏策略，这是一种通用的可插入和可播放的方法，与判别和生成的KGC框架兼容。我们的方法首先指导大型语言模型（LLMs）将紧凑的结构化三元组转换为上下文丰富的段落。随后，我们引入了两个定制的辅助任务，重建和上下文化，使较小的KGC模型能够吸收这些丰富的三元组中的见解。对多种数据集和KGC技术的全面评估突出了我们方法的功效和适应性，揭示了无论基础管道如何，始终能提高性能。

    While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
    
[^185]: 硬件Phi-1.5B：一个大型语言模型编码硬件领域专业知识

    Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge

    [https://arxiv.org/abs/2402.01728](https://arxiv.org/abs/2402.01728)

    硬件Phi-1.5B是一个专门针对半导体产业硬件领域的大型语言模型，通过预训练和专业分层数据集解决了硬件领域的复杂性和数据稀缺性问题。

    

    在快速发展的半导体产业中，研究、设计、验证和制造是紧密相连的，大型语言模型在革新硬件设计和安全验证方面有巨大潜力。然而，主要挑战在于硬件特定问题的复杂性，这些问题在预训练阶段通常不能充分解决自然语言或软件代码知识。此外，缺乏与硬件领域相关的数据集也是开发基础模型的重要障碍。针对这些挑战，本文介绍了硬件Phi 1.5B，这是一个专门针对半导体产业硬件领域的创新大型语言模型。我们开发了一个专业分层数据集，包括小、中和大型子集，并将重点放在使用中型数据集进行预训练。这种方法充分利用了Ph模型的紧凑但高效的架构。

    In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi 1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset comprising small, medium, and large subsets and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Ph
    
[^186]: AI中介交流的指导：AI不改变对文本消息的感知

    Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages

    [https://arxiv.org/abs/2402.01726](https://arxiv.org/abs/2402.01726)

    这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。

    

    对于许多人来说，焦虑、抑郁和其他社交和心理因素可能使撰写文本消息成为一项积极的挑战。为解决这个问题，大型语言模型（LLM）可能是帮助那些本来会觉得发送短信困难或有压力的用户的完美工具。然而，尽管大型语言模型的使用快速普及，但对其在文本消息撰写中的辅助使用的考虑还未被探索。关于大型语言模型使用的一个主要关注点是，AI的公众情绪较差可能导致其辅助的文本消息的使用对感知产生负面影响，从而使使用适得其反。为了验证这种可能性，我们探讨了人们是否认为一条文本消息是否在撰写过程中得到了AI的辅助，会改变其感知的语调、清晰度和表达意图的能力。在这项研究中，我们调查了26名参与者对18条随机标记的预先撰写的文本消息的感知。通过分析参与者对消息语调的评分，我们发现。

    For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
    
[^187]: 在人工智能中加强道德界限：在大型语言模型中增强安全的高级策略

    Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models

    [https://arxiv.org/abs/2402.01725](https://arxiv.org/abs/2402.01725)

    本篇论文介绍了一种多方面的方法来解决大型语言模型中的道德和安全挑战，包括过滤敏感词汇、检测角色扮演、实施自定义规则引擎，以及应用到不同的派生模型中。这些方法可以提高模型的安全性，降低道德和隐私方面的风险。

    

    最近在大型语言模型（LLMs）方面的进展显著增强了自然语言处理和人工智能的能力。这些模型，包括GPT-3.5和LLaMA-2，由于具有变革性的Transformer模型，在文本生成、翻译和问答任务中取得了革命性的突破。尽管被广泛使用，LLMs也带来了一些挑战，例如在被迫做出不当回应时产生的道德困境，易受网络钓鱼攻击和侵犯隐私等问题。本文通过引入多方面的方法来解决这些挑战，包括：1）从用户输入中过滤敏感词汇，以防止产生不道德的回应；2）检测角色扮演，以阻止可能引发“越狱”情境的互动；3）实施自定义规则引擎，限制禁止内容的生成；以及4）将这些方法应用于各种LLM派生模型，如Multi-Model Large Language Models (MLLMs)。我们的方法不仅可以提高语言模型的安全性，还可以降低道德和隐私方面的风险。

    Recent advancements in large language models (LLMs) have significantly enhanced capabilities in natural language processing and artificial intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized text generation, translation, and question-answering tasks due to the transformative Transformer model. Despite their widespread use, LLMs present challenges such as ethical dilemmas when models are compelled to respond inappropriately, susceptibility to phishing attacks, and privacy violations. This paper addresses these challenges by introducing a multi-pronged approach that includes: 1) filtering sensitive vocabulary from user input to prevent unethical responses; 2) detecting role-playing to halt interactions that could lead to 'prison break' scenarios; 3) implementing custom rule engines to restrict the generation of prohibited content; and 4) extending these methodologies to various LLM derivatives like Multi-Model Large Language Models (MLLMs). Our approach not onl
    
[^188]: CERM: 通过情感分析进行基于文献的上下文感知发现

    CERM: Context-aware Literature-based Discovery via Sentiment Analysis

    [https://arxiv.org/abs/2402.01724](https://arxiv.org/abs/2402.01724)

    CERM是一个通过情感分析进行基于文献的上下文感知发现的系统，旨在理解食品与健康之间的关系。通常情况下，基于食材营养成分或基于标记数据的计算模型已被用于食谱推荐和分析系统。然而，本研究提出了一种增强模型，通过捕捉食材与生物医学概念之间的固有关系，利用标记和未标记的数据来更好地支持食品相关研究。

    

    鉴于生物医学出版物的丰富，我们引入了一项情感分析任务来理解食品与健康之间的关系。之前将健康纳入食谱推荐和分析系统的尝试主要集中在食材营养成分上，或者利用基于标记数据的基本计算模型进行训练。捕捉食材和生物医学概念之间固有关系的增强模型对于食品相关研究更有益处，鉴于生物医学文本中的丰富信息。考虑到昂贵的数据标记过程，这些模型应该有效利用标记和未标记的数据。本文介绍了一项名为实体关系情感分析（ERSA）的新任务，该任务基于实体对捕捉文本的情感。ERSA扩展了广泛研究的基于方面的情感分析（ABSA）任务。具体而言，我们的研究集中在应用于生物医学文本的ERSA任务上，重点关注(entity-ent

    Driven by the abundance of biomedical publications, we introduce a sentiment analysis task to understand food-health relationship. Prior attempts to incorporate health into recipe recommendation and analysis systems have primarily focused on ingredient nutritional components or utilized basic computational models trained on curated labeled data. Enhanced models that capture the inherent relationship between food ingredients and biomedical concepts can be more beneficial for food-related research, given the wealth of information in biomedical texts. Considering the costly data labeling process, these models should effectively utilize both labeled and unlabeled data. This paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that captures the sentiment of a text based on an entity pair. ERSA extends the widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our study concentrates on the ERSA task applied to biomedical texts, focusing on (entity-ent
    
[^189]: 在中国工业场景下对大型语言模型的准确性和鲁棒性的实证研究

    An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios

    [https://arxiv.org/abs/2402.01723](https://arxiv.org/abs/2402.01723)

    本文对中国工业领域的大型语言模型进行了实证研究，评估了其在准确性和鲁棒性方面的表现。

    

    近年来，大型语言模型（LLMs）在各个领域取得了快速发展。为了更好地服务大量的中国用户，中国的许多商业供应商采取了本地化战略，训练并提供专门为中国用户定制的本地LLMs。此外，展望未来，LLMs的一个重要应用领域将是企业和用户在工业生产领域实际部署。然而，LLMs在工业场景中的准确性和鲁棒性尚未得到很好的研究。本文在中国工业生产领域的背景下，对LLMs的准确性和鲁棒性进行了全面的实证研究。我们手动收集了来自8个不同工业部门的1200个领域特定问题来评估LLMs的准确性。此外，我们设计了一个包含四个工业特定稳定性类别和八个能力的变态测试框架，总计13,631个问题用于评估LLMs的鲁棒性。

    Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions wi
    
[^190]: 提升大型语言模型的性能，以更准确地回答问题和提取信息

    Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately

    [https://arxiv.org/abs/2402.01722](https://arxiv.org/abs/2402.01722)

    通过使用反馈和示例的精调过程，结合余弦相似度、LLM评估和Rouge-L得分等指标，可以提升大型语言模型（LLMs）在回答问题和提取信息方面的准确性。与零-shot LLMs相比，经过精调的模型展示了出色的问答能力，并且通过结合RAG过程进一步提高了其效果。

    

    大型语言模型（LLMs）生成问题的回答，然而它们的有效性常常受到答案质量的不佳和偶尔无法准确回答问题的影响。为了应对这些挑战，采用了一种精调过程，利用反馈和示例来优化模型。优化的目标是通过持续的反馈循环和利用余弦相似度、LLM评估和Rouge-L得分等指标来提升AI模型的性能。利用像GPT-3.5、GPT4ALL、LLaMA2和Claude这样的LLMs，并在包括FinanceBench和RAG Instruct Benchmark Tester Dataset在内的金融数据集上进行基准测试，展示了精调的必要性。结果表明，经过精调的模型能够超越零-shot LLMs的准确性，提供卓越的问答能力。值得注意的是，将LLM与一种名为RAG的检索增强生成过程相结合的方法证明了其有效性。

    Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves
    
[^191]: 基于深度学习的阿姆哈拉语常见问题解答聊天机器人

    Deep Learning Based Amharic Chatbot for FAQs in Universities

    [https://arxiv.org/abs/2402.01720](https://arxiv.org/abs/2402.01720)

    本文提出了一个基于深度学习的阿姆哈拉语常见问题解答聊天机器人模型，可以帮助大学生解答常见问题，通过使用自然语言处理和深度学习技术，采用多种机器学习模型算法进行分析和分类，取得了最好的成绩。

    

    大学生常常花费大量时间向管理员或教师寻求常见问题的答案。这对双方来说都很繁琐，需要找到一个解决方案。为此，本文提出了一个聊天机器人模型，利用自然语言处理和深度学习技术，在阿姆哈拉语中回答常见问题。聊天机器人是通过人工智能模拟人类对话的计算机程序，作为虚拟助手处理问题和其他任务。所提出的聊天机器人程序使用标记化、规范化、去除停用词和词干提取对阿姆哈拉语输入句子进行分析和分类。采用了三种机器学习模型算法来分类标记和检索合适的回答：支持向量机（SVM）、多项式朴素贝叶斯和通过TensorFlow、Keras和NLTK实现的深度神经网络。深度学习模型取得了最好的成绩。

    University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
    
[^192]: 在大型语言模型中测量道德不一致性

    Measuring Moral Inconsistencies in Large Language Models

    [https://arxiv.org/abs/2402.01719](https://arxiv.org/abs/2402.01719)

    本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。

    

    如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。

    A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
    
[^193]: 从RAG到QA-RAG：将生成式AI应用于药品监管合规流程

    From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process

    [https://arxiv.org/abs/2402.01717](https://arxiv.org/abs/2402.01717)

    本文介绍了一种利用生成式AI和检索增强生成（RAG）方法的聊天机器人模型，名为QA-RAG，用于解决制药行业中的合规性挑战，并在准确性上取得了显著改进。

    

    制药行业的合规性要求需要面对复杂且大量的指南文件，通常需要大量人力资源。为了解决这些挑战，我们的研究引入了一种利用生成式AI和检索增强生成（RAG）方法的聊天机器人模型。该聊天机器人旨在搜索与用户查询相关的指南文件，并根据检索到的指南提供答案。考虑到这个领域对高可靠性的需求，我们提出了问题回答检索增强生成（QA-RAG）模型。在对比实验中，QA-RAG模型在准确性上显示出显著改进，优于包括传统RAG方法在内的所有其他基线模型。本文详细介绍了QA-RAG的结构和性能评估，并强调其在药品监管合规领域及其他领域的潜力。我们已将我们的工作公开提供给进一步研究。

    Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further researc
    
[^194]: Bloom-认知和情感分析层次分类在课程讨论论坛中的应用

    Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums

    [https://arxiv.org/abs/2402.01716](https://arxiv.org/abs/2402.01716)

    本研究提出了一种称为布鲁姆-认知和情感分析（BE-Sent）的层次化方法，用于评估教育讨论论坛中的情绪和布鲁姆的认知分类。方法包括数据收集、文本预处理、情感分析和认知分类。研究结果有助于了解学生在学习过程中的进展情况和知识水平。

    

    在线讨论论坛广泛被用于讲师和学生之间的积极文本交流，以及检查学生在学习过程中的进展情况。本研究的目标是比较适合的机器学习模型，以评估教育讨论论坛中基于文本评论的情绪和布鲁姆的认知分类。我们提出的方法被称为布鲁姆认知和情感分析的层次化方法（BE-Sent）。研究方法包括三个主要步骤。第一步是从内部讨论论坛和YouTube频道的评论中收集数据。下一步是对文本进行预处理，对文本进行标注并清除不重要的单词。此外，对已成功清理的文本数据集，将在每个句子中进行情感分析和认知分类。情感分析分为三个类别：积极的，消极的和中性的。布鲁姆（Bloom）的认知分类是根据认知过程的六个层次进行的，从低层次的知识记忆到高层次的评价和创造。

    Online discussion forums are widely used for active textual interaction between lecturers and students, and to see how the students have progressed in a learning process. The objective of this study is to compare appropriate machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy based on textual comments in educational discussion forums. Our proposed method is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis (BE-Sent). The research methodology consists of three main steps. The first step is the data collection from the internal discussion forum and YouTube comments of a Web Programming channel. The next step is text preprocessing to annotate the text and clear unimportant words. Furthermore, with the text dataset that has been successfully cleaned, sentiment analysis and epistemic categorization will be done in each sentence of the text. Sentiment analysis is divided into three categories: positive, negative, and neutral. Bloom\'s epistem
    
[^195]: ChatGPT、Gemini和LLaMA2在多语言情感分析方面的比较

    ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis

    [https://arxiv.org/abs/2402.01715](https://arxiv.org/abs/2402.01715)

    本研究对ChatGPT、Gemini和LLaMA2等多语言情感分析模型进行了比较，发现这些模型在处理模棱两可的情境时表现良好，但在不同语言和评估中存在显著的偏差和性能不一致。这项工作为自动情感分析提供了标准化的评估方法，并呼吁改进算法和数据，以提高性能、可解释性和适用性。

    

    使用ChatGPT、Gemini或LLaMA2等基于大型语言模型（LLM）的模型进行自动情感分析，如今在学术研究和工业应用中越来越普遍。然而，在处理模棱两可或具有讽刺意味的文本时，对它们的性能进行评估和验证仍然不足。在本研究中，我们构建了细致和模棱两可的情境，并将其翻译成10种语言，然后使用流行的LLM进行情感预测。结果经过事后验证的人类回应来验证。ChatGPT和Gemini通常对模棱两可的情境处理得很好，但我们发现模型和评估的人类语言之间存在显著的偏差和性能不一致。这项工作为自动情感分析提供了标准化的评估方法，并呼吁进一步改进算法及其基础数据，以提高性能、可解释性和适用性。

    Automated sentiment analysis using Large Language Model (LLM)-based models like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic research and in industrial applications. However, assessment and validation of their performance in case of ambiguous or ironic text is still poor. In this study, we constructed nuanced and ambiguous scenarios, we translated them in 10 languages, and we predicted their associated sentiment using popular LLMs. The results are validated against post-hoc human responses. Ambiguous scenarios are often well-coped by ChatGPT and Gemini, but we recognise significant biases and inconsistent performance across models and evaluated human languages. This work provides a standardised methodology for automated sentiment analysis evaluation and makes a call for action to further improve the algorithms and their underlying data, to improve their performance, interpretability and applicability.
    
[^196]: TrICy: 通过意图感知的关注-复制机制引导的数据到文本生成

    TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy

    [https://arxiv.org/abs/2402.01714](https://arxiv.org/abs/2402.01714)

    TrICy是一种轻量级框架，利用意图和触发器引导数据到文本生成任务，并通过关注-复制机制有效地处理了词汇表之外的词汇。实验结果表明其在不同数据集上取得了良好的性能。

    

    数据到文本（D2T）生成是许多自然语言理解（NLU）应用中的关键任务，也是面向任务导向对话系统的基础。在可以直接与用户设备上的本地数据一起工作的会话型人工智能解决方案中，利用大型预训练语言模型（PLMs）的架构由于高内存占用而无法在设备上部署。为此，我们提出了TrICy，一种新颖的轻量级框架，用于增强D2T任务，根据上下文中的意图生成文本序列，并可进一步由用户提供的触发器进行引导。我们利用关注-复制机制准确地预测了词汇表之外的词汇（OOV）。对E2E NLG数据集（BLEU：66.43％，ROUGE-L：70.14％），WebNLG数据集（BLEU：Seen 64.08％，Unseen 52.35％）和与文本消息应用相关的自定义数据集的性能分析展示了我们的架构的有效性。此外，我们展示了通过利用可选的触发器输入，数据

    Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data
    
[^197]: 使用结构化纵向电子健康记录数据促使大型语言模型进行零样本临床预测

    Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data

    [https://arxiv.org/abs/2402.01713](https://arxiv.org/abs/2402.01713)

    本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。

    

    结构化纵向电子健康记录（EHR）数据的固有复杂性使其与传统上为自然语言处理而设计的大型语言模型（LLM）整合时面临重大挑战。受新疾病爆发时迅速决策的紧迫需求的驱使，本研究调查了类似GPT-4的LLM对EHR数据的适应性。我们特别关注它们的零样本能力，即在没有明确训练的情况下进行预测。针对EHR数据的纵向、稀疏和知识注入的特点，我们的提示方法考虑了特定的EHR特征，如单位和参考范围，并采用了与临床上下文相一致的上下文学习策略。通过在MIMIC-IV和TJH数据集上进行全面实验，我们证明了LLM能够通过我们的方法进行零样本临床预测，有效应对了EHR数据的挑战。

    The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
    
[^198]: 基于大型语言模型的社会感知式合成数据生成用于自杀意念检测

    Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models

    [https://arxiv.org/abs/2402.01712](https://arxiv.org/abs/2402.01712)

    通过利用生成式AI模型，我们提出了一种基于社会感知的合成数据生成方法，用于自杀意念检测。与传统模型相比，我们的方法在实际数据集上表现出有竞争力的性能。

    

    自杀意念检测是一个重要的研究领域，对于改进心理健康支持系统具有巨大潜力。然而，在自杀相关数据周围的敏感性导致难以访问到大规模的、注释的数据集，这是训练有效的机器学习模型所必需的。为了解决这个限制，我们引入了一种创新的策略，利用ChatGPT，Flan-T5和Llama等生成式AI模型的能力，为自杀意念检测创建合成数据。我们的数据生成方法基于从心理学文献中提取的社会因素，并旨在确保涵盖与自杀意念相关的基本信息。在我们的研究中，我们与基于BERT系列结构的现有NLP分类模型进行了基准测试。当在现实世界的UMD数据集上训练时，这些传统模型的F1分数通常在0.75到0.87之间。我们的合成数据驱动方法，提供了一种有效的替代选择，产生具有竞争力的性能。

    Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, i
    
[^199]: 不是我的声音！语音生成器的伦理和安全伤害分类法

    Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators

    [https://arxiv.org/abs/2402.01708](https://arxiv.org/abs/2402.01708)

    语音生成技术的广泛应用给社会带来了伦理和安全风险。本文通过分析语音生成事件，总结出特定伤害模式，并将其分类。这些特定伤害涉及到受影响个体的曝光程度以及相关利益相关者和技术系统之间的相互作用。

    

    人工智能广泛采用语音生成技术给社会带来了一系列重大的伦理和安全风险，亟需解决。例如，在美国，越来越多的语音生成事件与警察受到恶作剧袭击有关，匿名行为者制造合成的声音打电话给警察，要求关闭学校和医院，或者以暴力手段进入无辜市民的家中。这样的事件表明，多模态生成人工智能的风险和伤害并不存在于孤立状态，而是源于多个利益相关者和技术人工智能系统之间的互动。本文通过分析语音生成事件，研究特定伤害模式的出现。我们发现特定伤害可以根据受影响个体的曝光程度进行分类，即他们是语音生成系统的主体、与之互动、受其影响或被排除在外。同样，特定伤害也与相关利益相关者和技术系统之间的相互作用有关。

    The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a con
    
[^200]: MULTIVERSE: 在不同世界中揭示大型语言模型对齐问题

    MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds

    [https://arxiv.org/abs/2402.01706](https://arxiv.org/abs/2402.01706)

    本文通过构建多种语境，使用领域特定语言描述可能世界，并利用编译器，发现了大型语言模型在不同语境下的对齐问题。这种方法成本较低，能够更全面地研究LLM对齐问题。

    

    大型语言模型（LLM）对齐旨在确保LLM的输出与人类价值相匹配。研究人员通过一系列越狱技术展示了对齐问题的严重性，这些技术可以在对话中诱使LLMs产生恶意内容。通常需要大量的人类智能或计算资源才能找到相应的越狱提示。本文报告了LLMs在不同语境下对齐水平的差异。因此，通过系统地构建许多被称为世界的语境、利用描述可能世界（如时间、地点、角色、行为和语言）的领域特定语言和相应的编译器，我们能够以较低成本揭示潜在的对齐问题。鉴于我们方法的低成本，我们能够对不同世界中LLM对齐问题进行大规模研究。我们的结果表明，我们的方法在效果上优于现有的越狱技术。

    Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both eff
    
[^201]: 超越行为主义的表征伤害：度量和减轻计划

    Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation

    [https://arxiv.org/abs/2402.01705](https://arxiv.org/abs/2402.01705)

    本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。

    

    算法伤害通常被分为配置性或表征性。本研究专门针对后者，重点在于对当前表征性伤害定义的审查，以确定其中包含什么和不包含什么。这个分析促使我们扩展超越行为主义的定义范围，包括对认知和情感状态的伤害。本文概述了度量的高级要求：确定实施这种方法所需的专业知识，并通过案例研究进行说明。我们的工作凸显了大型语言模型在实施表征性伤害时的独特脆弱性，特别是当这些伤害未被度量和减轻时。该研究通过提出减轻措施并界定何时使用它们来结束。这项研究的总体目标是建立一个框架，扩大表征性伤害的定义，并将公平研究的见解转化为实际的度量方法。

    Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
    
[^202]: 作为策略的状态字符串：用博弈论求解器引导语言模型

    States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

    [https://arxiv.org/abs/2402.01704](https://arxiv.org/abs/2402.01704)

    本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。

    

    博弈论是研究理性主体间战略互动的数学模型。语言是人类互动的重要方式，但在历史上一直很难通过数学方法对对话及其战略动机建模。与语言互动相关的玩家、策略和回报的适当模型（即对游戏论常规符号逻辑的约束）将使现有的博弈论算法能够在语言领域提供战略解决方案。换句话说，这种约束可以为在对话中计算稳定、理性的对话策略提供一条途径。大型语言模型（LLM）可能已经达到了其生成能力足以实现自然对话真实、类似人类的模拟的程度。通过以不同的方式提示它们，我们可以将其响应引导到不同的输出话语。利用自然语言的表达能力，LLM还可以帮助我们快速生成新的对话。

    Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
    
[^203]: 语言模型的流畅梦境

    Fluent dreaming for language models

    [https://arxiv.org/abs/2402.01702](https://arxiv.org/abs/2402.01702)

    扩展了贪婪坐标梯度方法，设计了进化提示优化算法，实现了语言模型的流畅梦境，能够同时最大化内部特征和提示流畅性，可以自动探索模型对轻度分布之外提示的反应。

    

    特征可视化，也称为“梦境”，通过优化输入以最大化神经元的激活或其他内部组件，为视觉模型提供了洞察力。然而，由于输入空间是离散的，梦境在语言模型上尚未成功应用。我们对语言模型对抗攻击文献中的贪婪坐标梯度方法进行了扩展，设计了进化提示优化（EPO）算法。EPO优化输入提示，以同时最大化所选内部特征和提示流畅性之间的帕累托前沿，实现语言模型的流畅梦境。我们演示了使用神经元、输出logits和激活空间中的任意方向进行梦境。我们衡量了生成的提示的流畅性，并将语言模型的梦境与最大激活数据集示例进行了比较。关键是，流畅梦境允许自动探索模型内部对轻度分布之外的提示的行为反应。用于运行的代码

    Feature visualization, also known as "dreaming", offers insights into vision models by optimizing the inputs to maximize a neuron's activation or other internal component. However, dreaming has not been successfully applied to language models because the input space is discrete. We extend Greedy Coordinate Gradient, a method from the language model adversarial attack literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO optimizes the input prompt to simultaneously maximize the Pareto frontier between a chosen internal feature and prompt fluency, enabling fluent dreaming for language models. We demonstrate dreaming with neurons, output logits and arbitrary directions in activation space. We measure the fluency of the resulting prompts and compare language model dreaming with max-activating dataset examples. Critically, fluent dreaming allows automatically exploring the behavior of model internals in reaction to mildly out-of-distribution prompts. Code for runni
    
[^204]: 在临床护理点的医疗专业人员问答系统--一项系统综述

    Question answering systems for health professionals at the point of care -- a systematic review

    [https://arxiv.org/abs/2402.01700](https://arxiv.org/abs/2402.01700)

    这项系统综述旨在描述当前的医学问答系统，评估其在医疗保健中的适用性，并确定改进的方向。

    

    目标：问答（QA）系统有潜力通过为医疗专业人员提供最新和最相关的证据来提高临床护理的质量。然而，QA系统尚未得到广泛应用。本系统综述旨在描述当前的医学QA系统，评估其在医疗保健领域的适用性，并确定改进的方向。材料与方法：我们于2023年2月7日在PubMed、IEEE Xplore、ACM Digital Library、ACL Anthology以及前后引用中进行了搜索。我们包括了描述生物医学QA系统设计和评估的同行评议期刊和会议论文。两个评审人员筛选了标题、摘要和全文文章。我们对每个研究进行了叙事综合和偏倚评估。我们评估了生物医学QA系统的效用。结果：我们包括了79个研究，并确定了主题，包括问题的真实性，答案的可靠性，答案的效用，临床特殊性，系统和可用性。

    Objective: Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement.   Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology and forward and backward citations on 7th February 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems.   Results: We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usabili
    
[^205]: 大型语言模型赋能参与式城市规划

    Large language model empowered participatory urban planning

    [https://arxiv.org/abs/2402.01698](https://arxiv.org/abs/2402.01698)

    本研究将大型语言模型（LLMs）应用于城市规划的参与式过程中，通过角色扮演、协作生成和反馈迭代，解决了社区级土地利用任务。实证实验表明LLM在不同规划场景上具有适应性和有效性，能够超过人类专家满意度和包容性，并与最先进的强化学习方法在服务和生态方面媲美。

    

    参与式城市规划是现代城市规划的主流，涉及各个利益相关者的积极参与。然而，传统的参与式范式在时间和人力上面临挑战，而生成式规划工具在提供可调整和包容性解决方案方面失败。本研究介绍了一种创新的城市规划方法，将大型语言模型（LLMs）整合到参与过程中。该框架基于精心设计的LLM代理，包括角色扮演、协作生成和反馈迭代，解决了一个服务于1000个不同利益的社区级土地利用任务。在各种城市社区的实证实验表明，LLM在不同规划场景上的适应性和有效性。结果根据四个指标进行评估，在满意度和包容性方面超过人类专家，并在服务和生态方面与最先进的强化学习方法相媲美。进一步分析显示了该方法的优势。

    Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage
    
[^206]: APT-Pipe: 用于社交计算数据标注的自动提示调整工具

    APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation

    [https://arxiv.org/abs/2402.01697](https://arxiv.org/abs/2402.01697)

    APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.

    

    最近的研究突出了像ChatGPT这样的LLM应用在社交计算文本标注中的潜力。然而，已经人们已经知道性能取决于输入提示的质量。为了解决这个问题，已经有了大量的研究来探索提示调整的技术和指南，试图改善提示的质量。然而，这些方法往往依赖于手工努力和对正在标注的数据集的先前知识。为了解决这个限制，我们提出了一个自动化的提示调整流水线APT-Pipe。APT-Pipe旨在自动调整提示，以提高ChatGPT在任何给定数据集上的文本分类性能。我们实现了APT-Pipe，并在12个不同的文本分类数据集上进行了测试。我们发现APT-Pipe调整的提示有助于ChatGPT在12个实验数据集中有9个获得更高的加权F1分数，平均改进了7.01％。我们进一步突出了APT-Pipe作为一个框架的灵活性。

    Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
    
[^207]: HiGen: 层次感知的层级文本分类序列生成

    HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification

    [https://arxiv.org/abs/2402.01696](https://arxiv.org/abs/2402.01696)

    HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。

    

    层次文本分类（HTC）是多标签文本分类中的一个复杂子任务，其特点是具有层级标签分类法和数据不平衡。最佳性能模型旨在通过结合文档和层级标签信息来学习静态表示。然而，文档各个部分的相关性可能因层级水平的不同而变化，需要动态的文档表示。为了解决这个问题，我们提出了HiGen，一个利用语言模型编码动态文本表示的基于文本生成的框架。我们引入了一种层级引导的损失函数，以捕捉文本和标签名称语义之间的关系。我们的方法采用了一个特定任务的预训练策略，将语言模型调整到领域知识上，并显著提高了对样本有限的类别的性能。此外，我们还提供了一个命名为ENZYME的新颖和有价值的用于HTC的数据集，该数据集由来自PubMed的文章组成，旨在预测...

    Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
    
[^208]: 语言引导的世界模型：一种基于模型的人工智能控制方法

    Language-Guided World Models: A Model-Based Approach to AI Control

    [https://arxiv.org/abs/2402.01695](https://arxiv.org/abs/2402.01695)

    语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。

    

    将概率世界模型安装到人工智能代理中，为人类与这些代理沟通和控制打开了一个高效的渠道。除了更新代理策略，人类还可以修改他们的内部世界模型，以影响代理的决策。然而，当前现有的世界模型难以适应人类，因为它们缺乏自然的通信界面。为了解决这个问题，我们开发了语言引导的世界模型（LWMs），它们可以通过阅读语言描述来捕捉环境动态。这些模型提高了代理的沟通效率，使人类能够通过简洁的语言反馈同时改变他们在多个任务上的行为。它们还使代理能够从最初用于指导人类的文本中进行自我学习。为了促进LWMs的发展，我们设计了一个基于MESSENGER游戏（Hanjie等人，2021）的挑战基准，需要对新场景进行组合泛化。

    Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
    
[^209]: ARGS: 对齐作为奖励导向的搜索

    ARGS: Alignment as Reward-Guided Search

    [https://arxiv.org/abs/2402.01694](https://arxiv.org/abs/2402.01694)

    ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。

    

    将大规模语言模型与人类目标对齐是至关重要的，然而常见的方法包括RLHF在训练过程中存在不稳定和资源密集的问题。为应对这一挑战，我们引入了一个新的框架ARGS，即对齐作为奖励导向的搜索，它将对齐融入到解码过程中，消除了昂贵的RL训练的需求。通过使用奖励信号调整模型的概率预测，ARGS生成具有语义多样性的文本，同时与人类偏好对齐，为对齐语言模型提供了一种有前景且灵活的解决方案。值得注意的是，在不同的对齐任务和不同的模型维度下，ARGS相对于基线显示出持续的奖励改进。例如，采用相同的贪婪解码策略，我们的方法相对于基线提高了19.56%的平均奖励，并在GPT-4评估中获得了64.33%的偏好或并列分数。我们相信，我们的框架强调了解码的创新性和效果。

    Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco
    
[^210]: 大型语言生成模型与人类患者相比，对非专业患者解释实验室测试结果的回答质量的评估研究

    Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study

    [https://arxiv.org/abs/2402.01693](https://arxiv.org/abs/2402.01693)

    本研究评估了使用大型语言生成模型（LLMs）与人类患者相比，解答非专业患者关于实验室测试结果的问题的回答质量。评估结果表明LLMs在相关性、正确性和有帮助性方面具有一定潜力，但还存在潜在问题需要改进。

    

    实验室检测结果常常令人困惑和难以理解。大型语言生成模型（LLMs），如ChatGPT，为患者提供了获取问题答案的有前景的途径。我们的目标是评估使用LLMs回答患者有关实验室测试问题的相关、准确、有帮助并且无害的回答的可行性，以及识别可以通过增强方法来缓解的潜在问题。我们首先从Yahoo! Answers收集了与实验室测试结果相关的问题和答案数据，并选择了53个问答对进行本研究。使用LangChain框架和ChatGPT互联网门户，我们从四个LLMs（包括GPT-4、Meta LLaMA 2、MedAlpaca和ORCA_mini）生成了对这53个问题的回答。我们首先使用标准的问答相似度评估指标（包括ROUGE、BLEU、METEOR和BERTScore）评估了它们回答的相似性。我们还利用基于LLMs的评估器判断目标模型在相关性、正确性和有帮助性方面的质量是否更高。

    Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpf
    
[^211]: 通过自监督表示混合和嵌入初始化实现跨语言TTS自适应的最大数据效率

    Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization

    [https://arxiv.org/abs/2402.01692](https://arxiv.org/abs/2402.01692)

    本文提出了一种有效的转移学习框架，用于通过最少的标记和未标记数据实现语言自适应。该方法使用自监督特征进行预训练，在细调期间替换伪标签噪声部分，并结合嵌入初始化技巧，有效利用未标记数据的信息。实验证明，即使在仅有很少的数据情况下，该框架也能合成可理解的未知语言语音，并超越传统技术。这一研究结果展示了该高效语言自适应框架的潜力。

    

    本文提出了一种有效的转移学习框架，用于文本到语音系统中的语言自适应，重点是使用最少的标记和未标记数据实现语言自适应。虽然许多工作侧重于减少标记数据的使用，但很少有人考虑尽量减少未标记数据的使用。通过在预训练阶段利用自监督特征，替换细调期间伪标签中的噪声部分，并结合嵌入初始化技巧，我们的方法与传统方法相比利用了更多未标记数据的信息。实验结果表明，我们的框架能够使用仅4个标记数据和15分钟未标记数据合成可理解的未知语言语音。我们的方法即使在更多数据可用的情况下，仍然超过了传统技术。这些发现突显了我们的高效语言自适应框架的潜力。

    This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.
    
[^212]: 基于信息损失的语言学方法用于检测轻度认知障碍

    Linguistic-Based Mild Cognitive Impairment Detection Using Informative Loss

    [https://arxiv.org/abs/2402.01690](https://arxiv.org/abs/2402.01690)

    本论文提出了一种基于自然语言处理技术的深度学习方法，用于在老年人中区分轻度认知障碍和正常认知条件。该方法使用了句子嵌入和句子交叉注意力模块，通过分析视频访谈的转录文本，提取时序特征进行分类。同时，引入了一种新的损失函数来建立稳健的模型。

    

    本论文提出了一种使用自然语言处理（NLP）技术的深度学习方法，以区分老年人中的轻度认知障碍（MCI）和正常认知（NC）条件。我们提出了一个框架，该框架分析了在I-CONECT研究项目中收集的视频访谈中生成的转录文本，该项目是一项旨在通过视频聊天改善认知功能的随机对照试验。我们提出的NLP框架包括两个基于Transformer的模块，即句子嵌入（SE）和句子交叉注意力（SCA）。首先，SE模块捕捉每个句子中单词之间的上下文关系。接下来，SCA模块提取句子序列的时序特征。然后，这些特征由多层感知器（MLP）用于将被试分为MCI或NC。为了建立一个稳健的模型，我们提出了一种新的损失函数，称为信息损失（InfoLoss），该函数通过观察每个句子序列来考虑熵的减少。

    This paper presents a deep learning method using Natural Language Processing (NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and Normal Cognitive (NC) conditions in older adults. We propose a framework that analyzes transcripts generated from video interviews collected within the I-CONECT study project, a randomized controlled trial aimed at improving cognitive functions through video chats. Our proposed NLP framework consists of two Transformer-based modules, namely Sentence Embedding (SE) and Sentence Cross Attention (SCA). First, the SE module captures contextual relationships between words within each sentence. Subsequently, the SCA module extracts temporal features from a sequence of sentences. This feature is then used by a Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC. To build a robust model, we propose a novel loss function, called InfoLoss, that considers the reduction in entropy by observing each sequence of sentences
    
[^213]: SMUTF：使用生成标签和混合特征的模式匹配方法

    SMUTF: Schema Matching Using Generative Tags and Hybrid Features

    [https://arxiv.org/abs/2402.01685](https://arxiv.org/abs/2402.01685)

    SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。

    

    我们引入了SMUTF，一种用于大规模表格数据模式匹配的独特方法，该方法假设在开放域任务中，监督学习不会影响性能，从而实现了有效的跨域匹配。这个系统独特地结合了基于规则的特征工程、预训练语言模型和生成式大语言模型。受人道主义交换语言的启发，我们使用“生成标签”为每个数据列部署了创新的适应性，提高了模式匹配的效果。SMUTF具有广泛的灵活性，可以与任何现有的预训练嵌入、分类方法和生成模型无缝配合使用。鉴于模式匹配缺乏广泛的公开数据集，我们已经创建并开源了HDXSM数据集，该数据集来自公共人道主义数据，我们相信这是目前最全面的模式匹配数据集。

    We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
    
[^214]: 使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的框架

    A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm

    [https://arxiv.org/abs/2402.01684](https://arxiv.org/abs/2402.01684)

    提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。

    

    随着自然语言处理领域中大型语言模型（LLMs）的不断演进，人们为了有效地微调常见的预训练LLMs以完成各种任务，在一个或多个特定领域中进行了大量努力。在实践中，有两种主要的适应方式：（i）多个独立模型：使用每个任务的相应训练样本对预训练LLMs进行独立的微调；（ii）集成模型：使用所有任务的样本来联合微调预训练LLMs 。为了同时解决高计算成本和摇摆问题，我们提出了一个统一的框架，使用一种新颖的定制门控（CGC）低秩自适应（LoRA）算法在LLMs中实现了1 + N多任务微调模式。我们的工作旨在充分利用MTL（即CGC）和PEFT（即LoRA）方案。对于给定的任务集群，我们设计了一个创新的层，其中包含...

    With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai
    
[^215]: 基于社交媒体数据的社区行为对危机活动关注的理解：以2023年纽约市加拿大野火为例的研究

    Community-based Behavioral Understanding of Crisis Activity Concerns using Social Media Data: A Study on the 2023 Canadian Wildfires in New York City

    [https://arxiv.org/abs/2402.01683](https://arxiv.org/abs/2402.01683)

    本研究利用大规模社交媒体数据研究了2023年加拿大野火烟雾在纽约市引发的危机活动关注。通过整合地理标记的Twitter数据和国家数据库，开发模型对不同活动关注进行社区推断。研究结果发现，在野火期间，纽约市居民的活动关注发生了变化。

    

    2023年6月，由加拿大野火飘入纽约市的烟雾使得该市空气污染达到全球最严重水平。这种前所未有的情况导致纽约市居民的出行和传统活动模式发生了重大变化。本研究利用大规模社交媒体数据研究了2023年加拿大野火烟雾在纽约市出现时的不同危机活动关注（包括疏散、呆在室内、购物和娱乐活动等）。为此，我们收集了纽约市一个星期（2023年6月2日至6月9日）的地理标记的Twitter数据，并使用先进的文本分类技术对这些推文进行了处理，然后将其与社保局数据、人口普查和美国社区调查等国家数据库进行了整合。最后，我们开发了一个模型来对重大野火中不同活动关注进行社区推断。研究结果表明，在野火期间，纽约市居民的活动关注发生了变化。

    New York City (NYC) topped the global chart for the worst air pollution in June 2023, owing to the wildfire smoke drifting in from Canada. This unprecedented situation caused significant travel disruptions and shifts in traditional activity patterns of NYC residents. This study utilized large-scale social media data to study different crisis activity concerns (i.e., evacuation, staying indoors, shopping, and recreational activities among others) in the emergence of the 2023 Canadian wildfire smoke in NYC. In this regard, one week (June 02 through June 09, 2023) geotagged Twitter data from NYC were retrieved and used in the analysis. The tweets were processed using advanced text classification techniques and later integrated with national databases such as Social Security Administration data, Census, and American Community Survey. Finally, a model has been developed to make community inferences of different activity concerns in a major wildfire. The findings suggest, during wildfires, f
    
[^216]: 利用社交媒体数据识别影响公众对可及性、社会经济差距和公共交通态度的因素

    Leveraging Social Media Data to Identify Factors Influencing Public Attitude Towards Accessibility, Socioeconomic Disparity and Public Transportation

    [https://arxiv.org/abs/2402.01682](https://arxiv.org/abs/2402.01682)

    本研究利用社交媒体数据提出了一种新方法，通过分析大规模回应和建立统计模型，深入了解了个体对交通可及性、社会经济差距和公共基础设施的感知。研究结果显示，女性、亚洲裔和经历高交通流量的个体更关注交通可及性，而具有社会经济劣势的个体更关注公共交通问题并表达更强烈的关切。

    

    本研究提出了一种新方法来理解影响个体对交通可及性、社会经济差距和公共基础设施感知的因素。与耗时昂贵的调查方法相反，这种方法可以从社交媒体生成有机的大规模回应，并开发统计模型来理解个体对各种交通问题的感知。本研究从2020年3月19日至2022年5月15日，检索并分析了纽约市的36,098条推文。采用先进的自然语言处理算法进行文本挖掘和分类。采用数据融合技术生成一系列用作模型解释变量的社会经济特征。模型结果显示，女性和亚洲裔个体更倾向于讨论交通可及性，而那些经历高交通流量的人更加积极发声。然而，具有社会经济劣势的个体更关注公共交通问题，并表达出更强烈的关切。

    This study proposes a novel method to understand the factors affecting individuals' perception of transport accessibility, socioeconomic disparity, and public infrastructure. As opposed to the time consuming and expensive survey-based approach, this method can generate organic large-scale responses from social media and develop statistical models to understand individuals' perceptions of various transportation issues. This study retrieved and analyzed 36,098 tweets from New York City from March 19, 2020, to May 15, 2022. A state-of-the-art natural language processing algorithm is used for text mining and classification. A data fusion technique has been adopted to generate a series of socioeconomic traits that are used as explanatory variables in the model. The model results show that females and individuals of Asian origin tend to discuss transportation accessibility more than their counterparts, with those experiencing high neighborhood traffic also being more vocal. However, disadvan
    
[^217]: 表情符号解密：利用ChatGPT提升社交媒体沟通的理解能力

    Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications

    [https://arxiv.org/abs/2402.01681](https://arxiv.org/abs/2402.01681)

    在表情符号研究中，我们评估了ChatGPT在处理注释和下游任务中的有效性。我们的研究结果表明ChatGPT可以作为一个可行的替代人类注释者的工具，有效地解释表情符号。

    

    表情符号在社交网络沟通中已经普遍存在，它们承载了超越文字或短语的语义，这引发了学术界对其属性和功能的越来越多的研究兴趣。然而，与表情符号相关的研究和应用面临两个主要挑战。首先，研究者通常依赖众包来注释表情符号，以了解其情感、使用意图和语义含义。其次，用户的主观解释往往会导致对表情符号的误解，并造成沟通障碍。大型语言模型（LLMs）在各种注释任务中取得了显著的成功，ChatGPT在多个领域展示了专业能力。在我们的研究中，我们评估了ChatGPT在处理以前注释和下游任务中的有效性。我们的目标是验证ChatGPT可以在表情符号研究中作为人类注释者的可行替代者，并验证其解释表情符号的能力。

    Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
    
[^218]: 大型语言模型基于多智能体系统：进展与挑战综述

    Large Language Model based Multi-Agents: A Survey of Progress and Challenges

    [https://arxiv.org/abs/2402.01680](https://arxiv.org/abs/2402.01680)

    大型语言模型(LLMs)已广泛应用于各种任务。基于LLMs的多智能体系统在复杂问题求解和世界模拟方面取得了重要进展。这篇综述给出了基于LLMs的多智能体系统的重要方面和面临的挑战的全面讨论。

    

    大型语言模型(LLMs)在各种任务上取得了显著的成功。由于LLMs具有令人印象深刻的规划和推理能力，它们被用作自主智能体来自动完成许多任务。最近，基于将一个LLM用作单个规划或决策智能体的发展，基于LLMs的多智能体系统在复杂问题求解和世界模拟方面取得了相当大的进展。为了为社区提供这个充满活力领域的综述，我们提供了这篇综述文章，深入讨论了基于LLMs的多智能体系统的基本方面以及面临的挑战。我们的目标是让读者对以下问题获得实质性见解：LLM-based多智能体模拟哪些领域和环境？这些智能体是如何建模和通信的？什么机制有助于智能体能力的增长？对于那些对这个领域的研究感兴趣的人，我们还总结了一些要点和挑战.

    Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize t
    
[^219]: StickerConv: 从零开始生成多模态共情回应

    StickerConv: Generating Multimodal Empathetic Responses from Scratch

    [https://arxiv.org/abs/2402.01679](https://arxiv.org/abs/2402.01679)

    本文介绍了StickerConv代理(Agent4SC)，该代理通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。为了利用构建的多模态共情对话数据集StickerConv，作者提出了PErceive and Generate Stickers (PEGS)模型，该模型能够生成情境相关和情感丰富的回应。

    

    在当前的共情对话研究中，贴纸尽管被广泛认可为提高在线交流中的共情能力，但仍未得到充分探索。本文介绍了StickerConv代理(Agent4SC)，通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。在此基础上，我们构建了一个多模态共情对话数据集StickerConv，包括12.9K个对话会话，5.8K个独特贴纸和2K个多样化会话场景，专门设计用于增强多模态情境下的共情回应生成。为了利用这个数据集的丰富性，我们提出了PErceive and Generate Stickers (PEGS)，一种多模态共情回应生成模型，并结合基于LLM的全面共情评估指标。我们的实验表明，PEGS在生成情境相关和情感丰富的回应方面具有很好的效果。

    Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
    
[^220]: 通过整合外延知识和内涵知识嵌入本体

    Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge

    [https://arxiv.org/abs/2402.01677](https://arxiv.org/abs/2402.01677)

    本文提出了一种新型本体嵌入方法EIKE，通过整合外延知识和内涵知识，在外延空间和内涵空间中表示本体，并采用基于几何的方法和预训练的语言模型对实例、概念和关系进行嵌入建模。

    

    本体包含领域内丰富的知识，可以分为两个类别，即外延知识和内涵知识。外延知识提供关于本体中特定概念所属的具体实例的信息，而内涵知识详细描述了概念之间的内在属性、特征和语义关联。然而，现有的本体嵌入方法未能同时充分考虑外延知识和内涵知识。在本文中，我们提出了一种名为EIKE（Extensional and Intensional Knowledge Embedding）的新型本体嵌入方法，通过在外延空间和内涵空间中表示本体。EIKE提出了一个统一的框架，用于将实例、概念及其关系嵌入到本体中，采用基于几何的方法对外延知识进行建模，并使用预训练的语言模型对内涵知识进行建模。

    Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
    
[^221]: 语言模型与人类在关键语法结构上的判断一致性

    Language models align with human judgments on key grammatical constructions

    [https://arxiv.org/abs/2402.01676](https://arxiv.org/abs/2402.01676)

    本研究通过对比评估发现，大型语言模型（LLMs）在俘获人类行为方面的表现非常出色，不仅整体准确率高，而且能够捕捉到人类语言判断中的细微差异。

    

    大型语言模型（LLMs）是否具有类似人类的语言普遍性？Dentella等人（2023年；“DGL”）使用多个LLMs提示语法正确性问题，以获取80个英语句子的语法句子判断，得出LLMs存在“是”偏向和“不能区分语法和非语法句子”的结论。我们采用了既定的实践方法重新评估LLM的性能，并发现DGL的数据实际上证明了LLM如何准确捕捉人类行为。模型不仅整体上实现了高准确率，还捕捉到了人类语言判断的细微变化。

    Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.
    
[^222]: 使用大型语言模型嵌入追踪思想的系谱

    Tracing the Genealogies of Ideas with Large Language Model Embeddings

    [https://arxiv.org/abs/2402.01661](https://arxiv.org/abs/2402.01661)

    本文提出了一种使用大型语言模型嵌入追踪思想系谱的新方法，在大型语料库中检测思想影响。通过结合通用文本嵌入、句子嵌入和抽象意义表示图的方法，可以高效搜索实质上相似的思想和思想影响的迹象。

    

    本文提出了一种新颖的方法，用于检测大型语料库中的思想影响。利用大型语言模型在编码语义和结构意义方面的独特优势，并在保持对换句方式鲁棒性的同时，我们可以以计算高效的方式搜索具有实质性相似思想和思想影响迹象。这种方法允许我们操作不同的置信水平：我们可以允许直接引用、改写或推测性相似性，同时对每个阈值的限制保持开放态度。我应用了综合方法，结合了通用文本嵌入、一种优化捕捉语义内容的最先进的句子嵌入方法，以及一种用于捕捉论证风格和隐喻使用上结构相似性的抽象意义表示图表示法。我将这种方法应用于大约400,000本非小说书籍和学术出版物的语料库中以向量化句子。

    In this paper, I present a novel method to detect intellectual influence across a large corpus. Taking advantage of the unique affordances of large language models in encoding semantic and structural meaning while remaining robust to paraphrasing, we can search for substantively similar ideas and hints of intellectual influence in a computationally efficient manner. Such a method allows us to operationalize different levels of confidence: we can allow for direct quotation, paraphrase, or speculative similarity while remaining open about the limitations of each threshold. I apply an ensemble method combining General Text Embeddings, a state-of-the-art sentence embedding method optimized to capture semantic content and an Abstract Meaning Representation graph representation designed to capture structural similarities in argumentation style and the use of metaphor. I apply this method to vectorize sentences from a corpus of roughly 400,000 nonfiction books and academic publications from t
    
[^223]: L-TUNING：用于LLMs中的提示和前缀的同步标签调整

    L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs

    [https://arxiv.org/abs/2402.01643](https://arxiv.org/abs/2402.01643)

    本文介绍了L-Tuning，一种在自然语言推理（NLI）框架内的高效微调方法，通过对预训练的LLM中的标签标记进行微调，提高了训练准确性和效率，并增强了模型的训练细微差别。

    

    高效地针对特定任务对大型语言模型（LLMs）进行微调在自然语言处理中面临着重大挑战。传统方法，如提示或前缀调整，通常依赖于任意标记进行训练，从而导致训练时间延长并且通用标记在各种类别标签中使用。为了解决这些问题，本文引入了L-Tuning，这是一种在自然语言推理（NLI）框架内设计的用于分类任务的高效微调方法。与传统方法不同，L-Tuning专注于通过预训练的LLM处理的标签标记的微调，从而利用其预先存在的语义知识。这种技术不仅提高了微调的准确性和效率，还促进了为每个类别生成不同的标签嵌入，增强了模型的训练细微差别。我们的实验结果表明，使用L-Tuning可以显著提高训练效率和分类准确性。

    Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
    
[^224]: 机器生成文本的检测：文献综述

    Detection of Machine-Generated Text: Literature Survey

    [https://arxiv.org/abs/2402.01642](https://arxiv.org/abs/2402.01642)

    这项论文对机器生成文本的检测进行了文献综述，指出了语言模型生成的虚假文本大量存在于公共领域，因此需要采取预防措施来应对其可能带来的危险影响。

    

    由于语言模型能够快速轻松地产生虚假文本，公共领域中出现了大量此类内容。在不断提升的复杂度和写作风格下，几乎无法区分人类撰写和机器生成的内容。因此，与人工作者相比，语言模型生成的作品引起了巨大的媒体关注并引起了争议。对于先进语言模型可能对社会产生的影响的担忧也应运而生，需要对这些过程有更充分的了解。自然语言生成（NLG）和生成预训练转换器（GPT）模型已经在各个领域引起了革命性的影响：其范围不仅渗透到新闻报道和客户服务，还涉及到学术界。为了减轻使用这些模型可能带来的危险影响，必须采取预防措施，例如为人类操作员提供区分虚假文本和真实文本的能力。

    Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artif
    
[^225]: 通用句法结构：对各种自然语言的句法建模

    Universal Syntactic Structures: Modeling Syntax for Various Natural Languages

    [https://arxiv.org/abs/2402.01641](https://arxiv.org/abs/2402.01641)

    该研究提出了一种新颖的句法建模方法，通过分析多种自然语言的语料库，可能揭示了适用于所有自然语言的通用句法结构的存在。这对理解人类大脑中语言的运作方式以及相关学科的理论有重要意义。

    

    我们旨在解释人类大脑是如何将词连接起来形成句子的。引入了一种新颖的句法表示建模方法，可能表明存在着适用于所有自然语言的通用句法结构。就像发现DNA的双螺旋结构揭示了基因组的内部运作一样，我们希望能对人脑中语言的运作方式提供基本的理解。这可能是大脑对知识进行编码和解码的方式。它还为语言学、心理学和认知科学的理论提供了一些洞见。通过研究通用句法结构的逻辑以及建模技术的方法论，我们尝试分析展示不同自然语言（如英语和韩语）语言过程中普遍性的语料库。最后，我们讨论了关键期假说、通用语法和关于语言的几种其他主张，以推进研究的目的。

    We aim to provide an explanation for how the human brain might connect words for sentence formation. A novel approach to modeling syntactic representation is introduced, potentially showing the existence of universal syntactic structures for all natural languages. As the discovery of DNA's double helix structure shed light on the inner workings of genetics, we wish to introduce a basic understanding of how language might work in the human brain. It could be the brain's way of encoding and decoding knowledge. It also brings some insight into theories in linguistics, psychology, and cognitive science. After looking into the logic behind universal syntactic structures and the methodology of the modeling technique, we attempt to analyze corpora that showcase universality in the language process of different natural languages such as English and Korean. Lastly, we discuss the critical period hypothesis, universal grammar, and a few other assertions on language for the purpose of advancing o
    
[^226]: 通过数据压缩评估大型语言模型的泛化性和鲁棒性

    Evaluating Large Language Models for Generalization and Robustness via Data Compression

    [https://arxiv.org/abs/2402.00861](https://arxiv.org/abs/2402.00861)

    通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。

    

    现有的大型语言模型评估方法面临数据污染、对提示敏感以及基准测试创建成本高等挑战。为了解决这些问题，我们提出了一种基于无损数据压缩的评估方法，测试模型的预测能力在其训练截止日期之后的泛化情况。具体而言，我们收集了从2017年到2023年共83个月的全面测试数据，并根据模型的训练数据截止日期将数据分为训练和测试期。我们使用两个指标进行评估：1）测试期的压缩性能作为对未见数据的泛化能力的衡量；2）训练期和测试期之间的性能差距作为鲁棒性的衡量。我们的实验证明，许多模型的压缩率在截止日期之后显著降低，但像... (内容过长，省略)

    Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
    
[^227]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^228]: DetectGPT是否充分利用了扰动？基于模型对比学习的选择性扰动会更好

    Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better

    [https://arxiv.org/abs/2402.00263](https://arxiv.org/abs/2402.00263)

    我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动和多对比学习，在减少随机屏蔽引起的信息丢失的同时，进一步提升少样本和个体输入的性能。

    

    大型语言模型的不断发展引发了对其滥用的增长关注。DetectGPT是一种零-shot基于度量的无监督机器生成文本检测器，首次引入了扰动并展现了巨大的性能提升。然而，DetectGPT的随机扰动策略可能会引入噪声，限制了可区分性和进一步的性能提升。此外，它的逻辑回归模块依赖于设置阈值，这会影响个体或小批量输入的泛化性和适用性。因此，我们提出了一种新颖的检测器，模型名，它使用选择性策略扰动来缓解随机屏蔽所引起的重要信息丢失，并利用多对比学习捕捉扰动期间的隐含模式信息，便于少量样本的性能提升。实验结果表明，模型名在四个公共数据集上的平均准确率比SOTA方法高出1.20\%。我们进一步分析了...

    The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
    
[^229]: LLaMA和ChatGPT嵌入在分子嵌入中的比较分析

    Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding

    [https://arxiv.org/abs/2402.00024](https://arxiv.org/abs/2402.00024)

    LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。

    

    目的：ChatGPT和LLaMA等大型语言模型在化学信息学领域越来越受到重视，特别是在解释Simplified Molecular Input Line Entry System (SMILES)方面。这些语言模型可以将SMILES字符串解码为向量表示，为理解化学图提供了一种新的方法。方法：我们研究了ChatGPT和LLaMA在嵌入SMILES字符串方面的性能。我们的评估集中在两个关键应用领域：分子性质（MP）预测和药物-药物相互作用（DDI）预测，这在药物开发和医疗保健中至关重要。结果：我们发现，使用LLaMA生成的SMILES嵌入在MP和DDI预测任务中表现优于ChatGPT。值得注意的是，基于LLaMA的SMILES嵌入在这两个预测任务中显示了与现有方法相当的结果。结论：在化学信息学中应用LLMs，特别是在利用SMILES进行嵌入方面，是可行的。

    Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
    
[^230]: 推理束搜索：为链式思维推断寻找可推导的理由

    Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning

    [https://arxiv.org/abs/2401.17686](https://arxiv.org/abs/2401.17686)

    本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。

    

    最近的研究通过各种方法，尤其是链式思维推理，极大增强了大型语言模型（LLMs）的推理能力。然而，以往的方法未能解决中间步骤的推理错误问题，导致错误的累积。本文提出了一种称为推理束搜索（DBS）的方法，它将链式思维和演绎推理与逐步束搜索无缝集成到LLMs中。我们的方法部署了一个验证器，用于验证推理步骤及其前提的可推导性，从而减少错误的累积。此外，我们引入了一种可扩展且无需人工劳动的数据构建方法，来增强我们模型的验证能力。广泛的实验证明，我们的方法显著提升了各种规模的LLMs（7B、13B、70B和ChatGPT）的基础性能，在3种不同的推理场景（算术、常识和符号）的8个推理数据集中都表现出色。此外，我们的分析证明了

    Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
    
[^231]: 鲁棒的提示优化用于对抗语言模型的破解攻击

    Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

    [https://arxiv.org/abs/2401.17263](https://arxiv.org/abs/2401.17263)

    该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。

    

    尽管在人工智能对齐方面取得了一些进展，但语言模型（LM）仍然容易受到对抗性攻击或破解攻击的影响，其中对手修改输入提示以诱导有害行为。虽然已经提出了一些防御方法，但它们仅关注狭窄的威胁模型，并不能提供强大的防御。为了实现强大的防御，我们首次提出了用于对抗破解攻击的对抗目标，并提出了一种名为鲁棒提示优化（RPO）的算法，该算法利用基于梯度的令牌优化来确保输出的无害性。通过这种方法，我们得到了一个易于访问的后缀，显著改善了对破解攻击的强韧性，包括优化过程中出现的破解攻击以及未知的破解攻击，将攻击成功率从84%降低到8.66%，在20个破解攻击中。此外，我们还发现RPO对正常LM使用的影响较小，在适应性攻击下仍然有效，并且可以迁移到黑盒模型中，降低攻击成功率。

    Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
    
[^232]: 大规模语言模型的弱到强破解

    Weak-to-Strong Jailbreaking on Large Language Models

    [https://arxiv.org/abs/2401.17256](https://arxiv.org/abs/2401.17256)

    通过弱到强破解攻击，对手可以利用较小的不安全/对齐LLMs指导对显著较大的对齐LLMs进行破解，与解码较大的LLMs相比，其计算和延迟成本较小。

    

    尽管已经付出了大量努力来对齐大规模语言模型（LLMs），但红队测试报告表明，这些经过精心对齐的LLMs仍然可以通过对抗性提示、调优或解码进行破解。在调查对齐LLMs的破解漏洞时，我们观察到破解和对齐模型的解码分布仅在初始生成中存在差异。这一观察结果激发了我们提出的弱到强破解攻击，敌对方可以利用较小的不安全/对齐LLMs（例如7B）指导对显著较大的对齐LLMs（例如70B）进行破解。要进行破解，只需额外解码两个较小的LLMs一次，与解码较大的LLMs相比，其计算和延迟成本较小。通过在三个不同组织的五个模型上进行实验，我们证明了该攻击的有效性。我们的研究揭示了一种以前未注意到但高效的破解方式，

    Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, expo
    
[^233]: 对齐和有用性之间的权衡：语言模型的研究

    Tradeoffs Between Alignment and Helpfulness in Language Models

    [https://arxiv.org/abs/2401.16332](https://arxiv.org/abs/2401.16332)

    本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。

    

    语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望行为和抑制非期望行为，实现人类与语言模型之间的安全交互。通常通过调整模型或插入预设的对齐提示来实现。最近，通过改变训练后的表示来改变模型行为的表示工程方法在对齐语言模型方面表现出了有效性。表示工程在面对对抗攻击和降低社会偏见等对齐导向任务方面取得了增益，但也导致了模型执行基本任务能力的降低。本文研究了增加对齐度和减少模型有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。有趣的是，我们发现，尽管模型的有用性通常会减少

    Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
    
[^234]: 在对抗的草堆中找到针：一种针对最小分布失真的边缘情况的目标改写方法

    Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion

    [https://arxiv.org/abs/2401.11373](https://arxiv.org/abs/2401.11373)

    本文提出了一种名为TPRL的方法，通过生成具有挑战性的样本来改善自然语言处理模型的性能，以对抗攻击中的分布失真问题。该方法利用近端策略梯度自动生成对抗性样本，并通过Mutual Implication分数保持原始文本的语义含义。

    

    对语言模型(LMs)进行对抗攻击是一个重要的问题。特别是，对抗样本利用模型对输入的微小变化的敏感性。虽然这些变化对输入样本的语义来说似乎微不足道，但却导致模型性能的显著下降。本文提出了一种称为Targeted Paraphrasing via RL (TPRL)的方法，通过自动学习生成具有挑战性的样本的策略，从而最可能提高模型的性能。TPRL利用FLAN T5作为生成器，并使用近端策略梯度来自动生成对抗性样本。TPRL的奖励基于分类器中引发的困惑程度，通过Mutual Implication分数保留原始文本的含义。我们通过对四个不同的自然语言处理分类任务进行大量实验，证明了TPRL发现自然对抗攻击和提高模型性能的有效性。

    Adversarial attacks against language models(LMs) are a significant concern. In particular, adversarial samples exploit the model's sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that most likely improve the model's performance. TPRL leverages FLAN T5, a language model, as a generator and employs a self learned policy using a proximal policy gradient to generate the adversarial examples automatically. TPRL's reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate and evaluate TPRL's effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks
    
[^235]: AI作为探索：导航智能空间

    AI-as-exploration: Navigating intelligence space

    [https://arxiv.org/abs/2401.07964](https://arxiv.org/abs/2401.07964)

    这篇论文阐述了一个被忽视但重要的科学角色，即AI作为探索。它强调通过创建和研究智能系统来揭示可能与人类和动物的智能形式不同的候选构建模块。论文通过讨论人类和大型语言模型在组合新颖和创造性概念方面的能力，说明了AI作为探索的价值。

    

    人工智能是一个拥有许多生命的领域，这个术语已经包含了一系列科学和商业努力。在这篇论文中，我阐述了人工智能具有一个被忽视但十分重要的科学角色，即“AI作为探索”。AI作为探索的基本思想是创建和研究能够揭示智能候选构建模块的系统，这些模块可能不同于我们熟悉的人类和动物智能形式。换句话说，我认为人工智能是探索智能空间，即可能的智能系统空间，的最佳工具之一。我通过关注一个具体的案例研究，即人类和大型语言模型在组合新颖和创造性概念方面的能力，来说明AI作为探索的价值。我展示了尽管后者在这样的任务中表现出人类水平的准确性，但很可能以根本不同的方式解决这个问题。

    Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no 
    
[^236]: 语言模型在某种程度上理解数字

    Language Models Understand Numbers, at Least Partially

    [https://arxiv.org/abs/2401.03735](https://arxiv.org/abs/2401.03735)

    本研究表明，大型语言模型在某种程度上理解数字，可以通过压缩和编码的方式执行算术计算。

    

    大型语言模型(LLMs)在各种任务中展现出令人印象深刻的能力，但其不透明的内部机制限制了它们在数学问题中的应用。在本文中，我们研究了一个基本问题：语言模型是否理解数字，数学中的基本元素。基于一个假设，即LLMs应该能够在其隐藏状态中压缩数字以解决数学问题，我们构建了一个合成数据集，包括加法问题，并利用线性探测器从隐藏状态中读取输入数字。实验结果支持LLMs中存在压缩的数字。然而，精确重建原始数字是困难的，表明压缩过程可能不是无损的。进一步的实验证明，LLMs可以利用编码的数字来执行算术计算，并且计算能力随模型大小的增加而扩展。我们的初步研究表明，LLMs在数字上展现出部分理解。

    Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number
    
[^237]: 结构化概率编码

    Structured Probabilistic Coding

    [https://arxiv.org/abs/2312.13933](https://arxiv.org/abs/2312.13933)

    结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。

    

    本论文提出了一种新的监督式表示学习框架，即结构化概率编码（SPC），用于从与目标任务相关的输入中学习紧凑和信息丰富的表示。SPC是一种仅有编码器的概率编码技术，具有来自目标空间的结构化正则化。它可以提高预训练语言模型的泛化能力，以实现更好的语言理解。具体而言，我们的概率编码在一个模块中同时进行信息编码和任务预测，以更充分地利用输入数据中的有效信息。它使用输出空间的变分推断来减少随机性和不确定性。此外，为了更好地控制概率表示的学习过程，在潜在空间中提出了结构化正则化，以促进类别之间的均匀性。通过正则化项，SPC可以保持潜在编码的高斯结构，并实现更好的覆盖率。

    This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
    
[^238]: 评估和增强用于知识图谱上的对话推理的大型语言模型

    Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs

    [https://arxiv.org/abs/2312.11282](https://arxiv.org/abs/2312.11282)

    该论文评估了当前最先进的大型语言模型（GPT-4）在知识图谱上的对话推理能力，提出了一种基于KG推理的LLM基准代理（LLM-ARK），该代理利用全文环境提示来实现精确和适应性强的KG路径预测，并采用近端策略优化算法进行训练。

    

    大型语言模型（LLM）的发展得益于预训练技术的进展。通过手动设计的提示，这些模型展示了强大的推理能力。在这项工作中，我们评估了当前最先进的LLM（GPT-4）在知识图谱（KG）上的对话推理能力。然而，由于缺乏KG环境意识和开发有效的中间推理阶段优化机制的困难，LLM的性能受到限制。我们进一步引入了LLM-ARK，一个基于KG推理的LLM基准代理，旨在提供精确和适应性强的KG路径预测。LLM-ARK利用全文环境（FTE）提示来吸收每个推理步骤中的状态信息。我们将KG上的多跳推理挑战重新框定为顺序决策任务。利用近端策略优化（PPO）在线策略梯度强化学习算法，我们的模型...

    The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model i
    
[^239]: 在课堂人机交互中利用非语言行为和社交注视的应用

    Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications

    [https://arxiv.org/abs/2312.06825](https://arxiv.org/abs/2312.06825)

    本文主要研究课堂人机交互中利用非语言行为和社交注视的应用。通过使用人类启发的社交注视模型，我们希望在机器人认知架构中实现更加流畅的社交互动。

    

    这篇摘要探讨了课堂人机交互中的场景，并着重介绍了人类启发的社交注视模型在机器人认知架构中的应用，以促进更加流畅的社交互动。首先，我们详细介绍了我们在研究中探索的人机交互场景，然后描述了我们研究中使用的社交注视模型。我们强调了在课堂人机交互场景中利用这种关注模型的优势。我们还详细介绍了我们即将进行的关于这个社交注视模型的研究的目标。

    This abstract explores classroom Human-Robot Interaction (HRI) scenarios with an emphasis on the adaptation of human-inspired social gaze models in robot cognitive architecture to facilitate a more seamless social interaction. First, we detail the HRI scenarios explored by us in our studies followed by a description of the social gaze model utilized for our research. We highlight the advantages of utilizing such an attentional model in classroom HRI scenarios. We also detail the intended goals of our upcoming study involving this social gaze model.
    
[^240]: 在Web上揭示语言模型代理在顺序任务组合中的局限性

    Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web

    [https://arxiv.org/abs/2311.18751](https://arxiv.org/abs/2311.18751)

    本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。

    

    最近，语言模型代理(LMA)作为一种在多步决策任务上的有希望的范例出现，通常表现优于人类和其他强化学习代理。尽管有这种希望，但它们在通常涉及任务组合的现实应用中的性能仍未得到充分探索。在这项工作中，我们引入了一个新的基准，叫做CompWoB-反映更现实假设的50个组合性网站自动化任务。我们发现，虽然现有的提示型LMA（gpt-3.5-turbo或gpt-4）在基本任务上实现了94.0％的平均成功率，但在组合任务上降至24.9％的成功率。另一方面，只在基本任务上进行微调的转移性LMA表现出更小的泛化性差距，从85.4％下降到54.8％。通过平衡任务之间的数据分布，我们训练了一个新模型HTML-T5++，在MiniWoB上超过了人类水平的性能（95.2％），并在CompWoB上实现了最佳的零-shot性能（61.5%）。

    Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
    
[^241]: ChatTraffic：通过扩散模型实现文本到交通生成

    ChatTraffic: Text-to-Traffic Generation via Diffusion Model

    [https://arxiv.org/abs/2311.16203](https://arxiv.org/abs/2311.16203)

    本研究提出了一种名为ChatTraffic的扩散模型，用于实现文本到交通生成。通过将生成模型与描述交通系统的文本结合，该方法能够解决传统交通预测方法中的两个挑战，并得到与真实交通数据一致的合成数据。

    

    交通预测是智能交通系统中最重要的基础之一。传统的交通预测方法只依赖历史交通数据来预测交通趋势，面临两个主要挑战：1）对异常事件不敏感；2）在长期预测方面性能有限。本文探讨了如何将生成模型与描述交通系统的文本结合起来用于交通生成，将此任务命名为文本到交通生成（TTG）。TTG任务的关键挑战是如何将文本与道路网络的空间结构和交通数据相关联，用于生成交通情况。为此，我们提出了ChatTraffic，这是第一个用于文本到交通生成的扩散模型。为了保证合成数据与真实数据的一致性，我们用图卷积网络（GCN）来扩展扩散模型，以提取交通数据的空间相关性。此外，我们构建了一个包含...

    Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing t
    
[^242]: 数据多样性对鲁棒指令调整至关重要

    Data Diversity Matters for Robust Instruction Tuning

    [https://arxiv.org/abs/2311.14736](https://arxiv.org/abs/2311.14736)

    数据多样性对鲁棒指令调整非常重要，我们提出了一种新算法(QDIT)，通过同时控制数据集的多样性和质量，我们深入研究了多样性和质量对指令调整性能的影响，并得出了两个关键观点。

    

    最近的研究表明，通过精选高质量且多样化的指令调整数据集，我们可以显著提高指令跟随能力。然而，创建这样的数据集非常困难，大多数研究依赖于手动精选或专有语言模型。自动数据精选很困难，因为仍不清楚如何为指令调整定义多样性，多样性和质量如何相互关联，以及如何优化数据集的质量和多样性。为解决这些问题，我们提出了一种新算法，质量-多样性指令调整(QDIT)。QDIT提供了一种简单的方法来同时控制数据集的多样性和质量，使我们能够深入研究多样性和质量对指令调整性能的影响。从这项研究中，我们得出了两个关键观点：(1)数据多样性和质量之间存在自然的权衡关系，(2)增加数据多样性显著提高最坏情况下的指令跟随性能。

    Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following perform
    
[^243]: DURel注释工具：人类和计算测量语义接近度、语义聚类和语义变化

    The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change

    [https://arxiv.org/abs/2311.12664](https://arxiv.org/abs/2311.12664)

    DURel注释工具是一个在线的、开源的工具，通过人类和计算机的注释实现了对单词使用之间的语义接近度的测量，并提供了对语义聚类和语义变化的分析功能。

    

    我们提出了DURel工具，该工具实现了在在线、开源界面中对单词使用之间的语义接近度进行注释。该工具支持标准化的人类注释和计算机注释，利用最近的上下文词模型的进展进行构建。注释者的判断通过自动图形聚类技术进行聚类，并进行可视化分析。这允许通过简单而直观的微任务判断来测量单词词义，并且需要最小的准备工作。该工具提供了额外的功能，以比较注释者之间的一致性，以确保获得判断的主观性，并计算总结统计数据，以揭示词义频率分布、语义变异或词义随时间的变化。

    We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time.
    
[^244]: 重新思考注意力：探索将浅层前馈神经网络作为Transformers中注意力层的替代方法

    Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers

    [https://arxiv.org/abs/2311.10642](https://arxiv.org/abs/2311.10642)

    本研究探索使用浅层前馈神经网络替代注意力机制，通过知识蒸馏方法训练，实验证明了这种"无注意力的Transformers"可以与原始架构的性能媲美，并揭示了其简化复杂架构的潜力。

    

    本研究分析了使用标准的浅层前馈网络来模仿Transformer模型中注意力机制的有效性。我们使用知识蒸馏的方法，将Transformer中的关键元素替换为简单的前馈网络，并使用原始组件进行训练。我们在IWSLT2017数据集上进行实验证明了这种“无注意力的Transformers”可以与原始架构的性能媲美。通过严谨的实验和不同替代网络类型和大小的尝试，我们提供了支持我们方法可行性的见解。这不仅揭示了浅层前馈网络在模仿注意力机制方面的适应性，而且强调了它们在简化序列任务的复杂架构方面的潜力。

    This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these "attentionless Transformers" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.
    
[^245]: 语言模型与人脑的差异

    Divergences between Language Models and Human Brains

    [https://arxiv.org/abs/2311.09308](https://arxiv.org/abs/2311.09308)

    该论文系统地探索了语言模型（LMs）和人类大脑在语言处理方面的差异，发现在社交/情感智能和物理常识领域，LMs无法很好地捕捉到人类的表现，但在这些领域对LMs进行微调可以提高其性能。

    

    机器和人类是否以相似的方式处理语言？最近的研究暗示肯定，发现大脑信号可以通过语言模型（LMs）的内部表示有效地进行预测。尽管这样的结果被认为反映了LMs和人类大脑之间的共享计算原理，但LMs和人类在语言表示和使用上也存在明显的差异。在这项工作中，我们通过检查LM表示和人类大脑对语言的响应之间的差异，通过采用两个数据集对受试者阅读和听叙述故事的方式，系统地探索了人类和机器语言处理之间的分歧。通过数据驱动的方法，我们确定了两个领域，即社交/情感智能和物理常识，这些领域在LMs中无法很好地捕捉到。然后，我们使用人类行为实验验证了这些领域，并证明在这些领域对LMs进行微调可以改善其性能。

    Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
    
[^246]: 数据污染问题: 一种检测和估计大型语言模型中污染的工具

    Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models

    [https://arxiv.org/abs/2311.06233](https://arxiv.org/abs/2311.06233)

    这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。

    

    我们提出了数据污染问题（DCQ），这是一种简单而有效的方法，用于检测大型语言模型（LLM）中的数据污染并估计其数量。具体而言，我们将数据污染检测视为一系列的多项选择问题，并设计了一种测验形式，其中创建了每个数据集实例的三个扰动版本。这些变化仅包括词级扰动。生成的扰动版本与原始实例一起形成DCQ中的选项，额外的选项适应了提供的选择都不正确的可能性。鉴于在选择之间唯一的区别信号是与原始实例的确切措辞相关，如果在预训练阶段已经接触到原始实例，语言模型当被要求从选项中识别原始实例时，倾向于选择原始实例--这是语言模型固有的特性。在使用GPT-4/3.5进行多个数据集的测试中，我们的结果完全缺少准确性。

    We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
    
[^247]: 在仅使用编码器的变压器模型中，对离散化的令牌ASR不需要使用损失遮蔽

    Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR

    [https://arxiv.org/abs/2311.04534](https://arxiv.org/abs/2311.04534)

    在离散化令牌的ASR中，仅使用解码器的Transformer模型不需要使用损失遮蔽。取而代之的是，我们提出了一种名为平滑标签蒸馏的新方法，它在语音令牌上应用了带有平滑标签的KL散度损失，并且在实验证明效果优于损失遮蔽方法。

    

    最近，统一的语音-文本模型，例如SpeechGPT、VioLA和AudioPaLM，在各种语音任务上取得了显著的性能。这些模型将语音信号离散化为令牌（语音离散化），并对文本和语音令牌使用共享词汇表。然后，在混合语音任务上训练单个只有解码器的变压器。然而，这些模型依赖于ASR任务中的损失遮蔽策略，该策略忽略语音令牌之间的依赖关系。在本文中，我们提出以类似于文本的自回归方式对语音令牌进行建模。我们发现，对输入的语音令牌应用传统的交叉熵损失并不能始终改善ASR性能，相比之下，损失遮蔽方法更有效。为了解决这个问题，我们提出了一种新的方法，名为平滑标签蒸馏（SLD），它在语音令牌上应用了带有平滑标签的KL散度损失。我们的实验证明，SLD有效地对语音令牌进行建模，并胜过了损失遮蔽方法。

    Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for 
    
[^248]: 语言模型就像超级马里奥：通过吸收同源模型的能力来实现免费午餐

    Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch

    [https://arxiv.org/abs/2311.03099](https://arxiv.org/abs/2311.03099)

    本文揭示了语言模型可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。作者提出了DARE技术来稀疏化参数并将多个同源模型合并为一个模型。实验证明，DARE可以轻松删除大部分参数并实现多任务融合。

    

    在本文中，我们揭示了语言模型(LMs)可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。我们首先引入了DARE来将大多数delta参数（即微调和预训练参数之间的差异）设置为零，而不会影响监督微调(SFT) LMs的能力，DARE通过随机删除比率为p的delta参数，并通过1/(1 - p)重新缩放剩余参数来近似原始嵌入。然后，我们将DARE作为一种通用的即插即用技术来稀疏化多个SFT同源模型的delta参数，以减轻参数干扰，并通过参数融合将它们合并为一个模型。我们通过编码器和解码器为基础的LM进行实验，结果表明：（1）SFT delta参数值范围通常很小（在0.005以内），具有极高的冗余，DARE可以轻松删除90%甚至99%的参数。（2）DARE可以将多个任务特定的LM合并为一个LM，并有驾驶技能

    In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
    
[^249]: PhoGPT: 越南语的生成式预训练模型

    PhoGPT: Generative Pre-training for Vietnamese

    [https://arxiv.org/abs/2311.02945](https://arxiv.org/abs/2311.02945)

    PhoGPT是一个用于越南语的生成式预训练模型系列，具有40亿参数的基础模型PhoGPT-4B以及其聊天变体PhoGPT-4B-Chat，展示了在越南语任务上优于之前的7亿参数模型的强大性能。

    

    我们开源了一个拥有40亿参数的最先进的越南语生成模型系列，其中包括基础的预训练单语模型PhoGPT-4B和其聊天变体PhoGPT-4B-Chat。基础模型PhoGPT-4B有37亿参数，从零开始在包含1020亿标记的越南语语料库上进行预训练，使用长度为8192的上下文，使用20480个标记类型的词汇表。聊天变体PhoGPT-4B-Chat是在70000个指导提示和回应以及额外的290000个对话数据集上对PhoGPT-4B进行微调得到的模型输出。我们展示了相比之前闭源和开源的70亿参数模型，它的强大性能。我们的PhoGPT模型可在以下链接下载：https://github.com/VinAIResearch/PhoGPT

    We open-source a state-of-the-art 4B-parameter generative model series for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens, with an 8192 context length, employing a vocabulary of 20480 token types. The chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along with an additional 290K conversations. We demonstrate its strong performance compared to previous closed-source and open-source 7B-parameter models. Our PhoGPT models are available at: https://github.com/VinAIResearch/PhoGPT
    
[^250]: 学习从错误中使LLM成为更好的推理者

    Learning From Mistakes Makes LLM Better Reasoner

    [https://arxiv.org/abs/2310.20689](https://arxiv.org/abs/2310.20689)

    本研究探索了大型语言模型（LLMs）是否可以从错误中学习，类似于人类学习的过程，并通过引入错误纠正的数据对来改进LLMs的推理能力。实验结果表明，这种方法能够持续提升仅使用CoT进行微调后的性能。

    

    最近，大型语言模型（LLM）在解决数学问题方面展示出了卓越的推理能力。为了进一步提高它们的推理能力，本研究探讨了LLM是否可以学习从错误中获益（LEMA），类似于人类的学习过程。考虑一个未能解决数学问题的人类学生，他会从自己犯的错误中学习，并纠正它。模仿这种错误驱动的学习过程，LEMA在LLM的微调过程中引入了错误纠正的数据对。具体而言，我们首先收集来自各种LLM的错误推理路径，然后使用GPT-4作为“纠正者”来识别错误步骤，解释错误原因，纠正错误并生成最终答案。此外，我们还应用了一种基于纠正的进化策略，有效地扩展了生成纠正数据的问题集。在各种LLM和推理任务上的实验表明，LEMA始终可以提升仅使用CoT的微调。我们...

    Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu
    
[^251]: 用规划标记引导语言模型的数学推理

    Guiding Language Model Math Reasoning with Planning Tokens

    [https://arxiv.org/abs/2310.05707](https://arxiv.org/abs/2310.05707)

    本论文介绍了一种通过引入规划标记来引导语言模型进行数学推理的方法。这种方法在保持推理过程中的一致性方面具有显著的准确性提升，而增加的训练参数很少。

    

    大型语言模型（LLMs）近来因其进行复杂推理任务的能力（如思维链推理）而引起了广泛关注。然而，大多数现有的增强模型推理能力方法过于依赖数据驱动方法，忽视了模型推理能力的结构化方面。我们发现，虽然LLMs可以很好地处理个别推理步骤，但在整个推理链上保持一致性方面却存在困难。为了解决这个问题，我们在每个推理步骤的开始处引入规划标记，作为模型的引导，并将它们的嵌入添加到模型参数中。我们的方法对于可训练参数的增加非常小（仅为0.001%），可以通过完全微调或更高效的参数方案来应用。我们通过将其应用于三种不同的LLMs，在三个数学单词问题数据集上展示了我们方法的有效性，相对于标准方法，准确性显著提高。

    Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
    
[^252]: Fabricator: 一个用于生成带有Teacher LLMs标注训练数据的开源工具集

    Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs

    [https://arxiv.org/abs/2309.09582](https://arxiv.org/abs/2309.09582)

    Fabricator是一个开源工具集，用于生成带有Teacher LLMs标注训练数据。它通过零样本学习的方式，利用强大的LLM根据任务描述生成标注数据，可以用于训练下一阶段的自然语言处理模型。

    

    大多数自然语言处理任务是通过监督学习来建模的，因此需要标注的训练数据来训练有效的模型。然而，手动生成足够质量和数量的标注数据被认为是昂贵和耗时的。当前的研究通过探索一种称为零样本学习的新范式来解决这个瓶颈，该范式通过数据集生成模型。在这种模式下，一个强大的LLM根据给定的任务描述生成标注数据，这些数据可以用于训练下一阶段的自然语言处理模型。例如，可以让LLM生成500个积极情绪的电影评论和另外500个消极情绪的评论。然后，生成的数据可以用于训练一个情感分类器，从而有效地利用LLM作为较小规模的学生模型的教师。通过这个演示，我们介绍了Fabricator，一个用于数据集生成的开源Python工具集。Fabricator实现了常见的数据集生成工作流程，支持广泛的自然语言处理任务（例如文本分类）。

    Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to "generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment." The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classi
    
[^253]: 上下文感知对抗攻击在命名实体识别中的应用

    Context-aware Adversarial Attack on Named Entity Recognition

    [https://arxiv.org/abs/2309.08999](https://arxiv.org/abs/2309.08999)

    本研究关注命名实体识别任务，研究了上下文感知的对抗攻击方法，通过扰动最具信息量的单词来创建对抗样本，并使用不同的替换方法生成自然且可信的对抗示例。实验证明，这些方法比强基准方法更有效地欺骗模型做出错误预测。

    

    最近几年，大规模预训练语言模型（PLM）在许多自然语言处理基准测试中取得了显著的性能。尽管取得了成功，但之前的研究表明，PLM容易受到对抗样本的攻击。在本文中，我们关注命名实体识别任务，并研究上下文感知的对抗攻击方法，以检验模型的鲁棒性。具体而言，我们提出扰动用于识别实体的最具信息量的单词，从而创建对抗样本，并研究不同的候选替换方法来生成自然且可信的对抗样本。实验和分析表明，我们的方法在欺骗模型做出错误预测方面比强基准方法更有效。

    In recent years, large pre-trained language models (PLMs) have achieved remarkable performance on many natural language processing benchmarks. Despite their success, prior studies have shown that PLMs are vulnerable to attacks from adversarial examples. In this work, we focus on the named entity recognition task and study context-aware adversarial attack methods to examine the model's robustness. Specifically, we propose perturbing the most informative words for recognizing entities to create adversarial examples and investigate different candidate replacement methods to generate natural and plausible adversarial examples. Experiments and analyses show that our methods are more effective in deceiving the model into making wrong predictions than strong baselines.
    
[^254]: 在大型语言模型中重新思考STS和NLI

    Rethinking STS and NLI in Large Language Models

    [https://arxiv.org/abs/2309.08969](https://arxiv.org/abs/2309.08969)

    这篇论文重新思考了在大型语言模型中的STS和NLI问题。通过在临床/生物医学领域评估性能，以及评估LLMs的预测置信度和捕捉集体人类意见的能力，发现这些问题在LLMs时代仍未得到妥善解决。

    

    近年来，大型语言模型（LLMs）的兴起使从业者能够使用特定任务提示，这在各种任务中被证明是有效的。然而，当应用于语义文本相似性（STS）和自然语言推理（NLI）时，LLMs的有效性受到限制，原因是低资源领域准确性、模型自信度不足以及捕捉人类判断之间的分歧困难。基于这一思考，我们试图重新思考LLMs时代的STS和NLI。我们首先评估了临床/生物医学领域的STS和NLI性能，然后评估了LLMs的预测置信度和捕捉集体人类意见的能力。我们发现在LLMs时代，这些老问题仍未得到妥善解决。

    Recent years have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs' predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.
    
[^255]: LAraBench：基于大语言模型进行阿拉伯语AI的基准测试

    LAraBench: Benchmarking Arabic AI with Large Language Models

    [https://arxiv.org/abs/2305.14982](https://arxiv.org/abs/2305.14982)

    LAraBench是一个针对阿拉伯语自然语言处理和语音处理任务的基准测试平台，通过多种实验设置和性能衡量指标，证明最新模型通常表现优于大语言模型（LLMs）。

    

    近期大语言模型（LLMs）的进展显著影响了语言和语音研究领域。尽管取得了进步，但这些模型尚缺乏特定语言和任务的最新模型进行对比的基准测试。LAraBench针对阿拉伯自然语言处理（NLP）和语音处理任务提供了这方面的解决方案，包括序列标注和跨不同领域的内容分类。我们采用了GPT-3.5-turbo、GPT-4、BLOOMZ、Jais-13b-chat、Whisper和USM等模型，运用零样本学习和少样本学习技术，应对了33个独立任务和61个公开可用的数据集。这涉及98个实验设置，包括约296K个数据点、约46小时的语音和30个用于文本到语音（TTS）的句子。这一努力产生了330+组实验。我们的分析重点是衡量最新模型和LLMs之间的性能差距。总体趋势表明，最新模型一般表现更优。

    Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperf
    
[^256]: 用于双曲知识图嵌入的三维旋转和平移

    3D Rotation and Translation for Hyperbolic Knowledge Graph Embedding

    [https://arxiv.org/abs/2305.13015](https://arxiv.org/abs/2305.13015)

    本研究引入了一种新的模型3H-TH，用于双曲知识图嵌入，可以同时捕捉关系模式，包括对称性、反对称性、反转、可交换组合、非可交换组合、层次结构和多重性。实验证明，该模型在低维空间的准确度、层次结构和其他关系模式方面优于现有模型，在高维空间中表现类似。

    

    知识图（KG）嵌入的主要目标是学习实体和关系的低维表示，以便预测缺失的事实。在实现更好的KG嵌入中的一个重要挑战是捕捉关系模式，包括对称性、反对称性、反转、可交换组合、非可交换组合、层次结构和多重性。本研究引入了一种称为3H-TH（双曲空间中的三维旋转和平移）的新模型，可以同时捕捉这些关系模式。相比之下，先前的尝试没有同时在所有提到的属性上实现令人满意的性能。实验证明，新模型在低维空间的精度、层次结构和其他关系模式方面优于现有的最先进模型，同时在高维空间中表现类似。

    The main objective of Knowledge Graph (KG) embeddings is to learn low-dimensional representations of entities and relations, enabling the prediction of missing facts. A significant challenge in achieving better KG embeddings lies in capturing relation patterns, including symmetry, antisymmetry, inversion, commutative composition, non-commutative composition, hierarchy, and multiplicity. This study introduces a novel model called 3H-TH (3D Rotation and Translation in Hyperbolic space) that captures these relation patterns simultaneously. In contrast, previous attempts have not achieved satisfactory performance across all the mentioned properties at the same time. The experimental results demonstrate that the new model outperforms existing state-of-the-art models in terms of accuracy, hierarchy property, and other relation patterns in low-dimensional space, meanwhile performing similarly in high-dimensional space.
    
[^257]: 通过跨模态选择性自训练实现零射击端到端口语理解

    Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training

    [https://arxiv.org/abs/2305.12793](https://arxiv.org/abs/2305.12793)

    本文提出了一种通过跨模态选择性自训练的方法，以解决在零射击端到端口语理解中的不平衡和标签噪声问题。

    

    现有的端到端口语理解 (SLU) 受到了收集语音-语义对的成本的约束，特别是当标签域发生变化时。因此，我们探索了"零射击"的端到端口语理解，即在没有语音-语义对的情况下学习端到端口语理解，而是仅使用语音-文本和文本-语义对。以前的工作通过使用在文本-语义语料库上学到的自然语言理解 (NLU) 模型对所有的语音-文本转录进行伪标签化来实现零射击。然而，这种方法要求语音-文本和文本-语义的领域匹配，而由于采集不同的语料库，通常会出现不匹配的情况。此外，使用从任何领域收集的整个语音-文本语料库会导致"不平衡"和"噪声"问题。为了解决这些问题，我们提出了"跨模态选择性自训练" (CMSST)。CMSST通过在三种模态 (语音、文本和语义) 的联合空间中进行聚类来解决不平衡问题，并借助选择网络来处理标签噪声。

    End-to-end (E2E) spoken language understanding (SLU) is constrained by the cost of collecting speech-semantics pairs, especially when label domains change. Hence, we explore \textit{zero-shot} E2E SLU, which learns E2E SLU without speech-semantics pairs, instead using only speech-text and text-semantics pairs. Previous work achieved zero-shot by pseudolabeling all speech-text transcripts with a natural language understanding (NLU) model learned on text-semantics corpora. However, this method requires the domains of speech-text and text-semantics to match, which often mismatch due to separate collections. Furthermore, using the entire collected speech-text corpus from any domains leads to \textit{imbalance} and \textit{noise} issues. To address these, we propose \textit{cross-modal selective self-training} (CMSST). CMSST tackles imbalance by clustering in a joint space of the three modalities (speech, text, and semantics) and handles label noise with a selection network. We also introdu
    
[^258]: 一种用于文本生成的重新参数化离散扩散模型的研究

    A Reparameterized Discrete Diffusion Model for Text Generation

    [https://arxiv.org/abs/2302.05737](https://arxiv.org/abs/2302.05737)

    本文提出了一种重新参数化离散扩散模型，该模型在文本生成方面表现出更好的灵活性、训练技术和生成效果，实验证明其较现有的扩散模型有显著的改进。

    

    本文研究了应用于自然语言生成的离散扩散概率模型。我们推导出了从离散扩散过程中采样的另一种等价形式，并利用这一洞见开发了一族重新参数化离散扩散模型。这个派生的通用框架非常灵活，为离散扩散模型中的生成过程提供了新的视角，并具备更有效的训练和解码技术。我们进行了大量实验证明我们模型的文本生成能力，在现有的扩散模型上取得了显著的改进。

    This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.
    
[^259]: 电影中少样本情感理解作为元学习心智模型评价的研究

    Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind

    [https://arxiv.org/abs/2211.04684](https://arxiv.org/abs/2211.04684)

    人们在阅读故事时，通过对虚构和真实人物的类比，可以快速理解新的虚构角色。本研究填补了现有研究中忽视的少样本和元学习的心智模型（ToM）的重要性。我们提供了一个新的NLP数据集ToM-in-AMC，该数据集提供了一个评价机器元学习心智模型的现实叙事理解场景。

    

    在阅读故事时，人类可以通过将其与他们已经了解的虚构和真实人物进行类比，迅速理解新的虚构角色。这反映了人类对角色内心状态（即心智模型）的推理中少样本和元学习的本质，现有研究在这方面很大程度上被忽视了。我们通过提供一个新颖的NLP数据集ToM-in-AMC来填补这一空白，这是第一个以现实叙事理解场景为背景的机器元学习心智模型评价。我们的数据集包含约1000个分析过的电影剧本，每个剧本对应于一个需要模型模仿人类快速理解新电影中的角色的少样本情感理解任务。

    When reading a story, humans can quickly understand new fictional characters with a few observations, mainly by drawing analogies to fictional and real people they already know. This reflects the few-shot and meta-learning essence of humans' inference of characters' mental states, i.e., theory-of-mind (ToM), which is largely ignored in existing research. We fill this gap with a novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario. Our dataset consists of ~1,000 parsed movie scripts, each corresponding to a few-shot character understanding task that requires models to mimic humans' ability of fast digesting characters with a few starting scenes in a new movie.   We propose a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions. It surpasses existing baseline models, underscoring the significance of modeling multiple ToM dimensions for our task. Our extensive hu
    
[^260]: BoAT v2 -一种以粘聚性语言为重点的基于Web的依存性标注工具

    BoAT v2 - A Web-Based Dependency Annotation Tool with Focus on Agglutinative Languages

    [https://arxiv.org/abs/2207.01327](https://arxiv.org/abs/2207.01327)

    BoAT v2是一种基于Web的依存性标注工具，专注于以粘聚性语言为重点的语言，可以实现多用户同时标注，为使用者提供良好的用户体验。

    

    随着树库质量的不断提高，树库在自然语言处理工具开发中发挥的关键作用日益增加。创建这种树库需要极大的人力和时间成本。特别是考虑到树库的规模时，支持标注过程的工具至关重要。虽然已经提出了各种标注工具，但对于像土耳其语这样的粘聚性语言往往不适用。 BoAT v1是为标注依存关系而开发的，随后用于创建手动标注的BOUN树库（UD_Turkish-BOUN）。在这项工作中，我们报道了一个基于BoAT v1使用经验的设计和实现的依存注释工具BoAT v2，发现了一些改进的机会。BoAT v2是一个多用户和基于Web的依存注释工具，其设计侧重于注释者的用户体验以产生有效的注释。

    The value of quality treebanks is steadily increasing due to the crucial role they play in the development of natural language processing tools. The creation of such treebanks is enormously labor-intensive and time-consuming. Especially when the size of treebanks is considered, tools that support the annotation process are essential. Various annotation tools have been proposed, however, they are often not suitable for agglutinative languages such as Turkish. BoAT v1 was developed for annotating dependency relations and was subsequently used to create the manually annotated BOUN Treebank (UD_Turkish-BOUN). In this work, we report on the design and implementation of a dependency annotation tool BoAT v2 based on the experiences gained from the use of BoAT v1, which revealed several opportunities for improvement. BoAT v2 is a multi-user and web-based dependency annotation tool that is designed with a focus on the annotator user experience to yield valid annotations. The main objectives of 
    
[^261]: SemScore: 基于语义文本相似度的指令调校大型语言模型的自动评估

    SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity. (arXiv:2401.17072v1 [cs.CL])

    [http://arxiv.org/abs/2401.17072](http://arxiv.org/abs/2401.17072)

    这项研究提出了一个名为SemScore的评估指标，通过语义文本相似度直接比较模型输出和黄金目标回应，用于评估指令调校大型语言模型。实验证明，SemScore指标在与人工评估的相关性方面表现优于其他评估指标。

    

    最近，指令调校的大型语言模型（LLMs）在生成适合自然语言指令的回应方面展示出了令人瞩目的进展。然而，许多当前的研究依赖于手动评估来判断生成回应的质量。由于这种手动评估耗时，不容易扩展到对多个模型和模型变体的评估。在本短文中，我们提出了一种简单但非常有效的评估指标SemScore，通过语义文本相似度（STS）直接将模型输出与黄金目标回应进行比较。我们对12个知名的指令调校LLMs的模型输出进行了基于8个广泛使用的文本生成评估指标的比较评估。我们发现我们提出的SemScore指标在与人工评估的相关性方面优于所有其他、在许多情况下更复杂的评估指标。这些发现表明我们提出的指标对于评估指令调校LLMs的实用性。

    Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for 
    
[^262]: 跨语言转移研究：将低资源马耳他语视为多语言代码切换

    Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching. (arXiv:2401.16895v1 [cs.CL])

    [http://arxiv.org/abs/2401.16895](http://arxiv.org/abs/2401.16895)

    本研究关注马耳他语这种混合语言，采用了一种新颖的数据集和分类器来提高跨语言转移能力，解决了混合语言文字差异的问题。

    

    尽管多语言语言模型在未见过的语言上表现出令人印象深刻的跨语言转移能力，但在与多语言模型的预训练数据中使用的语言存在文字差异时，其在下游任务中的性能受到影响。使用音译提供了一种直接而有效的方法，可以将资源丰富的语言的文字与目标语言的文字对齐，从而增强跨语言转移能力。然而，对于混合语言来说，这种方法并不是最佳选择，因为只有语言的某个子集从跨语言转移中受益，而其余部分受到阻碍。在这项工作中，我们专注于马耳他语，这是一种受到阿拉伯语、意大利语和英语重大影响，并且采用拉丁文脚本的闪米特语言。我们提供了一个新的数据集，其中包含了单词级别的词源学注释。我们使用这个数据集训练了一个分类器，使我们能够明智决策如何处理马耳他语的每个标记。

    Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model's pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language, thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese la
    
[^263]: 从零开始构建一个大型语言模型

    Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])

    [http://arxiv.org/abs/2401.16736](http://arxiv.org/abs/2401.16736)

    Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。

    

    深度学习在自然语言处理（NLP）领域的普及导致了能够理解和生成人类语言的创新技术的开发和发布。Atinuke是一种基于Transformer的神经网络，通过利用独特的配置，在各种语言任务上优化性能。该架构通过将处理时序数据的层与注意机制交织在一起，从而在输入和输出之间建立有意义的关联。由于其拓扑结构和超参数调整的配置，它可以提取特征并学习复杂的映射，从而模仿人类语言。Atinuke是模块化、可扩展的，并可以与现有的机器学习流程无缝集成。softmax、嵌入和多头注意力等高级矩阵操作使得对文本、声音和视觉信号的细致处理成为可能。通过将现代深度学习技术与软件设计原则和数学方法相结合

    The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
    
[^264]: 在临床文本中预测实体修饰语的迁移学习：以阿片类物质使用障碍病例检测为应用

    Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])

    [http://arxiv.org/abs/2401.15222](http://arxiv.org/abs/2401.15222)

    本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。

    

    背景：从临床文本中提取的实体的语义可能会受到修饰语的显著改变，包括实体的否定、不确定性、条件性、严重性和主观性。现有的确定临床实体修饰语的模型涉及使用正则表达式或特征权重，这些权重是独立训练每个修饰语的。方法：我们开发并评估了一个多任务变换器架构设计，在公开可用的SemEval 2015任务14语料库和一个新的阿片类物质使用障碍（OUD）数据集上共同学习和预测修饰语，该数据集包含与SemEval共享的修饰语以及OUD特定的新修饰语。我们评估了我们的多任务学习方法与以前发表的系统的效果，并评估了仅共享部分临床修饰语时的临床实体修饰语的迁移学习的可行性。结果：我们的方法在来自SemEval 2015的ShARe语料库上取得了最新技术的结果。

    Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
    
[^265]: EAGLE: 推测采样需要重新思考特征不确定性

    EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])

    [http://arxiv.org/abs/2401.15077](http://arxiv.org/abs/2401.15077)

    EAGLE是一个无损加速语言模型推理的框架，通过在次顶层特征层面上自回归推理，并解决采样不确定性问题，实现了比传统方法更快3倍的速度。

    

    自回归解码使得大型语言模型（LLMs）的推理变得耗时。我们提出了一个简单的框架，EAGLE（用于提高语言模型效率的外推算法），实现了无损加速。与传统的推测采样方法不同，EAGLE在更规律的（次顶层）特征层面上自回归进行编写，并通过整合提前一个时间步的标记来解决下一个特征预测问题中的采样不确定性。EAGLE所提供的加速是无损的：它不需要微调目标LLM，并且生成的文本与原始的自回归解码的分布相同。截至本文提交时，EAGLE是已知推测采样家族中速度最快的框架。在MT-bench上，EAGLE比原始解码快3倍，比Lookahead快2倍，比Medusa快1.6倍。使用gpt-fast，EAGLE平均每秒达到160个标记与LLaMA2-Chat搭配。

    Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
    
[^266]: 作为一种语言处理任务的参数高效的对话推荐系统

    Parameter-Efficient Conversational Recommender System as a Language Processing Task. (arXiv:2401.14194v1 [cs.CL])

    [http://arxiv.org/abs/2401.14194](http://arxiv.org/abs/2401.14194)

    本文将对话推荐系统作为一种语言处理任务进行建模，利用预训练的语言模型来编码项目、理解用户意图，通过语义匹配进行项目推荐，并生成对话。实验证明了该方法的有效性。

    

    对话式推荐系统旨在通过自然语言对话来向用户推荐相关的项目。之前的工作通常利用外部知识图谱来提供项目的语义信息，利用语言模型进行对话生成，以及利用推荐模块进行相关项目的排序。这种多组件的组合导致训练过程繁琐，并且导致对话生成和项目推荐之间的语义不配对问题。在本文中，我们使用自然语言表示项目，并将对话式推荐系统作为一种自然语言处理任务进行建模。因此，我们利用预训练的语言模型来编码项目，在对话中理解用户意图，通过语义匹配进行项目推荐，并生成对话。作为一个统一的模型，我们的PECRS（参数高效的对话推荐系统）可以在单个阶段进行优化，而不依赖非文本元数据，如知识图谱。实验证明了我们方法的有效性。

    Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on
    
[^267]: 基于Mistral的大规模马来西亚语言模型，提升本地语言理解能力

    Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v1 [cs.CL])

    [http://arxiv.org/abs/2401.13565](http://arxiv.org/abs/2401.13565)

    本文介绍了Mistral 7B大规模语言模型在马来西亚语言数据集上的预训练进展和性能优化，证明了继续预训练和扩展上下文长度对提升语言理解能力的有效性，并对比了其在Tatabahasa上的优越性能。

    

    本文提出了Mistral 7B的预训练的重要进展，使用了32.6GB的数据集，相当于11亿个标记。我们研究了扩展上下文长度的影响，发布了上下文长度为4096和32768的模型，并使用特定的16384上下文长度的指令调整模型，我们称之为马来西亚Mistral。我们的实验证明了继续预训练的有效性以及扩展上下文长度对Mistral 7B语言理解能力的影响。此外，我们发布了一个专门调整了16384上下文长度的模型，展示了其捕捉微妙语言细节的潜力。此外，我们的研究还对比了马来西亚Mistral与ChatGPT3.5和Claude 2等著名语言模型，并呈现了令人信服的结果表明马来西亚Mistral在Tatabahasa上的优越性能。

    In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.  Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.  Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (
    
[^268]: SLANG: 大型语言模型对新概念的理解

    SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])

    [http://arxiv.org/abs/2401.12585](http://arxiv.org/abs/2401.12585)

    本研究提出了一个新的基准SLANG，旨在增强大型语言模型LLMs对互联网上新概念的理解能力，同时提出了一种基于因果推断的基准方法FOCUS，能帮助LLMs更好地理解新的短语和用法模式。

    

    语言的动态性，尤其在互联网上的俚语和表情包等方面的体现，给大型语言模型（LLMs）的适应性带来了严峻挑战。传统上，这些模型通常仅绑定在静态数据集上，很难跟上在线社区中快速语言进化的步伐。本研究解决了弥合这一差距的迫切需求，旨在增强LLMs对互联网上新概念的理解能力，同时避免高成本和不切实际的持续重训练。为应对这个问题，我们提出了一个新的评估LLMs在理解新兴语言趋势方面能力的基准 - SLANG，并提出了一种基于因果推断的基准方法 FOCUS，它能增强LLMs对新的短语和用法模式的理解。该方法包括对语言转变的真实世界实例进行详细研究，作为背景依据，以形成更精确和具有上下文相关性的新连接。

    The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
    
[^269]: 单词级别的ASR质量评估用于通过分析参考无关的指标的注意力进行高效语料库采样和后期编辑

    Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric. (arXiv:2401.11268v1 [cs.CL])

    [http://arxiv.org/abs/2401.11268](http://arxiv.org/abs/2401.11268)

    本研究介绍了一种使用质量估计度量来增强自动语音识别系统(XAI)的方法。实验证明了NoRefER度量在识别单词错误和提供有价值的模型行为见解方面的能力。研究还发现NoRefER在语料库构建和后期编辑工作流程中的实用性，表明其有潜力成为提高ASR系统效率和可解释性的关键工具。

    

    在自动语音识别（ASR）领域中，不仅要有高准确性的模型，还要提供决策过程的可解释性是至关重要的。引入和评估了质量估计（QE）度量作为增强ASR系统可解释人工智能（XAI）的新工具。通过实验证明了NoRefER（无参考错误率）度量在识别单词级错误方面的能力，以帮助后期编辑者改进ASR假设。研究还扩展到NoRefER在语料库构建过程中的实用性，展示了它在增强具有有见地注释的数据集方面的有效性。对NoRefER的诊断特性进行了研究，揭示了其提供有价值的模型行为和决策模式见解的能力。这对于在后期编辑工作流程和微调ASR模型中优先考虑假设是有益的。研究结果表明NoRefER具有潜力成为提高ASR系统可解释性和效率的关键工具。

    In the realm of automatic speech recognition (ASR), the quest for models that not only perform with high accuracy but also offer transparency in their decision-making processes is crucial. The potential of quality estimation (QE) metrics is introduced and evaluated as a novel tool to enhance explainable artificial intelligence (XAI) in ASR systems. Through experiments and analyses, the capabilities of the NoRefER (No Reference Error Rate) metric are explored in identifying word-level errors to aid post-editors in refining ASR hypotheses. The investigation also extends to the utility of NoRefER in the corpus-building process, demonstrating its effectiveness in augmenting datasets with insightful annotations. The diagnostic aspects of NoRefER are examined, revealing its ability to provide valuable insights into model behaviors and decision patterns. This has proven beneficial for prioritizing hypotheses in post-editing workflows and fine-tuning ASR models. The findings suggest that NoRef
    
[^270]: 多语言语言模型中的跨语言编辑

    Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])

    [http://arxiv.org/abs/2401.10521](http://arxiv.org/abs/2401.10521)

    本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。

    

    大规模语言模型（LLM）的训练需要大量的数据和计算资源，而更新过时的LLM需要大量的工作和资源。虽然出现了许多模型编辑技术（MET）以便在不重新训练的情况下高效更新模型输出，但在多语言LLM中，其中的知识以多种语言存储，这仍然是一个未深入研究的领域。本研究介绍了跨语言模型编辑（XME）范式，在该范式中，一个事实在一种语言中被编辑，观察其在其他语言中的更新传播。为了研究XME范式，我们使用BLOOM、mBERT和XLM-RoBERTa进行了实验，使用了两种写作脚本，即拉丁语（英语、法语和西班牙语）和印地语（印地语、古吉拉特语和孟加拉语）。结果显示，在XME设置下，当前最先进的MET存在明显的性能限制，特别是当涉及的语言属于两个不同的语族时。

    The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
    
[^271]: 基于知识图谱驱动的图神经网络推荐模型

    Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])

    [http://arxiv.org/abs/2401.10244](http://arxiv.org/abs/2401.10244)

    提出了一种基于知识图谱的图神经网络推荐模型KGLN，通过合并节点特征、调整聚合权重和迭代演化，提高了个性化推荐的准确性和效果。在实验中相对于已有基准方法，KGLN在不同数据集上的AUC提高了0.3%至5.9%和1.1%至8.2%。

    

    提出了一种新的基于图神经网络的推荐模型KGLN，该模型利用知识图谱（KG）信息，提高了个性化推荐的准确性和效果。该模型首先利用单层神经网络将图中的个体节点特征合并，然后通过结合影响因素调整相邻实体的聚合权重。通过迭代，模型从单层逐渐演变为多层，使实体能够获取丰富的多阶关联实体信息。最后，将实体和用户的特征结合起来产生推荐分数。通过比较不同聚合方法和影响因素的效果，评估了模型的性能。在使用MovieLen-1M和Book-Crossing数据集进行测试时，KGLN相对于LibFM和D等已有基准方法，AUC（ROC曲线下的面积）提高了0.3%至5.9%和1.1%至8.2%。

    A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
    
[^272]: LoMA: 无损压缩的内存注意力

    LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])

    [http://arxiv.org/abs/2401.09486](http://arxiv.org/abs/2401.09486)

    LoMA是一种无损压缩的内存注意力方法，可以有效地处理长文本并减少资源消耗。

    

    处理长文本是大型语言模型（LLMs）最重要的能力之一，但随着文本长度的增加，资源消耗也急剧增加。目前，通过压缩KV缓存来减少资源消耗是一种常见的方法。尽管存在许多现有的压缩方法，但它们都有一个共同的缺点：压缩是有损的。也就是说，在压缩过程中信息不可避免地会丢失。如果压缩率很高，丢失重要信息的概率会大大增加。我们提出了一种新方法，无损压缩的内存注意力（LoMA），可以根据一组压缩比率将信息无损压缩成特殊的内存令牌KV对。我们的实验证明，LoMA具有出色的性能，可以高效训练且具有非常有效的性能。

    The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
    
[^273]: DrawTalking：通过草图和语言建立互动世界

    DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])

    [http://arxiv.org/abs/2401.05631](http://arxiv.org/abs/2401.05631)

    用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。

    

    我们引入了一种交互式方法，DrawTalking，用户可以通过草图和语言建立互动世界。它强调用户控制和灵活性，并且在没有编程的情况下提供了类似编程的能力。我们在iPad上实现了它。一项开放式研究表明，这种机制与许多创造性探索性用例相契合和适用。我们希望能够激发和指导未来自然用户中心界面的研究。

    We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
    
[^274]: 多用户聊天助手（MUCA）：一种使用LLMs框架促进群体对话的方法

    Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations. (arXiv:2401.04883v1 [cs.CL])

    [http://arxiv.org/abs/2401.04883](http://arxiv.org/abs/2401.04883)

    这篇论文介绍了一种基于大规模语言模型的多用户聊天机器人框架（MUCA），该框架支持群组讨论，并提供了三个主要模块来确定回应内容、时机和适当的接收者。同时，作者还提出了一个基于语言模型的多用户模拟器（MUS），用于模拟真实用户行为，以便更高效地测试和优化聊天机器人。

    

    最近大规模语言模型（LLMs）的进展为聊天机器人的发展提供了新的途径，而大部分现有研究主要集中在单用户的聊天机器人上，重点放在用户输入后决定“回答什么”。在本文中，我们发现多用户聊天机器人有更复杂的3W设计维度——如何回答，“何时”回应，“回答谁”。此外，我们提出了一个名为Multi-User Chat Assistant (MUCA)的基于LLM的聊天机器人框架，专门用于群组讨论。MUCA由三个主要模块组成：子主题生成器，对话分析器和话语策略仲裁器。这些模块共同确定合适的回应内容、时机和适当的接收者。为了使MUCA的优化过程更容易，我们进一步提出了一个基于LLM的多用户模拟器（MUS），可以模拟真实用户行为。这使得聊天机器人和模拟用户之间的对话进行更快速的模拟，从而使得早期测试和优化过程更高效。

    Recent advancements in large language models (LLMs) have provided a new avenue for chatbot development, while most existing research has primarily centered on single-user chatbots that focus on deciding "What" to answer after user inputs. In this paper, we identified that multi-user chatbots have more complex 3W design dimensions -- "What" to say, "When" to respond, and "Who" to answer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an LLM-based framework for chatbots specifically designed for group discussions. MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and Utterance Strategies Arbitrator. These modules jointly determine suitable response contents, timings, and the appropriate recipients. To make the optimizing process for MUCA easier, we further propose an LLM-based Multi-User Simulator (MUS) that can mimic real user behavior. This enables faster simulation of a conversation between the chatbot and simulated users, making the earl
    
[^275]: 模型编辑可能会损害大型语言模型的通用能力

    Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])

    [http://arxiv.org/abs/2401.04700](http://arxiv.org/abs/2401.04700)

    这项论文指出，模型编辑可能会改善模型的事实性，但会以降低模型的通用能力为代价。在这项研究中，作者通过评估四种编辑方法在两个大型语言模型上的表现，发现这些编辑方法往往忽视了对模型通用能力可能产生的负面影响。

    

    大型语言模型（LLM）的最新进展为我们获取其参数中存储的知识提供了新的范式。一个关键的挑战是LLM输出中存在错觉，这是由于错误或过时知识引起的。由于使用更新后的信息重新训练LLM需要大量资源，因此人们对模型编辑产生了越来越多的兴趣。然而，许多模型编辑方法在各种场景中很有效，但往往过于强调编辑性能的功效、泛化性和局部性，常常忽视了对LLM的通用能力可能产生的副作用。本文提出了改善模型的事实性可能会以相当大的通用能力下降为代价的担忧，这不符合LLM可持续发展的要求。我们通过评估四种常用的编辑方法在两个LLM上进行了系统分析副作用，并涵盖了八个代表性任务类别。

    Recent advances in large language models (LLMs) have opened up new paradigms for accessing the knowledge stored in their parameters. One critical challenge that has emerged is the presence of hallucinations in LLM outputs due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, many model editing methods, while effective in various scenarios, tend to overemphasize aspects such as efficacy, generalization, and locality in editing performance, often overlooking potential side effects on the general abilities of LLMs. In this paper, we raise concerns that the improvement of model factuality may come at the cost of a significant degradation of these general abilities, which is not conducive to the sustainable development of LLMs. Systematically, we analyze side effects by evaluating four popular editing methods on two LLMs across eight representative task categories. Extensive empi
    
[^276]: 自扩展LLM:无需调整的LLM上下文窗口。

    LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])

    [http://arxiv.org/abs/2401.01325](http://arxiv.org/abs/2401.01325)

    本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。

    

    本研究揭示了LLM在处理长上下文时的固有能力，而无需进行精调。在训练过程中，训练序列的有限长度可能限制了大型语言模型（LLMs）在推理过程中对长输入序列的应用。在本研究中，我们认为现有的LLMs本身具有处理长上下文的固有能力。基于这一观点，我们建议通过自身扩展LLMs的上下文窗口，以充分利用其固有能力。我们提出了Self-Extend方法来激发LLMs的长上下文处理潜力。基本思想是构建双层注意信息：群组级和邻居级。这两个级别通过原始模型的自注意力计算，这意味着所提方法不需要任何训练。只需修改四行代码，所提方法就可以轻松扩展现有LLMs的上下文窗口，而无需进行任何精调。我们进行了全面的实验证明，结果表明所提方法可以+摘要减掉文章最后一句話

    This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
    
[^277]: LLM训练中的结构化填充改进了长上下文利用

    Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17296](http://arxiv.org/abs/2312.17296)

    本论文研究了长上下文大型语言模型（LLM）中上下文利用不足的问题，并通过将相关文档纳入训练示例中来改进模型的困惑度。通过引入Structured Packing for Long Context (SPLiCe)方法，使用检索方法将最互相关文档汇集到单个训练上下文中，进一步提高了模型的性能。

    

    长上下文大型语言模型（LCLM）的最新进展引起了广泛关注，特别是在查询科学研究论文等应用中。然而，它们的潜力往往受到上下文利用不足的限制。我们确定典型训练数据中缺乏长程语义依赖是主要障碍。为了解决这个问题，我们深入研究了频繁将相关文档纳入训练输入的好处。利用代码数据的固有目录结构作为训练示例的来源，我们证明了即使对于与编码无关的任务，囊括相关文档能够改进模型的困惑度。基于这些发现，并且更具广泛的关注，我们引入了一种名为Structured Packing for Long Context (SPLiCe)的创新方法。 SPLiCe是一种使用检索方法将最互相关文档汇集到单个训练上下文中的方法。我们的结果表明，\method{}提高了模型的性能，并可用于t

    Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \method{} enhances model performance and can be used to t
    
[^278]: LLMs是否展现出类似于人类的反应偏倚？一项关于调查设计的案例研究。

    Do LLMs exhibit human-like response biases? A case study in survey design. (arXiv:2311.04076v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04076](http://arxiv.org/abs/2311.04076)

    本研究以调查设计为案例研究，探讨了LLMs是否展现类似于人类的反应偏差的问题。

    

    随着大型语言模型（LLMs）的能力增强，人们对将LLMs用作代理人类进行主观标签任务（如调查和舆论调查）的可能性越来越兴奋。然而，LLMs对提示措辞的敏感性是其广泛引述的限制之一，但有趣的是，人类在回应中也显示出对指令变化的敏感性，表现为反应偏倚。因此，我们认为，如果要使用LLMs近似人类意见，有必要调查LLMs是否也反映了人类的反应偏差。在本研究中，我们以调查设计为案例研究，调查问卷中由于“提示”措辞的变化导致的人类反应偏差已经得到广泛研究。借鉴社会心理学的先前工作，我们设计了一个数据集并提出了一个评估框架，以评估LLMs是否在调查问卷中展现类似于人类的反应偏差。

    As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of "prompts" have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models s
    
[^279]: 语言模型是否容易受到语言幻觉的欺骗？在语法方面容易，在语义方面困难。（arXiv:2311.01386v1 [cs.CL]）

    Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics. (arXiv:2311.01386v1 [cs.CL])

    [http://arxiv.org/abs/2311.01386](http://arxiv.org/abs/2311.01386)

    通过研究语言模型在与"语言幻觉"相关的判断中的行为，我们发现语言模型更容易受到结构依赖性的幻觉的影响，而在语义方面则较困难。

    

    虽然语言模型（LMs）在判断语法性方面与人类有很大重叠，但是当人类在语言处理中系统性地出现错误时，我们是否期望LMs能像语言的认知模型那样模仿人类行为？通过研究与“语言幻觉”相关的LMs的更微妙判断，我们回答了这个问题——这些句子在意义上模糊、不合情理或语法错误，但却受到人类意外高接受度的判断。我们研究了三种幻觉：比较幻觉（例如“去过俄罗斯的人比我多”），深度冲击幻觉（例如“没有轻微的头部伤害可以被忽视”）和否定极性项（NPI）幻觉（例如“没有一个乡村人相信是可信赖的猎人会向熊射击”）。我们发现，LMs表示的概率更有可能与人类对于被NPI幻觉“欺骗”的判断一致，这一幻觉检验了一种结构依赖性。

    Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion which examines a structural dep
    
[^280]: 基于能源的法律领域文本分类常见方法的比较分析

    An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])

    [http://arxiv.org/abs/2311.01256](http://arxiv.org/abs/2311.01256)

    本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。

    

    大部分机器学习研究评估最佳解决方案的性能。然而，在追求最佳性能的竞争中，经常忽视许多重要因素，而事实上，这些因素应该被仔细考虑。实际上，有时不同方法之间的性能差距可以忽略不计，而生产成本、能源消耗和碳足迹等因素必须考虑在内。大型语言模型（LLMs）被广泛应用于学术界和工业界的NLP问题。在这项工作中，我们在LexGLUE基准上对LLM和传统方法（例如SVM）进行了详细的定量比较，同时考虑性能（标准指标）和其他指标，如时间、耗能和成本，总之就是碳足迹。在我们的分析中，我们分别考虑了原型设计阶段（通过训练-验证-测试迭代进行模型选择）和生产阶段。

    Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
    
[^281]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^282]: DoGE: 使用泛化估计进行领域重新加权

    DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])

    [http://arxiv.org/abs/2310.15393](http://arxiv.org/abs/2310.15393)

    DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。

    

    预训练数据语料库的覆盖范围和组成对大型语言模型的泛化能力有着重要影响。传统上，预训练语料库由各种来源领域（如CommonCrawl、Wikipedia、Github等）按照特定的采样概率（领域权重）组成。然而，当前的方法缺乏一种基于最终泛化目标优化领域权重的原则方法。我们提出了一种称为DOmain reweighting with Generalization Estimation（DoGE）的方法，其中我们重新调整了每个领域的采样概率，根据它对最终泛化目标的贡献进行了基于梯度的泛化估计函数评估。首先，我们使用最小最大优化训练了一个小规模的代理模型来获取重新加权的领域权重。在每一步中，通过镜像下降法更新领域权重以最大化整体的泛化增益。最后，我们使用获得的领域权重来训练一个规模更大的完整语言模型。

    The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
    
[^283]: ICU：通过将任务划分为图像字幕和语言理解来克服视觉与语言建模中的语言障碍

    ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])

    [http://arxiv.org/abs/2310.12531](http://arxiv.org/abs/2310.12531)

    ICU提出了一种解决视觉与语言建模中语言障碍的方法，通过将任务划分为图像字幕和语言理解两个阶段，将多语言处理负担转移到多语言语言模型上。实验结果显示，ICU在多个语言上取得了最先进的结果。

    

    多语言视觉与语言(V&L)研究旨在在一个模型中实现多语言和多模态的能力。然而，图像的多语言字幕稀缺一直以来一直阻碍了该领域的发展。为了克服这个障碍，我们提出了ICU（Image Caption Understanding），将V&L任务分为两个阶段：一个V&L模型以英文进行图像字幕生成，然后一个多语言语言模型（mLM）以字幕作为替代文本进行跨语言语言理解。这种方式减轻了V&L模型的多语言处理负担，将其转移到了mLM上。由于多语言文本数据相对丰富和质量较高，ICU可以帮助克服V&L模型中的语言障碍。在IGLUE基准测试的两个任务中，涉及9种语言的实验中，我们展示了ICU可以在五种语言上实现新的最先进结果，并在其余语言上取得了可比较的结果。

    Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.
    
[^284]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^285]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^286]: 有效且参数高效的重复使用微调模型

    Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])

    [http://arxiv.org/abs/2310.01886](http://arxiv.org/abs/2310.01886)

    本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。

    

    许多在线提供的预训练大规模模型在传递到下游任务中变得非常有效。与此同时，各种在这些预训练模型上微调的任务特定模型也可供公众使用。在实践中，由于收集任务特定数据耗时且微调大规模预训练模型计算复杂，可以重复使用任务特定微调模型来处理下游任务。然而，为每个任务使用一个模型会给存储和服务带来巨大负担。最近，有许多无需训练且参数高效的方法被提出，将多个微调的任务特定模型重复使用到一个多任务模型中。然而，与为每个任务使用微调模型相比，这些方法表现出较大的准确性差距。本文中，我们提出了参数高效方法来重复使用微调模型。针对重复使用全面微调模型，我们提出了PERU-FFT，通过将稀疏任务向量注入到一个mer模型中来实现。

    Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
    
[^287]: MentaLLaMA：利用大型语言模型在社交媒体上进行可解释的心理健康分析

    MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models. (arXiv:2309.13567v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13567](http://arxiv.org/abs/2309.13567)

    本研究利用大型语言模型在社交媒体上进行可解释的心理健康分析。针对解释性不足的问题，研究发现ChatGPT能够生成接近人类解释的分类结果。然而，LLMs在零 shot/few-shot 方式下的分类性能仍不理想。为了解决缺乏训练数据和开源LLMs的问题，研究建立了第一个多任务和多源的解释性心理健康指导数据集。

    

    随着网络技术的发展，社交媒体文本正在成为自动心理健康分析的丰富数据源。由于传统的判别方法存在解释性不足的问题，最近开始探索利用大型语言模型进行社交媒体上可解释的心理健康分析，旨在提供详细的解释和预测。结果表明，ChatGPT能够生成接近人类解释的正确分类。然而，LLMs在零 shot/few-shot 方式下仍然实现了不令人满意的分类性能。领域特定的微调是一个有效的解决方案，但面临两个挑战：1）缺乏高质量的训练数据。2）没有发布用于可解释的心理健康分析的开源 LLMs 以降低微调成本。为了缓解这些问题，我们在社交媒体上构建了第一个多任务和多源可解释的心理健康指导 (IMHI) 数据集，包含105K个数据样本。

    With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw socia
    
[^288]: 利用语义信息改进说话者分离: 利用联合成对约束传播

    Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation. (arXiv:2309.10456v1 [cs.SD])

    [http://arxiv.org/abs/2309.10456](http://arxiv.org/abs/2309.10456)

    该论文提出了一种利用语义信息改进说话者分离的方法，通过引入口语理解模块提取语义信息并构建成对约束，并将其集成到说话者分离流程中，从而提高系统性能。

    

    说话者分离已经引起了语音处理研究界的广泛关注。主流的说话者分离主要依赖于从声音信号中提取的说话者的声音特征，往往忽视了语义信息的潜力。考虑到语音信号能够有效传达语音的内容，我们有兴趣充分利用这些语义线索，利用语言模型。在这项工作中，我们提出了一种新的方法，以有效地利用基于聚类的说话者分离系统中的语义信息。首先，我们引入口语理解模块来提取与说话者相关的语义信息，并利用这些信息构建成对约束。其次，我们提出了一个新的框架来将这些约束集成到说话者分离流程中，提高整个系统的性能。在公开数据集上进行了大量实验证明了一致的优势。

    Speaker diarization has gained considerable attention within speech processing research community. Mainstream speaker diarization rely primarily on speakers' voice characteristics extracted from acoustic signals and often overlook the potential of semantic information. Considering the fact that speech signals can efficiently convey the content of a speech, it is of our interest to fully exploit these semantic cues utilizing language models. In this work we propose a novel approach to effectively leverage semantic information in clustering-based speaker diarization systems. Firstly, we introduce spoken language understanding modules to extract speaker-related semantic information and utilize these information to construct pairwise constraints. Secondly, we present a novel framework to integrate these constraints into the speaker diarization pipeline, enhancing the performance of the entire system. Extensive experiments conducted on the public dataset demonstrate the consistent superiori
    
[^289]: Talk2Care: 利用大型语言模型促进异步患者-医生通信

    Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09357](http://arxiv.org/abs/2309.09357)

    本研究利用大型语言模型（LLMs）来促进患者和医生之间的异步通信，通过访谈研究了解了他们对LLMs的需求，并构建了一个名为Talk2Care的LLM驱动的通信系统。

    

    尽管有大量的远程医疗应用程序来帮助家庭中的老年人和医疗提供者，但基本的消息和电话仍然是最常见的通信方法，这些方法存在有限的可用性、信息丢失和流程效率低下的问题。促进患者-医生通信的一个有希望的解决方案是利用大型语言模型(LLMs)及其强大的自然对话和摘要能力。然而，对于LLMs在通信过程中的作用还存在有限的理解。我们首先进行了两项访谈研究，分别与老年人(N=10)和医疗提供者(N=9)进行了交流，以了解他们在患者-医生异步通信中对LLMs的需求和机会。基于这些见解，我们构建了一个LLM驱动的通信系统Talk2Care，并为两个群体设计了交互组件: (1) 对于老年人，我们利用语音助手的便利性和易于获取性，构建了一个LLM驱动的语音助手

    Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
    
[^290]: 自训练在情感分析中的实例选择策略的实证研究

    An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis. (arXiv:2309.08777v1 [cs.CL])

    [http://arxiv.org/abs/2309.08777](http://arxiv.org/abs/2309.08777)

    本文对自训练在情感分析中的实例选择策略进行了实证研究，研究了策略和超参数对自训练性能的影响。

    

    情感分析是自然语言处理中的一个关键任务，涉及从文本中识别和提取主观情感。最近，通过利用少量标记数据和大量未标记数据，自训练已经成为一种经济高效的技术，用于开发情感分析模型。然而，自训练过程的性能严重依赖于实例选择策略的选择，而这方面的研究还不够充分。本文对自训练的各种实例选择策略在两个公开情感数据集上进行了实证研究，并研究了策略和超参数在各种少样本设置下对自训练性能的影响。

    Sentiment analysis is a crucial task in natural language processing that involves identifying and extracting subjective sentiment from text. Self-training has recently emerged as an economical and efficient technique for developing sentiment analysis models by leveraging a small amount of labeled data and a larger amount of unlabeled data. However, the performance of a self-training procedure heavily relies on the choice of the instance selection strategy, which has not been studied thoroughly. This paper presents an empirical study on various instance selection strategies for self-training on two public sentiment datasets, and investigates the influence of the strategy and hyper-parameters on the performance of self-training in various few-shot settings.
    
[^291]: 通过利用图像-文本辅助任务提高社交媒体帖子的多模态分类

    Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])

    [http://arxiv.org/abs/2309.07794](http://arxiv.org/abs/2309.07794)

    本研究通过引入图像-文本辅助任务，有效地提高了社交媒体帖子的多模态分类，通过两个辅助损失函数对图像-文本表示进行调整，捕捉底层依赖关系和语义对应关系，实现了一致的改进。

    

    有效地利用社交媒体帖子中的多模态信息对情感分析、讽刺检测和仇恨言论分类等多个下游任务至关重要。然而，由于匹配的图像-文本对中存在隐藏或互补信息的独特跨模态语义，将文本和图像信息结合起来是具有挑战性的。在本研究中，我们旨在通过在微调任何预训练的多模态模型时联合使用两个辅助损失函数来直接建模这一问题。图像-文本对比（ITC）将一篇帖子的图像-文本表示更加靠近，并将其与其他帖子分离开来，捕捉底层依赖关系。图像-文本匹配（ITM）通过惩罚不相关的对来促进理解图像和文本之间的语义对应关系。我们将这些目标与五个多模态模型相结合，证明了在四个热门社交媒体数据集上的一致改进。

    Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
    
[^292]: SIB-200: 包括200多种语言和方言的简单、全面和大型主题分类评估数据集

    SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects. (arXiv:2309.07445v1 [cs.CL])

    [http://arxiv.org/abs/2309.07445](http://arxiv.org/abs/2309.07445)

    本研究提出了SIB-200数据集，在200多种语言和方言中提供了一个大规模、全面的主题分类评估数据集。该数据集填补了自然语言理解领域中对评估数据集的缺乏，通过全监督、跨语言迁移和大型语言模型提示的评估，发现性能仍存在差距。

    

    尽管在多语言自然语言处理方面取得了进展，但评估通常仅限于一小部分带有可用数据集的语言，排除了许多资源匮乏的语言。本文创建了SIB-200，这是一个用于主题分类的大规模开放源代码基准数据集，涵盖了200多种语言和方言，以弥补自然语言理解（NLU）缺乏评估数据集的问题。对于SIB-200中涵盖的许多语言来说，这是首个公开可用的NLU评估数据集。该数据集基于Flores-200机器翻译语料库，并对该语料库涵盖的其他203种语言进行了句子级注释。尽管该任务简单，但我们在全监督设置、跨语言迁移设置和大型语言模型提示设置下的评估结果表明，性能仍存在较大差距。

    Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performa
    
[^293]: 无监督的对比一致排序与语言模型

    Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])

    [http://arxiv.org/abs/2309.06991](http://arxiv.org/abs/2309.06991)

    无监督的对比一致排序与语言模型，通过训练一个受逻辑约束引导的探测模型，实现在多个语句中始终映射到对比的真-假极点的排序任务。

    

    语言模型包含基于排序的知识，并且是处理上下文排名任务的强大解决者。最近的研究关注于配对、点对和列表提示技术，以揭示语言模型的排序知识。然而，我们发现，即使在仔细校准和限制解码的情况下，基于提示的技术在产生的排序中也不总是自洽的。这促使我们探索一种受无监督探测方法Contrast-Consistent Search（CCS）启发的替代方法。这个想法是训练一个受逻辑约束引导的探测模型：模型对一个语句及其否定的表示必须在多个语句中始终映射到对比的真-假极点。我们假设类似的约束适用于所有项通过一致性对相关排序任务。

    Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
    
[^294]: 基于框架的大型语言模型自由回答的定性分析：算法保真度

    Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])

    [http://arxiv.org/abs/2309.06364](http://arxiv.org/abs/2309.06364)

    本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。

    

    如今，使用大规模生成式语言模型（LLMs），可以模拟自由回答面试问题，就像传统上使用定性研究方法分析的那样。定性方法涵盖了一系列技术，涉及对开放式访谈或自由进行的自然语言对话的手动分析。本文考虑通过定性方法对LLMs生成的"硅参与者"进行研究，从而产生可能可以推广到真实人群的洞察力。我们分析的关键概念是算法保真度，这是由Argyle等人（2023年）引入的一个术语，用于描述LLM生成的输出与人类亚群体的信念和态度的程度相吻合。根据定义，高算法保真度表明从LLMs中提取的潜在信念可能可以推广到真实人类，而低算法保真度则使得这样的研究无效。本文使用LLM生成面试问答，...

    Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
    
[^295]: 阴谋者的解剖学：揭示性格特点的全面Twitter数据集

    The Anatomy of Conspirators: Unveiling Traits using a Comprehensive Twitter Dataset. (arXiv:2308.15154v1 [cs.SI])

    [http://arxiv.org/abs/2308.15154](http://arxiv.org/abs/2308.15154)

    本研究通过构建一种全面的Twitter数据集，揭示了参与阴谋相关活动的用户的特点和行为特征，为阴谋论的检测提供了新的方法和依据。

    

    在充斥着在线环境中的大量错误信息中，关于阴谋论的讨论正在蓬勃发展。在这个领域的研究主要集中在社交媒体上检测阴谋论，往往依赖于有限的数据集。在本研究中，我们提出了一种新的方法论，用于构建一个包含2022年全年涉及阴谋相关活动的Twitter数据集。我们的方法着重于独立于特定阴谋论和信息操作的数据收集。此外，我们的数据集包括一个对照组，其中随机选择用户可以与涉及阴谋活动的个体进行公正比较。这次全面的收集工作总共得到了15K个账户和从他们的时间线中提取的37M条推文。我们对两个群体在主题、个人资料和行为特征这三个维度上进行了比较分析。结果表明，阴谋和

    The discourse around conspiracy theories is currently thriving amidst the rampant misinformation prevalent in online environments. Research in this field has been focused on detecting conspiracy theories on social media, often relying on limited datasets. In this study, we present a novel methodology for constructing a Twitter dataset that encompasses accounts engaged in conspiracy-related activities throughout the year 2022. Our approach centers on data collection that is independent of specific conspiracy theories and information operations. Additionally, our dataset includes a control group comprising randomly selected users who can be fairly compared to the individuals involved in conspiracy activities. This comprehensive collection effort yielded a total of 15K accounts and 37M tweets extracted from their timelines. We conduct a comparative analysis of the two groups across three dimensions: topics, profiles, and behavioral characteristics. The results indicate that conspiracy and
    
[^296]: ESG主导的DLT研究的演化：对文献进行NLP分析

    Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])

    [http://arxiv.org/abs/2308.12420](http://arxiv.org/abs/2308.12420)

    本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。

    

    分布式账本技术(DLT)迅速发展，需要全面了解其各个组成部分。然而，针对DLT的环境、可持续性和治理(ESG)组成部分的系统文献综述还不足。为填补这一空白，我们选择了107篇种子文献，构建了一个包含63,083个参考文献的引用网络，并将其精炼为24,539篇文献的语料库进行分析。然后，我们根据一个已建立的技术分类法从46篇论文中标记了命名实体，并通过找出DLT的ESG要素来完善这个分类法。利用基于transformer的语言模型，我们对一个预先训练的语言模型进行了细化调整，用于命名实体识别任务，使用我们标记的数据集。我们利用我们调整后的语言模型对语料库进行了精简，得到了505篇关键论文，通过命名实体和时间图分析，促进了对DLT在ESG背景下的演化的文献综述。

    Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
    
[^297]: 大型多语言模型在跨语种零样本多模式学习中的作用。

    Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])

    [http://arxiv.org/abs/2308.12038](http://arxiv.org/abs/2308.12038)

    本论文提出了一种在低资源语言中训练大型多模式模型的有效方法，通过利用多语言模型实现了跨语种零样本多模式学习，在图像到文本和文本到图像的生成任务上具有竞争力。

    

    最近，在图像到文本和文本到图像的生成方面，多模式学习出现了显著增长。然而，成功通常仅限于英语，其他语言则相对落后。在其他语言中构建具有竞争力的对应物是非常具有挑战性的，因为非英语多模式数据具有低资源特性（即缺乏大规模、高质量的图像-文本数据）。在这项工作中，我们提出了MPM，一种在低资源语言中训练大型多模式模型的有效训练范例。MPM表明，多语言模型可以在跨语种零样本多模式学习中起到关键作用。具体而言，基于强大的多语言大语言模型，仅在英语图像-文本数据上预训练的多模式模型可以以零样本的方式很好地泛化到其他语言，用于图像到文本和文本到图像的生成，甚至超过在本地语言的图像-文本数据上训练的模型。以中文作为MPM实践的一个练习。

    Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP
    
[^298]: 提高ChatGPT生成的假科学检测的方法：引入xFakeBibs监督学习网络算法

    Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])

    [http://arxiv.org/abs/2308.11767](http://arxiv.org/abs/2308.11767)

    本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。

    

    ChatGPT正在成为现实。本文展示了如何区分ChatGPT生成的出版物与科学家生成的出版物。通过使用一种新设计的监督机器学习算法，我们演示了如何检测机器生成的出版物和科学家生成的出版物。该算法使用100个真实出版物摘要进行训练，然后采用10倍交叉验证方法建立了一个接受范围的下限和上限。与ChatGPT内容进行比较，明显可见ChatGPT仅贡献了23\%的二元组内容，这比其他10个交叉验证中的任何一个都少50\%。这个分析凸显了ChatGPT在技术术语上与真实科学的明显差异。在对每篇文章进行分类时，xFakeBibs算法准确地将98篇出版物识别为假的，有2篇文献错误地分类为真实出版物。尽管这项工作引入了一种算法应用

    ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
    
[^299]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^300]: 将顺序带入基于Transformer的语言模型中，用于人工智能和法律的应用

    Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])

    [http://arxiv.org/abs/2308.05502](http://arxiv.org/abs/2308.05502)

    本文提供了第一个对基于Transformer的语言模型在法律领域的人工智能问题和任务中的方法的系统概述。文章旨在突出这一领域的研究进展，以进一步了解Transformer在支持法律流程中的AI成功贡献以及当前的局限性。

    

    基于Transformer的语言模型（TLM）被广泛认可是一种先进的技术，能够成功开发出基于深度学习的解决方案，用于需要自然语言处理和理解的问题和应用。与其他文本领域一样，TLM确实推动了法律领域许多感兴趣任务对人工智能方法的最新进展。尽管第一个Transformer模型提出了大约6年时间，但这项技术以前所未有的速度迅猛发展，BERT和相关模型成为主要参考，也在法律领域占有重要地位。本文首次系统概述了TLM在法律领域的人工智能驱动问题和任务中的方法。一个主要目标是突出研究在这一领域的进展，以便一方面了解Transformer在支持法律流程中取得的AI成功贡献是什么，另一方面了解当前的局限性是什么。

    Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
    
[^301]: 实时作曲辅助的混合检索增强生成

    Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])

    [http://arxiv.org/abs/2308.04215](http://arxiv.org/abs/2308.04215)

    提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。

    

    检索增强模型在提升传统语言模型的上下文理解、整合私人数据和减少幻觉方面显示出了潜力。然而，应用于需要实时响应的任务（如作曲辅助）时，检索增强的大型语言模型所需的处理时间存在挑战。为了克服这一限制，我们提出了Hybrid Retrieval-Augmented Generation (HybridRAG)框架，利用了将客户端模型和云模型结合起来的混合设置。HybridRAG通过异步生成的检索增强内存，将大型语言模型（LLM）在云端生成的检索增强内存整合到客户端模型中。通过整合这种检索增强内存，客户端模型能够生成高效的响应，从LLM的能力中受益。此外，通过异步内存集成，客户端模型能够实时响应用户请求，无需等待云端处理。

    Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
    
[^302]: DialogStudio：面向会话 AI 的最丰富和最多样化的统一数据集集合

    DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])

    [http://arxiv.org/abs/2307.10172](http://arxiv.org/abs/2307.10172)

    DialogStudio是迄今为止最大且最多样化的对话数据集合，包含从开放领域对话到任务导向对话、自然语言理解、会话推荐、对话摘要和知识驱动对话的数据。它为对话研究和模型训练提供了丰富而多样化的资源。

    

    尽管会话 AI 取得了进展，但语言模型在处理多样化的对话任务时面临挑战，现有的对话数据集往往缺乏多样性和全面性。为解决这些问题，我们介绍了 DialogStudio：最大、最多样化的对话数据集集合，以一致的格式统一，同时保留其原始信息。我们的集合包括来自开放领域对话、任务导向对话、自然语言理解、会话推荐、对话摘要和知识驱动对话的数据，为对话研究和模型训练提供了非常丰富和多样化的资源。为了进一步增强 DialogStudio 的实用性，我们为每个数据集确定了许可证，并为选定对话设计了领域感知提示，以便促进指导感知微调。此外，我们使用数据集集合开发了会话 AI 模型，并在零摘要生成和分布式文字基准对话任务上进行了实验。

    Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
    
[^303]: MorphPiece: 远离统计语言表示的一步

    MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])

    [http://arxiv.org/abs/2307.07262](http://arxiv.org/abs/2307.07262)

    本文提出了一种基于语言学动机的分词方案MorphPiece，并使用该方案训练了一个称为MorphGPT的语言模型。MorphGPT在语言建模以及各种NLP任务上都表现出了比传统模型更优异的性能。

    

    分词是现代自然语言处理流程中至关重要的一部分。然而，用于大型语言模型的当代分词器基于对文本语料库的统计分析，对语言特征的考虑较少。我们提出了一种基于语言学动机的分词方案MorphPiece，部分基于底层文本的形态分割。使用该分词器（称为MorphGPT）训练的类GPT的因果语言模型显示出比在标准BPE分词器上训练时更优越的收敛性。具体来说，我们获得了与规模大6倍的模型相媲美的语言建模性能。此外，我们在监督和无监督的条件下对MorphGPT在各种NLP任务上进行了评估，并发现在各个方面与GPT-2模型相比有更出色的性能。

    Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. We propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows superior convergence compared to the same architecture trained on a standard BPE tokenizer. Specifically we get Language Modeling performance comparable to a 6 times larger model. Additionally, we evaluate MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and find superior performance across the board, compared to GPT-2 model.
    
[^304]: DRAGON: 一种基于对话的带有视觉语言关联的辅助导航机器人

    DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])

    [http://arxiv.org/abs/2307.06924](http://arxiv.org/abs/2307.06924)

    DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。

    

    视力受损者在理解和导航周围空间方面存在困难。目前的导航技术要么只关注导航，要么提供有限的关于环境的沟通。受到最近在视觉语言关联和语义导航方面的进展的启发，我们提出了DRAGON，一种由对话系统驱动的导航机器人，并具有将环境与自然语言关联的能力。通过理解用户的指令，DRAGON能够引导用户到地图上的目标地标，描述环境，并通过视觉观察回答问题。通过有效利用对话，机器人可以将用户的自由形式描述与环境中的地标关联起来，并通过口语提供语义信息给用户。我们在日常室内环境中进行了盲目参与者的用户研究。我们的结果表明，DRAGON能够与用户顺畅地沟通，

    Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
    
[^305]: 生成用于语言模型事实性评估的基准数据集

    Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])

    [http://arxiv.org/abs/2307.06908](http://arxiv.org/abs/2307.06908)

    该论文提出了一个名为FACTOR的方法，用于生成用于语言模型事实性评估的基准数据集。通过自动转换事实语料库，评估语言模型根据语料库生成真实事实的倾向与生成不正确陈述的能力。实验结果表明，该基准数据集的分数随模型大小增加而增加，在LM与检索方法结合时性能得到改善。困惑度和基准数据集分数之间存在相关性，但不总是一致。

    

    在将语言模型（LM）部署到特定领域之前，衡量其在该领域中生成事实错误信息的倾向很重要。现有的事实生成评估方法集中于从LM自身中采样的事实，因此无法控制评估事实的集合，并且可能低估了罕见和不太可能的事实。我们提出了FACTOR：通过语料库变换进行事实评估的方法，这是一种可扩展的方法来评估LM的事实性。FACTOR会自动将感兴趣的事实语料库转化为一个基准数据集，评估LM根据语料库生成真实事实的倾向与生成类似但不正确的陈述的能力。我们使用我们的框架创建了两个基准数据集：Wiki-FACTOR和News-FACTOR。我们的实验结果表明：（i）我们的基准数据集分数随模型大小增加而增加，并且当LM与检索方法结合使用时，性能得到改善；（ii）基准数据集分数与困惑度之间存在相关性，但这两个指标在模型排序上并不总是一致；以及（iii）当困惑度和基准数据集分数发生冲突时，基准数据集分数更能准确反映LM的事实性能。

    Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score 
    
[^306]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^307]: LLaVAR:增强的视觉指令调整用于文本丰富的图像理解

    LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])

    [http://arxiv.org/abs/2306.17107](http://arxiv.org/abs/2306.17107)

    LLaVAR是一个增强的视觉指令调整模型，通过使用文本丰富的图像数据，它能够显著提升在文本为基础的视觉问答数据集上的准确率。

    

    指令调整可以发挥大型语言模型（LLM）与人类互动的出色能力。此外，最近的指令遵循数据集包括图像作为视觉输入，收集图像指令的响应。然而，视觉指令调整的模型不能很好地理解图像中的文本细节。本研究增强了当前的视觉指令调整流程，使用文本丰富的图像（如电影海报、图书封面等）。具体地，我们首先使用公开可用的OCR工具从LAION数据集的422K个文本丰富的图像上提取结果。此外，我们使用识别到的文本和图像标题来启动仅文本的GPT-4生成16K个对话，每个对话包含文本丰富的图像的问答对。通过将我们收集的数据与先前的多模态指令遵循数据组合，我们的模型LLaVAR在文本为基础的VQA数据集上显著提高了LLaVA模型的能力（准确率提高了20%）同时 achieving an accur

    Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accur
    
[^308]: 系统级自然语言反馈

    System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])

    [http://arxiv.org/abs/2306.13588](http://arxiv.org/abs/2306.13588)

    本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了通过任务度量设计和语言模型提示设计，如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型，并展示了使用系统级别反馈和实例级别反馈的有效性。

    

    自然语言反馈包含了丰富的用户体验信息。现有研究聚焦于实例级别的方法，即将反馈用于细化特定例子，而忽略了其系统范围的应用。本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型。具体而言，这是通过以下两方面实现的：(i) 任务度量设计; (ii) 用于改进模型响应的语言模型提示设计。我们进行了两项案例研究，来改进搜索查询生成和对话响应生成，展示了使用系统级别反馈的有效性。我们表明系统级别反馈和实例级别反馈的组合带来了进一步的收益，并且由人类撰写的实例级别反馈导致比GPT-3.5撰写的反馈更加扎实。

    Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
    
[^309]: SqueezeLLM：密集稀疏量化

    SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])

    [http://arxiv.org/abs/2306.07629](http://arxiv.org/abs/2306.07629)

    本文提出了一种基于训练后的量化框架——SqueezeLLM，它不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。

    

    生成式大型语言模型(LLMs)已经证明在广泛领域的任务中取得了非凡的成果。但是由于其前所未有的资源需求，将这些模型用于推理一直是一个巨大的挑战。这导致现有的部署框架需要使用多GPU推理管道，这通常是复杂和昂贵的，或者使用更小且性能更低的模型。在这项工作中，我们证明了用于LLMs生成推断的主要瓶颈是内存带宽，而不是计算，尤其是单个批次推理。虽然通过使用减少精度来表示模型权重，量化已经成为一种有前途的解决方案，但是以前的努力通常导致性能下降。为了解决这个问题，我们引入SqueezeLLM，这是一种基于训练后的量化框架，不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。

    Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
    
[^310]: 基于最近邻的大语言模型的测试时间训练

    Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])

    [http://arxiv.org/abs/2305.18466](http://arxiv.org/abs/2305.18466)

    该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。

    

    最近的许多工作都旨在在测试时从数据库中检索相关信息以增强语言模型。我们通过直接在测试时使用其标准训练设置对检索到的数据对模型进行微调，避免了提示工程的需要。为此，我们建立了一个基于“Pile”数据集的文本嵌入的大规模分布式最近邻索引。给定一个语言模型的查询，我们的系统检索查询的邻居，并在对应于这些邻居的文本数据上微调模型。令人惊讶的是，检索和训练仅20个邻居，每个邻居仅进行一次梯度迭代，就显著提高了在“Pile”基准测试中超过二十个语言建模任务的性能。例如，测试时间训练显著缩小了小型GPT2模型和GPTNeo模型之间的性能差距，后者是专门对“Pile”进行收敛训练的，体积却是前者的十倍以上。然而，其方法的成功还取决于充分的索引质量和大小。

    Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
    
[^311]: CODET：机器翻译对比方言评估的基准测试

    CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation. (arXiv:2305.17267v1 [cs.CL])

    [http://arxiv.org/abs/2305.17267](http://arxiv.org/abs/2305.17267)

    CODET是一个对比方言的评估基准测试，用于评估机器翻译系统在处理方言变体时的表现，该基准测试包含九种不同语言的882个不同变体。

    

    神经机器翻译系统在处理源语言的语言变化方面表现出有限的鲁棒性。当面临即使是语言使用中的细微差异（例如不同的领域或由第二语言使用者引入的变体）时，其性能往往会下降。直观上，将这种观察推广到涵盖方言变体，而允许社区在这个维度上评估MT系统的工作是有限的。为了解决这个问题，我们编译和发布了对比方言基准测试 \dataset，其中包括来自九种不同语言的882个不同变体。我们还在数量上展示了大型MT模型在有效翻译方言变体方面面临的挑战。我们发布所有代码和数据。

    Neural machine translation (NMT) systems exhibit limited robustness in handling source-side linguistic variations. Their performance tends to degrade when faced with even slight deviations in language usage, such as different domains or variations introduced by second-language speakers. It is intuitive to extend this observation to encompass dialectal variations as well, but the work allowing the community to evaluate MT systems on this dimension is limited. To alleviate this issue, we compile and release \dataset, a contrastive dialectal benchmark encompassing 882 different variations from nine different languages. We also quantitatively demonstrate the challenges large MT models face in effectively translating dialectal variants. We are releasing all code and data.
    
[^312]: GPT究竟有多老？HumBEL框架通过人群数据评估语言模型

    How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14195](http://arxiv.org/abs/2305.14195)

    本论文提出了一种使用人类人口数据对语言模型进行评估的框架，并发现GPT-3.5在不同任务中表现不同，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。

    

    近年来，大型预训练语言模型在自然语言处理中得到广泛应用，然而目前的评估方法并未考虑模型的语言使用与特定人群之间的一致性，尤其是在对话式人工智能应用中这一点非常重要。为了填补这一空白，我们探讨了如何测量和比较语言模型的语言能力与人类子群之间的差异。我们借助语言病理学的临床技术，该学科已经建立了不同（人类）年龄阶段的语言能力发展规范，对技能进行评估，我们与领域专家（即持有临床许可证的语言病理学家）进行了评估，并提出了自动化的评估技术以实现规模化评估。我们发现，GPT-3.5的能力因任务而异，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。GPT-3.5（InstructGPT）在社交交互任务中也存在一定的困难。

    While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
    
[^313]: 理解和减少文本分类中的伪相关性

    Understanding and Mitigating Spurious Correlations in Text Classification. (arXiv:2305.13654v1 [cs.CL])

    [http://arxiv.org/abs/2305.13654](http://arxiv.org/abs/2305.13654)

    本文研究了深度学习模型容易利用训练集中存在但通常不成立的伪相关性的问题，并提出了一种邻域分析框架以解释语言模型如何利用伪相关性。通过一系列正则化方法NFL（不要忘记你的语言）避免了这种情况，并在实验中证明了其鲁棒性方面的显著改进。

    

    最近的研究表明，深度学习模型容易利用训练集中存在但通常不成立的伪相关性。例如情感分类器可能会错误地学习到令人愉悦的电影评论总是与“Spielberg”这个词相关联。依赖于伪相关性可能会导致泛化性能显著降低，因此应该避免。本文提出了一种邻域分析框架来解释语言模型如何利用伪相关性。在此基础上，我们提出了一系列正则化方法NFL（不要忘记你的语言），以避免这种情况。在两个文本分类任务上的实验表明，NFL相对于标准的微调算法在鲁棒性方面带来了显著的改进，而没有牺牲在数据内部的准确性。

    Recent work has shown that deep learning models are prone to exploit spurious correlations that are present in the training set, yet may not hold true in general. A sentiment classifier may erroneously learn that the token spielberg is always tied to positive movie reviews. Relying on spurious correlations may lead to significant degradation in generalizability and should be avoided. In this paper, we propose a neighborhood analysis framework to explain how exactly language models exploit spurious correlations. Driven by the analysis, we propose a family of regularization methods, NFL (do Not Forget your Language) to prevent the situation. Experiments on two text classification tasks show that NFL brings a significant improvement over standard fine-tuning in terms of robustness without sacrificing in-distribution accuracy.
    
[^314]: 小型语言模型更适合作为黑匣子机器生成文本检测器

    Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])

    [http://arxiv.org/abs/2305.09859](http://arxiv.org/abs/2305.09859)

    本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。

    

    随着流畅的生成语言模型的出现，它们可以生成与人类写作的非常相似的令人信服的话语，因此区分一段文本是由机器生成的还是人类写作的变得更加具有挑战性和重要性，因为这样的模型可以用于传播错误信息、虚假新闻、虚假评论并模仿某些作者和人物。为此，已经提出了许多检测机器生成文本的方法。其中大部分方法需要访问目标模型的 logits，或需要可以从目标模型中进行采样的能力。其中一种黑匣子检测方法依赖于观察到生成文本在生成器的似然函数下是局部最优的，而人类写作的文本则不是。我们发现，总体而言，较小且部分训练的模型更适合作为通用文本检测器：它们可以更精确地检测来自小型和大型模型的生成文本。有趣的是，我们发现检测器和生成模型是否具有相同的架构或相同的语料库对检测性能没有显著影响。

    With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
    
[^315]: 利用词汇相似性实现极低资源语言的零样本机器翻译

    Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])

    [http://arxiv.org/abs/2305.05214](http://arxiv.org/abs/2305.05214)

    本文解决了极低资源语言到英语的机器翻译任务，利用密切相关的高资源语言的词汇相似性，注入噪声作为正则化器，使模型更能抵御词汇差异，从而更好地促进跨语言转移。

    

    我们解决了从极低资源语言（LRL）到英语的机器翻译任务，采用从密切相关的高资源语言（HRL）进行跨语言转移。对于许多这些语言，没有平行语料库可用，即使是单语料库也很有限，并且在预训练的序列到序列模型中表示也缺失。这些因素限制了从多语言模型中共享嵌入空间的跨语言转移的好处。然而，许多极低资源语言与相关的高资源语言具有很高的词汇相似性。我们利用这个属性，将字符和字符跨度的噪声注入到HRL的训练数据中，然后再学习词汇表。这作为一个正则化器，使模型更能抵御HRL和LRL之间的词汇差异，并更好地促进跨语言转移。在来自多个语言家族的密切相关的HRL和LRL对上，我们观察到我们的方法显著优于基线机器翻译模型。

    We address the task of machine translation from an extremely low-resource language (LRL) to English using cross-lingual transfer from a closely related high-resource language (HRL). For many of these languages, no parallel corpora are available, even monolingual corpora are limited and representations in pre-trained sequence-to-sequence models are absent. These factors limit the benefits of cross-lingual transfer from shared embedding spaces in multilingual models. However, many extremely LRLs have a high level of lexical similarity with related HRLs. We utilize this property by injecting character and character-span noise into the training data of the HRL prior to learning the vocabulary. This serves as a regularizer which makes the model more robust to lexical divergences between the HRL and LRL and better facilitates cross-lingual transfer. On closely related HRL and LRL pairs from multiple language families, we observe that our method significantly outperforms the baseline MT as we
    
[^316]: 一个提示和几个示例就足够了吗？使用GPT-4进行数据增强在低资源分类任务中

    Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])

    [http://arxiv.org/abs/2304.13861](http://arxiv.org/abs/2304.13861)

    本文使用GPT-4和ChatGPT对低资源分类任务进行数据增强，通过简单的提示将小型标记数据集扩充为合成数据集，在保留原始标签分布或平衡分布的情况下，产生了良好的下游性能。在测试集上，GPT-4和ChatGPT表现出出色的零-shot性能，尤其在低资源设置中能够较好地识别罕见类别。

    

    在复杂的低资源领域中，获取和注释数据可能是昂贵和耗时的。我们使用GPT-4和ChatGPT通过简单的提示将小型标记数据集扩充为合成数据集，应用于三个不同的分类任务中，复杂程度各异。对于每个任务，我们随机选择了500个文本作为基本样本，生成了5,000个新的合成样本。我们探索了两种增强策略：一种保留原始标签分布，另一种平衡分布。使用逐步变大的训练样本量，我们分别在真实数据和合成数据上训练和评估了一个1.1亿参数的多语言语言模型。我们还在测试集上测试了GPT-4和ChatGPT的零-shot设置。我们发现GPT-4和ChatGPT在所有任务中都具有很强的零-shot性能。我们发现，使用合成样本增强的数据在下游任务中产生了良好的性能，尤其在识别罕见类别等低资源设置方面表现突出。

    Obtaining and annotating data can be expensive and time-consuming, especially in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small labeled datasets with synthetic data via simple prompts, in three different classification tasks with varying complexity. For each task, we randomly select a base sample of 500 texts to generate 5,000 new synthetic samples. We explore two augmentation strategies: one that preserves original label distribution and another that balances the distribution. Using a progressively larger training sample size, we train and evaluate a 110M parameter multilingual language model on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have strong zero-shot performance across all tasks. We find that data augmented with synthetic samples yields a good downstream performance, and particularly aids in low-resource settings, such as in identifying rare classes
    
[^317]: 大型语言模型对齐的基本限制

    Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])

    [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082)

    本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。

    

    开发与人交互的语言模型的重要方面是对齐其行为，使其对其人类用户有用且无害。这通常通过调整模型的方式来实现，以增强所需的行为并抑制不希望的行为。在本文中，我们提出了一种名为行为期望边界(BEB)的理论方法，它允许我们正式研究大型语言模型中的几个内在特征和对齐的限制。重要的是，我们证明对于任何具有被该模型表现出的有限概率的行为，都存在可以触发模型输出此行为的提示，其概率随提示的长度增加而增加。这意味着任何减弱不希望的行为但未将其完全消除的对齐过程都无法抵御针对性攻击。此外，我们的框架提示了领先的

    An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
    
[^318]: REFINER: 基于中间表示的推理反馈。

    REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])

    [http://arxiv.org/abs/2304.01904](http://arxiv.org/abs/2304.01904)

    REFINER 是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互，其在三个不同推理任务上取得了显着改进。

    

    最近语言模型在推理任务上表现出了remarkable的性能，通过显式生成中间推理步骤，例如链式思考提示等。然而，这些中间推理步骤可能并不是根据初始上下文得出的适当推导，从而导致不正确的最终预测。在这里，我们介绍了REFINER，这是一个框架，用于微调语言模型以显式生成中间推理步骤，并与提供自动反馈的批判模型交互。具体而言，批评家提供了结构化反馈，推理语言模型使用它来迭代改进其中间参数。REFINER的三个不同推理任务的实证评估显示出了与基线具有可比规模的语言模型相比的显着改进。此外，当使用GPT3.5作为推理器时，经过训练的批评家显着改善了推理而无需微调推理器。最后，我们的批评模型是在没有昂贵的人类参与数据的情况下进行的，但可以通过对新颖上下文提供低成本反馈进行继续改进。

    Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be su
    
[^319]: 串行采样块式Conformer网络在流式端到端ASR中的应用

    Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.11419](http://arxiv.org/abs/2211.11419)

    本论文提出了一种称为 SSC-Conformer 的块式模型，利用串行采样块自注意力机制提高块间交互效率，同时保持线性复杂度，将块卷积与因果卷积相结合以达到更好的 CER 表现，实验结果表明 SSC-Conformer 在 AISHELL-1 基准测试中取得了最新的流式 E2E ASR 性能水平。

    

    本文针对流式端到端语音识别 (E2E ASR) 提出了一种名为 SSC-Conformer 的串行采样块式 Conformer 模型。该模型使用串行采样块多头自注意力机制 (SSC-MHSA) 来提高跨块交互的效率，同时保持线性复杂度。此外，本文还提出利用块卷积来增加块级未来上下文，并将其与卷积层的因果卷积相结合以进一步降低 CER。在 AISHELL-1 基准测试中，实验结果表明 SSC-Conformer 在无语言模型重打分的情况下可以实现 CER 5.33%，达到了流式 E2E ASR 的最新性能水平，并且由于其线性复杂度，可以使用更大的批量进行训练并更高效地推理。

    This paper presents an in-depth study on a Sequentially Sampled Chunk Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer first demonstrates the significant performance gains from using the sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the Conformer encoder by allowing efficient cross-chunk interactions while keeping linear complexities. Furthermore, it explores taking advantage of chunked convolution to make use of the chunk-wise future context and integrates with casual convolution in the convolution layers to further reduce CER. We verify the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results show that a state-of-the-art performance for streaming E2E ASR is achieved with CER 5.33% without LM rescoring. And, owing to its linear complexity, the SSC-Conformer can train with large batch sizes and infer more efficiently.
    

