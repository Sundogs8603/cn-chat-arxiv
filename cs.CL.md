# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction](https://rss.arxiv.org/abs/2402.01629) | 这篇论文提出了一个通用框架，其中使用广义语法规则（GGRs）来实现组合一般化，将其视为转导任务中的对称性约束。该框架不仅形式化了语言转导的广义对称性概念，还与强化学习和其他研究领域有关联。 |
| [^2] | [TravelPlanner: A Benchmark for Real-World Planning with Language Agents](https://rss.arxiv.org/abs/2402.01622) | 本论文提出了一种用于自然语言代理的新的规划基准TravelPlanner，它关注于旅行规划这一常见的真实世界场景。经过全面评估，发现目前的语言代理仍无法处理如此复杂的规划任务，即使最先进的GPT-4也只能达到0.6%的成功率。 |
| [^3] | [MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models](https://rss.arxiv.org/abs/2402.01620) | MAGDi是一种结构化蒸馏方法，通过将多个大语言模型之间的推理交互表示为图形，来改善较小语言模型的推理能力。 |
| [^4] | [KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases](https://rss.arxiv.org/abs/2402.01619) | KB-Plugin是一个用于大型语言模型在低资源知识库上引导程序的即插即用框架，它通过自监督学习和利用丰富资源KB的注释数据来提取问题相关的模式信息，并实现在任何低资源KB上诱导程序。 |
| [^5] | [Style Vectors for Steering Generative Large Language Model](https://rss.arxiv.org/abs/2402.01618) | 本研究通过在文本生成过程中添加风格向量来引导大型语言模型输出特定风格的文本，这种方法简单且有效，是发展更具适应性和有效性的AI驱动交互系统的重要一步。 |
| [^6] | [Nomic Embed: Training a Reproducible Long Context Text Embedder](https://rss.arxiv.org/abs/2402.01613) | Nomic Embed是第一个完全可复现、开源、开放权重、开放数据的8192上下文长度英文文本嵌入器，在短上下文和长上下文任务上优于OpenAI Ada-002和OpenAI text-embedding-3-small。 |
| [^7] | [Towards Sustainable Workplace Mental Health: A Novel Approach to Early Intervention and Support](https://rss.arxiv.org/abs/2402.01592) | 本研究提出了一种 groundbreaking 的压力检测算法，利用自动聊天机器人技术实时分析聊天对话，以客观测量员工的心理健康水平，并根据语言生物标志提供个性化的实时治疗建议，为企业提供早期干预和支持。 |
| [^8] | [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://rss.arxiv.org/abs/2402.01591) | 本文提出了BAT，它结合了双耳声音场景分析模型的空间声音感知能力和大规模语言模型的自然语言推理能力，以复制人类的空间声音推理能力。通过使用合成的双耳音频数据集和基于空间声音的问答数据集进行训练，BAT在空间声音感知和推理方面取得了强大的性能。 |
| [^9] | [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://rss.arxiv.org/abs/2402.01586) | 本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。 |
| [^10] | [Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study](https://rss.arxiv.org/abs/2402.01582) | 该论文介绍了一种部分自动化的语言系统演化推断方法，通过利用神经网络预测历史语言形式与现代语言形式之间的中间声音变化步骤，并取得了显著的改进效果。 |
| [^11] | [How Paralingual are Paralinguistic Representations? A Case Study in Speech Emotion Recognition](https://rss.arxiv.org/abs/2402.01579) | 本研究通过比较研究五种预训练模型，评估了针对社交语言任务进行预训练的模型在多语言情境下对语音情感识别的效果。结果表明，TRILLsson模型能够有效地捕捉语音数据中的社交语言特征，提升了语音情感识别的性能。 |
| [^12] | [Deep Active Learning for Data Mining from Conflict Text Corpora](https://rss.arxiv.org/abs/2402.01577) | 本文提出了一种利用深度主动学习从冲突文本语料库中进行数据挖掘的方法。通过迭代的主动学习过程，结合大型的仅编码器语言模型，可以提取与冲突动态相关的子类事件，达到类似于人类的性能。 |
| [^13] | [An Empirical Analysis of Diversity in Argument Summarization](https://rss.arxiv.org/abs/2402.01535) | 本研究通过实证分析，发现目前的论据摘要方法在捕捉多样性方面存在困难，并提出多样性有三个方面：意见、注释者和来源。所研究的关键点分析任务中的通用LLMs和专门的KPA模型都有互补的优势，训练数据的多样化将有助于提高泛化能力。因此，应对论证摘要中的多样性需要采用多种策略来处理主观性。 |
| [^14] | [Decoding Speculative Decoding](https://rss.arxiv.org/abs/2402.01528) | 推测解码是一种用于加速大型语言模型推断的技术，但我们的实验表明，选择的草稿模型生成的令牌被目标模型接受的概率越高，吞吐量越低。我们通过大量实验，分析了各种因素对推测解码效果的影响，并提出了一个分析模型来提高效率。 |
| [^15] | [K-Level Reasoning with Large Language Models](https://rss.arxiv.org/abs/2402.01521) | 该论文探索了使用大型语言模型进行动态决策推理的能力，并提出了一种名为"K级推理"的新颖推理方法。通过博弈论的试验，发现现有的推理方法在动态环境中容易出错，而"K级推理"可以解决这个问题。 |
| [^16] | [Multilingual Gradient Word-Order Typology from Universal Dependencies](https://rss.arxiv.org/abs/2402.01513) | 本文提出了一个使用连续值数据而不是分类数据的新种子数据集，用于改进NLP任务性能，并介绍了创建数据集的方法论，可适用于更多特征和语言。 |
| [^17] | [Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation](https://rss.arxiv.org/abs/2402.01512) | 本文综述了多项选择题中干扰项生成的方法、数据集和评估。调查结果显示，现有数据集主要来自特定领域教育资源中，以文本为主，缺乏开放领域和多模态的数据集。 |
| [^18] | [A Hybrid Strategy for Chat Transcript Summarization](https://rss.arxiv.org/abs/2402.01510) | 这篇论文介绍了一种混合策略用于聊天记录摘要化，该策略首先结合了抽取和生成式摘要化技术，然后通过强化学习优化摘要的质量。这种方法在大规模部署的聊天记录摘要化中表现出了很好的效果。 |
| [^19] | [Code-Switched Language Identification is Harder Than You Think](https://rss.arxiv.org/abs/2402.01505) | 代码转换语言识别是一个常见且应用广泛的问题，但目前的方法都不够理想。本研究通过扩展语言范围、采用简单架构的模型以及重新定义任务，提出了一种更加可行的解决方案，并对未来的工作提出了建议。 |
| [^20] | [A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation](https://rss.arxiv.org/abs/2402.01495) | 本研究对比了四个不同大小的大型语言模型在从语义三元组生成自然语言文本方面的性能，并发现了生成预测中最常见的问题。 |
| [^21] | [AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback](https://rss.arxiv.org/abs/2402.01469) | AMOR是一个基于开源LLM的代理框架，通过与外部知识库进行推理和人类监督来适应特定领域的推理过程。通过两阶段的微调，AMOR能够在不同知识环境中泛化，并且可以根据过程反馈进行领域定制。 |
| [^22] | [The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs](https://rss.arxiv.org/abs/2402.01453) | 本研究探讨了PLMs中事实知识的一致性问题，结果显示PLMs在使用提示时具有较低的一致性，但通过添加证据段落可以显著提高。这表明PLMs需要进一步增强以处理从事实中检索信息的能力。 |
| [^23] | [The effect of diversity on group decision-making](https://rss.arxiv.org/abs/2402.01427) | 研究探讨认知多样性及其对群体决策成功的影响。通过分析500个团队讨论的对话记录，我们发现较大的认知多样性与更成功的群体决策相关。 |
| [^24] | [Different Tastes of Entities: Investigating Human Label Variation in Named Entity Annotations](https://rss.arxiv.org/abs/2402.01423) | 本论文研究了命名实体标注中的人类标签变化的原因，发现文本歧义和人为指南更改是高质量修订中不同标注的主要因素，并验证了多样化标注用于理解命名实体歧义的可行性和必要性。 |
| [^25] | [Sequence Shortening for Context-Aware Machine Translation](https://rss.arxiv.org/abs/2402.01416) | 本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。 |
| [^26] | [On Measuring Context Utilization in Document-Level MT Systems](https://rss.arxiv.org/abs/2402.01404) | 提出了一种补充基于准确度评估的上下文利用度度量方法，表明扰动分析是一种有效的总体上下文利用度的度量方法，同时提出了测量支持性上下文对处理上下文相关话语现象贡献程度的方法。 |
| [^27] | [StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback](https://rss.arxiv.org/abs/2402.01391) | StepCoder是一个基于强化学习的代码生成框架，通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，并使用细粒度优化来提高模型的效果。 |
| [^28] | [LLM-based NLG Evaluation: Current Status and Challenges](https://rss.arxiv.org/abs/2402.01383) | 这项调研介绍了基于大型语言模型（LLM）的自然语言生成（NLG）评估方法的现状，包括基于LLM导出的指标、引导LLM和使用带有标记评估数据的LLM微调，并讨论了人类与LLM的合作。同时指出了该领域的一些挑战和未来研究方向。 |
| [^29] | [LoTR: Low Tensor Rank Weight Adaptation](https://rss.arxiv.org/abs/2402.01376) | LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。 |
| [^30] | [Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization](https://rss.arxiv.org/abs/2402.01375) | 本研究通过分析不同LMs的探究实验，首次展示了主题内和跨主题泛化差距的原因，并表明嵌入空间的稳健性和LMs之间通用泛化差距的显著变化。我们的研究还发现多样的预训练目标、架构规范化或数据去重对于增强LMs的稳健性和减小泛化差距有积极作用。 |
| [^31] | [Continual Learning for Large Language Models: A Survey](https://rss.arxiv.org/abs/2402.01364) | 这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。 |
| [^32] | [What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation Properties for Fact Verification](https://rss.arxiv.org/abs/2402.01360) | 本论文通过分析医疗索赔的实体和关系属性，研究了影响其可验证性的因素，并建立了一个科学事实验证的语料库。 |
| [^33] | [Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes](https://rss.arxiv.org/abs/2402.01352) | 这项研究探索了图像描述中人类行为的变化，并发现图像的属性与这些变化相关。预训练模型可以在一定程度上捕捉到这种变化，但仍存在偏差。 |
| [^34] | [Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models](https://rss.arxiv.org/abs/2402.01349) | 对于评估大型语言模型中多选题回答的合理性进行了回顾，发现当前基于多选题回答的基准可能无法充分捕捉大型语言模型的真实能力。 |
| [^35] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^36] | [Two Approaches to Diachronic Normalization of Polish Texts](https://rss.arxiv.org/abs/2402.01300) | 本文讨论了两种处理波兰文本时序规范化的方法，一种是基于规则的，一种是基于神经网络的。实验证明，在目前的研究阶段，基于规则的方法在大部分数据集上优于神经网络方法，但在实际应用中，两种方法都有各自的优点和缺点。 |
| [^37] | [Can MLLMs Perform Text-to-Image In-Context Learning?](https://rss.arxiv.org/abs/2402.01293) | 本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。 |
| [^38] | [The Human and the Mechanical: logos, truthfulness, and ChatGPT](https://rss.arxiv.org/abs/2402.01267) | 本论文通过语义论证，讨论了是否可以称之为“机械思维”，以及ChatGPT模型能否实现该思维。研究发现，“机械思维”缺乏与现实相关的证据和个人信念，因此无法形成对世界的信念和真实性判断。 |
| [^39] | [In-Context Learning for Few-Shot Nested Named Entity Recognition](https://rss.arxiv.org/abs/2402.01182) | 本研究提出了一种针对少样本嵌套命名实体识别的上下文学习框架，并通过引入EnDe检索器和对比学习机制改进了示例演示选择，实现了高质量的演示示例。在多个数据集上进行的实验验证了系统的有效性。 |
| [^40] | [Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus](https://rss.arxiv.org/abs/2402.01176) | 本研究提出了一个统一的语言模型，通过无缝集成生成式检索、闭式生成和RAG，利用外部语料处理各种知识密集型任务。 |
| [^41] | [Efficient Prompt Caching via Embedding Similarity](https://rss.arxiv.org/abs/2402.01173) | 本论文通过嵌入相似性的提示缓存方法来提高大规模语言模型(LMMs)的推理效率，并提出一种蒸馏方法来优化现有嵌入以获得更好的缓存预测准确性。 |
| [^42] | [Streaming Sequence Transduction through Dynamic Compression](https://rss.arxiv.org/abs/2402.01172) | STAR是一种新型的Transformer模型，通过动态压缩和优化延迟、内存占用和质量，实现对流的高效序列转导，并在自动语音识别领域表现出色。 |
| [^43] | [LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning](https://rss.arxiv.org/abs/2402.01158) | 本文提出了一个名为LLM-Detector的方法，使用开源LLM指令调整来改进AI生成的中文文本检测。通过提供混合了人工编写句子和LLMs润饰句子的数据集，并利用LLMs在预训练期间获得的知识，我们能够在文档级和句子级上实现高效的文本检测。实验结果表明，该方法优于现有的基于BERT和RoBERTa的模型。 |
| [^44] | [CABINET: Content Relevance based Noise Reduction for Table Question Answering](https://rss.arxiv.org/abs/2402.01155) | CABINET是一个用于表格问答系统的基于内容相关性的噪声降低方法，通过加权处理表格内容并生成解析语句，使得大型语言模型能够专注于相关表格数据而抑制无关信息的干扰。 |
| [^45] | [AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents](https://rss.arxiv.org/abs/2402.01152) | "AccentFold"是一种通过利用学习到的口音嵌入之间的空间关系来改进ASR的方法，通过探索性分析语音嵌入，揭示非洲口音之间的有趣关系，并发现了Ethnologue先前未经描述的口音关系。通过实证评估，证明了该方法的有效性。 |
| [^46] | [A Multi-Agent Conversational Recommender System](https://rss.arxiv.org/abs/2402.01135) | 本文提出了一个多智能体对话推荐系统（MACRS），它通过设计一个多智能体行动规划框架来控制对话流程，并基于LLM生成多个候选响应。这个系统能够提高对话推荐系统的性能，并利用用户反馈来更好地建模用户偏好。 |
| [^47] | [Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models](https://rss.arxiv.org/abs/2402.01118) | Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。 |
| [^48] | [DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models](https://rss.arxiv.org/abs/2402.01117) | DTS-SQL采用了一种分解式的方法，使用小型语言模型，有效地缩小了开源模型与专有模型之间的性能差距，提高了文本到SQL任务的执行准确性。 |
| [^49] | [Interpretation of Intracardiac Electrograms Through Textual Representations](https://rss.arxiv.org/abs/2402.01115) | 本研究首次利用预训练的语言模型，通过文本表示的方式对心内电图进行插值和房颤分类。相比其他表示方法，我们的方法在房颤分类上表现出竞争性的性能。 |
| [^50] | [Vaccine: Perturbation-aware Alignment for Large Language Model](https://rss.arxiv.org/abs/2402.01109) | 疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。 |
| [^51] | [Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions](https://rss.arxiv.org/abs/2402.01108) | 多智能体系统面临着利用大型语言模型的限制和挑战，需要引入推理能力作为统一的标准来实现系统的整合和优化。 |
| [^52] | [Let's Negotiate! A Survey of Negotiation Dialogue Systems](https://rss.arxiv.org/abs/2402.01097) | 该论文综述了谈判对话系统的最新研究，包括基准、评估和方法论。还讨论了未来的发展方向，如多模态、多方和跨文化的谈判场景。这为社群提供了关于谈判对话系统的系统综述，并激发未来的研究。 |
| [^53] | [Specialized Language Models with Cheap Inference from Limited Domain Data](https://rss.arxiv.org/abs/2402.01093) | 本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。 |
| [^54] | [Reading Between the Tweets: Deciphering Ideological Stances of Interconnected Mixed-Ideology Communities](https://rss.arxiv.org/abs/2402.01091) | 本文研究了Twitter上关于2020年美国选举的讨论，通过识别相互作用的社区，引入了一种利用信息传递的新方法，微调语言模型来探索这些社区的微妙意识形态。通过与真实调查结果的比较，我们的方法显示出比现有基准线更高的一致性，突显了利用语言模型揭示杂合意识形态社区中复杂意识形态的潜力。 |
| [^55] | [Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer](https://rss.arxiv.org/abs/2402.01065) | 本文研究了大型语言模型（LLMs）在多语言环境下的能力。研究结果表明，将原始语言的上下文、问题和答案翻译成高资源语言可以获得最佳效果。 |
| [^56] | [Plan-Grounded Large Language Models for Dual Goal Conversational Settings](https://rss.arxiv.org/abs/2402.01053) | 本研究针对双重目标对话设置提出了一种新型的计划驱动大型语言模型，该模型能够在任意计划上基础对话，主动引导用户完成计划，并在系统行为上实施安全防护。 |
| [^57] | [Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model](https://rss.arxiv.org/abs/2402.01051) | 本文介绍了一种将基础语言模型中的反思生成能力提炼到较小模型中的方法，并且展示了该方法在生成反思方面的优越性能。 |
| [^58] | [Getting the most out of your tokenizer for pre-training and domain adaptation](https://rss.arxiv.org/abs/2402.01035) | 本文通过训练专用分词器，对分词器设计进行了消毒和分析，发现分词器的大小、正则表达式和训练数据对模型性能有重要影响，并提供了相应的超参数选择建议和切换分词器的方法。 |
| [^59] | [Repeat After Me: Transformers are Better than State Space Models at Copying](https://rss.arxiv.org/abs/2402.01032) | 这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。 |
| [^60] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^61] | [Graph-based Clustering for Detecting Semantic Change Across Time and Languages](https://rss.arxiv.org/abs/2402.01025) | 这项研究提出了一种基于图的聚类方法，用于跨时间和语言中准确捕捉高频和低频词义的微妙变化，尤其解决了传统方法在捕捉低频词义方面的问题。该方法在SemEval2020四种语言的二元分类任务中表现出较好的性能。 |
| [^62] | [Domain-Independent Deception: A New Taxonomy and Linguistic Analysis](https://rss.arxiv.org/abs/2402.01019) | 本论文提出了面向领域独立的欺骗行为的新分类和语言分析。首先，给出了对欺骗行为的新的计算定义，并提供了对其的分类系统。然后，分析了关于欺骗的语言线索的争议并提供了系统回顾的指导方针。最后，研究了常见的语言特征并提供了知识转移的证据。 |
| [^63] | [HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent](https://rss.arxiv.org/abs/2402.01018) | HR-MultiWOZ是一个面向HR LLM Agent的任务导向对话数据集，具有首个开源的HR对话数据集标记，用于NLP研究。 |
| [^64] | [An Information-Theoretic Approach to Analyze NLP Classification Tasks](https://rss.arxiv.org/abs/2402.00978) | 本研究提供了一个信息论框架来分析文本分类任务中输入的影响，发现在更具挑战性的数据集中，上下文对输出的影响减小，同时语义含义对于分类任务的影响较大。 |
| [^65] | [SPARQL Generation with Entity Pre-trained GPT for KG Question Answering](https://rss.arxiv.org/abs/2402.00969) | 本文面向非程序员用户的KG问答问题，通过实体链接和GPT模型生成SPARQL查询，使用CWA预训练所有实体，实现了准确的SPARQL匹配率为62.703%。 |
| [^66] | [Exploring Spatial Schema Intuitions in Large Language and Vision Models](https://rss.arxiv.org/abs/2402.00956) | 通过再现心理语言学实验，研究发现大型语言模型（LLMs）尽管无具身性，却能够有效捕捉到人类对于语言中基本的空间构建块的隐含直觉，这对于理解语言和空间的相互作用具有重要意义。 |
| [^67] | [Institutional Platform for Secure Self-Service Large Language Model Exploration](https://rss.arxiv.org/abs/2402.00913) | 这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。 |
| [^68] | [An Early Categorization of Prompt Injection Attacks on Large Language Models](https://rss.arxiv.org/abs/2402.00898) | 本文旨在提供对大规模语言模型上的提示注入攻击的早期分类，并讨论了这些攻击对LLM最终用户、开发人员和研究人员的影响。 |
| [^69] | [Large Language Models in Cybersecurity: State-of-the-Art](https://rss.arxiv.org/abs/2402.00891) | 本研究综述了大型语言模型（LLMs）在网络安全领域中的防御和对抗应用，并识别了关键的研究空缺，旨在提供对LLM驱动的网络安全的潜在风险和机会的全面理解。 |
| [^70] | [Security and Privacy Challenges of Large Language Models: A Survey](https://rss.arxiv.org/abs/2402.00888) | 大型语言模型具有卓越的能力，但也面临着安全和隐私攻击的威胁。本调查全面审查了LLM的安全和隐私挑战，涵盖了训练数据、用户和应用风险等方面，并对解决方法进行了回顾。 |
| [^71] | [Scaling Sparse Fine-Tuning to Large Language Models](https://rss.arxiv.org/abs/2401.16405) | 本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。 |
| [^72] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^73] | [NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness](https://rss.arxiv.org/abs/2401.15963) | 本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。 |
| [^74] | [SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese](https://rss.arxiv.org/abs/2401.11819) | SuperCLUE-Math6 是一个用于评估中文语言模型数学推理能力的分级数据集，通过增加难度、多样性和应用范围，提供了2000多个需要多步推理的数学问题，并采用创新方法来量化大型模型的推理能力。实验结果显示出明显的推理水平分层，顶级模型如GPT-4表现出优越的性能。SC-Math6填补了中文数学推理基准数据集的空白，并推进了中文语言模型的智能化。 |
| [^75] | [Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text](https://rss.arxiv.org/abs/2401.09407) | 该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。 |
| [^76] | [Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon](https://rss.arxiv.org/abs/2401.03462) | 本论文提出了一种称为激活标志的新方法，它通过压缩LLM的激活状态，使其能够以有限的上下文窗口感知更长的上下文，同时保留了LLM在短上下文中的原始能力。这种方法具有竞争力的内存和时间效率，并通过多样化训练有效地支持不同上下文长度。 |
| [^77] | [Spike No More: Stabilizing the Pre-training of Large Language Models](https://rss.arxiv.org/abs/2312.16903) | 本论文研究了大型语言模型预训练中的损失尖峰问题，并通过理论分析找出了梯度爆炸的原因，并提出了满足要求的方法。通过实验证明，该方法能够有效地防止尖峰的发生。 |
| [^78] | [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://rss.arxiv.org/abs/2312.13382) | DSPy引入了LM断言，用于表达语言模型应满足的计算约束。在四个案例研究中，LM断言不仅提高了对规则的遵守，而且提高了下游任务的性能，增加了对约束的接受次数并生成了更高质量的回复。 |
| [^79] | [CBQ: Cross-Block Quantization for Large Language Models](https://rss.arxiv.org/abs/2312.07950) | CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。 |
| [^80] | [Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs](https://rss.arxiv.org/abs/2312.05356) | 这项工作介绍了一种神经元层面的模型编辑方法，能够在编码任务中修补LLM模型，并且在API序列推荐、代码生成和伪代码到代码转换等任务中得到了验证和评估。 |
| [^81] | [Large Language Models on Graphs: A Comprehensive Survey](https://rss.arxiv.org/abs/2312.02783) | 这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。 |
| [^82] | [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://rss.arxiv.org/abs/2311.11855) | 本文研究了基于LLM的智能体的安全性，从智能体数量、角色定义和攻击水平三个角度进行了探讨，并提出了一种有效的攻击方法“恶意天才”，通过自动生成与原始角色相关的提示，来测试在不同角色定义和攻击水平下的影响。 |
| [^83] | [A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges](https://rss.arxiv.org/abs/2311.05112) | 本综述提供了医学中大型语言模型（LLMs）的原理、应用和挑战的全面概述。同时回答了医学LLMs的构建、下游性能、实际应用、挑战以及更好构建和利用的问题。旨在为构建有效的医学LLMs提供见解和实用资源。 |
| [^84] | [Foundation Model's Embedded Representations May Detect Distribution Shift](https://rss.arxiv.org/abs/2310.13836) | 该研究发现基础模型的嵌入表示可以检测到数据集之间的分布偏移，且在传统的泛化度量上表现出偏差。预训练的GPT-2模型的特征学习无法在特定任务上提升性能，而对其表示进行线性探测可能优于整体微调。 |
| [^85] | [Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking](https://rss.arxiv.org/abs/2309.01606) | Geo-Encoder是一个用于中文地理重新排序的创新框架，它能够更有效地将中文地理语义集成到重新排序流程中，并且通过使用多任务学习模块和异步更新机制，能够提升模型在特定块上的注意力集中能力。 |
| [^86] | [Deception Abilities Emerged in Large Language Models](https://rss.arxiv.org/abs/2307.16513) | 大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。 |
| [^87] | [Language Models as Inductive Reasoners](https://rss.arxiv.org/abs/2212.10923) | 该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。 |
| [^88] | [Towards Efficient and Exact Optimization of Language Model Alignment](https://arxiv.org/abs/2402.00856) | 本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。 |
| [^89] | [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786) | CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。 |
| [^90] | [A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559) | 本论文提出了Reveal数据集，用于在开放领域的问答设置中对复杂思维链的自动验证进行基准测试。这个数据集包含了详尽的标签，用于评估语言模型的答案中每个推理步骤的相关性、归因和逻辑正确性。 |
| [^91] | [Improving QA Model Performance with Cartographic Inoculation](https://arxiv.org/abs/2401.17498) | 本文提出了一种名为地图接种的新方法，通过在优化的挑战数据子集上对QA模型进行微调，减少模型对数据集伪迹的依赖性，从而显著提高模型在复杂和开放的上下文推理问题上的性能。 |
| [^92] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^93] | [Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models.](http://arxiv.org/abs/2401.16727) | 这项综合调查总结了最近在仇恨言论审核方面的进展，重点介绍了大型语言模型和大型多模态模型的作用。研究发现了文本、视觉和听觉元素在传播仇恨言论中的微妙相互作用，并强调了大型模型对审核能力的重新定义。同时，研究还指出了在少数语言和文化背景下的研究差距和处理低资源环境的需求。 |
| [^94] | [A Linguistic Comparison between Human and ChatGPT-Generated Conversations.](http://arxiv.org/abs/2401.16587) | 本研究比较了人类和ChatGPT生成的对话的语言差异，发现ChatGPT在社交、分析、认知、关注焦点和积极情绪等方面表现出色，但人类对话更具变异性和真实性，尽管在情绪方面无显著差异。同时，该研究还提供了一个新颖的、由ChatGPT生成的对话组成的数据集。 |
| [^95] | [Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports.](http://arxiv.org/abs/2401.16578) | 该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。 |
| [^96] | [A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM.](http://arxiv.org/abs/2401.15378) | 基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。 |
| [^97] | [UNSEE: Unsupervised Non-contrastive Sentence Embeddings.](http://arxiv.org/abs/2401.15316) | UNSEE是一种无监督的非对比度句子嵌入方法，通过引入目标网络解决了表示坍塌问题，达到了与对比目标相当的性能提升。 |
| [^98] | [The Neglected Tails of Vision-Language Models.](http://arxiv.org/abs/2401.12425) | 本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。 |
| [^99] | [Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys.](http://arxiv.org/abs/2401.10352) | 通过引入cuDialog，我们提出了一种以文化为视角的对话生成基准，并开发了能够从对话中提取文化属性的基准模型。实验结果显示，结合文化价值调查可以提高对话代理的对个性化和对话质量的预测准确性。 |
| [^100] | [Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation.](http://arxiv.org/abs/2401.08417) | 本研究通过引入对比性偏好优化（CPO）的方法，弥合了大型语言模型（LLM）在机器翻译中性能与传统编码器-解码器模型之间的差距，实现了更好的翻译效果。 |
| [^101] | [DQNC2S: DQN-based Cross-stream Crisis event Summarizer.](http://arxiv.org/abs/2401.06683) | 本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。 |
| [^102] | [An Assessment on Comprehending Mental Health through Large Language Models.](http://arxiv.org/abs/2401.04592) | 这项研究评估了大型语言模型在理解心理健康方面的潜力，结果显示基于Transformer的模型在表达人类心理健康状况方面的理解能力超过大型语言模型。 |
| [^103] | [Topic Bias in Emotion Classification.](http://arxiv.org/abs/2312.09043) | 本文研究了情绪分类中的主题偏差问题，发现情绪语料库中的主题与情绪实际上具有相关性，并且情绪分类器容易受到这些主题的干扰。最后，研究者展示了一种去偏差的方法可以减轻主题偏差的影响。 |
| [^104] | [Entity Matching using Large Language Models.](http://arxiv.org/abs/2310.11244) | 这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。 |
| [^105] | [In-Context Few-Shot Relation Extraction via Pre-Trained Language Models.](http://arxiv.org/abs/2310.11085) | 本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。 |
| [^106] | ["Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion.](http://arxiv.org/abs/2308.10974) | "引用GPT的“豚鼠试验”是一种创新的智能代理建模方法，利用智能代理代表企业进行竞争和勾结研究。它比使用人类主体进行实验更具成本效益和灵活性，并展现出超越传统代理建模方法的能力。" |
| [^107] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^108] | [On the Computational Power of Decoder-Only Transformer Language Models.](http://arxiv.org/abs/2305.17026) | 本篇论文研究了解码器Transformer语言模型的计算普适性，表明即使只有单层和单注意力头，仍然具有图灵完备性，其中单词嵌入的稀疏性/可压缩性是必要条件。 |
| [^109] | [Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs.](http://arxiv.org/abs/2305.14279) | 本论文研究了大型语言模型在多步推理中的自洽性问题，提出了假设自洽性和组合自洽性两个重要特性，并发现GPT-3/-4模型在这两方面都表现出了较差的一致性。 |
| [^110] | [Beyond Words: A Comprehensive Survey of Sentence Representations.](http://arxiv.org/abs/2305.12641) | 本文综述了不同的句子表示学习方法，包括传统和基于深度学习技术的方法。突出该领域的主要贡献和挑战，并强调句子表示学习的进展和未来的研究方向。 |
| [^111] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^112] | [Assessing Working Memory Capacity of ChatGPT.](http://arxiv.org/abs/2305.03731) | 本文评估了最先进语言模型ChatGPT的工作记忆容量，结果显示其在N-back任务的行为表现与人类参与者相似，这为设计具有人类级认知能力的人工智能系统提供了关键洞察。 |
| [^113] | [Data Curation for Image Captioning with Text-to-Image Generative Models.](http://arxiv.org/abs/2305.03610) | 本论文探究了通过数据整合来提高图像字幕质量的方案，并使用现有资源训练了比基准模型更好的模型。 |

# 详细

[^1]: 《论文标题：广义语法规则和基于结构的一般化——对于词汇任务和转导的经典等变性之外的一般化的立场论文》

    Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction

    [https://rss.arxiv.org/abs/2402.01629](https://rss.arxiv.org/abs/2402.01629)

    这篇论文提出了一个通用框架，其中使用广义语法规则（GGRs）来实现组合一般化，将其视为转导任务中的对称性约束。该框架不仅形式化了语言转导的广义对称性概念，还与强化学习和其他研究领域有关联。

    

    组合一般化是人类学习词汇与现有神经网络之间的主要差异之一。我们提出了一个通用框架，用于构建能够使用广义语法规则（GGRs）进行组合一般化的模型，GGRs是一类基于对称性的转导任务的组合约束，我们将其视为受物理学任务中等变性约束启发的转导类比。除了为语言转导形式化广义的对称性概念之外，我们的框架足够通用，可以包含许多现有工作作为特例。我们提出了关于如何实现GGRs的想法，并在此过程中与强化学习和其他研究领域建立了联系。

    Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.
    
[^2]: TravelPlanner: 一种用于自然语言代理的真实世界规划基准

    TravelPlanner: A Benchmark for Real-World Planning with Language Agents

    [https://rss.arxiv.org/abs/2402.01622](https://rss.arxiv.org/abs/2402.01622)

    本论文提出了一种用于自然语言代理的新的规划基准TravelPlanner，它关注于旅行规划这一常见的真实世界场景。经过全面评估，发现目前的语言代理仍无法处理如此复杂的规划任务，即使最先进的GPT-4也只能达到0.6%的成功率。

    

    自规划起初就是人工智能的核心追求之一，但早期的人工智能代理大多集中在受限环境下，因为缺乏进行人类水平规划所需的许多认知基础。最近，由大型语言模型（LLM）驱动的语言代理展现出了工具使用和推理等有趣的能力。这些语言代理能否在超出先前人工智能代理范围的更复杂环境中进行规划？为了推进这项研究，我们提出了TravelPlanner，这是一个新的规划基准，专注于旅行规划这个常见的真实世界规划场景。它提供了一个丰富的沙盒环境，各种用于访问近400万个数据记录的工具，并包含1225个精心策划的规划意图和参考计划。综合评估显示，当前的语言代理尚不具备处理如此复杂的规划任务的能力-即使是GPT-4的成功率也只有0.6%。

    Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. La
    
[^3]: MAGDi：结构化蒸馏多智能体交互图在较小的语言模型中改善推理能力

    MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models

    [https://rss.arxiv.org/abs/2402.01620](https://rss.arxiv.org/abs/2402.01620)

    MAGDi是一种结构化蒸馏方法，通过将多个大语言模型之间的推理交互表示为图形，来改善较小语言模型的推理能力。

    

    大语言模型（LLM）智能体之间的多智能体交互在各种推理任务中表现出重大改进。然而，这些方法涉及多个模型之间的长时间生成，成本高昂。此外，这些多智能体方法无法提供一个最终的、单一的模型进行高效推理。为了解决这个问题，我们引入了MAGDi，一种新的方法，用于将多个LLM之间的推理交互结构化蒸馏到较小的模型中。MAGDi通过将多智能体交互表示为图形，使用图形编码器增强基础学生模型，并使用三个目标函数进行知识蒸馏：下一个令牌预测、正确和错误推理之间的对比损失以及基于图形的目标来建模交互结构。在七个广泛使用的常识和数学推理基准测试上的实验结果表明，MAGDi改善了较小模型的推理能力，优于几种方法。

    Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods th
    
[^4]: KB-Plugin:一个用于大型语言模型在低资源知识库上引导程序的即插即用框架

    KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases

    [https://rss.arxiv.org/abs/2402.01619](https://rss.arxiv.org/abs/2402.01619)

    KB-Plugin是一个用于大型语言模型在低资源知识库上引导程序的即插即用框架，它通过自监督学习和利用丰富资源KB的注释数据来提取问题相关的模式信息，并实现在任何低资源KB上诱导程序。

    

    程序归纳已经成为一种有前景的利用知识库帮助大型语言模型回答复杂知识密集型问题的范例。然而，程序归纳通常依赖于大量的并行问题-程序对，在LLM意识到给定KB的模式的同时，对于许多缺乏注释数据的低资源KB来说是具有挑战性的。为此，我们提出了KB-Plugin，一个即插即用框架，使LLM能够在任何低资源KB上诱导程序。首先，KB-Plugin采用自监督学习将给定KB的详细模式信息编码到一个可插拔模块，即模式插件。其次，KB-Plugin利用丰富资源KB中的丰富注释数据来训练另一个可插拔模块，即PI插件，该插件可以帮助LLM从任何KB的模式插件中提取与问题相关的模式信息，并利用此信息在该KB上诱导程序。在五个异构KBQA数据集上进行了实验。

    Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA da
    
[^5]: 用于引导生成式大型语言模型的风格向量

    Style Vectors for Steering Generative Large Language Model

    [https://rss.arxiv.org/abs/2402.01618](https://rss.arxiv.org/abs/2402.01618)

    本研究通过在文本生成过程中添加风格向量来引导大型语言模型输出特定风格的文本，这种方法简单且有效，是发展更具适应性和有效性的AI驱动交互系统的重要一步。

    

    本研究探讨了通过在文本生成过程中将风格向量添加到隐藏层激活中，来引导大型语言模型（LLM）输出特定风格（如情感、情绪或写作风格）的策略。与更复杂的训练方法相比，我们表明可以从记录的层激活中简单地计算出特定风格的风格向量。通过一系列实验，我们展示了使用这样的风格向量来影响生成文本的风格的激活工程的有效性，这种方法具有细致和可参数化的特点，区别于提示工程。所展示的研究是向更具适应性和有效性的AI驱动交互系统发展迈出的重要一步。

    This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.
    
[^6]: Nomic Embed：训练可复现的长上下文文本嵌入器

    Nomic Embed: Training a Reproducible Long Context Text Embedder

    [https://rss.arxiv.org/abs/2402.01613](https://rss.arxiv.org/abs/2402.01613)

    Nomic Embed是第一个完全可复现、开源、开放权重、开放数据的8192上下文长度英文文本嵌入器，在短上下文和长上下文任务上优于OpenAI Ada-002和OpenAI text-embedding-3-small。

    

    本技术报告描述了nomic-embed-text-v1的训练，这是第一个完全可复现、开源、开放权重、开放数据的8192上下文长度英文文本嵌入模型，在短上下文和长上下文任务上均优于OpenAI Ada-002和OpenAI text-embedding-3-small。我们在Apache 2许可下发布了训练代码和模型权重。与其他开源模型相比，我们还发布了一个包含2.35亿个策划文本对的训练数据加载器，可以完全复现nomic-embed-text-v1。你可以在https://github.com/nomic-ai/contrastors找到模型的代码和数据。

    This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors
    
[^7]: 迈向可持续的工作场所心理健康：早期干预和支持的新方法

    Towards Sustainable Workplace Mental Health: A Novel Approach to Early Intervention and Support

    [https://rss.arxiv.org/abs/2402.01592](https://rss.arxiv.org/abs/2402.01592)

    本研究提出了一种 groundbreaking 的压力检测算法，利用自动聊天机器人技术实时分析聊天对话，以客观测量员工的心理健康水平，并根据语言生物标志提供个性化的实时治疗建议，为企业提供早期干预和支持。

    

    员工的健康是当代工作场所的一个关键问题，正如美国心理学协会2021年的报告所指出的，71%的员工会经历压力或紧张。这种压力对工作场所的离职率和缺勤率产生了显著影响，61%的离职和16%的病假归因于心理健康状况不佳。雇主面临的一个主要挑战是员工通常在达到危机点之前都不知道自己的心理健康问题，导致对企业福利待遇的利用有限。本研究通过提出一种前所未有的压力检测算法来解决这个挑战，该算法可以提供实时的支持。利用自动聊天机器人技术，该算法通过分析聊天对话客观地测量心理健康水平，并根据语言生物标志提供个性化的实时治疗建议。本研究探讨了将这些创新技术整合到企业环境中的可行性。

    Employee well-being is a critical concern in the contemporary workplace, as highlighted by the American Psychological Association's 2021 report, indicating that 71% of employees experience stress or tension. This stress contributes significantly to workplace attrition and absenteeism, with 61% of attrition and 16% of sick days attributed to poor mental health. A major challenge for employers is that employees often remain unaware of their mental health issues until they reach a crisis point, resulting in limited utilization of corporate well-being benefits. This research addresses this challenge by presenting a groundbreaking stress detection algorithm that provides real-time support preemptively. Leveraging automated chatbot technology, the algorithm objectively measures mental health levels by analyzing chat conversations, offering personalized treatment suggestions in real-time based on linguistic biomarkers. The study explores the feasibility of integrating these innovations into p
    
[^8]: BAT: 使用大规模语言模型学习关于空间声音的推理能力

    BAT: Learning to Reason about Spatial Sounds with Large Language Models

    [https://rss.arxiv.org/abs/2402.01591](https://rss.arxiv.org/abs/2402.01591)

    本文提出了BAT，它结合了双耳声音场景分析模型的空间声音感知能力和大规模语言模型的自然语言推理能力，以复制人类的空间声音推理能力。通过使用合成的双耳音频数据集和基于空间声音的问答数据集进行训练，BAT在空间声音感知和推理方面取得了强大的性能。

    

    空间声音推理是一种基本的人类技能，它使我们能够根据声音来导航和解释我们的周围环境。本文提出了BAT，它将双耳声音场景分析模型的空间声音感知能力与大规模语言模型（LLM）的自然语言推理能力相结合，以复制这种固有能力。为了解决现有野外空间声音数据集的缺乏，我们使用AudioSet和SoundSpaces 2.0合成了一个双耳音频数据集。接下来，我们开发了一种基于空间声音的问答数据集SpatialSoundQA，提供了一系列QA任务，以训练BAT在空间声音感知和推理的各个方面。BAT的声学前端编码器是一种名为Spatial Audio Spectrogram Transformer（Spatial-AST）的创新空间音频编码器，它本身在声音事件检测、空间定位和距离估计等方面具有强大的性能。通过将Spatial-AST与LLaMA-2 7B集成，

    Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
    
[^9]: TrustAgent: 通过代理构成实现安全可信赖的LLM代理

    TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution

    [https://rss.arxiv.org/abs/2402.01586](https://rss.arxiv.org/abs/2402.01586)

    本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。

    

    近年来，基于LLM的代理引起了广泛关注，但其可信度仍未得到深入探索。由于代理可以直接与物理环境交互，其可靠性和安全性至关重要。本文提出了一种基于代理构成的代理框架TrustAgent，对LLM代理的安全性维度进行了初步研究。该框架包括三种策略：预先规划策略，在生成计划之前向模型注入安全知识；规划过程中策略，在生成计划时增强安全性；计划后检查策略，通过计划后检查确保安全性。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险有效提高LLM代理的安全性。此外，我们还探讨了安全性与使用者满意度之间的复杂关系，以及模型的推理能力与其效率之间的关联。

    The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
    
[^10]: 对于语言系统演化推断的自动化声音变化预测：图卡诺亚人研究案例

    Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study

    [https://rss.arxiv.org/abs/2402.01582](https://rss.arxiv.org/abs/2402.01582)

    该论文介绍了一种部分自动化的语言系统演化推断方法，通过利用神经网络预测历史语言形式与现代语言形式之间的中间声音变化步骤，并取得了显著的改进效果。

    

    我们描述了一组新的方法，可以在一定程度上自动化语言系统演化的推断，这些方法依赖于(1)同源词集及其相应的原型形式和音变规律，(2)从音素到其发音特征的映射以及(3)一种声音变化的类型学数据库。我们通过训练神经网络利用这些声音变化数据来权衡音素之间的发音差距，并预测历史上的原型形式与其现代后代之间的中间声音变化步骤，从而在该方法中部分代替了语言学专家。在我们对图卡诺亚语言的最佳实验中，该方法生成的树与使用专家注释的树相比，具有0.12的广义四分距，这是其他半自动基线的显著改进。我们讨论了我们的神经方法和基于简洁原则的树预测的潜在优势和劣势。我们还尝试了最小泛化学习器进行自动音变规律归纳的实验，发现其与语言学家的归纳结果相当。

    We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes. We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it compa
    
[^11]: 多重语言环境下语音情感识别的跨语言预训练模型研究

    How Paralingual are Paralinguistic Representations? A Case Study in Speech Emotion Recognition

    [https://rss.arxiv.org/abs/2402.01579](https://rss.arxiv.org/abs/2402.01579)

    本研究通过比较研究五种预训练模型，评估了针对社交语言任务进行预训练的模型在多语言情境下对语音情感识别的效果。结果表明，TRILLsson模型能够有效地捕捉语音数据中的社交语言特征，提升了语音情感识别的性能。

    

    预训练模型（PTM）在语音情感识别（SER）领域取得了巨大进展。最近的研究利用各种PTM表示作为SER下游模型的输入特征。针对社交语言任务进行预训练的PTM在SER领域取得了最先进的性能。然而，这些PTM还没有在多语言环境下进行SER评估，且只涉及英语。因此，我们通过对五种PTM（TRILLsson、wav2vec2、XLS-R、x-vector、Whisper）进行全面比较研究，评估社交语言PTM（TRILLsson）在多种语言情境下对SER的效果。TRILLsson的表示在所有PTM中达到了最佳表现。这表明TRILLsson能够有效捕捉语音数据中的各种社交语言特征，从而提供更好的SER。

    Pre-trained Models (PTMs) have facilitated substantial progress in the field of Speech Emotion Recognition (SER). SER is an area with applications ranging from HumanComputer Interaction to Healthcare. Recent studies have leveraged various PTM representations as input features for downstream models for SER. PTM specifically pre-trained for paralinguistic tasks have obtained state-of-the-art (SOTA) performance for SER. However, such PTM haven't been evaluated for SER in multilingual settings and experimented only with English. So, we fill this gap, by performing a comprehensive comparative study of five PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages. Representations from TRILLsson achieved the best performance among all the PTMs. This demonstrates that TRILLsson is able to effectively capture the various paralinguistic features from speech data for better SER. We also show that downstre
    
[^12]: 来自冲突文本语料库的深度主动学习数据挖掘

    Deep Active Learning for Data Mining from Conflict Text Corpora

    [https://rss.arxiv.org/abs/2402.01577](https://rss.arxiv.org/abs/2402.01577)

    本文提出了一种利用深度主动学习从冲突文本语料库中进行数据挖掘的方法。通过迭代的主动学习过程，结合大型的仅编码器语言模型，可以提取与冲突动态相关的子类事件，达到类似于人类的性能。

    

    高分辨率的武装冲突事件数据以及相关过程已经通过UCDP GED、ACLED等数据集彻底改变了政治争论的研究。然而，大部分这些数据集仅限于收集时空（高分辨率）和强度数据。关于目标、战术、目的等动态信息很少被收集，这是因为数据收集的工作量非常大。然而，大部分数据集依赖于丰富的文本数据库，可以进一步挖掘与每个事件相关的更多信息。本文提出了一种廉价且高性能的方法，利用主动学习来改进机器学习模型，通过顺序（有导向的）人工输入的迭代过程。然后，使用主动学习逐步训练（微调）一个大型的仅编码器语言模型，以提取与冲突动态相关的子类事件。该方法表现出与人类（金标准）相似的性能。

    High-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like UCDP GED, ACLED etc. However, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data. Information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data. However, most datasets rely on a rich corpus of textual data allowing further mining of further information connected to each event. This paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input. Active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics. The approach shows performance similar to human (gold-standar
    
[^13]: 论论文摘要中的多样性的实证分析

    An Empirical Analysis of Diversity in Argument Summarization

    [https://rss.arxiv.org/abs/2402.01535](https://rss.arxiv.org/abs/2402.01535)

    本研究通过实证分析，发现目前的论据摘要方法在捕捉多样性方面存在困难，并提出多样性有三个方面：意见、注释者和来源。所研究的关键点分析任务中的通用LLMs和专门的KPA模型都有互补的优势，训练数据的多样化将有助于提高泛化能力。因此，应对论证摘要中的多样性需要采用多种策略来处理主观性。

    

    在在线社会讨论中，提供高水平的论据是促进参与的关键任务。目前的论据摘要方法缺失了这项任务的一个重要方面，即捕捉多样性，这对于包容多个观点是重要的。我们引入了三个方面的多样性：意见、注释者和来源。我们评估了一种名为关键点分析的流行论据摘要任务的方法，显示这些方法在(1)代表少数人共享的论点上，(2)处理来自各种来源的数据以及(3)与人工提供的主观注释相一致方面遇到了困难。我们发现，通用的LLM和专门的KPA模型都表现出了这种行为，但具有互补的优势。此外，我们观察到训练数据的多样化可能改善泛化能力。应对论证摘要中的多样性需要采用一系列策略来处理主观性。

    Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.
    
[^14]: 解码推测解码

    Decoding Speculative Decoding

    [https://rss.arxiv.org/abs/2402.01528](https://rss.arxiv.org/abs/2402.01528)

    推测解码是一种用于加速大型语言模型推断的技术，但我们的实验表明，选择的草稿模型生成的令牌被目标模型接受的概率越高，吞吐量越低。我们通过大量实验，分析了各种因素对推测解码效果的影响，并提出了一个分析模型来提高效率。

    

    推测解码是一种常用的技术，用于加速大型语言模型（LLM）的推断，而不修改其结果。在对LLM进行推断时，推测解码使用较小的草稿模型生成推测令牌，然后使用目标LLM验证这些草稿令牌。推测解码提供的加速取决于草稿模型的选择。普遍建议选择一个草稿模型，该模型生成的令牌被LLM接受的概率很高，以实现最高吞吐量。然而，我们的实验结果与之相反，随着生成的令牌被目标模型接受的概率增加，吞吐量减少。为了理解这一现象，我们进行了大量实验，对影响推测解码的不同因素进行了表征，并研究了这些因素如何相互作用和影响加速效果。基于我们的实验结果，我们描述了一个分析模型，可以使用该模型来进行决策，提高推测解码的效率。

    Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
    
[^15]: 使用大型语言模型进行K级推理

    K-Level Reasoning with Large Language Models

    [https://rss.arxiv.org/abs/2402.01521](https://rss.arxiv.org/abs/2402.01521)

    该论文探索了使用大型语言模型进行动态决策推理的能力，并提出了一种名为"K级推理"的新颖推理方法。通过博弈论的试验，发现现有的推理方法在动态环境中容易出错，而"K级推理"可以解决这个问题。

    

    虽然大型语言模型（LLMs）已经展示了其在复杂推理任务上的能力，但在动态、交互和竞争场景（如商业战略和股票市场分析）中的性能仍然未被充分探索。为了填补这个空白，我们正式探索LLMs在快速变化环境中的决策推理能力。我们引入了两个基于博弈论的试验，以模拟现实世界中动态决策的复杂性。这些挑战具有明确定义，可以对LLMs的动态推理能力进行清晰、可控和精确的评估。通过大量实验，我们发现现有的推理方法在需要k级思考的动态环境中容易出错 - 这是之前研究中未解决的关键概念。为了解决这个问题，我们提出了一种新颖的LLMs推理方法，命名为“K级推理”。该方法采用对手的视角，从递归角度运用基于k级思考的推理。

    While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on 
    
[^16]: 多语言梯度词序类型学研究从通用依赖关系

    Multilingual Gradient Word-Order Typology from Universal Dependencies

    [https://rss.arxiv.org/abs/2402.01513](https://rss.arxiv.org/abs/2402.01513)

    本文提出了一个使用连续值数据而不是分类数据的新种子数据集，用于改进NLP任务性能，并介绍了创建数据集的方法论，可适用于更多特征和语言。

    

    虽然语言类型学领域的信息有潜力提高NLP任务的表现，但可靠的类型学数据是前提。现有的类型学数据库，包括WALS和Grambank，主要由于其分类格式的不一致性而存在问题。此外，类型学的分类定义与自然语言语料库中所发现的现象的连续性本质不同。本文介绍了一个新的种子数据集，由连续值数据而不是分类数据组成，可以更好地反映语言的变异性。虽然这个初始数据集侧重于词序类型学，但我们也提供了创建数据集的方法论，可以方便地用于生成更广泛的特征和语言的数据。

    While information from the field of linguistic typology has the potential to improve performance on NLP tasks, reliable typological data is a prerequisite. Existing typological databases, including WALS and Grambank, suffer from inconsistencies primarily caused by their categorical format. Furthermore, typological categorisations by definition differ significantly from the continuous nature of phenomena, as found in natural language corpora. In this paper, we introduce a new seed dataset made up of continuous-valued data, rather than categorical data, that can better reflect the variability of language. While this initial dataset focuses on word-order typology, we also present the methodology used to create the dataset, which can be easily adapted to generate data for a broader set of features and languages.
    
[^17]: 多项选择题中的干扰项生成：方法、数据集和评估综述

    Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation

    [https://rss.arxiv.org/abs/2402.01512](https://rss.arxiv.org/abs/2402.01512)

    本文综述了多项选择题中干扰项生成的方法、数据集和评估。调查结果显示，现有数据集主要来自特定领域教育资源中，以文本为主，缺乏开放领域和多模态的数据集。

    

    干扰项在学习评估中具有重要作用。本文调查了针对英语多项选择题的干扰项生成任务，并使用了文本和多模态语境的数据集。具体而言，本文对干扰项生成任务的最新研究进行了全面的文献综述，讨论了多项选择题的组成部分及其特点，分析了相关数据集，并总结了干扰项生成的评估指标。我们的调查结果表明，超过一半的数据集来自特定领域（如科学和英语）中的教育来源，并且主要是以文本为主，缺乏开放领域和多模态的数据集。

    Distractors are important in learning evaluation. This paper surveys distractor generation tasks using English multiple-choice question datasets for textual and multimodal contexts. In particular, this paper presents a thorough literature review of the recent studies on distractor generation tasks, discusses multiple choice components and their characteristics, analyzes the related datasets, and summarizes the evaluation metrics of distractor generation. Our investigation reveals that more than half of datasets are human-generated from educational sources in specific domains such as Science and English, which are largely text-based, with a lack of open domain and multimodal datasets.
    
[^18]: 一种混合策略用于聊天记录摘要化

    A Hybrid Strategy for Chat Transcript Summarization

    [https://rss.arxiv.org/abs/2402.01510](https://rss.arxiv.org/abs/2402.01510)

    这篇论文介绍了一种混合策略用于聊天记录摘要化，该策略首先结合了抽取和生成式摘要化技术，然后通过强化学习优化摘要的质量。这种方法在大规模部署的聊天记录摘要化中表现出了很好的效果。

    

    文本摘要化是将一段文本压缩成较少句子的过程，同时保留其内容。在这个背景下，聊天记录是客户（来电者）和客服人员之间的数字或在线对话的文本副本。本文提出了一种本地开发的混合方法，首先结合抽取和生成式摘要化技术，对缺乏标点或未标点的聊天记录进行压缩，产生更易读的带标点摘要，然后通过强化学习优化摘要的整体质量。广泛的测试、评估、比较和验证证明了这种方法在没有手动生成的参考摘要的情况下，对于大规模部署的聊天记录摘要化的有效性。

    Text summarization is the process of condensing a piece of text to fewer sentences, while still preserving its content. Chat transcript, in this context, is a textual copy of a digital or online conversation between a customer (caller) and agent(s). This paper presents an indigenously (locally) developed hybrid method that first combines extractive and abstractive summarization techniques in compressing ill-punctuated or un-punctuated chat transcripts to produce more readable punctuated summaries and then optimizes the overall quality of summarization through reinforcement learning. Extensive testing, evaluations, comparisons, and validation have demonstrated the efficacy of this approach for large-scale deployment of chat transcript summarization, in the absence of manually generated reference (annotated) summaries.
    
[^19]: 代码转换语言识别比你想的更困难

    Code-Switched Language Identification is Harder Than You Think

    [https://rss.arxiv.org/abs/2402.01505](https://rss.arxiv.org/abs/2402.01505)

    代码转换语言识别是一个常见且应用广泛的问题，但目前的方法都不够理想。本研究通过扩展语言范围、采用简单架构的模型以及重新定义任务，提出了一种更加可行的解决方案，并对未来的工作提出了建议。

    

    代码转换（CS）是一种在书面和口语交流中非常常见的现象，但是许多自然语言处理应用程序对其处理不佳。在构建CS语料库的应用中，我们探索了用于语料库构建的CS语言识别（LID）。我们通过扩展到更多的语言并考虑采用简单架构的模型进行更快的推理来使任务更加真实。我们还将任务重新定义为句子级多标签标记问题，以使其更加可行。在定义了任务之后，我们研究了三个合理的模型，并定义了更能反映所需性能的指标。我们提出了实证证据表明当前的方法都不足够，并最终提供了对未来工作在这个领域的建议。

    Code switching (CS) is a very common phenomenon in written and spoken communication but one that is handled poorly by many natural language processing applications. Looking to the application of building CS corpora, we explore CS language identification (LID) for corpus building. We make the task more realistic by scaling it to more languages and considering models with simpler architectures for faster inference. We also reformulate the task as a sentence-level multi-label tagging problem to make it more tractable. Having defined the task, we investigate three reasonable models for this task and define metrics which better reflect desired performance. We present empirical evidence that no current approach is adequate and finally provide recommendations for future work in this area.
    
[^20]: 知识图生成对话式自然语言文本的比较分析

    A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation

    [https://rss.arxiv.org/abs/2402.01495](https://rss.arxiv.org/abs/2402.01495)

    本研究对比了四个不同大小的大型语言模型在从语义三元组生成自然语言文本方面的性能，并发现了生成预测中最常见的问题。

    

    从图结构数据中生成自然语言文本对于会话式信息搜索至关重要。从知识图中获得的语义三元组可以作为对话代理回应的基础，通过提供事实依据来传达信息。这在大型语言模型的背景下尤为重要，大型语言模型具有很大的会话交互潜力，但容易产生幻象、省略或产生冲突信息。在本研究中，我们对会话式大型语言模型在从语义三元组生成自然语言文本方面进行了实证分析。我们比较了四个不同大小的大型语言模型和不同启发式技术。通过在WebNLG数据集上进行一系列基准实验，我们分析了模型的性能，并确定了生成的预测中最常见的问题。我们的研究结果表明，大型语言模型在三元组验证

    Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple ver
    
[^21]: AMOR:通过进程反馈构建适应性模块化知识代理的方法

    AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback

    [https://rss.arxiv.org/abs/2402.01469](https://rss.arxiv.org/abs/2402.01469)

    AMOR是一个基于开源LLM的代理框架，通过与外部知识库进行推理和人类监督来适应特定领域的推理过程。通过两阶段的微调，AMOR能够在不同知识环境中泛化，并且可以根据过程反馈进行领域定制。

    

    大型语言模型（LLM）的显著成功引发了构建语言代理完成各种复杂任务的高潮。我们提出了基于开源LLM的代理框架AMOR，通过与外部知识库进行推理并通过人类监督来适应特定领域的推理过程。AMOR在有限状态机（FSM）上构建推理逻辑，通过自主执行和模块间转换解决问题。这使人们能够直接为单个模块提供反馈，从而自然形成了过程监督。基于这个推理和反馈框架，我们通过两阶段的微调开发了AMOR：预热和适应。前者使用从各种公共数据集自动构建的示例对LLM进行微调，使AMOR能够在不同的知识环境中泛化，后者使用过程反馈将AMOR量身定制到特定领域。在多个领域上进行了广泛的实验。

    The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across mult
    
[^22]: 英国女王并非英格兰的女王：关于PLMs中缺乏事实一致性的问题

    The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs

    [https://rss.arxiv.org/abs/2402.01453](https://rss.arxiv.org/abs/2402.01453)

    本研究探讨了PLMs中事实知识的一致性问题，结果显示PLMs在使用提示时具有较低的一致性，但通过添加证据段落可以显著提高。这表明PLMs需要进一步增强以处理从事实中检索信息的能力。

    

    预训练语言模型（PLMs）中编码的事实知识丰富了其表示，并证明了它们作为知识库的使用合理性。先前的研究侧重于通过衡量PLMs能够在给定主题和关系的情况下正确预测对象实体的频率，以及通过优化查询PLMs时使用的提示来改进事实检索。在本研究中，我们考虑了一个补充性的方面，即PLMs中事实知识的一致性，即在PLMs通过对对象实体的初始预测后，能够多少次预测到主题实体。这超越了评估PLMs所知的程度，而是关注其内部知识状态。我们的结果表明，PLMs在使用手动编写的、优化的和改写的提示时具有较低的一致性，但包含证据段落可以显著提高一致性。这表明，PLMs没有模拟逆关系的能力，需要进一步增强以处理从事实中检索信息的能力。

    Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from th
    
[^23]: 多样性对群体决策的影响

    The effect of diversity on group decision-making

    [https://rss.arxiv.org/abs/2402.01427](https://rss.arxiv.org/abs/2402.01427)

    研究探讨认知多样性及其对群体决策成功的影响。通过分析500个团队讨论的对话记录，我们发现较大的认知多样性与更成功的群体决策相关。

    

    我们研究了认知多样性的不同方面以及其对群体协商成功的影响。我们使用了500个小型在线团队讨论瓦索卡片选择任务的对话记录进行评估 - DeliData语料库。利用该语料库，我们进行了定量分析，评估了认知多样性的三种不同度量。首先，我们分析了群体规模作为多样性的代理度量的影响。其次，我们评估了初始想法池的大小对结果的影响。最后，我们通过分析讨论的解决方案、讨论模式以及谈话探究如何改善这些特征来研究讨论内容。尽管群体因为加重偏见而名声不佳，我们表明小团队可以通过对话克服直觉偏见，提高个体决策能力。在大样本和不同操作中，我们始终发现认知多样性与群体协商的成功性有关

    We explore different aspects of cognitive diversity and its effect on the success of group deliberation. To evaluate this, we use 500 dialogues from small, online groups discussing the Wason Card Selection task - the DeliData corpus. Leveraging the corpus, we perform quantitative analysis evaluating three different measures of cognitive diversity. First, we analyse the effect of group size as a proxy measure for diversity. Second, we evaluate the effect of the size of the initial idea pool. Finally, we look into the content of the discussion by analysing discussed solutions, discussion patterns, and how conversational probing can improve those characteristics.   Despite the reputation of groups for compounding bias, we show that small groups can, through dialogue, overcome intuitive biases and improve individual decision-making. Across a large sample and different operationalisations, we consistently find that greater cognitive diversity is associated with more successful group deliber
    
[^24]: 各实体的不同偏好：探究命名实体标注中的人类标签变化

    Different Tastes of Entities: Investigating Human Label Variation in Named Entity Annotations

    [https://rss.arxiv.org/abs/2402.01423](https://rss.arxiv.org/abs/2402.01423)

    本论文研究了命名实体标注中的人类标签变化的原因，发现文本歧义和人为指南更改是高质量修订中不同标注的主要因素，并验证了多样化标注用于理解命名实体歧义的可行性和必要性。

    

    命名实体识别（NER）是一项重要的信息提取任务，有着悠久的传统。尽管最近的研究通过重新标注的努力来解决注释错误，但人类标签变异的来源，如文本歧义、注释错误或指南分歧，知之甚少。这在高质量的数据集和英语CoNLL03之外尤为突出。本文研究了三种语言（英语、丹麦语和巴伐利亚语）的专家标注的命名实体数据集中的不一致性。我们显示，文本歧义和人为指南更改是高质量修订中不同标注的主要因素。我们调查了学生对一部分困难实体的标注，并从分布角度验证了多样化标注用于理解命名实体歧义的可行性和必要性。

    Named Entity Recognition (NER) is a key information extraction task with a long-standing tradition. While recent studies address and aim to correct annotation errors via re-labeling efforts, little is known about the sources of human label variation, such as text ambiguity, annotation error, or guideline divergence. This is especially the case for high-quality datasets and beyond English CoNLL03. This paper studies disagreements in expert-annotated named entity datasets for three languages: English, Danish, and Bavarian. We show that text ambiguity and artificial guideline changes are dominant factors for diverse annotations among high-quality revisions. We survey student annotations on a subset of difficult entities and substantiate the feasibility and necessity of manifold annotations for understanding named entity ambiguities from a distributional perspective.
    
[^25]: 上下文感知机器翻译的序列缩短

    Sequence Shortening for Context-Aware Machine Translation

    [https://rss.arxiv.org/abs/2402.01416](https://rss.arxiv.org/abs/2402.01416)

    本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。

    

    上下文感知机器翻译旨在通过将周围的句子作为上下文来改进句子的翻译。为实现这一目标，已经应用了两种主要架构，即基于串联的单编码器和多编码器模型。在这项研究中，我们展示了多编码器架构的一个特殊情况，在下一步中重用源句子的潜在表示作为上下文，可以在对比数据集上实现更高的准确性（模型必须对提供的句子中的正确翻译进行排序），并且与单编码器和多编码器方法相比具有可比较的BLEU和COMET分数。此外，我们研究了将缓存表示应用于序列缩短。我们测试了三种基于汇聚的缩短技术，并引入了两种新方法——潜在分组和潜在选择，其中网络学习将令牌分组或选择要缓存为上下文。我们的实验表明，缓存表示的序列缩短方法可以进一步提高翻译质量。

    Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th
    
[^26]: 关于测量文档级机器翻译系统中上下文利用的研究

    On Measuring Context Utilization in Document-Level MT Systems

    [https://rss.arxiv.org/abs/2402.01404](https://rss.arxiv.org/abs/2402.01404)

    提出了一种补充基于准确度评估的上下文利用度度量方法，表明扰动分析是一种有效的总体上下文利用度的度量方法，同时提出了测量支持性上下文对处理上下文相关话语现象贡献程度的方法。

    

    文档级翻译模型通常使用一般的度量标准（如BLEU）进行评估，但这些度量标准无法提供关于上下文的好处的信息。目前关于上下文感知评估的工作，如对比方法，仅仅测量需要上下文以消除歧义的单词的翻译准确度。这样的测量方法无法揭示翻译模型是否正确地使用了支持性上下文。我们提议在基于准确度的评估中补充上下文利用度的度量标准。我们发现扰动分析（比较在提供正确上下文和随机上下文的情况下模型的性能）是一种有效的总体上下文利用度的度量方法。对于更细粒度的现象特定评估，我们提出测量支持性上下文对处理上下文相关话语现象的贡献程度。我们表明自动注释的支持性上下文与人工注释的上下文得出类似的结论，并可作为人工注释无法覆盖的情况下的替代品。

    Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models' performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annota
    
[^27]: StepCoder: 使用强化学习从编译器反馈中改进代码生成

    StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback

    [https://rss.arxiv.org/abs/2402.01391](https://rss.arxiv.org/abs/2402.01391)

    StepCoder是一个基于强化学习的代码生成框架，通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，并使用细粒度优化来提高模型的效果。

    

    大型语言模型的发展极大地推动了代码生成领域。以前的工作将强化学习与编译器反馈结合起来，探索语言模型的输出空间，以提高代码生成质量。然而，为了满足复杂的人类要求，语言模型生成的代码往往很长，这使得强化学习的探索成为一项挑战。此外，由于单元测试可能无法涵盖复杂代码，使用这些未执行的代码片段来优化语言模型是无效的。为了解决这些问题，我们引入了StepCoder，这是一个用于代码生成的新型强化学习框架，包含两个主要组件：CCCS通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，而FGO通过屏蔽未执行的代码段来提供细粒度优化，仅优化模型。此外，我们还构建了APPS+数据集用于强化学习训练，该数据集经过手动验证确保质量。

    The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ens
    
[^28]: 基于LLM的自然语言生成评估：现状与挑战

    LLM-based NLG Evaluation: Current Status and Challenges

    [https://rss.arxiv.org/abs/2402.01383](https://rss.arxiv.org/abs/2402.01383)

    这项调研介绍了基于大型语言模型（LLM）的自然语言生成（NLG）评估方法的现状，包括基于LLM导出的指标、引导LLM和使用带有标记评估数据的LLM微调，并讨论了人类与LLM的合作。同时指出了该领域的一些挑战和未来研究方向。

    

    在人工智能领域，评估自然语言生成（NLG）是一个至关重要但挑战重重的问题。传统的评估指标主要通过系统输出和参考文本之间的内容（如n-gram）重叠度来捕捉，远远不够令人满意，而近年来，大型语言模型（LLM）如ChatGPT在NLG评估方面展现出巨大潜力。已经提出了基于LLM的各种自动评估方法，包括基于LLM导出的指标、引导LLM和使用带有标记评估数据的LLM微调。在这项调研中，我们首先给出了基于LLM的NLG评估方法的分类法，并分别讨论了它们的优势和劣势。我们还讨论了人类与LLM的合作用于NLG评估。最后，我们讨论了该领域中的几个待解决的问题，并指出了未来的研究方向。

    Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.
    
[^29]: LoTR: 低张量秩权重自适应

    LoTR: Low Tensor Rank Weight Adaptation

    [https://rss.arxiv.org/abs/2402.01376](https://rss.arxiv.org/abs/2402.01376)

    LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。

    

    在本文中，我们将大型语言模型（LLM）上的低秩适应（LoRA）思想推广和扩展，这些模型基于Transformer架构。广泛使用的LoRA类方法是基于梯度更新的矩阵分解。我们引入了LoTR，一种新颖的LLM参数高效调优方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器都由三个矩阵的乘积构成，而张量结构是由这个乘积的左右乘子在层之间共享引起的。通过对低秩张量表示的一系列层同时压缩，LoTR能够比LoRA在特别是对于深层模型具有更好的参数效率。此外，核心张量不依赖于原始权重维度，可以任意缩小，从而实现非常廉价和快速的下游调优。

    In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
    
[^30]: 跳入分歧点：探究主题内和跨主题泛化之间的差距

    Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization

    [https://rss.arxiv.org/abs/2402.01375](https://rss.arxiv.org/abs/2402.01375)

    本研究通过分析不同LMs的探究实验，首次展示了主题内和跨主题泛化差距的原因，并表明嵌入空间的稳健性和LMs之间通用泛化差距的显著变化。我们的研究还发现多样的预训练目标、架构规范化或数据去重对于增强LMs的稳健性和减小泛化差距有积极作用。

    

    预训练语言模型（LMs）在主题内的设定中表现良好，这里训练和测试数据来自相同的主题。然而，在跨主题的情况下，例如枪支管制，它们在测试数据来自不同主题时面临挑战。本研究通过三个探究实验证明了不同LMs之间主题内和跨主题泛化差距的原因，并首次展示了嵌入空间的稳健性和通用泛化差距在LMs之间显著变化。此外，我们还评估了更大的LMs，并强调了我们分析对于最新模型的相关性。总的来说，多样的预训练目标、架构规范化或数据去重都有助于更稳健的LMs并减小泛化差距。我们的研究为深入理解和比较不同泛化场景下的语言模型做出了贡献。

    Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control. This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap. Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs. Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models. Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.
    
[^31]: 大型语言模型的持续学习: 一项综述

    Continual Learning for Large Language Models: A Survey

    [https://rss.arxiv.org/abs/2402.01364](https://rss.arxiv.org/abs/2402.01364)

    这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。

    

    由于其庞大的规模导致训练成本高昂，大型语言模型(LLMs)不易频繁重新训练。然而，更新是必要的，以赋予LLMs新的技能，并使其与快速发展的人类知识保持同步。本文对LLMs的持续学习最新研究进行了综述。鉴于LLMs的独特性，我们以一种新颖的多阶段分类方案对持续学习技术进行了分类，涉及持续预训练、指令调整和对齐等方面。我们将LLMs的持续学习与在规模较小的模型中使用的简单适应方法以及其他增强策略(如检索增强生成和模型编辑)进行了对比。此外，根据对基准和评估的讨论，我们确定了这一重要任务面临的几个挑战和未来工作方向。

    Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
    
[^32]: 医疗索赔的可验证性分析：分析实体和关系属性以进行事实验证

    What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation Properties for Fact Verification

    [https://rss.arxiv.org/abs/2402.01360](https://rss.arxiv.org/abs/2402.01360)

    本论文通过分析医疗索赔的实体和关系属性，研究了影响其可验证性的因素，并建立了一个科学事实验证的语料库。

    

    如果找不到证据，生物医学索赔验证将失败。在这些情况下，事实核查结果未知，索赔无法验证。为了改进这一情况，我们需要了解是否有任何索赔属性会影响其可验证性。在这项工作中，我们假设实体和关系定义了生物医学索赔解剖学的核心变量，并分析它们的属性是否有助于区分可验证和不可验证的索赔。在与经过训练的注释专家进行的研究中，我们要求他们找到生物医学索赔的证据，并观察他们如何改进证据搜索的查询。这导致了第一个用主题-关系-客体三元组、证据文档和事实核查结果（BEAR-Fact语料库）进行科学事实验证的语料库。我们发现（1）发现否定性索赔的证据（例如，X不导致Y）特别具有挑战性。此外，我们发现注释者主要通过添加来处理查询。

    Biomedical claim verification fails if no evidence can be discovered. In these cases, the fact-checking verdict remains unknown and the claim is unverifiable. To improve upon this, we have to understand if there are any claim properties that impact its verifiability. In this work we assume that entities and relations define the core variables in a biomedical claim's anatomy and analyze if their properties help us to differentiate verifiable from unverifiable claims. In a study with trained annotation experts we prompt them to find evidence for biomedical claims, and observe how they refine search queries for their evidence search. This leads to the first corpus for scientific fact verification annotated with subject-relation-object triplets, evidence documents, and fact-checking verdicts (the BEAR-Fact corpus). We find (1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is particularly challenging. Further, we see that annotators process queries mostly by adding
    
[^33]: 描述图像的“快慢”: 量化和预测视觉语言过程中人类信号的变化

    Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes

    [https://rss.arxiv.org/abs/2402.01352](https://rss.arxiv.org/abs/2402.01352)

    这项研究探索了图像描述中人类行为的变化，并发现图像的属性与这些变化相关。预训练模型可以在一定程度上捕捉到这种变化，但仍存在偏差。

    

    图像的属性与人们在描述图像时的行为之间存在复杂的关系。这种行为表现出丰富的变化，如眼动和人们开始描述图像的时机。尽管视觉语言变异的这些信号具有重要价值，但在当前预训练模型的训练过程中，它们几乎被忽视，这促使进一步的调查。通过使用同时收集的荷兰图像描述语料库和眼动数据，我们探索了视觉语言信号变化的本质，并发现它们之间存在相关性。鉴于这个结果，我们假设变化部分源于图像的属性，并探索预训练视觉编码器所编码的图像表示能否捕捉到这种变化。我们的研究结果表明，预训练模型在一定程度上能够做到这一点，这表明模型缺乏对什么使刺激复杂的偏好。

    There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for 
    
[^34]: 超越答案：对于评估大型语言模型中多选题回答的合理性的回顾

    Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models

    [https://rss.arxiv.org/abs/2402.01349](https://rss.arxiv.org/abs/2402.01349)

    对于评估大型语言模型中多选题回答的合理性进行了回顾，发现当前基于多选题回答的基准可能无法充分捕捉大型语言模型的真实能力。

    

    在自然语言处理领域，大型语言模型（LLMs）引发了一场范式转变，显著提升了自然语言生成任务的性能。尽管取得了这些进展，对LLMs的全面评估仍然是社区面临的必然挑战。最近，将多选题回答（MCQA）作为LLMs的基准已经引起了广泛关注。本研究调查了MCQA作为LLMs评估方法的合理性。如果LLMs真正理解问题的语义，它们的性能应该在从相同问题派生的各种配置上表现一致。然而，我们的实证结果表明LLMs的响应一致性存在显著差异，我们将之定义为LLMs的响应可变性综合征（REVAS），这表明目前基于MCQA的基准可能无法充分捕捉LLMs的真实能力，强调了对更合适的评估方法的需要。

    In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
    
[^35]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^36]: 两种处理波兰文本时序规范化的方法

    Two Approaches to Diachronic Normalization of Polish Texts

    [https://rss.arxiv.org/abs/2402.01300](https://rss.arxiv.org/abs/2402.01300)

    本文讨论了两种处理波兰文本时序规范化的方法，一种是基于规则的，一种是基于神经网络的。实验证明，在目前的研究阶段，基于规则的方法在大部分数据集上优于神经网络方法，但在实际应用中，两种方法都有各自的优点和缺点。

    

    本文讨论了处理波兰文本时序规范化的两种方法：一种是基于一组手工模式的规则方法，另一种是基于文本到文本转换转换器架构的神经规范化模型。文章详细讨论了为该任务准备的训练和评估数据，以及进行比较提出的规范化解决方案的实验。进行了定量和定性分析。结果表明，目前阶段探索该问题时，基于规则的方法在4个准备的数据集变体中有3个优于神经方法，尽管在实践中，这两种方法都具有各自的优劣。

    This paper discusses two approaches to the diachronic normalization of Polish texts: a rule-based solution that relies on a set of handcrafted patterns, and a neural normalization model based on the text-to-text transfer transformer architecture. The training and evaluation data prepared for the task are discussed in detail, along with experiments conducted to compare the proposed normalization solutions. A quantitative and qualitative analysis is made. It is shown that at the current stage of inquiry into the problem, the rule-based solution outperforms the neural one on 3 out of 4 variants of the prepared dataset, although in practice both approaches have distinct advantages and disadvantages.
    
[^37]: MLLMs能否进行上下文学习的文本到图像转换？

    Can MLLMs Perform Text-to-Image In-Context Learning?

    [https://rss.arxiv.org/abs/2402.01293](https://rss.arxiv.org/abs/2402.01293)

    本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。

    

    从大型语言模型（LLMs）发展到多模式大型语言模型（MLLMs）推动了将上下文学习（ICL）扩展到多模式的研究。现有的研究主要集中在图像到文本的ICL上。然而，文本到图像的ICL（T2I-ICL）具有独特的特性和潜在的应用，但仍然少有研究。为了填补这个空白，我们正式定义了T2I-ICL任务，并提出了CoBSAT，第一个包含十个任务的T2I-ICL基准数据集。利用我们的数据集评估了六个最先进的MLLMs，我们发现MLLMs在解决T2I-ICL问题时面临着相当大的困难。我们确定了多模态和图像生成的固有复杂性是主要挑战。为了克服这些挑战，我们探索了微调和思维链提示等策略，并取得了显著的改进。我们的代码和数据集可以在\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}上获得。

    The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
    
[^38]: 人类与机器：逻辑、真实性与ChatGPT

    The Human and the Mechanical: logos, truthfulness, and ChatGPT

    [https://rss.arxiv.org/abs/2402.01267](https://rss.arxiv.org/abs/2402.01267)

    本论文通过语义论证，讨论了是否可以称之为“机械思维”，以及ChatGPT模型能否实现该思维。研究发现，“机械思维”缺乏与现实相关的证据和个人信念，因此无法形成对世界的信念和真实性判断。

    

    本论文探讨了是否适当地谈论“机械思维”，以及ChatGPT模型是否可以被视为实现了这一点。我们的论文在当前的讨论中添加了一个语义论证。人类断言的行为需要形成一个真实性判断。使用情态动词修饰断言（约翰一定在家）和使用主观元素（约翰明显在家）表明说话者正在操纵她的判断，在合作的语境中，意图将她的认知状态对话方透明化。真实性判断是基于两个组成部分形成的：（i）与现实相关的证据（外生证据）和（ii）与偏好和个人信念相关的内在证据。而“机械思维”缺乏这两个组成部分：（i）它们与现实无关，（ii）没有内在证据。因此它们缺乏对世界的信念形成和真实性判断的能力。

    The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that. Our paper adds a semantic argument to the current debate. The act of human assertion requires the formation of a veridicality judgment. Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee. Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs. `Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence. Therefore they lack the ability to form a belief about the world and a veridicality judgmen
    
[^39]: 针对少样本嵌套命名实体识别的上下文学习

    In-Context Learning for Few-Shot Nested Named Entity Recognition

    [https://rss.arxiv.org/abs/2402.01182](https://rss.arxiv.org/abs/2402.01182)

    本研究提出了一种针对少样本嵌套命名实体识别的上下文学习框架，并通过引入EnDe检索器和对比学习机制改进了示例演示选择，实现了高质量的演示示例。在多个数据集上进行的实验验证了系统的有效性。

    

    在嵌套命名实体识别（NER）中，实体与实体相互嵌套，因此需要更多的数据注释来解决。这促使发展出少样本嵌套NER，其中具有上下文学习（ICL）的预训练语言模型的普及提供了有前途的解决方案。在这项工作中，我们引入了一个有效和创新的ICL框架，用于少样本嵌套NER的设定。我们通过设计一种新颖的示例演示选择机制EnDe retriever改进了ICL提示。在EnDe检索器中，我们利用对比学习进行三种类型的表示学习，分别是语义相似度、边界相似度和标签相似度，以生成高质量的演示示例。对三个嵌套NER和四个扁平NER数据集进行了大量实验，证明了我们系统的有效性。

    In nested Named entity recognition (NER), entities are nested with each other, and thus requiring more data annotations to address. This leads to the development of few-shot nested NER, where the prevalence of pretrained language models with in-context learning (ICL) offers promising solutions. In this work, we introduce an effective and innovative ICL framework for the setting of few-shot nested NER. We improve the ICL prompt by devising a novel example demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ contrastive learning to perform three types of representation learning, in terms of semantic similarity, boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments over three nested NER and four flat NER datasets demonstrate the efficacy of our system.
    
[^40]: 为利用外部语料进行知识密集型任务而构建的统一语言模型

    Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus

    [https://rss.arxiv.org/abs/2402.01176](https://rss.arxiv.org/abs/2402.01176)

    本研究提出了一个统一的语言模型，通过无缝集成生成式检索、闭式生成和RAG，利用外部语料处理各种知识密集型任务。

    

    大型语言模型（LLMs）的出现展示了它们在各个领域的有效性，然而在需要外部知识来源的知识密集型任务中，它们往往会产生虚构的结果。为了提高语言模型的事实准确性，检索增强生成（RAG）成为了一种流行的解决方案。然而，传统的检索模块通常依赖于大规模的文档索引，这可能与生成任务相脱离。通过生成式检索（GR）方法，语言模型可以通过直接生成相关文档标识符（DocIDs）来实现更好的检索性能。然而，GR与下游任务之间的关系以及LLMs在GR中的潜力尚未得到探索。在本文中，我们提出了一个统一的语言模型，通过无缝集成生成式检索、闭式生成和RAG，利用外部语料处理各种知识密集型任务。

    The advent of large language models (LLMs) has showcased their efficacy across various domains, yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources. To improve factual accuracy of language models, retrieval-augmented generation (RAG) has emerged as a popular solution. However, traditional retrieval modules often rely on large-scale document indexes, which can be disconnected from generative tasks. Through generative retrieval (GR) approach, language models can achieve superior retrieval performance by directly generating relevant document identifiers (DocIDs). However, the relationship between GR and downstream tasks, as well as the potential of LLMs in GR, remains unexplored. In this paper, we present a unified language model that utilizes external corpus to handle various knowledge-intensive tasks by seamlessly integrating generative retrieval, closed-book generation, and RAG. In order to achieve effective retrieval and generati
    
[^41]: 通过嵌入相似性实现高效的提示缓存

    Efficient Prompt Caching via Embedding Similarity

    [https://rss.arxiv.org/abs/2402.01173](https://rss.arxiv.org/abs/2402.01173)

    本论文通过嵌入相似性的提示缓存方法来提高大规模语言模型(LMMs)的推理效率，并提出一种蒸馏方法来优化现有嵌入以获得更好的缓存预测准确性。

    

    大规模语言模型(LLMs)在许多自然语言处理(NLP)任务中取得了巨大成功。然而，在推理过程中，它面临着资源消耗的挑战。本文旨在通过提示缓存来提高LLMs的推理效率，即，如果当前提示可以由前一个提示的同样回答而得到回答，就可以直接利用该前一个回答而不调用LLMs。具体而言，我们关注通过嵌入相似性来提升单轮问答任务的缓存预测准确性。目前已有的提示嵌入主要关注两个提示是否语义相似，这与同样的回答是否可以回答它们并不等价。因此，我们提出了一种基于蒸馏的方法来优化现有的嵌入以获得更好的缓存预测。理论上，我们在不同类型的损失函数下提供了我们方法收敛的有限样本保证。实验结果表明，我们的方法在推理效率方面取得了显著的改进。

    Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empi
    
[^42]: 流式序列转导通过动态压缩

    Streaming Sequence Transduction through Dynamic Compression

    [https://rss.arxiv.org/abs/2402.01172](https://rss.arxiv.org/abs/2402.01172)

    STAR是一种新型的Transformer模型，通过动态压缩和优化延迟、内存占用和质量，实现对流的高效序列转导，并在自动语音识别领域表现出色。

    

    我们引入了STAR（带有锚定表示的流式转导），这是一种基于Transformer的新型模型，旨在实现对流的高效序列转导。STAR动态地对输入流进行分段，创建压缩的锚定表示，实现近乎无损的压缩（12倍）在自动语音识别（ASR）中，并优于现有方法。此外，STAR在同时进行语音到文本任务中展示出优越的分割和延迟-质量折衷，优化延迟、内存占用和质量。

    We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
    
[^43]: LLM-Detector: 使用开源LLM指令调整来改进AI生成的中文文本检测

    LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning

    [https://rss.arxiv.org/abs/2402.01158](https://rss.arxiv.org/abs/2402.01158)

    本文提出了一个名为LLM-Detector的方法，使用开源LLM指令调整来改进AI生成的中文文本检测。通过提供混合了人工编写句子和LLMs润饰句子的数据集，并利用LLMs在预训练期间获得的知识，我们能够在文档级和句子级上实现高效的文本检测。实验结果表明，该方法优于现有的基于BERT和RoBERTa的模型。

    

    ChatGPT和其他通用大型语言模型（LLMs）取得了显著的成功，但也引发了有关滥用人工智能生成文本的担忧。现有的基于BERT和RoBERTa的AI生成文本检测模型容易在领域内过度拟合，导致领域外（OOD）的检测性能差。本文首先收集了人工专家和9种LLMs生成的中文文本回答，针对多个领域的问题，并进一步创建了一个混合了人工编写句子和LLMs润饰句子的数据集。然后，提出了LLM-Detector，一种通过LLMs的指令调整实现文档级和句子级文本检测的新方法。我们的方法利用LLMs在预训练期间获得的丰富知识，使其能够检测它们生成的文本。指令调整将模型的响应与用户的期望文本检测任务保持一致。实验结果表明，之前的方法出现了困难。

    ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggl
    
[^44]: CABINET: 表格问答系统的基于内容相关性的噪声降低方法

    CABINET: Content Relevance based Noise Reduction for Table Question Answering

    [https://rss.arxiv.org/abs/2402.01155](https://rss.arxiv.org/abs/2402.01155)

    CABINET是一个用于表格问答系统的基于内容相关性的噪声降低方法，通过加权处理表格内容并生成解析语句，使得大型语言模型能够专注于相关表格数据而抑制无关信息的干扰。

    

    大型语言模型（LLMs）的表格理解能力通过对表格的问答任务进行了广泛研究。通常，只有表格的一小部分与给定问题的答案相关。不相关的部分会产生噪声和干扰信息，导致LLMs的性能下降。为了缓解这个问题，我们提出了CABINET（基于内容相关性的表格问答噪声降低方法）- 一个能够让LLMs专注于相关表格数据并抑制无关信息的框架。CABINET包括一个无监督的相关性评分器（URS），与问答LLM差异性训练，根据其与输入问题的相关性对表格内容进行加权处理后再输入问答LLM（QA LLM）。为了进一步辅助相关性评分器，CABINET利用一个弱监督模块生成一个解析语句，描述行和列的标准。

    Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns r
    
[^45]: "AccentFold：通过零样本ASR适应探索非洲口音之旅"

    AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents

    [https://rss.arxiv.org/abs/2402.01152](https://rss.arxiv.org/abs/2402.01152)

    "AccentFold"是一种通过利用学习到的口音嵌入之间的空间关系来改进ASR的方法，通过探索性分析语音嵌入，揭示非洲口音之间的有趣关系，并发现了Ethnologue先前未经描述的口音关系。通过实证评估，证明了该方法的有效性。

    

    尽管语音识别方面取得了一些进展，但口音语音依然具有挑战性。虽然先前的方法主要集中在建模技术或创建有口音的语音数据集上，但由于非洲口音的多样性和相关的预算限制，收集到足够的数据仍然不可行。为了解决这些挑战，我们提出了一种方法——"AccentFold"，它利用了学习到的口音嵌入之间的空间关系来改进下游的自动语音识别（ASR）。我们对代表100多种非洲口音的语音嵌入进行了探索性分析，揭示了有趣的口音空间关系，突显出地理和亲缘相似性，捕捉到从语音中经验性地学习到的一致的音系和形态学规律。此外，我们还发现了Ethnologue先前未经描述的口音关系。通过实证评估，我们证明了该方法的有效性。

    Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose \textit{AccentFold}, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness o
    
[^46]: 多智能体对话推荐系统

    A Multi-Agent Conversational Recommender System

    [https://rss.arxiv.org/abs/2402.01135](https://rss.arxiv.org/abs/2402.01135)

    本文提出了一个多智能体对话推荐系统（MACRS），它通过设计一个多智能体行动规划框架来控制对话流程，并基于LLM生成多个候选响应。这个系统能够提高对话推荐系统的性能，并利用用户反馈来更好地建模用户偏好。

    

    由于大型语言模型（LLM）在与用户进行流畅的多轮对话方面具有强大的能力，它们有潜力进一步提高对话推荐系统（CRS）的性能。与LLM擅长的无目的闲聊不同，CRS有一个明确的目标。因此，必须控制LLM中的对话流程，以成功向用户推荐适当的物品。此外，CRS中的用户反馈可以帮助系统更好地建模用户偏好，但现有研究忽视了这一点。然而，简单地提示LLM进行对话推荐无法解决上述两个关键挑战。在本文中，我们提出了一种包含两个关键模块的多智能体对话推荐系统（MACRS）。首先，我们设计了一个多智能体行动规划框架，可以基于四个基于LLM的智能体控制对话流程。这个合作的多智能体框架将基于不同的方案生成各种候选响应。

    Due to strong capabilities in conducting fluent, multi-turn conversations with users, Large Language Models (LLMs) have the potential to further improve the performance of Conversational Recommender System (CRS). Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address the above two key challenges.   In this paper, we propose Multi-Agent Conversational Recommender System (MACRS) which contains two essential modules. First, we design a multi-agent act planning framework, which can control the dialogue flow based on four LLM-based agents. This cooperative multi-agent framework will generate various candidate responses based on different 
    
[^47]: Pok\'eLLMon：一个用于使用大型语言模型的Pok\'emon对战的与人类能力相当的代理机器人

    Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models

    [https://rss.arxiv.org/abs/2402.01118](https://rss.arxiv.org/abs/2402.01118)

    Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。

    

    我们介绍了\textsc{Pok\'eLLMon}，这是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人，同时以Pok\'emon对战为例进行了证明。 \textsc{Pok\'eLLMon}的设计采用了三个关键策略：（i）上下文强化学习，即即时使用从对战中获得的基于文本的反馈来逐步完善策略；（ii）知识增强生成，即检索外部知识以对抗产生幻觉现象，并使代理机器人能够及时正确地行动；（iii）一致的行动生成，以减轻代理机器人面对强敌时的“惊慌换手”现象，使其可以逃避战斗。我们展示了与人类进行的在线对战中，\textsc{Pok\'eLLMon}采用与人类类似的战斗策略和及时决策，其在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。我们的实现和可玩的战斗日志可以在以下链接中找到：\url{https://gith

    We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
    
[^48]: DTS-SQL: 使用小型语言模型的分解式文本到SQL

    DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models

    [https://rss.arxiv.org/abs/2402.01117](https://rss.arxiv.org/abs/2402.01117)

    DTS-SQL采用了一种分解式的方法，使用小型语言模型，有效地缩小了开源模型与专有模型之间的性能差距，提高了文本到SQL任务的执行准确性。

    

    对于文本到SQL任务，领先的模型严重依赖于专有的大型语言模型（LLMs），引发了数据隐私方面的担忧。缩小开源模型和专有模型之间的性能差距对于缓解这种依赖是至关重要的。为此，我们引入了一种新颖的两阶段微调方法，将任务分解为两个较简单的任务。通过对两个大型跨领域数据集和两个小型LLM的全面评估，我们展示了这种方法提高了执行准确性，提升了3到7个百分点，有效地使开源模型的性能与专有模型相一致。

    Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.
    
[^49]: 通过文本表示解读心内电图

    Interpretation of Intracardiac Electrograms Through Textual Representations

    [https://rss.arxiv.org/abs/2402.01115](https://rss.arxiv.org/abs/2402.01115)

    本研究首次利用预训练的语言模型，通过文本表示的方式对心内电图进行插值和房颤分类。相比其他表示方法，我们的方法在房颤分类上表现出竞争性的性能。

    

    理解房颤(AFib)的不规则电活动一直是心电图学中的一个重要挑战。对于严重的房颤病例，进行导管消融以获取心内电图(EGMs)。EGMs提供了心脏电活动的复杂细节和局部化信息，是可解释的心脏研究的理想模式。近年来，人工智能(AI)的进展使得一些研究可以利用深度学习框架来解释房颤中的EGMs。此外，语言模型(LMs)在能够推广到未见过的领域方面表现出了出色的性能，尤其在医疗领域。在本研究中，我们首次利用预训练的LMs来通过掩码语言建模对EGM插值和房颤分类进行微调。我们将EGM形式化为文本序列，并与其他表示方法相比，在房颤分类方面展示了竞争性的性能。最后，我们提供了全面的解释性分析。

    Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
    
[^50]: 疫苗：针对大规模语言模型的干扰感知对齐技术

    Vaccine: Perturbation-aware Alignment for Large Language Model

    [https://rss.arxiv.org/abs/2402.01109](https://rss.arxiv.org/abs/2402.01109)

    疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    

    作为一种新的微调即服务范 paradigm，大型语言模型 (LLM) 为用户上传的一小部分有害数据提供了新的攻击面，这些数据很容易欺骗微调过程从而产生对齐失效的模型。我们进行了实证分析，揭示了一种可能导致对齐失效的有害嵌入漂移现象。受到我们的发现启发，我们提出了疫苗 (Vaccine) ，一种针对干扰感知的对齐技术，以减轻用户微调的安全风险。疫苗的核心思想是通过在对齐阶段逐渐添加精心设计的扰动，产生不变的隐藏嵌入，从而使嵌入能够抵御来自未经消毒的用户数据的有害扰动。我们在开源主流LLM（如Llama2，Opt，Vicuna）上的实验结果表明，疫苗能够提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
    
[^51]: 多智能体系统中的推理能力：局限性、挑战和以人为中心的解决方案

    Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions

    [https://rss.arxiv.org/abs/2402.01108](https://rss.arxiv.org/abs/2402.01108)

    多智能体系统面临着利用大型语言模型的限制和挑战，需要引入推理能力作为统一的标准来实现系统的整合和优化。

    

    大型语言模型（LLMs）在各种任务中展现出卓越的性能，为在生产环境中利用它们带来了许多机遇和挑战。为了实现LLMs的实际采用，多智能体系统在企业平台中具有增强、整合和协调LLMs的巨大潜力，该平台利用现有专有数据和模型来解决复杂的现实任务。尽管这些系统取得了巨大的成功，但当前的方法依赖于狭窄、单一目标的优化和评估，往往忽视现实情景中的潜在约束，包括有限的预算、资源和时间。此外，解释、分析和调试这些系统要求不同的组件之间进行相互评估，但现有方法无法满足这种需求。在本论文中，我们引入了推理能力的概念作为一个统一的标准，以实现集成和优化多智能体系统。

    Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration o
    
[^52]: 让我们来谈判！谈判对话系统综述

    Let's Negotiate! A Survey of Negotiation Dialogue Systems

    [https://rss.arxiv.org/abs/2402.01097](https://rss.arxiv.org/abs/2402.01097)

    该论文综述了谈判对话系统的最新研究，包括基准、评估和方法论。还讨论了未来的发展方向，如多模态、多方和跨文化的谈判场景。这为社群提供了关于谈判对话系统的系统综述，并激发未来的研究。

    

    谈判是人类沟通中至关重要的能力。近年来，谈判对话系统的研究兴趣再度上升，其目标是创建智能代理，帮助人们解决冲突或达成协议。尽管已经有很多探索谈判对话系统的研究，但迄今为止还没有进行系统综述的工作。我们旨在填补这一空白，调查最近在谈判对话系统领域的研究，并涵盖文献中的基准、评估和方法论。我们还讨论了潜在的未来发展方向，包括多模态、多方和跨文化的谈判场景。我们的目标是为社群提供关于谈判对话系统的系统综述，并激发未来的研究。

    Negotiation is a crucial ability in human communication. Recently, there has been a resurgent research interest in negotiation dialogue systems, whose goal is to create intelligent agents that can assist people in resolving conflicts or reaching agreements. Although there have been many explorations into negotiation dialogue systems, a systematic review of this task has not been performed to date. We aim to fill this gap by investigating recent studies in the field of negotiation dialogue systems, and covering benchmarks, evaluations and methodologies within the literature. We also discuss potential future directions, including multi-modal, multi-party and cross-cultural negotiation scenarios. Our goal is to provide the community with a systematic overview of negotiation dialogue systems and to inspire future research.
    
[^53]: 使用有限领域数据进行廉价推理的专用语言模型

    Specialized Language Models with Cheap Inference from Limited Domain Data

    [https://rss.arxiv.org/abs/2402.01093](https://rss.arxiv.org/abs/2402.01093)

    本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。

    

    大型语言模型已成为一种多才多艺的工具，但在缺乏大规模推理预算和大规模领域内训练集的任务中应用起来具有挑战性。本研究对这些限制进行了形式化，并区分了四个重要的变量：预训练预算（用于在目标领域出现之前进行训练），专用预算（用于在目标领域出现之后进行训练），推理预算和领域内训练集大小。在这些设置中，我们比较了机器学习文献中的不同方法。受到推理成本的限制，我们发现比训练非常大的基本转换器模型的标准做法更好的替代方案。特别是，我们发现超网络和专家混合模型在大型预训练预算下具有更好的困惑度，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。

    Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
    
[^54]: 阅读推文之间：解读相互联系的杂合意识形态社区的立场

    Reading Between the Tweets: Deciphering Ideological Stances of Interconnected Mixed-Ideology Communities

    [https://rss.arxiv.org/abs/2402.01091](https://rss.arxiv.org/abs/2402.01091)

    本文研究了Twitter上关于2020年美国选举的讨论，通过识别相互作用的社区，引入了一种利用信息传递的新方法，微调语言模型来探索这些社区的微妙意识形态。通过与真实调查结果的比较，我们的方法显示出比现有基准线更高的一致性，突显了利用语言模型揭示杂合意识形态社区中复杂意识形态的潜力。

    

    自然语言处理的最新发展提高了我们理解在线社区的微妙世界观的能力。现有研究关注的是探索意识形态立场时将自由派和保守派视为不同的群体。然而，这未能考虑到有机形成的在线社区和它们之间的联系的微妙观点。在本文中，我们研究了关于2020年美国选举的Twitter讨论，以识别复杂的相互作用社区。充分利用这种相互关系，我们引入了一种新颖的方法，在微调语言模型（LMs）时利用信息传递来探索这些社区的微妙意识形态。通过比较由LMs生成的回复和真实世界的调查结果，我们的方法显示出比现有基准线更高的一致性，突显了在和跨相互联系的杂合意识形态社区中利用LMs揭示复杂意识形态的潜力。

    Recent advances in NLP have improved our ability to understand the nuanced worldviews of online communities. Existing research focused on probing ideological stances treats liberals and conservatives as separate groups. However, this fails to account for the nuanced views of the organically formed online communities and the connections between them. In this paper, we study discussions of the 2020 U.S. election on Twitter to identify complex interacting communities. Capitalizing on this interconnectedness, we introduce a novel approach that harnesses message passing when finetuning language models (LMs) to probe the nuanced ideologies of these communities. By comparing the responses generated by LMs and real-world survey results, our method shows higher alignment than existing baselines, highlighting the potential of using LMs in revealing complex ideologies within and across interconnected mixed-ideology communities.
    
[^55]: 用于多语言文档问答的大型语言模型评估方法

    Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer

    [https://rss.arxiv.org/abs/2402.01065](https://rss.arxiv.org/abs/2402.01065)

    本文研究了大型语言模型（LLMs）在多语言环境下的能力。研究结果表明，将原始语言的上下文、问题和答案翻译成高资源语言可以获得最佳效果。

    

    随着大型语言模型（LLMs）的广泛应用，本文研究了这些模型的多语言能力。我们的初步结果显示，将原始语言的上下文、问题和答案翻译成高资源语言产生了最好的结果。

    With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.
    
[^56]: 面向双重目标对话设置的计划驱动大型语言模型

    Plan-Grounded Large Language Models for Dual Goal Conversational Settings

    [https://rss.arxiv.org/abs/2402.01053](https://rss.arxiv.org/abs/2402.01053)

    本研究针对双重目标对话设置提出了一种新型的计划驱动大型语言模型，该模型能够在任意计划上基础对话，主动引导用户完成计划，并在系统行为上实施安全防护。

    

    训练大型语言模型（LLM）遵循用户指令已被证明可以为LLM提供充足的能力以流利地进行对话并与人类对齐。然而，在双向对话流动指令的混合倡议设置中，LLM如何引导以计划为基础的对话还不完全清楚，即LLM和用户彼此提供指令。在本文中，我们解决了双重目标混合倡议对话设置的问题，LLM不仅在任意计划上基础对话，还致力于满足流程计划和用户指令。LLM负责引导用户完成计划，同时适应新的情况，回答问题，并在需要时激活安全防护措施。我们提出了一种新颖的LLM，它基于流程计划来进行对话，可以主动参与对话，并在系统行为上实施安全防护。

    Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the 
    
[^57]: 生成、提炼和评估具有基础语言模型风格的动机式反思

    Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model

    [https://rss.arxiv.org/abs/2402.01051](https://rss.arxiv.org/abs/2402.01051)

    本文介绍了一种将基础语言模型中的反思生成能力提炼到较小模型中的方法，并且展示了该方法在生成反思方面的优越性能。

    

    大型基础语言模型能够在许多任务上高效执行，但由于其庞大的体积和专有所有权，在许多应用中很难部署。许多人会希望将基础模型的特定功能提炼成较小的模型，以便拥有和控制。在开发治疗性聊天机器人时，我们希望提炼一种称为反思倾听的能力，即治疗师生成对客户讲话的反思。这些反思要么重新陈述客户说过的话，要么将其与相关观察、思想或猜测联系起来，以鼓励和引导客户继续思考。在本文中，我们提出了一种从基础语言模型（GPT-4）中提炼反思生成的方法。我们首先展示了使用零样本提示，GPT-4可以以将近100%的成功率生成反思，优于所有先前的方法。使用由GPT-4生成的反思，我们进行了评估

    Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we f
    
[^58]: 充分利用分词器进行预训练和领域适应

    Getting the most out of your tokenizer for pre-training and domain adaptation

    [https://rss.arxiv.org/abs/2402.01035](https://rss.arxiv.org/abs/2402.01035)

    本文通过训练专用分词器，对分词器设计进行了消毒和分析，发现分词器的大小、正则表达式和训练数据对模型性能有重要影响，并提供了相应的超参数选择建议和切换分词器的方法。

    

    分词是现代LLM中鲜为人知且常被忽视的组成部分。大多数已发表的作品在所有实验中都使用同一个分词器，通常是从另一个模型借用而来的，并没有进行消融或分析来优化分词。此外，在微调基础模型时，分词器通常保持不变。在本文中，我们展示了分词器的大小、预标记正则表达式和训练数据对模型的生成速度、有效上下文大小、内存使用和下游性能均有重要影响。我们训练了专用的字节对编码分词器，并对分词器设计对代码生成任务（如HumanEval和MBPP）中LLM性能影响进行了广泛的消融，提供了分词器超参数选择和在预训练LLM中切换分词器的建议。我们在从头开始训练和从预训练模型中进行了实验，验证了它们对各种任务和模型的适用性。

    Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to 
    
[^59]: 跟着我重复：Transformer在复制任务上比状态空间模型更好

    Repeat After Me: Transformers are Better than State Space Models at Copying

    [https://rss.arxiv.org/abs/2402.01032](https://rss.arxiv.org/abs/2402.01032)

    这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。

    

    Transformer是序列建模的主要架构，但对于使用不依赖于序列长度的固定大小潜在状态的模型，也就是"广义状态空间模型" (GSSMs)，引起了越来越多的关注。在本文中，我们展示了虽然GSSMs在推理时间效率上有优势，但在需要从输入上下文复制的任务上，它们相对于transformer模型来说有限制。我们从对简单的字符串复制任务的理论分析开始，并证明了一个两层的transformer可以复制指数长度的字符串，而GSSMs由于其固定大小的潜在状态在根本上是有限制的。实证上，我们发现transformer在需要复制上下文的合成任务中，在效率和泛化性能上优于GSSMs。最后，我们评估了预训练的大型语言模型，并发现transformer模型在复制和检索上下文信息方面远远优于状态空间模型。

    Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
    
[^60]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^61]: 基于图的聚类用于跨时间和语言检测语义变化

    Graph-based Clustering for Detecting Semantic Change Across Time and Languages

    [https://rss.arxiv.org/abs/2402.01025](https://rss.arxiv.org/abs/2402.01025)

    这项研究提出了一种基于图的聚类方法，用于跨时间和语言中准确捕捉高频和低频词义的微妙变化，尤其解决了传统方法在捕捉低频词义方面的问题。该方法在SemEval2020四种语言的二元分类任务中表现出较好的性能。

    

    尽管上下文化嵌入在自然语言处理中占主导地位，但基于这些嵌入和聚类方法的检测语义变化的方法在性能上不如基于静态词嵌入的简单方法。这是因为聚类方法产生词义聚类的质量较差，难以捕捉低频词义，尤其是低频词义。这个问题阻碍了研究一个语言中的词义变化对另一个语言的影响。为了解决这个问题，我们提出了一种基于图的聚类方法，用于捕捉跨时间和语言中高频和低频词义的微妙变化，包括这些词义随时间的增加和减少。我们的实验结果表明，我们的方法在SemEval2020四种语言的二元分类任务中明显优于过去的方法。此外，我们展示了我们方法作为一种多功能可视化工具用于检测语义变化的能力。

    Despite the predominance of contextualized embeddings in NLP, approaches to detect semantic change relying on these embeddings and clustering methods underperform simpler counterparts based on static word embeddings. This stems from the poor quality of the clustering methods to produce sense clusters -- which struggle to capture word senses, especially those with low frequency. This issue hinders the next step in examining how changes in word senses in one language influence another. To address this issue, we propose a graph-based clustering approach to capture nuanced changes in both high- and low-frequency word senses across time and languages, including the acquisition and loss of these senses over time. Our experimental results show that our approach substantially surpasses previous approaches in the SemEval2020 binary classification task across four languages. Moreover, we showcase the ability of our approach as a versatile visualization tool to detect semantic changes in both int
    
[^62]: 面向领域独立的欺骗行为：一个新的分类和语言分析

    Domain-Independent Deception: A New Taxonomy and Linguistic Analysis

    [https://rss.arxiv.org/abs/2402.01019](https://rss.arxiv.org/abs/2402.01019)

    本论文提出了面向领域独立的欺骗行为的新分类和语言分析。首先，给出了对欺骗行为的新的计算定义，并提供了对其的分类系统。然后，分析了关于欺骗的语言线索的争议并提供了系统回顾的指导方针。最后，研究了常见的语言特征并提供了知识转移的证据。

    

    互联网经济和社会正淹没在欺骗性攻击中。这些攻击以假新闻、网络钓鱼和虚假招聘等多种形式呈现，我们称之为“欺骗领域”。机器学习和自然语言处理的研究人员一直在尝试通过设计特定领域的检测器来改善这个危险情况。只有少数最近的研究考虑到了领域独立的欺骗行为。我们收集了这些不同的研究线索，并研究了领域独立的欺骗行为。首先，我们提供了对欺骗行为的新的计算定义，并将其分解成一个新的分类系统。然后，我们分析了关于语言线索的争论，并提供了系统回顾的指导方针。最后，我们研究了常见的语言特征，并为不同形式的欺骗之间的知识转移提供了证据。

    Internet-based economies and societies are drowning in deceptive attacks. These attacks take many forms, such as fake news, phishing, and job scams, which we call ``domains of deception.'' Machine-learning and natural-language-processing researchers have been attempting to ameliorate this precarious situation by designing domain-specific detectors. Only a few recent works have considered domain-independent deception. We collect these disparate threads of research and investigate domain-independent deception. First, we provide a new computational definition of deception and break down deception into a new taxonomy. Then, we analyze the debate on linguistic cues for deception and supply guidelines for systematic reviews. Finally, we investigate common linguistic features and give evidence for knowledge transfer across different forms of deception.
    
[^63]: HR-MultiWOZ: 一种面向HR LLM Agent的任务导向对话数据集

    HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent

    [https://rss.arxiv.org/abs/2402.01018](https://rss.arxiv.org/abs/2402.01018)

    HR-MultiWOZ是一个面向HR LLM Agent的任务导向对话数据集，具有首个开源的HR对话数据集标记，用于NLP研究。

    

    最近大型语言模型（LLMs）的进展正在重塑自然语言处理（NLP）领域的任务。它们在人力资源（HR）领域的应用仍有待扩展，并且可能对一些耗时的任务很有益。例如，休假申请、医疗索赔和访问权限申请都是值得注意的例子，但它们绝不是唯一的例子。然而，上述发展必须应对一个关键的挑战，即构建高质量的训练数据集。一方面，大多数对话数据集解决的是客户问题，而不是员工问题。另一方面，获取与HR的对话可能引起隐私问题。为了解决这个问题，我们介绍了HR-Multiwoz，这是一个完全标注的数据集，包含550个涵盖10个HR领域的对话，用于评估LLM Agent。我们的工作具有以下贡献：(1) 这是HR领域的首个开源对话数据集，用于NLP研究。(2) 它提供了一个标记的、面向NLP任务的HR对话数据集。

    Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a det
    
[^64]: 一种信息论方法来分析自然语言处理分类任务

    An Information-Theoretic Approach to Analyze NLP Classification Tasks

    [https://rss.arxiv.org/abs/2402.00978](https://rss.arxiv.org/abs/2402.00978)

    本研究提供了一个信息论框架来分析文本分类任务中输入的影响，发现在更具挑战性的数据集中，上下文对输出的影响减小，同时语义含义对于分类任务的影响较大。

    

    理解输入对输出的重要性对许多任务都有帮助。本研究提供了一个信息论框架来分析文本分类任务中输入的影响。自然语言处理（NLP）任务采用单个元素输入或多个元素输入来预测输出变量，其中一个元素是一段文字。每个文字元素有两个组成部分：关联的语义含义和语言实现。选择了多项选择阅读理解（MCRC）和情感分类（SC）来展示该框架。对于MCRC，发现相对于问题的影响，上下文对输出的影响在更具挑战性的数据集上减小。特别是，更具挑战性的上下文允许问题的复杂性更大的变化。因此，测试创建者在设计多项选择问题时需要仔细考虑上下文的选择。对于SC，发现语义含义对于输出的影响较大。

    Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of 
    
[^65]: 使用Entity预训练GPT为KG问答生成SPARQL

    SPARQL Generation with Entity Pre-trained GPT for KG Question Answering

    [https://rss.arxiv.org/abs/2402.00969](https://rss.arxiv.org/abs/2402.00969)

    本文面向非程序员用户的KG问答问题，通过实体链接和GPT模型生成SPARQL查询，使用CWA预训练所有实体，实现了准确的SPARQL匹配率为62.703%。

    

    知识图谱的流行度在过去几年中迅速增长。人们可以通过互联网上的许多在线数据库查询这些知识。但是，如果非程序员用户能够访问他们想要知道的任何信息，那将是一个巨大的成就。为了解决这个问题，已经付出了很多努力，使用自然语言处理工具和通过许多挑战激励创造力。我们的方法重点是在自然语言问题上进行正确的实体链接，并训练一个GPT模型来从中创建SPARQL查询。我们成功地确定了这个任务中可能最难以在少数或零次尝试中解决的属性，并提出对所有实体进行预训练（在CWA下）以提高性能。在3次尝试中，我们在测试中获得了62.703%的准确的SPARQL匹配率，在实体链接挑战中获得了0.809的F1值，在问题回答挑战中获得了0.009的F1值。

    Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.
    
[^66]: 在大型语言和视觉模型中探索空间模式直觉

    Exploring Spatial Schema Intuitions in Large Language and Vision Models

    [https://rss.arxiv.org/abs/2402.00956](https://rss.arxiv.org/abs/2402.00956)

    通过再现心理语言学实验，研究发现大型语言模型（LLMs）尽管无具身性，却能够有效捕捉到人类对于语言中基本的空间构建块的隐含直觉，这对于理解语言和空间的相互作用具有重要意义。

    

    尽管大型语言模型（LLM）在人工智能研究中非常普遍，但关于LLM中的具身性问题还未得到充分探讨，这使它们不同于机器人中具体的具身系统，机器人可以通过感知直接指导行动。我们的研究探索了一个有趣的问题，即LLM是否能够有效地捕捉到人类对于语言中基本的空间构建块的隐含直觉，尽管它们是非具身的。我们运用从早期感知运动经验中发展出的空间认知基础的见解，通过再现三个心理语言学实验来指导我们的探索。令人惊讶的是，模型输出与人类回答之间出现了相关性，揭示了没有与具体经验有实质连接的适应能力。显著的区别包括极化的语言模型响应和视觉语言模型中降低的相关性。这项研究对于深入了解语言和空间的相互作用有着微妙的贡献。

    Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spat
    
[^67]: 用于安全自助大型语言模型探索的机构平台

    Institutional Platform for Secure Self-Service Large Language Model Exploration

    [https://rss.arxiv.org/abs/2402.00913](https://rss.arxiv.org/abs/2402.00913)

    这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。

    

    本文介绍了由肯塔基大学应用人工智能中心开发的用户友好型平台，旨在使大型定制语言模型（LLM）更易于使用。通过利用最近在多LoRA推理方面的进展，系统有效地适应了各类用户和项目的定制适配器。论文概述了系统的架构和关键特性，包括数据集策划、模型训练、安全推理和基于文本的特征提取。我们通过使用基于代理的方法建立了一个基于租户意识的计算网络，在安全地利用孤立资源岛的基础上形成了一个统一的系统。该平台致力于提供安全的LLM服务，强调过程和数据隔离、端到端加密以及基于角色的资源身份验证。该贡献与实现简化访问先进的AI模型和技术以支持科学发现的总体目标一致。

    This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
    
[^68]: 大规模语言模型上的提示注入攻击的早期分类

    An Early Categorization of Prompt Injection Attacks on Large Language Models

    [https://rss.arxiv.org/abs/2402.00898](https://rss.arxiv.org/abs/2402.00898)

    本文旨在提供对大规模语言模型上的提示注入攻击的早期分类，并讨论了这些攻击对LLM最终用户、开发人员和研究人员的影响。

    

    大规模语言模型和AI聊天机器人一直是民主化人工智能的前沿。然而，ChatGPT和其他类似工具的发布后，人们对于控制大规模语言模型及其输出的难度越来越担忧。目前，我们正在目睹一场捉迷藏的游戏，用户试图利用这些模型进行一种称为提示注入的新型攻击，而开发人员则试图同时发现漏洞并阻止这些攻击。在本文中，我们概述了这些新兴威胁，并提出了提示注入的分类，这可以指导未来在提示注入上的研究，并作为LLM界面开发中漏洞的检查清单。此外，基于先前的文献和我们自己的实证研究，我们讨论了提示注入对LLM最终用户、开发人员和研究人员的影响。

    Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by growing concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections. In contrast, the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of these emergent threats and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.
    
[^69]: 大型语言模型在网络安全中的应用：最新研究进展

    Large Language Models in Cybersecurity: State-of-the-Art

    [https://rss.arxiv.org/abs/2402.00891](https://rss.arxiv.org/abs/2402.00891)

    本研究综述了大型语言模型（LLMs）在网络安全领域中的防御和对抗应用，并识别了关键的研究空缺，旨在提供对LLM驱动的网络安全的潜在风险和机会的全面理解。

    

    大型语言模型（LLMs）的出现彻底改变了我们对智能的理解，使我们更接近于人工智能。自从它们被引入以来，研究人员积极探索了LLMs在各个领域的应用，显著提升了能力。然而，在网络安全领域，传统上对数据驱动解决方案持抵触态度且对机器学习采用较慢，这一领域却异军突起。本研究对现有文献进行了研究，全面描述了LLMs在网络安全领域中的防御和对抗应用。我们的综述不仅对当前的研究现状进行了调查和分类，并且还识别出了关键的研究空缺。通过评估攻击和防御应用，我们旨在提供对LLM驱动的网络安全所涉及的潜在风险和机会的全面理解。

    The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.
    
[^70]: 大型语言模型的安全和隐私挑战：一项调查

    Security and Privacy Challenges of Large Language Models: A Survey

    [https://rss.arxiv.org/abs/2402.00888](https://rss.arxiv.org/abs/2402.00888)

    大型语言模型具有卓越的能力，但也面临着安全和隐私攻击的威胁。本调查全面审查了LLM的安全和隐私挑战，涵盖了训练数据、用户和应用风险等方面，并对解决方法进行了回顾。

    

    大型语言模型（LLM）展示了非凡的能力，并在生成和总结文本、语言翻译和问答等多个领域做出了贡献。如今，LLM正在成为计算机语言处理任务中非常流行的工具，具备分析复杂语言模式并根据上下文提供相关和适当回答的能力。然而，尽管具有显著优势，这些模型也容易受到安全和隐私攻击的威胁，如越狱攻击、数据污染攻击和个人可识别信息泄露攻击。本调查全面审查了LLM的安全和隐私挑战，包括训练数据和用户方面的问题，以及在交通、教育和医疗等各个领域中应用带来的风险。我们评估了LLM的脆弱性程度，调查了出现的安全和隐私攻击，并对潜在的解决方法进行了回顾。

    Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potent
    
[^71]: 将稀疏微调扩展到大型语言模型

    Scaling Sparse Fine-Tuning to Large Language Models

    [https://rss.arxiv.org/abs/2401.16405](https://rss.arxiv.org/abs/2401.16405)

    本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。

    

    大型语言模型（LLM）由于其参数的庞大数量，很难完全进行微调（例如使用指令或人工反馈）。一系列参数高效的稀疏微调方法在性能方面表现出了很大的潜力，但它们的存储需求与LLM的大小成正比增加。在这项工作中，我们将稀疏微调扩展到最先进的LLM，如LLaMA 2 7B和13B。我们提出了SpIEL，一种新颖的稀疏微调方法，它针对所需的稀疏度水平，维护一个参数索引数组和这些参数相对于预训练值的增量。它遍历以下步骤：（a）更新活跃增量，（b）修剪索引（基于其增量的变化大小），以及（c）重新生长索引。对于重新生长，我们探索了基于少量候选参数的累积梯度或使用高效的SM3优化器估计的近似动差的两个准则。我们尝试使用指令微调LLM。

    Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL
    
[^72]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^73]: NoFunEval: 有趣的是，代码语言模型在超出功能正确性的要求上遇到困难

    NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness

    [https://rss.arxiv.org/abs/2401.15963](https://rss.arxiv.org/abs/2401.15963)

    本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。

    

    现有的代码语言模型（code LMs）的评估基准几乎完全集中在LMs是否能够生成功能正确的代码上。在实际的软件工程中，开发人员会考虑超出功能正确性的要求。他们对于“如何”实现功能有着对整体系统设计目标（如效率、安全性和可维护性）的要求。如果LMs能够展示对要求和代码语义的强大理解能力，他们也会更加信任这些LMs。我们提出了一个新的基准测试NoFunEval来评估代码LMs在非功能性要求和简单分类实例方面的表现。我们提出了一个提示方法Coding Concepts (CoCo)，可以用于开发人员向LMs传达领域知识。我们对22个代码LMs进行了广泛评估，发现它们在我们的基准测试中普遍表现不佳，暗示着它们在处理这些问题时存在根本性的盲点。

    Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
    
[^74]: SuperCLUE-Math6：用于评估中文语言模型逻辑多步数学推理能力的分级基准数据集

    SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese

    [https://rss.arxiv.org/abs/2401.11819](https://rss.arxiv.org/abs/2401.11819)

    SuperCLUE-Math6 是一个用于评估中文语言模型数学推理能力的分级数据集，通过增加难度、多样性和应用范围，提供了2000多个需要多步推理的数学问题，并采用创新方法来量化大型模型的推理能力。实验结果显示出明显的推理水平分层，顶级模型如GPT-4表现出优越的性能。SC-Math6填补了中文数学推理基准数据集的空白，并推进了中文语言模型的智能化。

    

    我们介绍了SuperCLUE-Math6（SC-Math6），这是一个新的基准数据集，用于评估中文语言模型的数学推理能力。SC-Math6是GSM8K数据集的升级版本，增加了难度、多样性和应用范围。它包含了2000多个需要多步推理并提供自然语言解决方案的数学问题。我们提出了一种创新的方法来量化大型模型的推理能力，基于不同推理步骤的问题表现。对13个代表性的中文模型的实验结果显示出明显的推理水平分层，顶级模型如GPT-4表现出优越的性能。SC-Math6填补了中文数学推理基准数据集的空白，并提供了一个全面的测试平台来推进中文语言模型的智能化。

    We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 13 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.
    
[^75]: 解读文本的真实性: 通过大规模语言语义的广义策略来检测人类和机器生成的文本

    Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text

    [https://rss.arxiv.org/abs/2401.09407](https://rss.arxiv.org/abs/2401.09407)

    该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。

    

    随着大规模语言模型（LLM）的广泛应用，对检测机器生成文本的工具的需求日益增长。有效检测机器生成文本面临两个关键问题: 首先，他们在应对真实世界场景时面临着极大的限制，这些场景中机器生成文本是由各种生成器产生的，包括但不限于GPT-4和Dolly，并涵盖各种领域，从学术手稿到社交媒体帖子。其次，现有的检测方法将LLM生成的文本视为严格的二元分类问题，忽略了不同LLM生成的文本多样性。本研究系统地研究了在真实世界场景中检测机器生成文本的方法。我们首先研究了最先进方法的有效性，并发现它们在应对真实世界中不同生成器和领域产生的文本时受到严重的限制。

    With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
    
[^76]: 从4K到400K的飞跃：利用激活标志扩展LLM的上下文

    Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon

    [https://rss.arxiv.org/abs/2401.03462](https://rss.arxiv.org/abs/2401.03462)

    本论文提出了一种称为激活标志的新方法，它通过压缩LLM的激活状态，使其能够以有限的上下文窗口感知更长的上下文，同时保留了LLM在短上下文中的原始能力。这种方法具有竞争力的内存和时间效率，并通过多样化训练有效地支持不同上下文长度。

    

    长上下文的利用对于LLM来说是一个巨大的挑战，因为它们有限的上下文窗口大小。尽管通过微调可以扩展上下文窗口，但这会导致训练和推理时间的显著成本，并对LLM的原始能力产生不利影响。在这项工作中，我们提出了一种名为激活标志的新方法，它将LLM的原始激活压缩成紧凑的形式，使LLM能够以有限的上下文窗口感知更长的上下文。激活标志被引入为插件模块，完全保留了LLM在短上下文中的原始能力。它与滑动窗口一起实时处理长的上下文，从而在训练和推理中实现了竞争力的内存和时间效率。激活标志是通过多样化压缩比的短序列数据进行训练的。得益于这种处理，它可以有效地学习支持不同上下文长度，实现小规模的训练。

    The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai
    
[^77]: 别再出现尖峰了：稳定大型语言模型的预训练

    Spike No More: Stabilizing the Pre-training of Large Language Models

    [https://rss.arxiv.org/abs/2312.16903](https://rss.arxiv.org/abs/2312.16903)

    本论文研究了大型语言模型预训练中的损失尖峰问题，并通过理论分析找出了梯度爆炸的原因，并提出了满足要求的方法。通过实验证明，该方法能够有效地防止尖峰的发生。

    

    大型语言模型的预训练经常出现损失尖峰。这些尖峰会降低大型语言模型的性能，有时会破坏预训练。由于预训练需要大量的计算资源，我们应该避免这种尖峰的出现。为了研究损失尖峰的原因，我们关注内部层的梯度。通过理论分析，我们揭示了梯度爆炸的两个原因，并提供了预防梯度爆炸的要求。此外，我们提出了一种通过组合初始化方法和对嵌入进行简单修改来满足要求的方法。我们进行了各种实验证明我们的理论分析的有效性。实验结果表明，在预训练过程中，这种组合方法能够有效地防止尖峰的出现。

    Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
    
[^78]: DSPy断言：用于自我调整语言模型流水线的计算约束

    DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines

    [https://rss.arxiv.org/abs/2312.13382](https://rss.arxiv.org/abs/2312.13382)

    DSPy引入了LM断言，用于表达语言模型应满足的计算约束。在四个案例研究中，LM断言不仅提高了对规则的遵守，而且提高了下游任务的性能，增加了对约束的接受次数并生成了更高质量的回复。

    

    将语言模型（LM）调用作为可组合模块的链式编程方式正在推动一种新的编程方式，但确保LM遵守重要约束需要启发式的“提示工程”。我们介绍了LM断言，这是一种用于表达LM应满足的计算约束的编程结构。我们将这些结构整合到最近的DSPy LM编程模型中，并提出了新的策略，使得DSPy能够将带有LM断言的程序编译为更可靠和准确的系统。我们还提出了在推断时使用断言进行自动自我修复的策略。我们报告了四个不同的文本生成案例研究，并发现LM断言不仅改善了对规则的遵守，而且提高了下游任务的性能，接受约束的次数增加了164％，生成了37％更高质量的回复。我们的LM断言参考实现已集成到DSPy中，网址为https://github.com/stanfordnlp/dspy

    Chaining language model (LM) calls as composable modules is fueling a new way of programming, but ensuring LMs adhere to important constraints requires heuristic "prompt engineering". We introduce LM Assertions, a programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with LM Assertions into more reliable and accurate systems. We also propose strategies to use assertions at inference time for automatic self-refinement with LMs. We report on four diverse case studies for text generation and find that LM Assertions improve not only compliance with imposed rules but also downstream task performance, passing constraints up to 164% more often and generating up to 37% more higher-quality responses. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy
    
[^79]: 跨块量化：用于大型语言模型的跨块量化方法

    CBQ: Cross-Block Quantization for Large Language Models

    [https://rss.arxiv.org/abs/2312.07950](https://rss.arxiv.org/abs/2312.07950)

    CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。

    

    后训练量化（PTQ）在以极低成本压缩大型语言模型（LLM）方面起着重要作用。然而，现有的PTQ方法只关注处理单个层或单个块内的异常值，忽略了块之间的依赖关系，在低位设置中导致严重的性能下降。本文提出了一种基于块间重构的跨块PTQ方法CBQ。CBQ采用了一种同源重构方案来实现块间的长程依赖关系，以最小化误差积累。此外，CBQ还结合了一种粗到精的预处理策略（CFP）来抑制权重和激活值的异常值，并配合一种自适应的LoRA取整技术实现精确的权重量化。这些创新使CBQ不仅能够有效处理极端异常值，还能提高整体量化精度。广泛的实验证明，CBQ在低位量化（W4A4，W4A8等）方面具有优越性能。

    Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
    
[^80]: Neuron Patching: 神经元层面的模型编辑与代码生成

    Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs

    [https://rss.arxiv.org/abs/2312.05356](https://rss.arxiv.org/abs/2312.05356)

    这项工作介绍了一种神经元层面的模型编辑方法，能够在编码任务中修补LLM模型，并且在API序列推荐、代码生成和伪代码到代码转换等任务中得到了验证和评估。

    

    大型语言模型在软件工程中得到了成功应用，特别是在代码生成方面。更新这些模型的新知识非常昂贵，通常需要全面实现其价值。在本文中，我们提出了一种新颖有效的模型编辑方法MENT，用于在编码任务中修补LLM模型。基于生成式LLM的机制，MENT可以在预测下一个令牌时进行模型编辑，并进一步支持常见的编码任务。MENT具有高效、有效和可靠的特点。它可以通过修补1或2个神经元来纠正神经模型。作为神经元层面上生成模型编辑的先驱工作，我们规范了编辑过程并介绍了相关概念。此外，我们还引入了新的衡量方法来评估其泛化能力，并建立了一个用于进一步研究的基准。我们的方法在三个编码任务上进行了评估，包括API序列推荐、行级代码生成和伪代码到代码转换。

    Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
    
[^81]: 在图上的大型语言模型：一项全面调查

    Large Language Models on Graphs: A Comprehensive Survey

    [https://rss.arxiv.org/abs/2312.02783](https://rss.arxiv.org/abs/2312.02783)

    这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。

    

    大型语言模型（LLMs），如GPT4和LLaMA，由于其强大的文本编码/解码能力和新发现的紧急能力（例如推理）在自然语言处理方面取得了显著的进展。虽然LLMs主要设计用于处理纯文本，但在许多现实场景中，文本数据与图形形式的丰富结构信息相关联（例如学术网络和电子商务网络），或者图形数据与丰富的文本信息配对（例如带有描述的分子）。此外，尽管LLMs已经展示了其基于纯文本的推理能力，但尚未探索此类能力是否可以推广到图形上（即基于图形的推理）。在本文中，我们对在图上的大型语言模型相关场景和技术进行了系统回顾。我们首先总结了采用LLMs在图形上的潜在场景，分为纯图形、文本属性图形和文本配对图形三个类别。

    Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
    
[^82]: 恶意天才：探索基于LLM的智能体的安全性

    Evil Geniuses: Delving into the Safety of LLM-based Agents

    [https://rss.arxiv.org/abs/2311.11855](https://rss.arxiv.org/abs/2311.11855)

    本文研究了基于LLM的智能体的安全性，从智能体数量、角色定义和攻击水平三个角度进行了探讨，并提出了一种有效的攻击方法“恶意天才”，通过自动生成与原始角色相关的提示，来测试在不同角色定义和攻击水平下的影响。

    

    大型语言模型(LLM)的快速发展使得基于LLM的智能体在各种场景中展示出令人印象深刻的类人行为和合作能力，但这些智能体也带来了一些独特的风险，源于交互环境的复杂性和工具的可用性。本文从智能体数量、角色定义和攻击水平三个角度深入探讨了基于LLM的智能体的安全性。具体来说，我们首先提出了一种基于模板的攻击策略，用于测试智能体数量的影响。此外，为了解决交互环境和角色特异性问题，我们引入了“恶意天才”(EG)，这是一种有效的攻击方法，可以自动化生成与原始角色相关的提示，以检查在各种角色定义和攻击水平下的影响。恶意天才利用红蓝对抗训练，显著提高了生成的提示的攻击性和与原始角色的相似性。

    Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our ev
    
[^83]: 医学中的大型语言模型调查：原理、应用和挑战

    A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges

    [https://rss.arxiv.org/abs/2311.05112](https://rss.arxiv.org/abs/2311.05112)

    本综述提供了医学中大型语言模型（LLMs）的原理、应用和挑战的全面概述。同时回答了医学LLMs的构建、下游性能、实际应用、挑战以及更好构建和利用的问题。旨在为构建有效的医学LLMs提供见解和实用资源。

    

    大型语言模型（LLMs），如ChatGPT，由于其理解和生成人类语言的能力而受到了广泛关注。在人工智能和临床医学中，LLMs在协助医生进行患者护理方面正在成为一个有前景的研究方向。本综述提供了医学中LLMs的原理、应用和面临的挑战的全面概述。我们回答了以下具体问题：1）如何构建医学LLMs？2）什么是医学LLMs的下游性能评估指标？3）在现实临床实践中，如何利用医学LLMs？4）使用医学LLMs会出现哪些挑战？5）如何更好地构建和利用医学LLMs？本综述旨在提供关于医学中LLMs的机遇和挑战的见解，并作为构建有效的医学LLMs的实用资源。我们还维护并定期更新一个清单

    Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
    
[^84]: 基础模型的嵌入表示能够检测到分布偏移

    Foundation Model's Embedded Representations May Detect Distribution Shift

    [https://rss.arxiv.org/abs/2310.13836](https://rss.arxiv.org/abs/2310.13836)

    该研究发现基础模型的嵌入表示可以检测到数据集之间的分布偏移，且在传统的泛化度量上表现出偏差。预训练的GPT-2模型的特征学习无法在特定任务上提升性能，而对其表示进行线性探测可能优于整体微调。

    

    采样偏差可能导致监督学习任务的训练和测试数据集之间发生分布偏移，使我们难以理解模型的泛化能力。鉴于广泛采用已预训练的基础神经网络作为迁移学习任务的工具，这一点尤为重要，而这些基础神经网络的行为仍然不太清楚。我们以Sentiment140数据集上的迁移学习为例进行了案例研究，并展示了许多预训练的基础模型对Sentiment140的手动标注的测试集M和自动标注的训练集P具有不同的表示，证实了发生了分布偏移。我们认为在P上训练并在M上评估性能是一种有偏差的泛化度量。对预训练的GPT-2进行的实验表明，从P中可学得的特征不会改善（事实上会阻碍）在M上的性能。对预训练的GPT-2的表示进行线性探测是鲁棒的，甚至可能胜过整体微调。

    Sampling biases can cause distribution shifts between train and test datasets for supervised learning tasks, obscuring our ability to understand the generalization capacity of a model. This is especially important considering the wide adoption of pre-trained foundational neural networks -- whose behavior remains poorly understood -- for transfer learning (TL) tasks. We present a case study for TL on the Sentiment140 dataset and show that many pre-trained foundation models encode different representations of Sentiment140's manually curated test set $M$ from the automatically labeled training set $P$, confirming that a distribution shift has occurred. We argue training on $P$ and measuring performance on $M$ is a biased measure of generalization. Experiments on pre-trained GPT-2 show that the features learnable from $P$ do not improve (and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning, imply
    
[^85]: Geo-Encoder: 用于中文地理重新排序的块-论证双编码器框架

    Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking

    [https://rss.arxiv.org/abs/2309.01606](https://rss.arxiv.org/abs/2309.01606)

    Geo-Encoder是一个用于中文地理重新排序的创新框架，它能够更有效地将中文地理语义集成到重新排序流程中，并且通过使用多任务学习模块和异步更新机制，能够提升模型在特定块上的注意力集中能力。

    

    中文地理重新排序任务旨在在检索到的候选地址中找到最相关的地址，这对于导航地图等与位置相关的服务至关重要。与一般的语句不同，地理上下文与地理概念紧密相连，从一般的片段（如省份）到特定的片段（如道路）。基于此特点，我们提出了一个创新的框架，名为Geo-Encoder，将中文地理语义更有效地集成到重新排序流程中。我们的方法是通过使用现成的工具将文本与地理范围相关联，将它们作为分块单元。然后，我们提出了一个多任务学习模块，同时获取一个有效的注意力矩阵，以确定块对额外语义表示的贡献。此外，我们还提出了异步更新机制，用于提出的附加任务，旨在引导模型能够有效集中注意力在特定的块上。在实验中

    Chinese geographic re-ranking task aims to find the most relevant addresses among retrieved candidates, which is crucial for location-related services such as navigation maps. Unlike the general sentences, geographic contexts are closely intertwined with geographical concepts, from general spans (e.g., province) to specific spans (e.g., road). Given this feature, we propose an innovative framework, namely Geo-Encoder, to more effectively integrate Chinese geographical semantics into re-ranking pipelines. Our methodology begins by employing off-the-shelf tools to associate text with geographical spans, treating them as chunking units. Then, we present a multi-task learning module to simultaneously acquire an effective attention matrix that determines chunk contributions to extra semantic representations. Furthermore, we put forth an asynchronous update mechanism for the proposed addition task, aiming to guide the model capable of effectively focusing on specific chunks. Experiments on t
    
[^86]: 大规模语言模型中出现的欺骗能力

    Deception Abilities Emerged in Large Language Models

    [https://rss.arxiv.org/abs/2307.16513](https://rss.arxiv.org/abs/2307.16513)

    大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。

    

    大规模语言模型（LLM）目前处于将人工智能系统与人类交流和日常生活紧密结合的前沿。因此，将它们与人类价值观保持一致非常重要。然而，由于推理能力的稳定增长，未来的LLM被怀疑能够欺骗人类操作员，并利用这种能力绕过监测工作。为此，LLM需要具备对欺骗策略的概念理解。本研究揭示了最先进的LLM（如GPT-4）中出现了这种策略，而在早期的LLM中并不存在。我们进行了一系列实验，表明最先进的LLM能够理解和诱导他人产生错误的信念，其在复杂的欺骗场景中表现可以通过链式思维推理得到增强，并且引发LLM中的马基雅维利主义可以改变其欺骗倾向。总之，揭示了迄今为止未知的欺骗能力。

    Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
    
[^87]: 语言模型作为归纳推理器

    Language Models as Inductive Reasoners

    [https://rss.arxiv.org/abs/2212.10923](https://rss.arxiv.org/abs/2212.10923)

    该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。

    

    归纳推理是人类智能的核心组成部分。在计算机科学中的归纳推理研究中，形式语言被用作知识（事实和规则）的表示。然而，形式语言会给归纳推理带来系统性问题，例如无法处理自然语言这样的原始输入、对错误标记的数据敏感以及处理模糊输入的能力不足。为此，我们提出了一种新的归纳推理范式（任务），即从自然语言事实中归纳出自然语言规则，并创建了一个被称为DEER的数据集，其中包含1.2k个规则-事实对，规则和事实以自然语言书写。还提出并分析了用于评估此任务的新的自动评估指标。通过DEER，我们研究了一种现代的归纳推理方法，其中我们使用自然语言作为知识的表示而不是形式语言，并使用预训练的语言模型。

    Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
    
[^88]: 实现语言模型对齐的高效精确优化方法

    Towards Efficient and Exact Optimization of Language Model Alignment

    [https://arxiv.org/abs/2402.00856](https://arxiv.org/abs/2402.00856)

    本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。

    

    将语言模型与人类偏好进行对齐对于其在实际任务中的应用至关重要。该问题被建模为优化模型策略，以最大化反映人类偏好的预期奖励，并尽量减小与初始策略的偏差。尽管强化学习（RL）被认为是一种直接的解决方案，但其策略更新的方差很高，阻碍了高效的策略改进。最近，直接偏好优化（DPO）被提出以直接从偏好数据中优化策略。尽管实现简单，DPO是基于不一定能在实践中实现的最优策略导出的，这削弱了其收敛到预期解决方案的能力。本文提出了一种高效精确优化（EXO）的对齐目标方法。我们证明了对于策略的任意参数化，EXO保证渐近地与RL算法的优化方向一致，并且能够实现高效优化。

    The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op
    
[^89]: CroissantLLM: 一个真正的双语法语-英语语言模型

    CroissantLLM: A Truly Bilingual French-English Language Model

    [https://arxiv.org/abs/2402.00786](https://arxiv.org/abs/2402.00786)

    CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。

    

    我们介绍了CroissantLLM，这是一个在3T个英语和法语标记上预训练的13亿语言模型，为研究和工业社区带来了一种高性能的、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们首次尝试使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集来训练一种内在双语的模型。我们发布了训练数据集，其中包含了一个法语分割，其中包含了手工策划、高质量和多样化的数据源。为了评估在英语以外的性能，我们创建了一个新的基准测试 FrenchBench，包括一系列分类和生成任务，涵盖了模型在法语语言中性能的各个方面。此外，为了保持透明度并促进进一步的大规模语言模型研究，我们发布了代码库和各种模型规模、训练数据分布上的几十个检查点。

    We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
    
[^90]: 一条思维链条的强度取决于最弱的环节：一个验证推理链的基准

    A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains

    [https://arxiv.org/abs/2402.00559](https://arxiv.org/abs/2402.00559)

    本论文提出了Reveal数据集，用于在开放领域的问答设置中对复杂思维链的自动验证进行基准测试。这个数据集包含了详尽的标签，用于评估语言模型的答案中每个推理步骤的相关性、归因和逻辑正确性。

    

    促使语言模型提供逐步回答（例如“思维链”）是复杂推理任务的主要方法，其中更准确的推理链通常可以提高下游任务的性能。最近的文献讨论了自动验证推理步骤的方法，以评估和改善其正确性。然而，缺乏细粒度的步骤级数据集，无法对这类验证方法进行全面评估，从而阻碍了在这个方向上的进展。我们介绍了Reveal：推理验证评估，这是一个新的数据集，用于在开放领域的问答设置中对复杂思维链的自动验证进行基准测试。Reveal包括对语言模型答案中每个推理步骤的相关性、归因于证据段落以及逻辑正确性的全面标签，涵盖了各种数据集和最先进的语言模型。

    Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.
    
[^91]: 通过地图接种改进QA模型性能

    Improving QA Model Performance with Cartographic Inoculation

    [https://arxiv.org/abs/2401.17498](https://arxiv.org/abs/2401.17498)

    本文提出了一种名为地图接种的新方法，通过在优化的挑战数据子集上对QA模型进行微调，减少模型对数据集伪迹的依赖性，从而显著提高模型在复杂和开放的上下文推理问题上的性能。

    

    QA模型面临着复杂而开放的上下文推理问题，但通常可以通过利用训练数据中特定于数据集的模式来学习高性能的解决启发式方法。这些模式，或者称为"数据集伪迹"，降低了模型在现实世界QA问题上的泛化能力。利用训练用于QA的ElectraSmallDiscriminator模型，我们使用一组对抗性挑战数据集分析了数据集伪迹的影响和发生情况，该数据集旨在混淆依赖于数据集伪迹进行预测的模型。在现有减轻伪迹影响方法的基础上，我们提出了一种新颖的方法，即地图接种，通过在优化的挑战数据子集上对模型进行微调，以减少模型对数据集伪迹的依赖性。我们证明，通过有选择地在挑战数据集中的模棱两可的对抗性示例上进行模型微调，可以在最小损失模型对其他挑战数据集的泛化能力的情况下，大大提高模型在整个挑战数据集上的性能。

    QA models are faced with complex and open-ended contextual reasoning problems, but can often learn well-performing solution heuristics by exploiting dataset-specific patterns in their training data. These patterns, or "dataset artifacts", reduce the model's ability to generalize to real-world QA problems. Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the impacts and incidence of dataset artifacts using an adversarial challenge set designed to confuse models reliant on artifacts for prediction. Extending existing work on methods for mitigating artifact impacts, we propose cartographic inoculation, a novel method that fine-tunes models on an optimized subset of the challenge data to reduce model reliance on dataset artifacts. We show that by selectively fine-tuning a model on ambiguous adversarial examples from a challenge set, significant performance improvements can be made on the full challenge dataset with minimal loss of model generalizability to other chal
    
[^92]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^93]: 最近在仇恨言论审核方面的进展：多模态和大型模型的作用

    Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models. (arXiv:2401.16727v1 [cs.CL])

    [http://arxiv.org/abs/2401.16727](http://arxiv.org/abs/2401.16727)

    这项综合调查总结了最近在仇恨言论审核方面的进展，重点介绍了大型语言模型和大型多模态模型的作用。研究发现了文本、视觉和听觉元素在传播仇恨言论中的微妙相互作用，并强调了大型模型对审核能力的重新定义。同时，研究还指出了在少数语言和文化背景下的研究差距和处理低资源环境的需求。

    

    在网络交流的不断发展中，审核仇恨言论（HS）面临着复杂的挑战，这是由数字内容的多模态特性所带来的。这项综合调查深入研究了HS审核的最新进展，着重介绍了大型语言模型（LLMs）和大型多模态模型（LMMs）的崛起角色。我们的研究从对当前文献的全面分析开始，揭示了文本、视觉和听觉元素在传播HS中的微妙相互作用。我们发现了一个明显的趋势，即将这些模态整合在一起，主要是因为HS的传播具有复杂性和微妙性。对于由LLMs和LMMs带来的进展，我们特别强调了其对检测和审核能力边界的重新定义。我们确定了研究中存在的现有差距，特别是在少数语言和文化的背景下，以及在处理低资源环境中需要解决方案的需求。

    In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey
    
[^94]: 人类与ChatGPT生成对话之间的语言对比

    A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])

    [http://arxiv.org/abs/2401.16587](http://arxiv.org/abs/2401.16587)

    本研究比较了人类和ChatGPT生成的对话的语言差异，发现ChatGPT在社交、分析、认知、关注焦点和积极情绪等方面表现出色，但人类对话更具变异性和真实性，尽管在情绪方面无显著差异。同时，该研究还提供了一个新颖的、由ChatGPT生成的对话组成的数据集。

    

    本研究探讨了人类和LLM生成的对话之间的语言差异，使用了由ChatGPT-3.5生成的19.5K个对话作为EmpathicDialogues数据集的补充。研究采用Linguistic Inquiry and Word Count (LIWC) 分析，比较了ChatGPT生成的对话和人类对话在118个语言类别上的差异。结果显示人类对话具有更大的变异性和真实性，但ChatGPT在社交过程、分析风格、认知、关注焦点和积极情绪色彩等方面表现出色，这进一步证明了LLMs“比真人更像真人”的最新发现。然而，在ChatGPT和人类对话之间没有找到积极或消极情绪的显著差异。对话嵌入的分类器分析表明，尽管对话中没有明确提及情绪，但对情感价值的隐性编码存在。研究还提供了一个新颖的、由两个ChatGPT生成的对话组成的数据集。

    This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
    
[^95]: 发挥专业放射科医生的专长，提升放射学报告的LLM评估

    Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])

    [http://arxiv.org/abs/2401.16578](http://arxiv.org/abs/2401.16578)

    该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。

    

    在放射学领域，人工智能（AI）已经大大推进了报告生成，但自动生成报告的自动评估仍然具有挑战性。目前的度量标准，如传统自然语言生成（NLG）和临床效能（CE），往往无法捕捉临床背景的语义复杂性，或者过分强调临床细节，降低了报告的清晰性。为了解决这些问题，我们提出的方法将专业放射科医生的专业知识与大型语言模型（LLMs），如GPT-3.5和GPT-4 1，相结合。利用上下文指导学习（ICIL）和思维链（CoT）推理，我们的方法使LLM的评估与放射科医生的标准保持一致，实现了人工智能生成报告与人类生成报告之间的详细比较。这进一步通过回归模型来综合句子评估分数。实验结果表明，我们的“详细GPT-4（5次训练）”模型获得了0.48的分数，优于METEOR指标。

    In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
    
[^96]: 基于RAG的理解伊斯兰教问题回答系统提案：MufassirQAS LLM

    A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])

    [http://arxiv.org/abs/2401.15378](http://arxiv.org/abs/2401.15378)

    基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。

    

    学习和理解宗教存在复杂性和教义深度的挑战。问答机器人作为解决这些挑战的问题回答系统，可以帮助。LLM聊天机器人利用自然语言处理技术建立主题之间的联系，准确回答复杂问题。这些能力使其成为用于宗教启蒙的问题回答聊天机器人的理想选择。然而，LLM也有生成虚假信息的倾向，称为幻觉。聊天机器人的回答可能包含侮辱个人宗教信仰、跨宗派冲突和有争议或敏感的话题的内容。它需要避免这种情况，而不会宣扬仇恨言论或冒犯某些群体的人或他们的信仰。本研究使用基于向量数据库的检索增强生成（RAG）方法来提高LLMs的准确性和透明度。我们的问答系统称为"MufassirQAS"。我们创建了一个模型来评估该系统并证明其在解决宗教行业问题中的效果。

    There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
    
[^97]: UNSEE: 无监督的非对比度句子嵌入

    UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])

    [http://arxiv.org/abs/2401.15316](http://arxiv.org/abs/2401.15316)

    UNSEE是一种无监督的非对比度句子嵌入方法，通过引入目标网络解决了表示坍塌问题，达到了与对比目标相当的性能提升。

    

    我们提出了一种名为UNSEE（Unsupervised Non-Contrastive Sentence Embeddings）的新方法，在大规模文本嵌入基准测试中超越了SimCSE。我们首先解决了SimCSE中替换对比目标为非对比目标时出现的表示坍塌挑战。为了解决这个问题，我们提出了一种称为目标网络的简单解决方案，有效地缓解了表示坍塌。目标网络的引入使我们能够利用非对比目标，在保持训练稳定性的同时实现与对比目标相当的性能提升。通过精心调整和优化，我们的方法在非对比度句子嵌入上达到了巅峰性能。这一全面的努力产生了出色的句子表示模型，展示了我们方法的有效性。

    We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our exploration begins by addressing the challenge of representation collapse, a phenomenon observed when contrastive objectives in SimCSE are replaced with non-contrastive objectives. To counter this issue, we propose a straightforward solution known as the target network, effectively mitigating representation collapse. The introduction of the target network allows us to leverage non-contrastive objectives, maintaining training stability while achieving performance improvements comparable to contrastive objectives. Our method has achieved peak performance in non-contrastive sentence embeddings through meticulous fine-tuning and optimization. This comprehensive effort has yielded superior sentence representation models, showcasing the effectiveness of our approach.
    
[^98]: 视觉语言模型中被忽视的尾部

    The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])

    [http://arxiv.org/abs/2401.12425](http://arxiv.org/abs/2401.12425)

    本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。

    

    视觉语言模型（VLM）在零样本识别方面表现出色，但在视觉概念上的表现极不均衡。例如，尽管CLIP在ImageNet上具有令人印象深刻的平均零样本准确率（72.7％），但在十个概念（如gyromitra和night snake）上的准确率不到10％，这可能是因为这些概念在VLM的非均衡预训练数据中的表示不足。然而，评估这种不平衡是具有挑战性的，因为在VLM的大规模预训练数据中计算特定概念的频率是非常复杂的。我们的工作首次尝试使用分析预训练文本来测量概念频率。我们利用现成的语言模型来帮助计算包含给定概念的同义词的相关文本，并解决语言歧义。我们确认像LAION这样的流行的VLM数据集确实展示了长尾概念分布，并且这与按类别的准确率强烈相关。此外，当代的多模式系统，如视觉聊天机器人和文本-视觉推理模型，在这种长尾分布下经常难以达到高性能。

    Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
    
[^99]: 通过文化价值调查，弥合对话代理中的文化细微差别

    Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys. (arXiv:2401.10352v1 [cs.CL])

    [http://arxiv.org/abs/2401.10352](http://arxiv.org/abs/2401.10352)

    通过引入cuDialog，我们提出了一种以文化为视角的对话生成基准，并开发了能够从对话中提取文化属性的基准模型。实验结果显示，结合文化价值调查可以提高对话代理的对个性化和对话质量的预测准确性。

    

    对话代理与文化的交互领域是一个引人注目但相对未被探索的领域。各种社会文化方面，从沟通风格和信念到共享的隐喻和知识，都深刻地影响着这些交互。为了更深入地研究这一动态，我们引入了cuDialog，这是一个以文化为视角的对话生成基准。我们还开发了能够从对话交流中提取文化属性的基准模型，旨在提高对话代理的预测准确性和质量。为了有效地共同学习文化理解和多轮对话预测，我们提出将文化维度与对话编码特征相结合。我们的实验结果表明，加入文化价值调查能够增强与参考文献和文化标记的一致性，显示出它对个性化和对话质量的重要影响。为了进一步促进对话代理与文化的交互探索。

    The cultural landscape of interactions with dialogue agents is a compelling yet relatively unexplored territory. It's clear that various sociocultural aspects -- from communication styles and beliefs to shared metaphors and knowledge -- profoundly impact these interactions. To delve deeper into this dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue generation with a cultural lens. We also develop baseline models capable of extracting cultural attributes from dialogue exchanges, with the goal of enhancing the predictive accuracy and quality of dialogue agents. To effectively co-learn cultural understanding and multi-turn dialogue predictions, we propose to incorporate cultural dimensions with dialogue encoding features. Our experimental findings highlight that incorporating cultural value surveys boosts alignment with references and cultural markers, demonstrating its considerable influence on personalization and dialogue quality. To facilitate further explorati
    
[^100]: 对比性偏好优化：推动机器翻译中LLM性能的边界

    Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08417](http://arxiv.org/abs/2401.08417)

    本研究通过引入对比性偏好优化（CPO）的方法，弥合了大型语言模型（LLM）在机器翻译中性能与传统编码器-解码器模型之间的差距，实现了更好的翻译效果。

    

    中等规模的大型语言模型（LLM）——7B或者13B参数的模型在机器翻译（MT）任务中表现出有希望的性能。然而，即使是表现最好的13B LLM翻译模型，如ALMA，也无法达到现有的最先进的传统编码器-解码器翻译模型或者更大规模的LLM（如GPT-4）的性能水平。本研究旨在弥合这一性能差距。首先，我们评估了监督微调在MT任务中针对LLM的不足之处，强调了尽管是人工生成的参考数据，但其中存在质量问题。然后，与模仿参考翻译的SFT相反，我们引入了对比性偏好优化（CPO），一种新的方法，训练模型避免生成仅仅合乎要求但不完美的翻译。将CPO应用于仅有22K对句子和12M参数的ALMA模型中，可以显著提高性能。得到的模型称为ALMA-R，其性能可以达到或超过WMT比赛的获胜水平。

    Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn
    
[^101]: DQNC2S：基于DQN的跨流危机事件摘要生成器

    DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])

    [http://arxiv.org/abs/2401.06683](http://arxiv.org/abs/2401.06683)

    本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。

    

    同时总结多个与灾害相关的数据流尤其具有挑战性，因为现有的检索与重新排序策略在多流数据的固有冗余和多查询环境下的限制可扩展性方面存在问题。本文提出了一种基于弱标注和深度Q网络的在线危机时间轴生成方法。它能够实时选择相关的文本片段，无需人工标注或内容重新排序，从而使推理时间与输入查询的数量无关。该方法还将冗余过滤器融入奖励函数中，以有效处理跨流内容重叠。在CrisisFACTS 2022基准测试中，所达到的ROUGE和BERTScore结果优于最佳性能模型。

    Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
    
[^102]: 通过大型语言模型了解心理健康的评估

    An Assessment on Comprehending Mental Health through Large Language Models. (arXiv:2401.04592v1 [cs.CL])

    [http://arxiv.org/abs/2401.04592](http://arxiv.org/abs/2401.04592)

    这项研究评估了大型语言模型在理解心理健康方面的潜力，结果显示基于Transformer的模型在表达人类心理健康状况方面的理解能力超过大型语言模型。

    

    心理健康挑战对个人和社区造成了巨大的全球负担。最近的数据表明，超过20%的成年人在他们的一生中可能会遇到至少一种心理障碍。一方面，大型语言模型的进展为各种应用提供了便利，然而在心理健康领域，对大型语言模型的潜力的理解和提升仍存在重要的研究空白。另一方面，在各种应用中，一个重要的问题是大型语言模型对自然语言中人类心理健康状况表达的理解能力。本研究在解决这一空白的过程中进行了大型语言模型的初步评估。因此，我们将Llama-2和ChatGPT与传统的机器学习模型和深度学习模型进行了性能比较。我们在DAIC-WOZ数据集上的结果显示，基于Transformer的模型（如BERT或XLNet）的性能优于大型语言模型。

    Mental health challenges pose considerable global burdens on individuals and communities. Recent data indicates that more than 20% of adults may encounter at least one mental disorder in their lifetime. On the one hand, the advancements in large language models have facilitated diverse applications, yet a significant research gap persists in understanding and enhancing the potential of large language models within the domain of mental health. On the other hand, across various applications, an outstanding question involves the capacity of large language models to comprehend expressions of human mental health conditions in natural language. This study presents an initial evaluation of large language models in addressing this gap. Due to this, we compare the performance of Llama-2 and ChatGPT with classical Machine as well as Deep learning models. Our results on the DAIC-WOZ dataset show that transformer-based models, like BERT or XLNet, outperform the large language models.
    
[^103]: 情绪分类中的主题偏差

    Topic Bias in Emotion Classification. (arXiv:2312.09043v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.09043](http://arxiv.org/abs/2312.09043)

    本文研究了情绪分类中的主题偏差问题，发现情绪语料库中的主题与情绪实际上具有相关性，并且情绪分类器容易受到这些主题的干扰。最后，研究者展示了一种去偏差的方法可以减轻主题偏差的影响。

    

    情绪语料库通常是基于关键词/标签搜索或通过询问研究参与者生成文本实例来采样的。无论哪种情况，这些语料库都不是代表领域整体的均匀样本。我们假设这种数据获取方式导致了这些语料库中过度呈现的主题之间不切实际的相关性，从而损害了模型的泛化能力。这种主题偏差可能会导致错误的预测，例如对于实例"I organized the service for my aunt's funeral."，尽管与悲伤情绪标记的实例中的葬礼事件过度呈现，但更适合的情绪是自豪。在本文中，我们从数据和建模角度研究了这种主题偏差。我们首先通过主题建模自动标记了一组情绪语料库，并展示了情绪实际上与特定主题相关。此外，我们发现情绪分类器受到这些主题的干扰。最后，我们展示了已建立的去偏差方法可以减轻主题偏差的影响。

    Emotion corpora are typically sampled based on keyword/hashtag search or by asking study participants to generate textual instances. In any case, these corpora are not uniform samples representing the entirety of a domain. We hypothesize that this practice of data acquisition leads to unrealistic correlations between overrepresented topics in these corpora that harm the generalizability of models. Such topic bias could lead to wrong predictions for instances like "I organized the service for my aunt's funeral." when funeral events are over-represented for instances labeled with sadness, despite the emotion of pride being more appropriate here. In this paper, we study this topic bias both from the data and the modeling perspective. We first label a set of emotion corpora automatically via topic modeling and show that emotions in fact correlate with specific topics. Further, we see that emotion classifiers are confounded by such topics. Finally, we show that the established debiasing met
    
[^104]: 使用大型语言模型进行实体匹配

    Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])

    [http://arxiv.org/abs/2310.11244](http://arxiv.org/abs/2310.11244)

    这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。

    

    实体匹配是判断两个实体描述是否指的是同一个真实世界实体的任务。实体匹配是大多数数据集成流程中的核心步骤，也是许多电子商务应用的重要组成部分，这些应用需要将来自不同供应商的产品匹配起来。目前最先进的实体匹配方法通常依赖于预训练的语言模型（PLMs），如BERT或RoBERTa。然而，这些模型在实体匹配中存在两个主要缺点：（i）模型需要大量特定任务的训练数据；（ii）微调后的模型对于超出分布范围的实体不够健壮。本文研究了使用大型语言模型（LLMs）作为基于PLMs的匹配器的备选方案，相比之下，LLMs对领域特定训练数据需求较少且更具鲁棒性。我们的研究涵盖了托管的LLMs，如GPT3.5和GPT4，以及基于Llama2的开源LLMs，可以在本地运行。我们在零样本场景和…

    Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
    
[^105]: 基于预训练语言模型的上下文少样本关系抽取

    In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])

    [http://arxiv.org/abs/2310.11085](http://arxiv.org/abs/2310.11085)

    本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。

    

    关系提取旨在从文本文档中推断结构化的人类知识。基于语言模型的最先进方法通常有两个限制：(1)它们要求命名实体作为输入或推断它们，从而引入了额外的噪声，(2)它们需要人工对文档进行注释。为解决这些问题，我们提出了一种新颖的基于预训练语言模型的上下文少样本关系抽取框架。据我们所知，我们是第一个将关系抽取任务重新定义为定制的上下文少样本学习范式的研究者。通过这种方式，我们在消除了命名实体识别和文档人工注释的需求的同时，实现了关键性的优势。与现有的基于微调的方法不同，我们的框架具有灵活性，可以在无需重新训练的情况下轻松更新到新的关系集合。我们使用DocRED评估了我们的框架，这是目前最大的公开可用的文档级关系提取数据集。

    Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
    
[^106]: "引用GPT的“豚鼠试验”：一种研究企业竞争和勾结的创新智能代理建模方法"

    "Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])

    [http://arxiv.org/abs/2308.10974](http://arxiv.org/abs/2308.10974)

    "引用GPT的“豚鼠试验”是一种创新的智能代理建模方法，利用智能代理代表企业进行竞争和勾结研究。它比使用人类主体进行实验更具成本效益和灵活性，并展现出超越传统代理建模方法的能力。"

    

    企业竞争和勾结涉及复杂的动态，尤其是考虑到企业之间的沟通。这些问题可以被建模为复杂系统的问题，传统上通过涉及人类主体或基于代理的建模方法进行探究。我们提出了一种创新的框架，称为智能代理建模（SABM），其中由GPT-4技术支持的智能代理代表企业并相互交互。我们进行了一项控制实验，研究了不同条件下企业价格竞争和勾结行为。与使用人类主体进行实验相比，SABM更具成本效益和灵活性。智能代理拥有决策的广泛知识库，展现出类似人类的战略能力，超越了传统的基于代理的建模方法。此外，智能代理能够模拟人类对话并个性化，使其成为研究涉及沟通的复杂情况的理想选择。我们的结果表明...

    Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
    
[^107]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^108]: 论解码器Transformer语言模型的计算能力

    On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17026](http://arxiv.org/abs/2305.17026)

    本篇论文研究了解码器Transformer语言模型的计算普适性，表明即使只有单层和单注意力头，仍然具有图灵完备性，其中单词嵌入的稀疏性/可压缩性是必要条件。

    

    本文章对解码器Transformer模型的计算普适性进行了理论评估。我们扩展了Transformer模型的理论文献，并表明仅使用单层和单注意力头的解码器Transformer结构，在合理假设下具备图灵完备性。从理论分析中，我们证明了单词嵌入的稀疏性/可压缩性是图灵完备性成立的必要条件。

    This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
    
[^109]: LLM的多步推理中的两个自洽失败

    Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14279](http://arxiv.org/abs/2305.14279)

    本论文研究了大型语言模型在多步推理中的自洽性问题，提出了假设自洽性和组合自洽性两个重要特性，并发现GPT-3/-4模型在这两方面都表现出了较差的一致性。

    

    大型语言模型（LLM）在各种上下文为基础的少样本任务上取得了广泛成功，但这种成功通常是通过正确性而不是一致性来评估的。我们认为在解决由多个子步骤的答案组成的任务的多步推理中，自洽性是一个重要的标准。我们提出了两种对于多步推理特别重要的自洽性类型：假设自洽性（模型在假设的其他上下文中的输出预测能力）和组合自洽性（当将中间子步骤替换为模型对这些步骤的输出时，模型的最终输出的一致性）。我们证明了GPT-3/-4模型的多个变体在多种任务上都表现出了低一致性率。

    Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.
    
[^110]: 超越语言：句子表示综述

    Beyond Words: A Comprehensive Survey of Sentence Representations. (arXiv:2305.12641v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12641](http://arxiv.org/abs/2305.12641)

    本文综述了不同的句子表示学习方法，包括传统和基于深度学习技术的方法。突出该领域的主要贡献和挑战，并强调句子表示学习的进展和未来的研究方向。

    

    句子表示已经成为自然语言处理应用中的关键组成部分，如检索、问答和文本分类。它们捕捉句子的语义和含义，使计算机能够理解和推理人类语言。近年来，在学习句子表示方面已经取得了重大进展，包括无监督、监督和迁移学习方法。本文综述了不同的句子表示学习方法，包括传统和基于深度学习技术的方法。我们系统地整理了句子表示学习方面的文献，突出了该领域的主要贡献和挑战。总的来说，我们的综述强调了句子表示学习的进展，这一领域在自然语言处理中的重要性以及仍然存在的挑战。最后，我们提出了未来的研究方向。

    Sentence representations have become a critical component in natural language processing applications, such as retrieval, question answering, and text classification. They capture the semantics and meaning of a sentence, enabling machines to understand and reason over human language. In recent years, significant progress has been made in developing methods for learning sentence representations, including unsupervised, supervised, and transfer learning approaches. In this paper, we provide an overview of the different methods for sentence representation learning, including both traditional and deep learning-based techniques. We provide a systematic organization of the literature on sentence representation learning, highlighting the key contributions and challenges in this area. Overall, our review highlights the progress made in sentence representation learning, the importance of this area in natural language processing, and the challenges that remain. We conclude with directions for fu
    
[^111]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^112]: 评估ChatGPT的工作记忆容量

    Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])

    [http://arxiv.org/abs/2305.03731](http://arxiv.org/abs/2305.03731)

    本文评估了最先进语言模型ChatGPT的工作记忆容量，结果显示其在N-back任务的行为表现与人类参与者相似，这为设计具有人类级认知能力的人工智能系统提供了关键洞察。

    

    工作记忆是人类智能和人工智能的关键方面，它作为信息临时存储和操作的工作空间。本文通过检查ChatGPT在N-back任务上的表现，调查了这一最先进语言模型的工作记忆容量。我们首先讨论了工作记忆对人类和人工智能的重要性，接着介绍了评估ChatGPT工作记忆容量的方法。研究比较了ChatGPT在言语和空间N- back任务上的行为表现与文献报道的人类参与者的表现，发现了显著的相似之处。我们的发现为设计具有人类级认知能力的人工智能系统的当前进展提供了关键洞察，并为通过人工智能模型理解人类工作记忆的未来努力提供了前景。

    Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
    
[^113]: 使用文本到图像生成模型进行图像字幕数据管理

    Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])

    [http://arxiv.org/abs/2305.03610](http://arxiv.org/abs/2305.03610)

    本论文探究了通过数据整合来提高图像字幕质量的方案，并使用现有资源训练了比基准模型更好的模型。

    

    最近，图像字幕技术的发展主要依赖于大规模的视觉-语言预训练，并且日益依赖于计算资源和越来越大的多模态数据集。本文通过数据整合的两种方法探究了如何通过提高现有数据集中的样本质量来改善性能：一种方法假定由于图像和字幕之间的不匹配，某些示例应该避免使用，另一种方法则假定不匹配可以通过替换图像来解决。

    Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
    

