# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Inner Sentiments of a Thought.](http://arxiv.org/abs/2307.01784) | 这篇论文探索了基于Transformer的大型语言模型中句子内部情感表示的方法，训练了预测器来分析句子的情感分布，并展示了即使是普通的连接词也可以显著改变话语的情感轨迹。 |
| [^2] | [Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data.](http://arxiv.org/abs/2307.01764) | 在有限的标注数据下，我们提出了一种基于知识感知的音频引导生成式插槽填充方法，KA2G，它通过将文本生成任务与音频形式相结合，并依赖于外部知识来实现对语音导向对话系统的稳健和数据高效的插槽填充。 |
| [^3] | [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework.](http://arxiv.org/abs/2307.01715) | 本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。 |
| [^4] | [Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting.](http://arxiv.org/abs/2307.01709) | 提出了CSProm-KG方法，通过条件软提示实现结构与文本的有效融合，在知识图谱补全任务中取得了新的最优性能。 |
| [^5] | [Racial Bias Trends in the Text of US Legal Opinions.](http://arxiv.org/abs/2307.01693) | 研究发现美国法律文书中存在广泛的种族偏见，传统上与黑人相关的名字更与“不愉快”术语相关，而与白人相关的名字则更与“愉快”术语相关。此外，并未发现1950年前的法律意见存在更高程度的偏见，也未发现南方州的意见比东北州的意见在种族偏见方面变化较小的证据。 |
| [^6] | [Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation.](http://arxiv.org/abs/2307.01680) | 本文通过跨数据集比较发现，结合多个恶意言论检测数据集可以开发出鲁棒的恶意言论检测模型，即使在控制数据大小的情况下，这种鲁棒性仍然存在。 |
| [^7] | [Disentanglement in a GAN for Unconditional Speech Synthesis.](http://arxiv.org/abs/2307.01673) | 在无条件语音合成中，我们提出了一种名为ASGAN的解耦生成对抗网络模型，该模型借鉴了StyleGAN图像合成模型，并引入了一些新技术。通过学习解耦的潜在空间，ASGAN能够从潜在空间中合成逼真的语音，即使在小字典数据集上也能取得最新成果。 |
| [^8] | [Boosting Norwegian Automatic Speech Recognition.](http://arxiv.org/abs/2307.01672) | 该论文通过比较不同大小和预训练方法的模型在多个挪威语音数据集上的性能，提高了挪威自动语音识别的技术水平，并讨论了进一步改进挪威语ASR模型的挑战和潜在解决方案。 |
| [^9] | [Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues.](http://arxiv.org/abs/2307.01664) | 本文研究了统一对话模型中系统发起的闲聊和任务导向对话之间的转换问题，并提出了两种有效的提示模型来主动触发转换。 |
| [^10] | [Insert-expansions for Tool-enabled Conversational Agents.](http://arxiv.org/abs/2307.01644) | 本文研究了特定工具支持下的对话式智能助手的插入扩展。通过将用户作为工具，在生成明确推理路径时提供更多细节和更精确的请求，从而解决了工具干扰用户意图的问题。通过两个实证研究，发现在推荐领域中使用这种方法带来了好处。 |
| [^11] | [Chain of Thought Prompting Elicits Knowledge Augmentation.](http://arxiv.org/abs/2307.01640) | 本文提出了一种Chain-of-Thought-based的方法，名为CoT-KA，用于深度学习的知识增强。该方法避免了传统增强方法中需要额外的知识检索或知识推理模型的需求，并在多个基准测试中显示出优于其他方法的表现。 |
| [^12] | [A Language Model for Grammatical Error Correction in L2 Russian.](http://arxiv.org/abs/2307.01609) | 该论文提出了一个用于L2俄语写作错误修正的语言模型，该模型在俄罗斯国家语料库的新闻子语料库未标注文本上训练，并通过RULEC-GEC语料库验证了其质量。 |
| [^13] | [Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases.](http://arxiv.org/abs/2307.01595) | 这个论文提出了一种使用连续提示增强的对比学习的两阶段去偏模型，以减轻预训练语言模型中的社会偏见。在第一阶段，通过提示调整推进不同人口群体之间的表示距离。 |
| [^14] | [Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation.](http://arxiv.org/abs/2307.01542) | 通过自对比训练，我们成功减轻了预训练语言模型在开放式生成中对重复的学习偏差。我们的方法有效地缓解了重复问题，并保持了流畅性。 |
| [^15] | [Learning to Prompt in the Classroom to Understand AI Limits: A pilot study.](http://arxiv.org/abs/2307.01540) | 在本研究中，通过学习提示，试图在课堂环境中理解人工智能的限制。人工智能的进展带来了巨大的潜力，但也引发了负面情绪。当前大型语言模型的能力限制被忽视，导致了错误的自信和不准确的建议。承认人工智能的不可靠性是解决这个问题的关键。 |
| [^16] | [On Evaluating and Mitigating Gender Biases in Multilingual Settings.](http://arxiv.org/abs/2307.01503) | 本研究探讨了在多语言环境中评估和缓解性别偏见的挑战，扩展了评估性别偏见的基准和资源，并在印度语言上创建了一个用于评估预训练的掩码语言模型中性别偏见的基准。研究还将去偏方法扩展到英语以外的语言，并在提议的度量标准上评估了它们的有效性。 |
| [^17] | [SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification.](http://arxiv.org/abs/2307.01488) | 本文提出了一种名为SCAT的自监督对比学习框架，通过对抗训练来生成对抗样本，从而实现在不依赖标签数据的情况下学习鲁棒的表示。评估结果显示，SCAT在两个文本分类数据集上取得了良好性能。 |
| [^18] | [CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care.](http://arxiv.org/abs/2307.01458) | CARE-MI是一个用于评估中国孕婴护理领域LLM虚假信息的基准，填补了这一领域的研究空白，并提供了构建长篇生成评估基准的创新范式。 |
| [^19] | [Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking.](http://arxiv.org/abs/2307.01453) | 该论文提出了RefPyDST，通过在对话状态跟踪中的上下文学习中引入Python编程任务、多样性检索相关示例和重新权重解码方法的改进，取得了更好的性能。 |
| [^20] | [ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision.](http://arxiv.org/abs/2307.01448) | 本论文提出了ReactIE，利用弱监督方法提高了化学反应提取技术。通过使用文本中的频繁模式和专利记录中的合成数据，实现了对化学反应的特定特征识别，取得了显著的改进并超过了现有的基线方法。 |
| [^21] | [On Conditional and Compositional Language Model Differentiable Prompting.](http://arxiv.org/abs/2307.01446) | 本论文研究了条件和组合的可微提示方法，提出了Prompt Production System（PRopS）模型，通过将任务说明或输入元数据转化为连续的提示，使预训练语言模型（PLM）能够生成任务特定的输出。该模型利用了神经网络结构和离散规则的学习，适用于组合式迁移学习和少样本学习。实证和理论分析表明，PRopS在PLM适应中始终优于其他技术，并且通常改进了完全微调的方法。 |
| [^22] | [Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users.](http://arxiv.org/abs/2307.01420) | 本研究通过对17个StackExchange社区用户的标签行为进行分析，开发了一个灵活的神经网络标签预测架构，可以预测每个问题的热门标签和更细粒度的标签。 |
| [^23] | [Multi-Task Learning Improves Performance In Deep Argument Mining Models.](http://arxiv.org/abs/2307.01401) | 多任务学习方法提高了深度论证挖掘模型的性能，通过构建共享表示并利用任务之间的相似性，这种方法在不同的论证挖掘任务上取得了更好的成果。 |
| [^24] | [ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis.](http://arxiv.org/abs/2307.01387) | 本研究提出了一种名为ALBERTI的多语言预训练语言模型，通过领域特定预训练，在12种语言的1200万行诗歌上进行训练。在结构性诗歌任务上的性能评估显示，ALBERTI优于多语言BERT和其他类似模型，并且在德语任务上实现了最新的成果，证明了领域特定语言模型在诗歌领域的可行性和有效性。 |
| [^25] | [Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation.](http://arxiv.org/abs/2307.01381) | 本文提出了一种隐式内存变换器，通过新的左上下文方法隐式保留记忆，从而实现了计算高效的同时语音翻译。在MuST-C数据集上的实验表明，该方法提供了显著的速度提升和少量的性能损失。 |
| [^26] | [Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models.](http://arxiv.org/abs/2307.01379) | 本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。 |
| [^27] | [Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation.](http://arxiv.org/abs/2307.01377) | 该论文提出了一种名为"可转移上下文"的简单而有效的方案，用于解决同时语音翻译中的训练-推理上下文不匹配问题。通过保持一致的段落和上下文大小，即使存在部分填充的段落，该方案在流式任务的分段Transformer中也是广泛适用的。实验证明，应用于Augmented Memory Transformer后可以提高BLEU得分。 |
| [^28] | [Multilingual Language Models are not Multicultural: A Case Study in Emotion.](http://arxiv.org/abs/2307.01370) | 研究发现，多语言语言模型未能成功学习到文化适宜情绪的微妙差别，并提出了纠正这一问题的可能研究方向。 |
| [^29] | [Semantic enrichment towards efficient speech representations.](http://arxiv.org/abs/2307.01323) | 这项研究通过在具有挑战性的口语理解任务上专注于少量转录数据的特定领域语义丰富化，对SAMU-XLSR模型进行了改进，同时还探索了在资源有限的语言可移植性方面的优势和丰富的SAMU-XLSR的跨领域能力。 |
| [^30] | [Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective.](http://arxiv.org/abs/2307.01310) | 本研究探索了口述命名实体识别（NER）的跨语言视角。通过使用荷兰语、英语和德语之间的迁移学习，以及管道和端到端方案，利用自定义的伪标注数据集和Wav2Vec2-XLS-R模型，研究了几种适应跨语言系统的架构。结果显示，端到端口述NER在有限的标注数据上表现出优于管道系统的性能。值得注意的是，从德语到荷兰语的迁移学习取得了较好的效果，超过了荷兰语系统的性能。 |
| [^31] | [The Evolution of Substance Use Coverage in the Philadelphia Inquirer.](http://arxiv.org/abs/2307.01299) | 该研究分析了费城查问报十年间发表的157,476篇文章，突出了对物质使用和成瘾的准确和包容性描述的重要性。 |
| [^32] | [Large Language and Text-to-3D Models for Engineering Design Optimization.](http://arxiv.org/abs/2307.01230) | 本文研究了深度文本到三维模型在工程设计优化中的潜力和挑战，提出并实现了一个自动化的进化设计优化框架。 |
| [^33] | [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport.](http://arxiv.org/abs/2307.01226) | vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。 |
| [^34] | [Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT).](http://arxiv.org/abs/2307.01225) | 通过提出的解释性和透明性驱动的检测与转换（IT-DT）框架，我们在检测和转换文本对抗示例方面注重解释性和透明性。这个框架利用了注意力图、集成梯度和模型反馈等技术，在检测阶段有助于识别对对抗性分类有贡献的显著特征和扰动词语，并在转换阶段使用预训练的嵌入和模型反馈来生成扰动词语的最佳替代，以将对抗性示例转换为正常示例。 |
| [^35] | [Discovering Patterns of Definitions and Methods from Scientific Documents.](http://arxiv.org/abs/2307.01216) | 本文提出了一种从科学文献中发现定义和方法模式的分析方法，并在语义、句法和词汇层面上保证了模式的完整性。 |
| [^36] | [Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search.](http://arxiv.org/abs/2307.01214) | 这篇论文提出了一种基于单词组搜索的自动反事实扩充方法，用于鲁棒文本分类。该方法通过捕捉关键字组合的因果效应，并排序最影响预测的组合，从而解决了由于只关注单个单词而导致的错误因果特征的问题。 |
| [^37] | [An automated method for the ontological representation of security directives.](http://arxiv.org/abs/2307.01211) | 本文提出了一种将大型法律文件自动转化为本体表示的方法，通过自然语言处理技术和本体发展原则的结合，展示了在欧洲网络和信息系统安全指令上的应用。 |
| [^38] | [Multi-Dialectal Representation Learning of Sinitic Phonology.](http://arxiv.org/abs/2307.01209) | 该论文提出了一种在汉字音韵学中获取多方言表示的方法，通过构建知识图谱和应用无监督聚类技术，这些表示可以捕捉输入方言的音位对比并展示古老的原始语言特征的潜力。 |
| [^39] | [Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT.](http://arxiv.org/abs/2307.01202) | 本研究采用ChatGPT技术，以OpenAI的最先进的文本嵌入为基础，通过深度学习预测模型实现了对专利创新成功和估值的准确预测，为专利估值提供了革命性的改进。此外，通过预测接受率构建的多空投资组合实现了显著的异常收益率。 |
| [^40] | [Schema-learning and rebinding as mechanisms of in-context learning and emergence.](http://arxiv.org/abs/2307.01201) | 本文研究了上下文学习的机制，发现使用克隆结构因果图可以实现与transformer-based语言模型相似的能力，并且这种方法可以解释上下文学习的工作原理。 |
| [^41] | [Improving Language Plasticity via Pretraining with Active Forgetting.](http://arxiv.org/abs/2307.01163) | 本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。 |
| [^42] | [Counterfactual Collaborative Reasoning.](http://arxiv.org/abs/2307.00165) | 本文提出了反事实协同推理（CCR）方法，通过整合反事实推理和逻辑推理来提高机器学习模型的准确性和可解释性。通过利用反事实推理生成困难的反事实训练样本进行数据增强，CCR在推荐系统中展示了如何缓解数据稀缺、提高准确性和增强透明度。 |
| [^43] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^44] | [The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps.](http://arxiv.org/abs/2306.17059) | 该论文介绍了一种名为mapKurator的系统，能完整地从历史地图中提取和链接文本信息。该系统解决了传统方法中对位置相关词语的忽略问题，并利用主题建模方法考虑更广的主题范围，能够识别文档的空间焦点。 |
| [^45] | [Biomedical Entity Recognition by Detection and Matching.](http://arxiv.org/abs/2306.15736) | 本研究提出了一种名为DMNER的新型生物医学实体识别框架，通过检测实体边界和匹配生物医学实体来提高NER的性能。在有监督NER、远程监督NER和多数据集合并训练NER等场景中，DMNER都展示了良好的适用性。 |
| [^46] | [Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking.](http://arxiv.org/abs/2306.12245) | BEER^2是一种用于Retriever和Reader的双向端到端训练框架，通过检索器和阅读器之间的相互学习，共同进步，实现端到端EL。 |
| [^47] | [$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue.](http://arxiv.org/abs/2306.03361) | 本文提出了一种针对商业环境的、能够平衡对话流畅性和趋向于理解对话系统的个性化开放领域对话系统方法，通过加权数据集混合、负角色信息增强方法，以及设计个性化对话数据集，解决了 $\textit{WHAT}$、$\textit{WHEN}$和$\textit{HOW}$ 等问题，同时提高了对话系统响应的可控性和解释性。 |
| [^48] | [LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning.](http://arxiv.org/abs/2305.18169) | 本文提出了一种基于释义引导的数据增强用于对比型Prompt的少样本微调方法。该方法利用基础Prompt的少样本释义生成语言模型完成数据增强。 |
| [^49] | [Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models.](http://arxiv.org/abs/2305.16243) | 本文研究发现，用基于表面级别的检索机制取代语义检索可以显著降低检索增强语言模型的困惑度。 |
| [^50] | [Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts.](http://arxiv.org/abs/2305.14705) | Flan-MoE是一种指令调优的稀疏Mixture of Experts（MoE）语言模型，相对于密集模型，在指令微调和任务特定微调后均表现更好。最大模型Flan-MoE-32B的性能在四个基准测试中超越Flan-PaLM-62B，同时只利用了1/3的FLOPs。 |
| [^51] | [Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network.](http://arxiv.org/abs/2305.12493) | 本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。 |
| [^52] | [Evaluation of medium-large Language Models at zero-shot closed book generative question answering.](http://arxiv.org/abs/2305.11991) | 本文评估了大小为中型的语言模型在没有外部检索的情况下完成问答任务的表现，结果表明使用适当的训练数据进行模型微调比单纯依赖参数数量更重要，最好的模型实现了46.4%的正确率。 |
| [^53] | [Multimodal Sentiment Analysis: A Survey.](http://arxiv.org/abs/2305.07611) | 本综述介绍了多模态情感分析的定义、发展和挑战，讨论了最新的数据集和先进模型，并提出了有前途的研究方向和构建更好性能的建议。 |
| [^54] | [Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models.](http://arxiv.org/abs/2305.02531) | 本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。 |
| [^55] | [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.](http://arxiv.org/abs/2305.02301) | 本研究提出了Distilling Step-by-Step机制，通过提取LLM基础信息为小型模型提供额外的监督训练，从而使它们胜过更大的LLM模型，并需更少的训练数据。 |
| [^56] | [Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?.](http://arxiv.org/abs/2304.14796) | 本文系统比较了从句子级别嵌入中产生文档级嵌入的方法，基于预训练的多语言模型LASER、LaBSE和Sentence BERT。我们着重比较了输入令牌数截断、句子平均以及一些简单的窗口方法，对三个多语言和跨语言任务表现进行了比较。 |
| [^57] | [BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations.](http://arxiv.org/abs/2304.03682) | 本论文介绍了一个包括四个不同领域Bengali文本的新数据集- BenCoref。该数据集可以帮助理解Bengali多个领域中共指消解现象的差异，并促进Bengali的资源开发。多个模型在该数据集上训练的性能也得到了报告。在跨语言测试中，从英语到Bengali的交叉语言性能较差，显示出需要语言特定的共指消解系统。 |
| [^58] | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.](http://arxiv.org/abs/2303.13988) | 本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。 |
| [^59] | [LEVER: Learning to Verify Language-to-Code Generation with Execution.](http://arxiv.org/abs/2302.08468) | 提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。 |
| [^60] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^61] | [Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges.](http://arxiv.org/abs/2301.10075) | 研究了性别中性化翻译（GNT）作为一种性别包容性的方法，从英语到意大利语的翻译是一个突出的性别相关的语言翻译问题。研究回顾了一些相关的性别包容性语言指南，探讨了使用GNT的情景，并探讨了在MT中执行GNT的技术挑战和解决方案。 |
| [^62] | [Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment.](http://arxiv.org/abs/2212.10549) | 本研究提出了一种跨模态注意力一致性正则化方法，用于视觉-语言关系对齐。通过鼓励语言注意力与视觉注意力的一致性来实现关系级别的对齐，从而提高视觉-语言模型在组合概括性基准测试中的性能。 |
| [^63] | [Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training.](http://arxiv.org/abs/2212.10503) | 使用迷你模型适应的方法，通过构建浅层迷你模型以及高效训练新的语言特定嵌入向量，实现了将预训练模型扩展到新语言上的快速跨语言传输。 |
| [^64] | [RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding.](http://arxiv.org/abs/2212.05961) | RPN是一种基于词向量级别的数据增强算法，通过引入噪声修改原始文本的词嵌入，更好地捕捉自然语言变化，并在自然语言理解任务中表现出优异的性能。 |
| [^65] | [Democratizing Neural Machine Translation with OPUS-MT.](http://arxiv.org/abs/2212.01936) | 本文介绍了OPUS-MT生态系统的发展，包括开放式机器翻译模型和工具的开发，以及它们与最终用户应用程序、开发平台和专业工作流程的整合。通过增加语言覆盖范围和翻译质量，实现了神经机器翻译的民主化。 |
| [^66] | [Sentiment analysis and opinion mining on E-commerce site.](http://arxiv.org/abs/2211.15536) | 本论文研究了电子商务网站上的情感分析和意见挖掘，提出了解决情感极性分类挑战的广泛技术，并进行了句子级分类和评论级分类。 |
| [^67] | [Language Detoxification with Attribute-Discriminative Latent Space.](http://arxiv.org/abs/2210.10329) | 本研究提出了一种使用属性辨别潜空间进行语言去毒化的方法，通过将原始Transformer语言模型的潜空间投影到一个能够通过属性将文本进行良好分离的潜空间上，最小化内存和计算开销，实现了对有毒文本的控制。 |
| [^68] | [Exclusive Supermask Subnetwork Training for Continual Learning.](http://arxiv.org/abs/2210.10209) | 本研究提出了一种连续学习方法ExSSNeT，通过独占超掩码子网络训练和KNN-based知识传递，解决了固定权重限制和知识积累问题。 |
| [^69] | [IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces.](http://arxiv.org/abs/2210.05098) | 这项研究介绍了IsoVec，一种控制词向量空间相对同构性的方法，通过在Skip-gram损失函数中加入全局同构度度量，提高了训练后词向量空间的同构性，进而改善了跨语言映射效果。 |
| [^70] | [Compositionality as Lexical Symmetry.](http://arxiv.org/abs/2201.12926) | 本文将组合性定义为对数据分布的对称性约束，而不是模型，通过自动发现数据转换并应用于训练数据，提高模型的组合归纳偏置。 |

# 详细

[^1]: 一个思想的内在情感

    The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])

    [http://arxiv.org/abs/2307.01784](http://arxiv.org/abs/2307.01784)

    这篇论文探索了基于Transformer的大型语言模型中句子内部情感表示的方法，训练了预测器来分析句子的情感分布，并展示了即使是普通的连接词也可以显著改变话语的情感轨迹。

    

    基于Transformer的大型语言模型能够生成高度逼真的文本。它们能够表达并至少暗示出一系列情感和色彩，从明显的价值和唤起到微妙的决心和赞赏。我们首次探索了这些表示及其如何用于理解单个句子内部的情感运作。我们训练了从增长长度的前缀中应用到LLM的隐藏表示的句子的最终情感的分布的定量预测器。在展示了价值、决心、赞赏、焦虑和烦恼的分布预测器是良好校准的基础上，我们提供了使用这些预测器分析句子的示例，例如，展示了即使是普通的连接词（例如，“但是”）也可以极大地改变话语的情感轨迹。然后，我们展示了如何利用这些预测器来利用分布表示。

    Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., "but") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the dis
    
[^2]: 有限已标注数据下基于知识感知的音频引导生成式插槽填充方法

    Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data. (arXiv:2307.01764v1 [cs.CL])

    [http://arxiv.org/abs/2307.01764](http://arxiv.org/abs/2307.01764)

    在有限的标注数据下，我们提出了一种基于知识感知的音频引导生成式插槽填充方法，KA2G，它通过将文本生成任务与音频形式相结合，并依赖于外部知识来实现对语音导向对话系统的稳健和数据高效的插槽填充。

    

    为任务导向对话系统手动注释细粒度插槽-值标签是一项昂贵且耗时的工作。这促使研究仅使用有限标注数据进行插槽填充的方法。此外，当前关于任务导向对话系统的大部分研究仅基于文本作为输入形式，忽视了在处理口语时的不完美自动语音识别（ASR）所带来的额外挑战。在本研究中，我们提出了一种名为KA2G的基于知识感知的音频引导生成式插槽填充框架，专注于有语音输入的少样本和零样本插槽填充。KA2G通过以下方式实现了语音导向任务导向对话系统的稳健和数据高效的插槽填充：1）将其构造为文本生成任务，2）在文本生成中额外使用音频形式，3）在外部可用的知识条件下（例如，预定义的可能插槽值列表）。我们证明在KA2G框架内结合两种形式可以改善插槽填充效果。

    Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot-filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by 1) framing it as a text generation task, 2) grounding text generation additionally in the audio modality, and 3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the
    
[^3]: 符合目标：使用通用的插入式框架在CTC模型中优化所需属性

    Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])

    [http://arxiv.org/abs/2307.01715](http://arxiv.org/abs/2307.01715)

    本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。

    

    连接主义时间分类（CTC）是训练监督序列到序列模型广泛使用的准则。它通过将完美对齐（产生基本事实）的边际化来学习输入和输出序列之间的关系，称为对其，以代价不完美对齐。这种对完美和不完美对齐的二元区分无法捕捉到在其他实际应用中具有重要意义的其他关键对齐属性。在这里，我们提出了$\textit{Align With Purpose}$，这是一个用于增强CTC条件下训练模型中所需属性的$\textbf{通用插入式框架}$。我们通过使用额外的损失项来补充CTC来优先考虑符合所需属性的对齐。我们的方法不需要干预CTC损失函数，能够轻松优化各种属性，并且可以区分完美和不完美的对齐。

    Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
    
[^4]: 通过条件软提示，将结构和文本相结合，实现有效的知识图谱补全

    Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting. (arXiv:2307.01709v1 [cs.CL])

    [http://arxiv.org/abs/2307.01709](http://arxiv.org/abs/2307.01709)

    提出了CSProm-KG方法，通过条件软提示实现结构与文本的有效融合，在知识图谱补全任务中取得了新的最优性能。

    

    知识图谱补全通常需要结构和文本信息的共同作用。预训练的语言模型(PLMs)已被用于学习文本信息，在知识图谱补全任务中通常采用精调模式。然而，精调后的PLMs往往过于注重文本信息，忽视了结构知识。为了解决这个问题，本文提出了CSProm-KG(用于知识图谱补全的条件软提示)，在结构信息和文本知识之间保持平衡。CSProm-KG只调整由实体和关系表示生成的条件软提示参数。我们验证了CSProm-KG在三个流行的静态KGC基准WN18RR、FB15K-237和Wikidata5M，以及两个时态KGC基准ICEWS14和ICEWS05-15上的有效性。CSProm-KG的性能超过了竞争性基准模型，并在这些基准上达到了新的最优水平。我们进行进一步的分析，展示了我们方法的有效性。

    Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our pr
    
[^5]: 美国法律文书中的种族偏见趋势

    Racial Bias Trends in the Text of US Legal Opinions. (arXiv:2307.01693v1 [cs.CL])

    [http://arxiv.org/abs/2307.01693](http://arxiv.org/abs/2307.01693)

    研究发现美国法律文书中存在广泛的种族偏见，传统上与黑人相关的名字更与“不愉快”术语相关，而与白人相关的名字则更与“愉快”术语相关。此外，并未发现1950年前的法律意见存在更高程度的偏见，也未发现南方州的意见比东北州的意见在种族偏见方面变化较小的证据。

    

    虽然广泛认识到美国法律中存在种族偏见，但目前尚不清楚这种偏见在法律语言中的表现方式，即司法意见中是否存在时期或地区的差异。我们在大规模语料库中的隐性种族偏见测量方法的基础上，对1860年至2009年的600多万个美国联邦和州级法院案件进行了GloVe词向量估算。我们发现几乎所有地区和时期都存在明显的种族偏见，传统上与黑人相关的名字更与预分类的“不愉快”术语密切相关，而传统上与白人相关的名字更与预分类的“愉快”术语密切相关。我们还测试了1950年前的法律意见是否比1950年后的意见更具隐性种族偏见，以及南方州的意见是否比东北州的意见在种族偏见方面变化较小。我们并未发现1950年前的法律意见存在更高程度的偏见，也未发现南方州的意见比东北州的意见在种族偏见方面变化较小的证据。

    Although there is widespread recognition of racial bias in US law, it is unclear how such bias appears in the language of law, namely judicial opinions, and whether it varies across time period or region. Building upon approaches for measuring implicit racial bias in large-scale corpora, we approximate GloVe word embeddings for over 6 million US federal and state court cases from 1860 to 2009. We find strong evidence of racial bias across nearly all regions and time periods, as traditionally Black names are more closely associated with pre-classified "unpleasant" terms whereas traditionally White names are more closely associated with pre-classified "pleasant" terms. We also test whether legal opinions before 1950 exhibit more implicit racial bias than those after 1950, as well as whether opinions from Southern states exhibit less change in racial bias than those from Northeastern states. We do not find evidence of elevated bias in legal opinions before 1950, or evidence that legal opi
    
[^6]: 社交媒体中对恶意言论的鲁棒性检测：跨数据集的实证评估

    Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation. (arXiv:2307.01680v1 [cs.CL])

    [http://arxiv.org/abs/2307.01680](http://arxiv.org/abs/2307.01680)

    本文通过跨数据集比较发现，结合多个恶意言论检测数据集可以开发出鲁棒的恶意言论检测模型，即使在控制数据大小的情况下，这种鲁棒性仍然存在。

    

    在自然语言处理领域，对在线恶意言论的自动检测是一个活跃的研究领域。迄今为止，大多数研究都基于社交媒体数据集，这些数据集有助于训练恶意言论检测模型。然而，数据创建过程中存在自身的偏见，并且模型在本地数据集的偏见下进行学习。在本文中，我们进行了一个大规模的跨数据集比较，使用不同的恶意言论检测数据集来进行语言模型的微调。这项分析显示了当作为训练数据时，一些数据集比其他数据集更具有泛化能力。关键是，我们的实验表明，结合恶意言论检测数据集可以有助于开发出鲁棒的恶意言论检测模型。这种鲁棒性甚至在控制数据大小、与最佳单个数据集进行比较时也成立。

    The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalisable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.
    
[^7]: 无条件语音合成中的生成对抗网络中的解耦技术

    Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])

    [http://arxiv.org/abs/2307.01673](http://arxiv.org/abs/2307.01673)

    在无条件语音合成中，我们提出了一种名为ASGAN的解耦生成对抗网络模型，该模型借鉴了StyleGAN图像合成模型，并引入了一些新技术。通过学习解耦的潜在空间，ASGAN能够从潜在空间中合成逼真的语音，即使在小字典数据集上也能取得最新成果。

    

    我们是否能够开发一个模型，能够直接从潜在空间合成逼真的语音，而无需显式条件？尽管过去十年中进行了几次尝试，之前的对抗性和扩散性方法仍然难以实现，即使在小字典数据集上也是如此。为了解决这个问题，我们提出了AudioStyleGAN(ASGAN)——一种用于无条件语音合成的生成对抗网络，旨在学习一个解耦潜在空间。在StyleGAN系列图像合成模型的基础上构建ASGAN，它将采样噪声映射到一个解耦潜在向量，然后将其映射到一个音频特征序列，以在每一层中抑制信号混叠。为了成功训练ASGAN，我们引入了一些新技术，包括对自适应鉴别器增强的修改，使其以概率方式跳过鉴别器更新。我们将其应用于小字典的谷歌语音命令数字数据集上，在该数据集上实现了无条件语音合成方面的最新成果。

    Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional
    
[^8]: 提升挪威自动语音识别的技术

    Boosting Norwegian Automatic Speech Recognition. (arXiv:2307.01672v1 [cs.CL])

    [http://arxiv.org/abs/2307.01672](http://arxiv.org/abs/2307.01672)

    该论文通过比较不同大小和预训练方法的模型在多个挪威语音数据集上的性能，提高了挪威自动语音识别的技术水平，并讨论了进一步改进挪威语ASR模型的挑战和潜在解决方案。

    

    在本论文中，我们介绍了几种面向挪威两种官方书面语言（Bokmål和Nynorsk）的自动语音识别（ASR）模型的基准。我们比较了不同大小和预训练方法的模型在多个挪威语音数据集上的性能。另外，我们还将这些模型与之前的最新ASR模型以及不同领域的数据集进行了性能比较。我们将在挪威议会演讲语料库（NPSC）上的错误率（WER）从17.10％降低到7.60％，Bokmål和Nynorsk分别达到5.81％和11.54％。我们还讨论了进一步改进挪威语ASR模型的挑战和潜在解决方案。

    In this paper, we present several baselines for automatic speech recognition (ASR) models for the two official written languages in Norway: Bokm{\aa}l and Nynorsk. We compare the performance of models of varying sizes and pre-training approaches on multiple Norwegian speech datasets. Additionally, we measure the performance of these models against previous state-of-the-art ASR models, as well as on out-of-domain datasets. We improve the state of the art on the Norwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of 17.10\% to 7.60\%, with models achieving 5.81\% for Bokm{\aa}l and 11.54\% for Nynorsk. We also discuss the challenges and potential solutions for further improving ASR models for Norwegian.
    
[^9]: 具有系统发起的闲聊和任务导向对话之间转换的统一对话模型

    Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues. (arXiv:2307.01664v1 [cs.CL])

    [http://arxiv.org/abs/2307.01664](http://arxiv.org/abs/2307.01664)

    本文研究了统一对话模型中系统发起的闲聊和任务导向对话之间的转换问题，并提出了两种有效的提示模型来主动触发转换。

    

    近年来，研究人员开始探索能够同时进行闲聊和任务导向对话的统一对话模型。然而，在对话模式之间发生变化时的“主动性”潜力很少得到探索。本研究探讨了两种对话场景，一种是从闲聊开始，隐含地涉及与任务相关的主题，并最终转为任务导向的请求；另一种是从任务导向的互动开始，最终在提供所有请求的信息后转为随意聊天。我们贡献了两种高效的提示模型，可以主动生成一个过渡句来触发系统发起的转换。

    Spoken dialogue systems (SDSs) have been separately developed under two different categories, task-oriented and chit-chat. The former focuses on achieving functional goals and the latter aims at creating engaging social conversations without special goals. Creating a unified conversational model that can engage in both chit-chat and task-oriented dialogue is a promising research topic in recent years. However, the potential ``initiative'' that occurs when there is a change between dialogue modes in one dialogue has rarely been explored. In this work, we investigate two kinds of dialogue scenarios, one starts from chit-chat implicitly involving task-related topics and finally switching to task-oriented requests; the other starts from task-oriented interaction and eventually changes to casual chat after all requested information is provided. We contribute two efficient prompt models which can proactively generate a transition sentence to trigger system-initiated transitions in a unified 
    
[^10]: 特定工具支持下的对话式智能助手的插入扩展

    Insert-expansions for Tool-enabled Conversational Agents. (arXiv:2307.01644v1 [cs.HC])

    [http://arxiv.org/abs/2307.01644](http://arxiv.org/abs/2307.01644)

    本文研究了特定工具支持下的对话式智能助手的插入扩展。通过将用户作为工具，在生成明确推理路径时提供更多细节和更精确的请求，从而解决了工具干扰用户意图的问题。通过两个实证研究，发现在推荐领域中使用这种方法带来了好处。

    

    本文深入研究了在大型语言模型中实现Chain-of-Thought-Prompting的高级方式，重点研究了在这种提示方法生成的明确推理路径中使用工具（或“插件”）。我们发现，使用工具的对话式智能助手往往会走偏，因为来自搜索引擎或计算器等工具的额外上下文会偏离原始用户意图。为了解决这个问题，我们探索了一种概念，即用户成为工具，提供必要的细节并完善他们的请求。通过对话分析，我们将这种交互称为插入扩展-一种旨在促进所需响应的中间对话。我们通过两个实证研究使用直接比较的方法探索了从这种“用户作为工具”的方法中产生的可能性，并在推荐领域中发现了好处。

    This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Language Models, focusing on the use of tools (or "plug-ins") within the explicit reasoning paths generated by this prompting method. We find that tool-enabled conversational agents often become sidetracked, as additional context from tools like search engines or calculators diverts from original user intents. To address this, we explore a concept wherein the user becomes the tool, providing necessary details and refining their requests. Through Conversation Analysis, we characterize this interaction as insert-expansion - an intermediary conversation designed to facilitate the preferred response. We explore possibilities arising from this 'user-as-a-tool' approach in two empirical studies using direct comparison, and find benefits in the recommendation domain.
    
[^11]: 链式思维启发促进了知识增强

    Chain of Thought Prompting Elicits Knowledge Augmentation. (arXiv:2307.01640v1 [cs.CL])

    [http://arxiv.org/abs/2307.01640](http://arxiv.org/abs/2307.01640)

    本文提出了一种Chain-of-Thought-based的方法，名为CoT-KA，用于深度学习的知识增强。该方法避免了传统增强方法中需要额外的知识检索或知识推理模型的需求，并在多个基准测试中显示出优于其他方法的表现。

    

    知识增强的深度学习范式是指将领域知识识别并整合到深度模型中的一种范式。传统方法通常采用任务特定的方法从各种来源收集外部知识。相比之下，大型语言模型通过广泛的预训练可以作为全面的外部知识来源。在本文中，我们提出了一种基于链式思维的CoT-KA方法，用于深度学习的知识增强。CoT-KA避免了传统增强方法中所需的额外知识检索或知识推理模型。我们的结果表明，在多个公开可用的各种推理任务的十一个基准测试中，CoT-KA在纯CoT方法和非增强方法之上表现出更好的效果。

    The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.
    
[^12]: 用于L2俄语语法错误修正的语言模型

    A Language Model for Grammatical Error Correction in L2 Russian. (arXiv:2307.01609v1 [cs.CL])

    [http://arxiv.org/abs/2307.01609](http://arxiv.org/abs/2307.01609)

    该论文提出了一个用于L2俄语写作错误修正的语言模型，该模型在俄罗斯国家语料库的新闻子语料库未标注文本上训练，并通过RULEC-GEC语料库验证了其质量。

    

    语法错误修正是自然语言处理中的基本任务之一。对于俄语，大多数可用的拼写检查程序可以高准确率地纠正拼写错误和其他简单错误，但在面对非母语（L2）写作时往往失效，因为后者含有对于母语者来说不典型的错误。本文提出了一种使用语言模型修正L2俄语写作错误的流程。所提出的语言模型是在俄罗斯国家语料库新闻子语料库的未标注文本上训练的，并通过RULEC-GEC语料库验证模型的质量。

    Grammatical error correction is one of the fundamental tasks in Natural Language Processing. For the Russian language, most of the spellcheckers available correct typos and other simple errors with high accuracy, but often fail when faced with non-native (L2) writing, since the latter contains errors that are not typical for native speakers. In this paper, we propose a pipeline involving a language model intended for correcting errors in L2 Russian writing. The language model proposed is trained on untagged texts of the Newspaper subcorpus of the Russian National Corpus, and the quality of the model is validated against the RULEC-GEC corpus.
    
[^13]: 提示调整进一步推进，对比学习拉近距离：一种两阶段方法来减轻社会偏见

    Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. (arXiv:2307.01595v1 [cs.CL])

    [http://arxiv.org/abs/2307.01595](http://arxiv.org/abs/2307.01595)

    这个论文提出了一种使用连续提示增强的对比学习的两阶段去偏模型，以减轻预训练语言模型中的社会偏见。在第一阶段，通过提示调整推进不同人口群体之间的表示距离。

    

    随着预训练语言模型（PLMs）的表示能力的提高，人们越来越担心它们会继承未经处理的语料库中的社会偏见。大多数先前的去偏技术使用对比数据增强（CDA）来平衡训练语料库。然而，CDA略微修改了原始语料库，限制了不同人口群体之间的表示距离在一个狭窄范围内。结果，去偏模型容易适应对比事实对之间的差异，这影响了它在有限的文本资源下的去偏性能。在本文中，我们提出了一种受对抗训练启发的两阶段去偏模型，使用连续提示增强的对比学习（称为CCPA）来减轻PLMs编码中的社会偏见。在第一阶段，我们提出了一种基于连续提示调整的数据增强方法，可以进一步推进不同人口群体之间的表示距离。在第二阶段，

    As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second sta
    
[^14]: 通过自对比训练减轻开放式生成中对重复的学习偏差

    Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation. (arXiv:2307.01542v1 [cs.CL])

    [http://arxiv.org/abs/2307.01542](http://arxiv.org/abs/2307.01542)

    通过自对比训练，我们成功减轻了预训练语言模型在开放式生成中对重复的学习偏差。我们的方法有效地缓解了重复问题，并保持了流畅性。

    

    尽管在各种生成任务中取得了巨大的进展，但预训练语言模型（如GPT2）仍倾向于使用基于最大化的解码算法生成重复的文本。我们将其对令牌级别重复概率的过度估计归因于学习偏差：语言模型使用MLE损失更快地捕捉到简单的重复模式。我们提出了自对比训练，以惩罚同一模型过早检查点的输出，当它错误地预测重复时，这在两个数据集上显示出有效减轻重复同时保持流畅的效果。此外，我们发现语言模型在预测重复令牌时使用的是更长范围的依赖关系，而非重复令牌则不然，这可能是造成句子级别重复循环的原因。

    Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.
    
[^15]: 在课堂上学习提示以了解人工智能的限制：一项试点研究

    Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])

    [http://arxiv.org/abs/2307.01540](http://arxiv.org/abs/2307.01540)

    在本研究中，通过学习提示，试图在课堂环境中理解人工智能的限制。人工智能的进展带来了巨大的潜力，但也引发了负面情绪。当前大型语言模型的能力限制被忽视，导致了错误的自信和不准确的建议。承认人工智能的不可靠性是解决这个问题的关键。

    

    人工智能的进展在帮助社会解决紧迫的社会问题方面具有巨大的潜力。特别是大型语言模型（LLM）和派生的聊天机器人，如ChatGPT，大大改进了AI系统的自然语言处理能力，使其能够处理前所未有的大量非结构化数据。由此产生的炒作也产生了负面情绪，即使在新颖的AI方法取得令人惊讶的贡献之后。造成这种情况的原因之一，但也是一个重要的问题本身，是越来越多人错误地认为自己能够轻松访问和处理任何形式的知识，以解决任何领域的问题，无需对AI或问题领域有任何专业知识，而忽视了当前LLMs的限制，例如幻觉和推理限制。承认人工智能的不可靠性对于解决由LLMs生成的可能错误建议可能产生的盲目过度自信的影响至关重要。同时，这可以减少恐惧和其他负面态度。

    Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitude
    
[^16]: 在多语言环境中评估和缓解性别偏见

    On Evaluating and Mitigating Gender Biases in Multilingual Settings. (arXiv:2307.01503v1 [cs.CL])

    [http://arxiv.org/abs/2307.01503](http://arxiv.org/abs/2307.01503)

    本研究探讨了在多语言环境中评估和缓解性别偏见的挑战，扩展了评估性别偏见的基准和资源，并在印度语言上创建了一个用于评估预训练的掩码语言模型中性别偏见的基准。研究还将去偏方法扩展到英语以外的语言，并在提议的度量标准上评估了它们的有效性。

    

    尽管在自然语言处理中理解和消除语言模型中的性别偏见是一个长期存在的问题，但先前的研究工作主要局限于英语。在这项工作中，我们研究了在多语言环境中评估和缓解偏见所面临的一些挑战，这些挑战源于缺乏用于非西方背景的性别偏见评估的现有基准和资源。在本文中，我们首先通过使用人工注释，将DisCo扩展到不同的印度语言，为评估预先训练的掩码语言模型中的性别偏见创建了一个基准。我们将各种去偏方法扩展到英语以外的语言，并在我们提出的度量标准上评估它们对SOTA大规模多语言模型的有效性。总的来说，我们的工作突出了在多语言环境中研究社会偏见时面临的挑战，并提供了资源和缓解技术，以推进更多语言的规模化进展。

    While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.
    
[^17]: SCAT：通过对抗训练的自监督对比学习技术实现文本分类的鲁棒性

    SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification. (arXiv:2307.01488v1 [cs.CL])

    [http://arxiv.org/abs/2307.01488](http://arxiv.org/abs/2307.01488)

    本文提出了一种名为SCAT的自监督对比学习框架，通过对抗训练来生成对抗样本，从而实现在不依赖标签数据的情况下学习鲁棒的表示。评估结果显示，SCAT在两个文本分类数据集上取得了良好性能。

    

    尽管在各种自然语言处理（NLP）任务中表现出很好的性能，但当前的NLP系统容易受到文本对抗攻击的影响。为了防御这些攻击，现有方法大多数采用对抗训练的方式来引入对抗样本。然而，这些方法通常需要依赖于标准标签来生成对抗样本，这在如今常用于NLP和其他任务的大规模模型预训练中是不可行的。本文提出了一种新的学习框架，名为SCAT（通过对抗训练的自监督对比学习），它可以在不需要标记数据的情况下学习鲁棒的表示。具体而言，SCAT通过对数据进行完全无标记的随机增强来生成对抗样本，并通过最小化增强和其对应的对抗样本之间的对比损失来实现对抗训练。我们在两个文本分类数据集上评估了SCAT的性能。

    Despite their promising performance across various natural language processing (NLP) tasks, current NLP systems are vulnerable to textual adversarial attacks. To defend against these attacks, most existing methods apply adversarial training by incorporating adversarial examples. However, these methods have to rely on ground-truth labels to generate adversarial examples, rendering it impractical for large-scale model pre-training which is commonly used nowadays for NLP and many other tasks. In this paper, we propose a novel learning framework called SCAT (Self-supervised Contrastive Learning via Adversarial Training), which can learn robust representations without requiring labeled data. Specifically, SCAT modifies random augmentations of the data in a fully labelfree manner to generate adversarial examples. Adversarial training is achieved by minimizing the contrastive loss between the augmentations and their adversarial counterparts. We evaluate SCAT on two text classification dataset
    
[^18]: CARE-MI: 中国孕婴护理领域的虚假信息评估基准

    CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])

    [http://arxiv.org/abs/2307.01458](http://arxiv.org/abs/2307.01458)

    CARE-MI是一个用于评估中国孕婴护理领域LLM虚假信息的基准，填补了这一领域的研究空白，并提供了构建长篇生成评估基准的创新范式。

    

    最近自然语言处理的进展导致了将LLM应用于现实场景的新趋势。尽管最新的LLM在与人类互动时令人惊叹地流利，但它们在生成错误事实陈述时会意外产生虚假信息问题。这可能导致有害后果，尤其是在敏感环境下，比如医疗保健领域。然而，之前很少有研究关注评估LLM长篇生成中的虚假信息，尤其是针对知识密集型主题。此外，尽管LLM在不同语言上表现良好，但虚假信息评估主要在英语中进行。为此，我们提供了一个基准，CARE-MI，用于评估LLM虚假信息在：1）一个敏感主题，具体是孕婴护理领域；和2）一种非英语语言，即中文。最重要的是，我们提供了一个创新的范式，用于构建长篇生成评估基准，可以

    The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can
    
[^19]: 多样的检索增强上下文学习用于对话状态跟踪

    Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. (arXiv:2307.01453v1 [cs.CL])

    [http://arxiv.org/abs/2307.01453](http://arxiv.org/abs/2307.01453)

    该论文提出了RefPyDST，通过在对话状态跟踪中的上下文学习中引入Python编程任务、多样性检索相关示例和重新权重解码方法的改进，取得了更好的性能。

    

    由于收集和注释面向任务对话的成本较高，对于零样本和少样本学习的对话状态跟踪 (DST) 引起了重大兴趣。最近的研究表明，在上下文学习中，只需要很少的数据和零个参数更新，甚至在少样本设置中优于训练方法 (Hu等，2022)。我们提出了RefPyDST，它通过三个改进推动了对DST的上下文学习的最新进展。首先，我们将DST形式化为Python编程任务，明确地将语言指代建模为Python中的变量引用。其次，由于上下文学习高度依赖上下文示例，我们提出了一种检索多样的相关示例以提高性能的方法。最后，在解码过程中引入了一种新颖的重新权重方法，考虑了竞争表面形式的概率，并产生了更准确的对话状态预测结果。我们使用MultiWOZ和进行了评估

    There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST. First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and 
    
[^20]: ReactIE: 借助弱监督提升化学反应提取技术

    ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision. (arXiv:2307.01448v1 [cs.CL])

    [http://arxiv.org/abs/2307.01448](http://arxiv.org/abs/2307.01448)

    本论文提出了ReactIE，利用弱监督方法提高了化学反应提取技术。通过使用文本中的频繁模式和专利记录中的合成数据，实现了对化学反应的特定特征识别，取得了显著的改进并超过了现有的基线方法。

    

    结构化化学反应信息对从事化学实验和计算辅助药物设计等领域的化学家来说至关重要。尽管从科学文献中提取结构化反应的重要性，但由于领域专家需要大量的工作，导致为此目的进行数据标注成本过高。因此，缺乏足够的训练数据成为该领域相关模型进展的障碍。本文提出了ReactIE，它结合两种弱监督方法进行预训练。我们的方法利用文本中的频繁模式作为语言线索来识别化学反应的特定特征。此外，我们采用专利记录中的合成数据作为远程监督，将领域知识纳入模型中。实验证明，ReactIE取得了显著的改进，并优于所有现有的基线方法。

    Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.
    
[^21]: 关于条件和组合语言模型可微提示的论文

    On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])

    [http://arxiv.org/abs/2307.01446](http://arxiv.org/abs/2307.01446)

    本论文研究了条件和组合的可微提示方法，提出了Prompt Production System（PRopS）模型，通过将任务说明或输入元数据转化为连续的提示，使预训练语言模型（PLM）能够生成任务特定的输出。该模型利用了神经网络结构和离散规则的学习，适用于组合式迁移学习和少样本学习。实证和理论分析表明，PRopS在PLM适应中始终优于其他技术，并且通常改进了完全微调的方法。

    

    提示已被证明是一种有效的方法，用于使预训练语言模型（PLM）在下游任务中表现出色。提示可以由人工设计的词序列或学习得到的连续嵌入来表示。在这项工作中，我们研究了条件和组合的可微提示。我们提出了一个新模型，Prompt Production System（PRopS），它学习将任务说明或输入元数据转化为连续的提示，从而激发PLM产生任务特定的输出。我们的模型使用基于我们对于产品系统的神经形式化的模块化网络结构，这使得模型能够学习离散规则——神经函数，这些函数学习专门将特定的提示输入模式转化为特定的输出，使其适用于组合式迁移学习和少样本学习。我们进行了广泛的实证和理论分析，并展示了PRopS始终超越其他PLM适应技术，并且通常改进了完全微调的方法。

    Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine
    
[^22]: 基于社区问答平台用户标签行为分析的标签预测模型

    Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users. (arXiv:2307.01420v1 [cs.CL])

    [http://arxiv.org/abs/2307.01420](http://arxiv.org/abs/2307.01420)

    本研究通过对17个StackExchange社区用户的标签行为进行分析，开发了一个灵活的神经网络标签预测架构，可以预测每个问题的热门标签和更细粒度的标签。

    

    在社区问答平台中，标签在有效的信息组织和检索、更好的问题路由、更快的问题响应以及话题热度评估方面都起着重要作用。因此，对于这类平台的用户来说，预测和建议帖子的标签是非常有用的自动辅助功能。为了在不同的社区和领域中开发更好的标签预测模型，我们对17个StackExchange社区的用户标签行为进行了深入分析。我们发现了这些不同领域中用户标签行为的一些共同属性。我们利用这些发现开发了一个灵活的神经网络标签预测架构，可以预测每个问题的热门标签和更细粒度的标签。我们的大量实验和获得的性能表明了我们模型的有效性。

    In community question-answering platforms, tags play essential roles in effective information organization and retrieval, better question routing, faster response to questions, and assessment of topic popularity. Hence, automatic assistance for predicting and suggesting tags for posts is of high utility to users of such platforms. To develop better tag prediction across diverse communities and domains, we performed a thorough analysis of users' tagging behavior in 17 StackExchange communities. We found various common inherent properties of this behavior in those diverse domains. We used the findings to develop a flexible neural tag prediction architecture, which predicts both popular tags and more granular tags for each question. Our extensive experiments and obtained performance show the effectiveness of our model
    
[^23]: 多任务学习提高深度论证挖掘模型的性能

    Multi-Task Learning Improves Performance In Deep Argument Mining Models. (arXiv:2307.01401v1 [cs.CL])

    [http://arxiv.org/abs/2307.01401](http://arxiv.org/abs/2307.01401)

    多任务学习方法提高了深度论证挖掘模型的性能，通过构建共享表示并利用任务之间的相似性，这种方法在不同的论证挖掘任务上取得了更好的成果。

    

    成功地从用户生成的文本中分析论证技巧对于许多下游任务（如政治和市场分析）至关重要。最近的论证挖掘工具使用先进的深度学习方法从各种在线文本语料库中提取和注释论证技巧，然而每个任务被视为独立的，不同的特定模型被针对每个数据集进行了微调。我们通过实施多任务方法来论证挖掘，表明不同的论证挖掘任务之间共享常见的语义和逻辑结构，这种方法在解决相同问题时达到了比最先进方法更好的性能。我们的模型构建了一个对所有任务都共有的输入文本的共享表示，并通过参数共享利用任务之间的相似性进一步提高性能。我们的结果对于论证挖掘非常重要，因为它们表明不同的任务之间存在相当大的相似之处，并提出了一种整体的论证提取方法。

    The successful analysis of argumentative techniques from user-generated text is central to many downstream tasks such as political and market analysis. Recent argument mining tools use state-of-the-art deep learning methods to extract and annotate argumentative techniques from various online text corpora, however each task is treated as separate and different bespoke models are fine-tuned for each dataset. We show that different argument mining tasks share common semantic and logical structure by implementing a multi-task approach to argument mining that achieves better performance than state-of-the-art methods for the same problems. Our model builds a shared representation of the input text that is common to all tasks and exploits similarities between tasks in order to further boost performance via parameter-sharing. Our results are important for argument mining as they show that different tasks share substantial similarities and suggest a holistic approach to the extraction of argume
    
[^24]: ALBERTI,一种用于诗歌分析的多语言领域特定语言模型.

    ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])

    [http://arxiv.org/abs/2307.01387](http://arxiv.org/abs/2307.01387)

    本研究提出了一种名为ALBERTI的多语言预训练语言模型，通过领域特定预训练，在12种语言的1200万行诗歌上进行训练。在结构性诗歌任务上的性能评估显示，ALBERTI优于多语言BERT和其他类似模型，并且在德语任务上实现了最新的成果，证明了领域特定语言模型在诗歌领域的可行性和有效性。

    

    诗歌的计算分析受到自动分析和扫描工具的稀缺性的限制。在多语言环境中，由于仅存在单个语言的扫描和韵律系统，因此比较性研究非常具有挑战性和耗时。在这项工作中，我们提出了\textsc{Alberti}，这是第一个用于诗歌的多语言预训练大型语言模型。通过领域特定的预训练(DSP)，我们在12种语言的1,200万行诗歌语料库上对多语言BERT进行了进一步训练。我们在两个结构性诗歌任务上评估了其性能: 西班牙诗歌篇章类型分类以及西班牙语、英语和德语的韵律模式预测。在这两种情况下，\textsc{Alberti}优于多语言BERT和其他相似规模的基于转换器的模型，甚至在与基于规则的系统进行比较时，对于德语实现了最新的成果，展示了领域特定语言模型在诗歌领域的可行性和有效性。

    The computational analysis of poetry is limited by the scarcity of tools to automatically analyze and scan poems. In a multilingual settings, the problem is exacerbated as scansion and rhyme systems only exist for individual languages, making comparative studies very challenging and time consuming. In this work, we present \textsc{Alberti}, the first multilingual pre-trained large language model for poetry. Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages. We evaluated its performance on two structural poetry tasks: Spanish stanza type classification, and metrical pattern prediction for Spanish, English and German. In both cases, \textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes, and even achieves state-of-the-art results for German when compared to rule-based systems, demonstrating the feasibility and effectiveness of DSP in the poetry domain.
    
[^25]: 隐式内存变换器用于计算高效的同时语音翻译

    Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])

    [http://arxiv.org/abs/2307.01381](http://arxiv.org/abs/2307.01381)

    本文提出了一种隐式内存变换器，通过新的左上下文方法隐式保留记忆，从而实现了计算高效的同时语音翻译。在MuST-C数据集上的实验表明，该方法提供了显著的速度提升和少量的性能损失。

    

    同时语音翻译是一项困难的人类交流任务，即在进行语音输入的同时生成翻译。对于这样的流式任务，使用块处理将输入序列分割成片段的Transformer在降低成本的同时实现了最先进的性能。当前的方法允许信息在片段之间传播，包括左上下文和存储器库，但它们既是不充分的表示又是不必要的计算开销。在本文中，我们提出了一种隐式内存变换器，通过一种新的左上下文方法隐式保留记忆，从而消除了需要用存储器库显式表示记忆的需求。我们通过前一个片段的注意力输出生成左上下文，并将其包含在当前片段的注意力计算的键和值中。对MuST-C数据集的实验证明，隐式内存变换器提供了显著的速度提升和少量的性能损失。

    Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedu
    
[^26]: 将关注点转移到相关性上: 探索大型语言模型的不确定性估计

    Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])

    [http://arxiv.org/abs/2307.01379](http://arxiv.org/abs/2307.01379)

    本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。

    

    虽然大型语言模型（LLMs）在自然语言生成方面表现出了巨大的潜力，但是对于模型生成的不确定性的特征化仍然具有挑战性，即用户何时可以信任模型的输出。我们的研究基于一些启发性的事实，即在自回归的LLMs中，令牌在反映生成的含义方面是不平等的，即一些令牌比其他令牌更相关（或更具代表性），然而在估计不确定性时所有的令牌被等值对待。这是由于语言冗余，其中大部分情况下，只需要几个关键词就足以传达一个长句的含义。我们将这些不平等称为生成的不平等，并研究它们如何影响不确定性的估计。我们的结果揭示，相当数量的令牌和包含有限语义的句子，在估计不确定性时被同等或甚至更加重视。为了解决由生成的不平等引起的这些偏差，我们提出了共同转移关注点来更好地估计不确定性。

    Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
    
[^27]: 可转移上下文：解决同时语音翻译中的训练-推理上下文不匹配问题

    Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])

    [http://arxiv.org/abs/2307.01377](http://arxiv.org/abs/2307.01377)

    该论文提出了一种名为"可转移上下文"的简单而有效的方案，用于解决同时语音翻译中的训练-推理上下文不匹配问题。通过保持一致的段落和上下文大小，即使存在部分填充的段落，该方案在流式任务的分段Transformer中也是广泛适用的。实验证明，应用于Augmented Memory Transformer后可以提高BLEU得分。

    

    在同时语音翻译中，使用分段处理的Transformer模型已经成为一种有效的架构。然而，这种模型在训练和推理环境之间创建了上下文不匹配的问题，阻碍了潜在的翻译准确性。我们通过提出可转移上下文来解决这个问题，这是一种简单而有效的方案，可以确保在训练和推理过程中始终维持一致的段落和上下文大小，即使由于流式翻译的性质导致部分填充的段落存在。可转移上下文在流式任务的分段Transformer中也是广泛适用的。我们在MUST-C数据集的英德、英法和英西语言对上进行的实验表明，将该方案应用于Augmented Memory Transformer（一种用于同时语音翻译的最先进模型）可以平均提高2.09、1.83和1.95个BLEU分数。

    Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value f
    
[^28]: 多语言语言模型并非多元文化的: 以情绪为案例研究

    Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v1 [cs.CL])

    [http://arxiv.org/abs/2307.01370](http://arxiv.org/abs/2307.01370)

    研究发现，多语言语言模型未能成功学习到文化适宜情绪的微妙差别，并提出了纠正这一问题的可能研究方向。

    

    情绪在世界各地的经历和表达方式不同。为了在需要情感敏感性的多语言任务中使用大型语言模型(LMs)，LMs必须反映情绪上的文化差异。在本研究中，我们调查了2023年广泛使用的多语言LMs是否反映了跨文化和跨语言情感表达的差异。我们发现从LMs(如XLM-RoBERTa)获得的嵌入是以英语为中心的，生成型LMs(如ChatGPT)在回应其他语言的提示时也体现了西方规范。我们的结果表明，多语言LMs并没有成功地学习文化上适当的情绪细微差别，并强调了可能的研究方向以纠正这一点。

    Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.
    
[^29]: 向语义丰富化的有效语音表示方法迈进

    Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])

    [http://arxiv.org/abs/2307.01323](http://arxiv.org/abs/2307.01323)

    这项研究通过在具有挑战性的口语理解任务上专注于少量转录数据的特定领域语义丰富化，对SAMU-XLSR模型进行了改进，同时还探索了在资源有限的语言可移植性方面的优势和丰富的SAMU-XLSR的跨领域能力。

    

    在过去几年中，自监督学习的语音表示在解决口语理解任务时已成为传统表面表示的有效替代品。与此同时，利用大规模文本数据训练的多语言模型被引入以编码语言无关的语义信息。最近，SAMU-XLSR方法提出了一种利用这种文本模型从而使多语言语音表示增加语言无关语义的方法。本研究针对具有挑战性的口语理解任务并考虑计算成本，通过专注于对下游任务的少量转录数据的特定领域语义丰富化，对SAMU-XLSR模型进行了研究。此外，我们展示了在资源有限的语言可移植性方面，使用同领域的法语和意大利语基准的好处，还探索了丰富的SAMU-XLSR的跨领域能力。

    Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.
    
[^30]: 探索口述命名实体识别：跨语言视角

    Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective. (arXiv:2307.01310v1 [cs.CL])

    [http://arxiv.org/abs/2307.01310](http://arxiv.org/abs/2307.01310)

    本研究探索了口述命名实体识别（NER）的跨语言视角。通过使用荷兰语、英语和德语之间的迁移学习，以及管道和端到端方案，利用自定义的伪标注数据集和Wav2Vec2-XLS-R模型，研究了几种适应跨语言系统的架构。结果显示，端到端口述NER在有限的标注数据上表现出优于管道系统的性能。值得注意的是，从德语到荷兰语的迁移学习取得了较好的效果，超过了荷兰语系统的性能。

    

    最近在命名实体识别（NER）方面取得了显著进展，提高了文本数据中实体的识别能力。然而，口述NER作为口述文档检索的专门领域，由于研究有限和数据稀缺而滞后。此外，口述NER中的跨语言迁移学习仍未被探索。本文利用荷兰语、英语和德语之间的迁移学习，使用管道和端到端（E2E）方案。我们利用自定义的伪标注数据集使用Wav2Vec2-XLS-R模型，并研究了几种适应跨语言系统的架构。我们的结果表明，端到端口述NER在我们有限的标注数据上优于基于管道的替代方案。值得注意的是，从德语到荷兰语的迁移学习超过了荷兰语E2E系统7%和荷兰语管道系统4%。这项研究不仅突出了口述NER中迁移学习的可行性，而且为未来的评估提供了有希望的结果。

    Recent advancements in Named Entity Recognition (NER) have significantly improved the identification of entities in textual data. However, spoken NER, a specialized field of spoken document retrieval, lags behind due to its limited research and scarce datasets. Moreover, cross-lingual transfer learning in spoken NER has remained unexplored. This paper utilizes transfer learning across Dutch, English, and German using pipeline and End-to-End (E2E) schemes. We employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and investigate several architectures for the adaptability of cross-lingual systems. Our results demonstrate that End-to-End spoken NER outperforms pipeline-based alternatives over our limited annotations. Notably, transfer learning from German to Dutch surpasses the Dutch E2E system by 7% and the Dutch pipeline system by 4%. This study not only underscores the feasibility of transfer learning in spoken NER but also sets promising outcomes for future evaluations, hint
    
[^31]: 费城查问报中物质使用报道的演变

    The Evolution of Substance Use Coverage in the Philadelphia Inquirer. (arXiv:2307.01299v1 [cs.CL])

    [http://arxiv.org/abs/2307.01299](http://arxiv.org/abs/2307.01299)

    该研究分析了费城查问报十年间发表的157,476篇文章，突出了对物质使用和成瘾的准确和包容性描述的重要性。

    

    媒体对非法物质使用的表述可能导致对与成瘾斗争的个体产生有害的刻板印象和污名化，最终影响公众对待成瘾问题的看法、政策和公共卫生结果。为了探索非法药物使用的言论和报道如何随时间变化，该研究分析了费城查问报在十年间发表的157,476篇文章。具体而言，该研究聚焦于提及至少一种常见滥用物质的文章，得到了3,903篇文章的样本。我们的分析显示，大麻和麻醉药是最经常被讨论的毒品类别。幻觉药物比其他类别更正面地呈现，而麻醉药则是最负面的。我们的研究旨在突出媒体对物质使用和成瘾的准确和包容性描述的必要性。

    The media's representation of illicit substance use can lead to harmful stereotypes and stigmatization for individuals struggling with addiction, ultimately influencing public perception, policy, and public health outcomes. To explore how the discourse and coverage of illicit drug use changed over time, this study analyzes 157,476 articles published in the Philadelphia Inquirer over a decade. Specifically, the study focuses on articles that mentioned at least one commonly abused substance, resulting in a sample of 3,903 articles. Our analysis shows that cannabis and narcotics are the most frequently discussed classes of drugs. Hallucinogenic drugs are portrayed more positively than other categories, whereas narcotics are portrayed the most negatively. Our research aims to highlight the need for accurate and inclusive portrayals of substance use and addiction in the media.
    
[^32]: 大规模语言和文本到三维模型用于工程设计优化

    Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])

    [http://arxiv.org/abs/2307.01230](http://arxiv.org/abs/2307.01230)

    本文研究了深度文本到三维模型在工程设计优化中的潜力和挑战，提出并实现了一个自动化的进化设计优化框架。

    

    当前生成式人工智能在学习大规模神经网络模型方面取得的进展，具有从文本提示生成论文、图像、音乐甚至三维资产的能力，为多学科提供了机会。本文研究了深度文本到三维模型在工程领域的潜力，重点关注在计算模拟设计优化中整合和交互三维资产的机会和挑战。与传统的基于数值表示的三维几何设计优化不同，自然语言要求对变异算子有不同的解释，同时也可以减轻和激发人类用户的交互。在这里，我们提出并实现了一个完全自动的进化设计优化框架，使用了最近推出的Shap-E模型。

    The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently pu
    
[^33]: vONTSS：基于vMF和最优传输的半监督神经主题建模

    vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])

    [http://arxiv.org/abs/2307.01226](http://arxiv.org/abs/2307.01226)

    vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。

    

    最近，受变分自编码器启发的神经主题模型（NTM）引起了很多研究兴趣，然而，由于整合人类知识的挑战，这些方法在实际应用中受到了限制。本研究提出了一种半监督神经主题建模方法vONTSS，该方法利用基于von Mises-Fisher（vMF）的变分自编码器和最优传输。在半监督设置中，当提供每个主题的少量关键词时，vONTSS生成潜在主题并优化主题-关键词质量和主题分类。实验证明，vONTSS在分类准确率和多样性方面优于现有的半监督主题建模方法。vONTSS还支持无监督主题建模。定量和定性实验证明，vONTSS在无监督设置下在多个方面优于最近的NTM：vONTSS在基准数据集上发现高度聚类和连贯的主题。它也比现有-手法快得多。

    Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
    
[^34]: 解释性和透明性驱动的文本对抗示例的检测与转换（IT-DT）

    Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])

    [http://arxiv.org/abs/2307.01225](http://arxiv.org/abs/2307.01225)

    通过提出的解释性和透明性驱动的检测与转换（IT-DT）框架，我们在检测和转换文本对抗示例方面注重解释性和透明性。这个框架利用了注意力图、集成梯度和模型反馈等技术，在检测阶段有助于识别对对抗性分类有贡献的显著特征和扰动词语，并在转换阶段使用预训练的嵌入和模型反馈来生成扰动词语的最佳替代，以将对抗性示例转换为正常示例。

    

    基于Transformer的文本分类器如BERT、Roberta、T5和GPT-3在自然语言处理方面展示了令人印象深刻的性能。然而，它们对于对抗性示例的脆弱性提出了安全风险。现有的防御方法缺乏解释性，很难理解对抗性分类并识别模型的漏洞。为了解决这个问题，我们提出了解释性和透明性驱动的检测与转换（IT-DT）框架。它专注于在检测和转换文本对抗示例时的解释性和透明性。IT-DT利用注意力图、集成梯度和模型反馈等技术进行解释性检测。这有助于识别对对抗性分类有贡献的显著特征和扰动词语。在转换阶段，IT-DT利用预训练的嵌入和模型反馈来生成扰动词语的最佳替代。通过找到合适的替换，我们的目标是将对抗性示例转换为正常示例。

    Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
    
[^35]: 从科学文献中发现定义和方法的模式

    Discovering Patterns of Definitions and Methods from Scientific Documents. (arXiv:2307.01216v1 [cs.CL])

    [http://arxiv.org/abs/2307.01216](http://arxiv.org/abs/2307.01216)

    本文提出了一种从科学文献中发现定义和方法模式的分析方法，并在语义、句法和词汇层面上保证了模式的完整性。

    

    从科学文献中自动提取定义和方法的困难在于两个方面：（1）自然语言文本的复杂性和多样性，要求一个分析方法来支持模式的发现；（2）科学论文中完整的定义或方法通常分布在文本中，因此一个有效的方法不仅应该提取单个句子的定义和方法，还应该整合这些句子以获得完整的定义或方法。本文提出了一种分析方法，用于发现定义和方法的模式，并使用该方法发现定义和方法的模式。语义层面上的模式的完整性由一组完整的语义关系保证，这些语义关系分别标识定义和方法。在句法和词汇层面上，模式的完整性由句法和词汇约束保证。

    The difficulties of automatic extraction of definitions and methods from scientific documents lie in two aspects: (1) the complexity and diversity of natural language texts, which requests an analysis method to support the discovery of pattern; and, (2) a complete definition or method represented by a scientific paper is usually distributed within text, therefore an effective approach should not only extract single sentence definitions and methods but also integrate the sentences to obtain a complete definition or method. This paper proposes an analysis method for discovering patterns of definition and method and uses the method to discover patterns of definition and method. Completeness of the patterns at the semantic level is guaranteed by a complete set of semantic relations that identify definitions and methods respectively. The completeness of the patterns at the syntactic and lexical levels is guaranteed by syntactic and lexical constraints. Experiments on the self-built dataset 
    
[^36]: 基于单词组搜索的鲁棒文本分类的自动反事实扩充

    Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])

    [http://arxiv.org/abs/2307.01214](http://arxiv.org/abs/2307.01214)

    这篇论文提出了一种基于单词组搜索的自动反事实扩充方法，用于鲁棒文本分类。该方法通过捕捉关键字组合的因果效应，并排序最影响预测的组合，从而解决了由于只关注单个单词而导致的错误因果特征的问题。

    

    尽管大规模预训练语言模型在文本分类方面取得了显著的成果，但最近的研究对于捷径学习的挑战提出了担忧。通常情况下，如果关键字与标签产生表面关联，从而导致错误预测，那么它被视为一种捷径。相反，如果模型依赖于能够产生准确预测的鲁棒因果特征，就可以缓解捷径学习。为此，许多研究探索了事后可解释的方法来挖掘鲁棒性和泛化性的捷径和因果特征。然而，大多数现有方法只关注句子中的单个单词，忽视了单词组的考虑，导致错误的因果特征。为了解决这个问题，我们提出了一种新的单词组挖掘方法，它能够捕捉任何关键字组合的因果效应，并对最影响预测的组合进行排序。我们的方法基于有效的事后分析和波束搜索，确保了算法的准确性。

    Despite large-scale pre-trained language models have achieved striking results for text classificaion, recent work has raised concerns about the challenge of shortcut learning. In general, a keyword is regarded as a shortcut if it creates a superficial association with the label, resulting in a false prediction. Conversely, shortcut learning can be mitigated if the model relies on robust causal features that help produce sound predictions. To this end, many studies have explored post-hoc interpretable methods to mine shortcuts and causal features for robustness and generalization. However, most existing methods focus only on single word in a sentence and lack consideration of word-group, leading to wrong causal features. To solve this problem, we propose a new Word-Group mining approach, which captures the causal effect of any keyword combination and orders the combinations that most affect the prediction. Our approach bases on effective post-hoc analysis and beam search, which ensures
    
[^37]: 一个用于安全指令本体表示的自动化方法

    An automated method for the ontological representation of security directives. (arXiv:2307.01211v1 [cs.AI])

    [http://arxiv.org/abs/2307.01211](http://arxiv.org/abs/2307.01211)

    本文提出了一种将大型法律文件自动转化为本体表示的方法，通过自然语言处理技术和本体发展原则的结合，展示了在欧洲网络和信息系统安全指令上的应用。

    

    难以解释的大型法律文件，长句导致名词之间错综复杂的关系。本文将这个问题放在最近欧洲安全指令的背景下。通过对自然语言处理（NLP）技术的特定定制，自动化提取关键信息，即每个从句的词类。这些与本体发展原则相结合，设计了我们的安全指令本体表示的自动化方法。该方法在一个实际问题上展示，即推导出表示欧洲层面上网络和信息系统安全指令的本体。尽管采用的NLP技术存在一些限制，并且需要通过手动分析进行补充，但总体结果为指令提供了有效的支持。

    Large documents written in juridical language are difficult to interpret, with long sentences leading to intricate and intertwined relations between the nouns. The present paper frames this problem in the context of recent European security directives. The complexity of their language is here thwarted by automating the extraction of the relevant information, namely of the parts of speech from each clause, through a specific tailoring of Natural Language Processing (NLP) techniques. These contribute, in combination with ontology development principles, to the design of our automated method for the representation of security directives as ontologies. The method is showcased on a practical problem, namely to derive an ontology representing the NIS 2 directive, which is the peak of cybersecurity prescripts at the European level. Although the NLP techniques adopted showed some limitations and had to be complemented by manual analysis, the overall results provide valid support for directive 
    
[^38]: 汉字音系的多方言表示学习

    Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])

    [http://arxiv.org/abs/2307.01209](http://arxiv.org/abs/2307.01209)

    该论文提出了一种在汉字音韵学中获取多方言表示的方法，通过构建知识图谱和应用无监督聚类技术，这些表示可以捕捉输入方言的音位对比并展示古老的原始语言特征的潜力。

    

    机器学习技术在语言和音韵等象征系统的表示和推理方面表现出了自己的能力。在汉字历史音韵学中，可以受益于机器学习的重要任务包括方言比较和原始语言系统的重建。本文提出了一种获取汉字音节的多方言表示的方法，通过从结构化音韵数据构建知识图谱，然后应用知识库学习中的BoxE技术。我们应用无监督的聚类技术对所得到的表示进行观察，发现这些表示捕捉到输入方言的音位对比。此外，我们训练了分类器来进行无法观察的中古汉语标签的推理，展示了这些表示揭示古老的原始语言特征的潜力。这些表示可以用于对碎片化的汉字音韵进行补全。

    Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data, then applying the BoxE technique from knowledge base learning. We applied unsupervised clustering techniques to the obtained representations to observe that the representations capture phonemic contrast from the input dialects. Furthermore, we trained classifiers to perform inference of unobserved Middle Chinese labels, showing the representations' potential for indicating archaic, proto-language features. The representations can be used for performing completion of fragmented Sinitic phonological 
    
[^39]: 预测性专利学：使用ChatGPT技术预测创新成功和估值

    Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])

    [http://arxiv.org/abs/2307.01202](http://arxiv.org/abs/2307.01202)

    本研究采用ChatGPT技术，以OpenAI的最先进的文本嵌入为基础，通过深度学习预测模型实现了对专利创新成功和估值的准确预测，为专利估值提供了革命性的改进。此外，通过预测接受率构建的多空投资组合实现了显著的异常收益率。

    

    传统方法对于创新的分析在广泛的结构变量方面存在根本性的局限性。本文通过开创性的ChatGPT技术采用LLM方法对专利进行分析，突破了边界。OpenAI的最先进的文本嵌入能够访问关于每个发明的质量和影响的复杂信息，用于驱动深度学习预测模型。细致的嵌入使预测专利价值的R-squared提高了24％，并明确地将最差和最佳应用程序分离开来。这些模型通过预测接受率构建的多空投资组合每年实现显著的异常收益率高达3.3%，同时也证明了市场无法及时整合有关应用程序的信息。这些模型为革命性改变科根、帕帕尼科洛、塞鲁和斯托夫曼（2017）对专利估值的更正提供了机会。

    Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutioniz
    
[^40]: 在上下文学习和出现中的模式学习和重新绑定机制

    Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])

    [http://arxiv.org/abs/2307.01201](http://arxiv.org/abs/2307.01201)

    本文研究了上下文学习的机制，发现使用克隆结构因果图可以实现与transformer-based语言模型相似的能力，并且这种方法可以解释上下文学习的工作原理。

    

    上下文学习（ICL）是近期基于Transformer的大型语言模型（LLMs）中最强大且最令人意外的能力之一。然而，它的基础机制尚不清楚。在本文中，我们证明可以通过使用克隆结构因果图（CSCGs）这种替代的序列预测学习方法获得可比较的ICL能力。此外，CSCGs的一个关键特性是，与基于Transformer的LLMs不同，它们是可解释的，这大大简化了解释ICL工作原理的任务。具体而言，我们显示它使用了以下组合：（a）学习模板（模式）电路进行模式完成，（b）以上下文敏感方式检索相关模板，以及（c）将新标记重新绑定到模板的适当位置。我们进一步提出了ICL在LLMs中采用类似机制的假设证据。例如，我们发现，与LLMs一样，使用CSCGs时会出现不同的能力。

    In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at d
    
[^41]: 通过主动遗忘在预训练中提高语言可塑性

    Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.01163](http://arxiv.org/abs/2307.01163)

    本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。

    

    预训练语言模型(PLMs)是自然语言处理中的主要模型。尽管它们在下游任务的性能令人印象深刻，但将PLMs应用于新语言可能很困难，这是使它们的能力普遍可访问的壁垒。先前的研究表明，通过为新语言学习新的嵌入层可以解决此问题，但这样做既浪费数据又浪费计算资源。我们建议在预训练期间使用主动遗忘机制，作为快速适应新语言的PLMs的简单方法。具体而言，通过在预训练期间的每K次更新时重置嵌入层，我们鼓励PLM在有限次更新内提高学习新嵌入的能力，类似于元学习的效果。使用RoBERTa进行的实验证明，使用我们的遗忘机制预训练的模型不仅在语言适应过程中显示出更快的收敛速度，而且在低数据的情况下也优于标准模型。

    Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
    
[^42]: 反事实协同推理

    Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR])

    [http://arxiv.org/abs/2307.00165](http://arxiv.org/abs/2307.00165)

    本文提出了反事实协同推理（CCR）方法，通过整合反事实推理和逻辑推理来提高机器学习模型的准确性和可解释性。通过利用反事实推理生成困难的反事实训练样本进行数据增强，CCR在推荐系统中展示了如何缓解数据稀缺、提高准确性和增强透明度。

    

    因果推理和逻辑推理是人类智能的两种重要推理能力。然而，在机器智能背景下，它们的关系还未得到广泛探索。本文探讨了如何共同建模这两种推理能力，以提高机器学习模型的准确性和可解释性。具体而言，通过整合反事实推理和（神经）逻辑推理两种重要的推理能力，我们提出了反事实协同推理（CCR），它通过进行反事实逻辑推理来改进性能。特别是，我们以推荐系统为例，展示了CCR如何缓解数据稀缺、提高准确性和增强透明度。从技术上讲，我们利用反事实推理来生成“困难”的反事实训练样本进行数据增强，这与原始的训练样本一起可以提升模型性能。

    Causal reasoning and logical reasoning are two important types of reasoning abilities for human intelligence. However, their relationship has not been extensively explored under machine intelligence context. In this paper, we explore how the two reasoning abilities can be jointly modeled to enhance both accuracy and explainability of machine learning models. More specifically, by integrating two important types of reasoning ability -- counterfactual reasoning and (neural) logical reasoning -- we propose Counterfactual Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to improve the performance. In particular, we use recommender system as an example to show how CCR alleviate data scarcity, improve accuracy and enhance transparency. Technically, we leverage counterfactual reasoning to generate "difficult" counterfactual training examples for data augmentation, which -together with the original training examples -- can enhance the model performance. Since the 
    
[^43]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^44]: The mapKurator系统：从历史地图中提取和链接文本的完整管道

    The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])

    [http://arxiv.org/abs/2306.17059](http://arxiv.org/abs/2306.17059)

    该论文介绍了一种名为mapKurator的系统，能完整地从历史地图中提取和链接文本信息。该系统解决了传统方法中对位置相关词语的忽略问题，并利用主题建模方法考虑更广的主题范围，能够识别文档的空间焦点。

    

    文档具有空间焦点和有价值的地方特征。例如，房地产或旅行博客中的列表描述包含有关特定地区社区的信息。这些信息对于描述人类如何感知他们的环境是有价值的。然而，利用这些信息的第一步是识别文档的空间焦点（例如，城市）。传统方法用于识别文档的空间焦点依赖于从文档中检测和消歧化地名。这种方法需要一个包含位置短语和临时规则的词汇集，这些规则忽略了与位置相关的重要词语。最近，使用大型语言模型的主题建模方法通常考虑几个广度的主题。相比之下，文档的空间焦点可以是一个国家、一个城市，甚至是一个社区，这些范围比这些方法考虑的主题数要大得多。

    Documents hold spatial focus and valuable locality characteristics. For example, descriptions of listings in real estate or travel blogs contain information about specific local neighborhoods. This information is valuable to characterize how humans perceive their environment. However, the first step to making use of this information is to identify the spatial focus (e.g., a city) of a document. Traditional approaches for identifying the spatial focus of a document rely on detecting and disambiguating toponyms from the document. This approach requires a vocabulary set of location phrases and ad-hoc rules, which ignore important words related to location. Recent topic modeling approaches using large language models often consider a few topics, each with broad coverage. In contrast, the spatial focus of a document can be a country, a city, or even a neighborhood, which together, is much larger than the number of topics considered in these approaches. Additionally, topic modeling methods a
    
[^45]: 通过检测和匹配进行生物医学实体识别

    Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v1 [cs.CL])

    [http://arxiv.org/abs/2306.15736](http://arxiv.org/abs/2306.15736)

    本研究提出了一种名为DMNER的新型生物医学实体识别框架，通过检测实体边界和匹配生物医学实体来提高NER的性能。在有监督NER、远程监督NER和多数据集合并训练NER等场景中，DMNER都展示了良好的适用性。

    

    生物医学命名实体识别（BNER）是许多生物医学文本挖掘任务的基础。与一般的NER不同，BNER需要全面掌握领域知识，而且在训练数据之外融入外部知识是一个重大挑战。本研究提出了一个新的BNER框架，称为DMNER。通过利用已有的实体表示模型SAPBERT，我们将BNER作为一个两步骤的过程来处理：实体边界检测和生物医学实体匹配。DMNER在多种NER场景中具有适用性：1）在有监督NER中，我们观察到DMNER有效纠正了基线NER模型的输出，从而进一步提高了性能。2）在远程监督NER中，将MRC和AutoNER作为跨度边界检测器相结合，使DMNER能够实现令人满意的结果。3）对于通过合并多个数据集进行NER训练，我们采用了与DS-NER类似的框架，但还额外利用ChatGPT来获得高质量的训练短语。

    Biomedical named entity recognition (BNER) serves as the foundation for numerous biomedical text mining tasks. Unlike general NER, BNER require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. In this study, we propose a novel BNER framework called DMNER. By leveraging existing entity representation models SAPBERT, we tackle BNER as a two-step process: entity boundary detection and biomedical entity matching. DMNER exhibits applicability across multiple NER scenarios: 1) In supervised NER, we observe that DMNER effectively rectifies the output of baseline NER models, thereby further enhancing performance. 2) In distantly supervised NER, combining MRC and AutoNER as span boundary detectors enables DMNER to achieve satisfactory results. 3) For training NER by merging multiple datasets, we adopt a framework similar to DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the training. Through
    
[^46]: 实体链接的检索器-阅读器范式的双向端到端学习

    Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])

    [http://arxiv.org/abs/2306.12245](http://arxiv.org/abs/2306.12245)

    BEER^2是一种用于Retriever和Reader的双向端到端训练框架，通过检索器和阅读器之间的相互学习，共同进步，实现端到端EL。

    

    实体链接（EL）是信息提取和知识图谱的基本任务，它的一般形式（即端到端EL）旨在首先在给定输入文档中找到提及，并将提及链接到特定知识库中的相应实体。最近，检索器-阅读器范式促进了端到端EL的进展，受益于密集的实体检索和机器阅读理解的优势。然而，现有研究仅以流水线方式单独训练检索器和阅读器，忽略了检索器和阅读器之间交互带来的益处。为了使检索器-阅读器范式更完美地执行端到端EL，我们提出了BEER$^2$，一种用于Retriever and Reader的双向端到端训练框架。通过我们设计的双向端到端训练，BEER$^2$指导检索器和阅读器互相学习，共同进步，并最终实现端到端EL。

    Entity Linking (EL) is a fundamental task for Information Extraction and Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first find mentions in the given input document and then link the mentions to corresponding entities in a specific knowledge base. Recently, the paradigm of retriever-reader promotes the progress of end-to-end EL, benefiting from the advantages of dense entity retrieval and machine reading comprehension. However, the existing study only trains the retriever and the reader separately in a pipeline manner, which ignores the benefit that the interaction between the retriever and the reader can bring to the task. To advance the retriever-reader paradigm to perform more perfectly on end-to-end EL, we propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever and Reader. Through our designed bidirectional end-to-end training, BEER$^2$ guides the retriever and the reader to learn from each other, make progress together, and ultimate
    
[^47]: 设计用户角色感知的对话代理进行有趣的对话：$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground

    $\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])

    [http://arxiv.org/abs/2306.03361](http://arxiv.org/abs/2306.03361)

    本文提出了一种针对商业环境的、能够平衡对话流畅性和趋向于理解对话系统的个性化开放领域对话系统方法，通过加权数据集混合、负角色信息增强方法，以及设计个性化对话数据集，解决了 $\textit{WHAT}$、$\textit{WHEN}$和$\textit{HOW}$ 等问题，同时提高了对话系统响应的可控性和解释性。

    

    本文提出了一种建立个性化开放领域对话系统以解决商业设置中涉及个性化对话响应与非正式响应交替的$\textit{WWH}$（$\textit{WHAT}$、$\textit{WHEN}$和$\textit{HOW}$）问题的方法。所提出的方法涉及加权数据集混合、负角色信息增强方法以及设计个性化对话数据集，以应对个性化、开放领域对话系统中$\textit{WWH}$的挑战。本文有效地平衡了对话流畅性和趋向于理解对话系统，同时还引入了响应类型标签来提高可控性和解释性。这些方法的组合导致了更加流畅的对话，证明了基于主观人类评估和客观评估的实验结果。

    This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
    
[^48]: LM-CPPF: 基于释义引导的数据增强用于对比型Prompt的少样本微调

    LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18169](http://arxiv.org/abs/2305.18169)

    本文提出了一种基于释义引导的数据增强用于对比型Prompt的少样本微调方法。该方法利用基础Prompt的少样本释义生成语言模型完成数据增强。

    

    近年来，预训练语言模型在自然语言处理领域取得了显著进展，但这些模型在少量数据集上的微调仍然存在缺陷。为了解决这个问题，研究人员提出了各种适应性方法。对基础Prompt进行微调是一种较为普遍的方式，尤其适用于大型模型。之前的研究显示，将对比学习添加到对Prompt的微调中是有效的，因为它帮助模型生成更能够区分不同分类之间的嵌入，而且它同时还能从正负示例中学习，更加节省样本。对比学习最重要的组成部分之一是数据增强，在计算机视觉领域得到广泛应用，但对于NLP来说，有效的数据增强仍然具有挑战性。本文提出了LM-CPPF，通过生成式语言模型引导的少样本释义型对比Prompt微调，它利用基础Prompt的少样本释义生成语言模型来完成数据增强。

    In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, e
    
[^49]: 基于表面的检索降低了检索增强语言模型的困惑度

    Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. (arXiv:2305.16243v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16243](http://arxiv.org/abs/2305.16243)

    本文研究发现，用基于表面级别的检索机制取代语义检索可以显著降低检索增强语言模型的困惑度。

    

    已经证明，通过检索机制增强语言模型可以显著提高性能，同时保持参数数量较低。检索增强模型通常依靠基于查询块和潜在邻居之间的密集表示相似性的语义检索机制。本文研究了最先进的Retro模型，并观察到其性能提升更好地解释为基于表面级别的相似性，例如标记重叠。受此启发，我们用BM25替换Retro中的语义检索，获得了显著的困惑度降低。由于完整的BM25检索可能在大型数据集上具有计算成本，因此我们还将其应用于重新排名场景中，以最小的计算开销获得部分困惑度降低。

    Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.
    
[^50]: Flan-MoE: 通过稀疏Mixture of Experts扩展指令调优的语言模型

    Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])

    [http://arxiv.org/abs/2305.14705](http://arxiv.org/abs/2305.14705)

    Flan-MoE是一种指令调优的稀疏Mixture of Experts（MoE）语言模型，相对于密集模型，在指令微调和任务特定微调后均表现更好。最大模型Flan-MoE-32B的性能在四个基准测试中超越Flan-PaLM-62B，同时只利用了1/3的FLOPs。

    

    语言模型的爆炸性增长和应用需求导致有效和可扩展方法的需求增加。本文介绍了一套Instruction-Finetuned Sparse Mixture-of-Expert (MoE)语言模型，即Flan-MoE。我们发现，仅针对任务特定数据集进行MoE模型的微调会导致性能不如相同计算复杂度的密集模型。然而，我们的Flan-MoE在多个实验设置下都优于密集模型：仅指令微调和指令微调后进行任务特定微调。这表明，指令微调是MoE模型的必要阶段。具体来说，我们的最大模型Flan-MoE-32B在四个基准测试中超越了Flan-PaLM-62B的性能，同时只利用了1/3的FLOPs。Flan-MoE的成功鼓舞我们重新思考大规模、高性能语言模型的设计，尤其是在任务无关学习的情况下。

    The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our Flan-MoE outperforms dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic lear
    
[^51]: 上下文化的短语预测网络在端到端语音识别中的应用

    Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.12493](http://arxiv.org/abs/2305.12493)

    本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法，通过计算偏置损失以帮助训练上下文化模型，在多种端到端语音识别模型上实现了显著的WER降低，相对于基线模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。

    

    上下文信息在语音识别技术中发挥着至关重要的作用，将其融入端到端语音识别模型近年来引起了极大的兴趣。然而，先前的深度偏置方法缺乏偏置任务的显式监督。本研究引入了一个上下文短语预测网络用于基于注意力的深度偏置方法。该网络利用上下文嵌入预测发音中的上下文短语，并计算偏置损失以帮助训练上下文化模型。我们的方法在多种端到端语音识别模型上实现了显著的单词错误率(WER)降低。对LibriSpeech语料库的实验结果表明，在基线模型上，我们提出的模型相对WER提高了12.1％，上下文短语的WER相对降低了40.5％。此外，通过应用上下文短语过滤策略，我们还有效消除了使用更大的偏置列表时的WER降级现象。

    Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
    
[^52]: 在零-shot封闭生成式问答中评估大小为中型-大型语言模型

    Evaluation of medium-large Language Models at zero-shot closed book generative question answering. (arXiv:2305.11991v1 [cs.CL])

    [http://arxiv.org/abs/2305.11991](http://arxiv.org/abs/2305.11991)

    本文评估了大小为中型的语言模型在没有外部检索的情况下完成问答任务的表现，结果表明使用适当的训练数据进行模型微调比单纯依赖参数数量更重要，最好的模型实现了46.4%的正确率。

    

    大型语言模型（LLMs）引起了重要关注，但“大”这个定义缺乏清晰度。本文关注中型语言模型（MLMs），这被定义为具有至少60亿参数但少于1000亿的模型。本研究评估MLMs在零-shot生成式问答方面的表现，这要求模型提供详细的答案而无需外部文档检索。本文引入了一个新的测试数据集，并给出了人类评估的结果，结果显示将不同MLMs的最佳答案组合可以实现82.7%的整体正确率，优于ChatGPT的60.9%。表现最好的MLM实现了46.4%，其具有70亿参数，强调了使用适当的训练数据进行微调的重要性，而不是仅仅依赖于参数数量。更细粒度的反馈应该被用于进一步提高答案的质量。

    Large language models (LLMs) have garnered significant attention, but the definition of "large" lacks clarity. This paper focuses on medium-sized lan-guage models (MLMs), defined as having at least six billion parameters but less than 100 billion. The study evaluates MLMs regarding zero-shot genera-tive question answering, which requires models to provide elaborate answers without external document retrieval. The paper introduces an own test da-taset and presents results from human evaluation. Results show that combin-ing the best answers from different MLMs yielded an overall correct answer rate of 82.7% which is better than the 60.9% of ChatGPT. The best MLM achieved 46.4% and has 7B parameters, which highlights the importance of using appropriate training data for fine-tuning rather than solely relying on the number of parameters. More fine-grained feedback should be used to further improve the quality of answers.
    
[^53]: 多模态情感分析：综述

    Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v1 [cs.CL])

    [http://arxiv.org/abs/2305.07611](http://arxiv.org/abs/2305.07611)

    本综述介绍了多模态情感分析的定义、发展和挑战，讨论了最新的数据集和先进模型，并提出了有前途的研究方向和构建更好性能的建议。

    

    多模态情感分析已成为人工智能领域的重要研究领域。随着深度学习的最新进展，这项技术已经达到了新的高度。它在应用和研究方面具有巨大的潜力，因此成为了一个热门研究课题。本综述提供了多模态情感分析的定义、背景和发展概述。它还涵盖了最新的数据集和先进模型，强调了该技术的挑战和未来前景。最后，它展望了未来的研究方向。需要指出的是，本综述为有前途的研究方向和构建更好性能的多模态情感分析模型提供了建设性的建议，有助于该领域的研究者。

    Multimodal sentiment analysis has become an important research area in the field of artificial intelligence. With the latest advances in deep learning, this technology has reached new heights. It has great potential for both application and research, making it a popular research topic. This review provides an overview of the definition, background, and development of multimodal sentiment analysis. It also covers recent datasets and advanced models, emphasizing the challenges and future prospects of this technology. Finally, it looks ahead to future research directions. It should be noted that this review provides constructive suggestions for promising research directions and building better performing multimodal sentiment analysis models, which can help researchers in this field.
    
[^54]: 语言、时间偏好和消费行为：大型语言模型的证据

    Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])

    [http://arxiv.org/abs/2305.02531](http://arxiv.org/abs/2305.02531)

    本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。

    

    语言对我们对时间和奖励的感知有很大的影响。这引发了一个问题，即当以不同的语言询问大型语言模型时，它们是否显示出不同的奖励时间偏好，并且它们的选择是否类似于人类的选择。本研究分析了GPT-3.5（以下简称GPT）在多种语言提示下的响应，探索了较小、较早的奖励和较大、较晚的奖励之间的偏好。我们的结果显示，当以语义含义较弱的未来时态参考（FTR），如德语和汉语，为提示语时，GPT表现出更大的耐心，相比英语和法语等具有强大FTR的语言。这些发现与现有文献一致，并表明了GPT的选择与这些语言的使用者的偏好之间的关联。然而，进一步的分析揭示了较早或较晚奖励的偏好并没有随着奖励差异系统地改变，这表明了一种词典序优先的选择。

    Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
    
[^55]: Distilling Step-by-Step！使用更少的训练数据和更小的模型尺寸胜过更大的语言模型

    Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])

    [http://arxiv.org/abs/2305.02301](http://arxiv.org/abs/2305.02301)

    本研究提出了Distilling Step-by-Step机制，通过提取LLM基础信息为小型模型提供额外的监督训练，从而使它们胜过更大的LLM模型，并需更少的训练数据。

    

    部署大型语言模型（LLM）面临内存效率低和计算密集度高的问题，研究人员通过微调或精炼使用LLM生成的标签来训练较小的任务特定模型。但是，要想达到LLM相当的性能，这需要大量的训练数据。我们引入了Distilling Step-by-Step，这是一种新的机制， (a)训练较小的模型比LLM表现更好，(b)并通过利用微调或精炼所需的更少的训练数据来实现。我们的方法在多任务训练框架中提取LLM基础，并作为额外的监督来训练小型模型。在四个NLP基准测试中，我们提出了三个发现：第一，与微调和精炼相比，我们的机制使用较少的标记/未标记训练示例取得更好的性能。第二，与LLM相比，即使使用更小的模型，我们也实现了更好的性能。

    Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
    
[^56]: 最好的多语言文档嵌入是否仅基于句子嵌入？

    Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])

    [http://arxiv.org/abs/2304.14796](http://arxiv.org/abs/2304.14796)

    本文系统比较了从句子级别嵌入中产生文档级嵌入的方法，基于预训练的多语言模型LASER、LaBSE和Sentence BERT。我们着重比较了输入令牌数截断、句子平均以及一些简单的窗口方法，对三个多语言和跨语言任务表现进行了比较。

    

    在现代自然语言处理中，文本数据的密集向量表征至关重要。从原始文本估计的词嵌入和句子嵌入是在多种需要语义理解的任务中实现最新成果的关键。然而，由于计算需求和缺乏适当的数据，获取文档级别的嵌入是具有挑战性的。相反，大多数方法退而使用基于句子表示的文档嵌入计算。尽管存在一些用于完全编码文档的体系结构和模型，但它们通常仅限于英语和其他几种高资源语言。在本文中，我们基于预训练的多语言模型LASER、LaBSE和Sentence BERT，系统比较从句子中产生文档级表示的方法。我们比较输入令牌数截断、句子平均以及一些简单的窗口方法，在三个多语言和跨语言任务中进行比较。

    Dense vector representations for textual data are crucial in modern NLP. Word embeddings and sentence embeddings estimated from raw texts are key in achieving state-of-the-art results in various tasks requiring semantic understanding. However, obtaining embeddings at the document level is challenging due to computational requirements and lack of appropriate data. Instead, most approaches fall back on computing document embeddings based on sentence representations. Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages. In this work, we provide a systematic comparison of methods to produce document-level representations from sentences based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare input token number truncation, sentence averaging as well as some simple windowing and in some cases new augmented and learnable approaches, on 3 multiand cross-lingual tasks 
    
[^57]: BenCoref:一种名词短语和代词指代注释的多领域数据集

    BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v1 [cs.CL])

    [http://arxiv.org/abs/2304.03682](http://arxiv.org/abs/2304.03682)

    本论文介绍了一个包括四个不同领域Bengali文本的新数据集- BenCoref。该数据集可以帮助理解Bengali多个领域中共指消解现象的差异，并促进Bengali的资源开发。多个模型在该数据集上训练的性能也得到了报告。在跨语言测试中，从英语到Bengali的交叉语言性能较差，显示出需要语言特定的共指消解系统。

    

    共指消解是自然语言处理中一个被广泛研究的问题。然而，由于缺乏相关数据集，Bengali 的共指消解研究主要未被探索。本文介绍了一个新的数据集BenCoref，包括来自四个不同领域的Bengali文本的共指注释。该数据集包含5200个提及注释，形成48,569个标记中的502个提及簇。我们描述了创建此数据集的过程，并报告了使用BenCoref训练的多个模型的性能。我们预计我们的工作将揭示Bengali多个领域中共指现象的差异，并鼓励开发其他Bengali资源。此外，我们发现在零样本设置下从英语到Bengali的交叉语言性能很差，这突显出需要语言特定的共指消解系统。

    Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morphological richness compared to English. In this article, we introduce a new dataset, BenCoref, comprising coreference annotations for Bengali texts gathered from four distinct domains. This relatively small dataset contains 5200 mention annotations forming 502 mention clusters within 48,569 tokens. We describe the process of creating this dataset and report performance of multiple models trained using BenCoref. We anticipate that our work sheds some light on the variations in coreference phenomena across multiple domains in Bengali and encourages the development of additional resources for Bengali. Furthermore, we found poor crosslingual performance at zero-shot setting from English, highlig
    
[^58]: 机器心理学：利用心理学方法探究大型语言模型的新兴能力和行为

    Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])

    [http://arxiv.org/abs/2303.13988](http://arxiv.org/abs/2303.13988)

    本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。

    

    大型语言模型（LLM）是将人工智能系统与人类交流和日常生活紧密结合的先锋。由于快速技术进步和其极高的通用性，现今LLM已经拥有数百万用户，并正处于成为主要信息检索、内容生成、问题解决等技术的前沿。因此，对其进行全面评估和审查显得尤为重要。由于当前LLM中出现愈加复杂和新颖的行为模式，可将其视为参与人类心理实验的对象，以便更为全面地评估其能力。为此，本文引入了一个名为"机器心理学"的新兴研究领域。本文概述了各类心理学分支如何为LLM的行为测试提供有用参考。同时，本文规范了机器心理学研究的方法论标准，特别是专注于提示设计政策的制定。此外，它还描述了行为测试结果如何为未来的LLM发展提供指导。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
    
[^59]: LEVER: 使用执行进行语言到代码生成的学习验证

    LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08468](http://arxiv.org/abs/2302.08468)

    提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。

    

    训练在代码上的大型语言模型（code LLMs）的出现，已经在语言到代码生成方面取得了显著进展。此领域的最新方法将LLM解码与使用测试用例或基于执行结果的启发式方法的样本修剪和重新排序相结合。然而，对于许多现实世界的语言到代码应用来说，获取测试用例是具有挑战性的，而启发式方法不能很好地捕捉执行结果的语义特征，比如数据类型和值范围，这往往表明程序的正确性。在这项工作中，我们提出了LEVER，一种通过学习使用执行结果来验证生成的程序，从而改进语言到代码生成的简单方法。具体地说，我们训练验证器根据自然语言输入、程序本身和执行结果来确定从LLM中抽样的程序是否正确。通过将验证分数与LLM生成分数相结合，对抽样的程序进行重新排序。

    The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
    
[^60]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^61]: 具有包容性的机器翻译的性别中性化：从理论基础到开放挑战

    Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10075](http://arxiv.org/abs/2301.10075)

    研究了性别中性化翻译（GNT）作为一种性别包容性的方法，从英语到意大利语的翻译是一个突出的性别相关的语言翻译问题。研究回顾了一些相关的性别包容性语言指南，探讨了使用GNT的情景，并探讨了在MT中执行GNT的技术挑战和解决方案。

    

    语言技术中的性别包容性已成为一个重要的研究课题。本研究探讨性别中性化翻译（GNT）作为一种性别包容性和机器翻译（MT）模型所要实现的目标，这些模型被发现具有延续性别偏见和歧视的倾向。具体而言，我们关注英译意这对语言，它代表了突出的与性别有关的语言转移问题。为了定义GNT，我们回顾了一些相关的机构性别包容性语言指南，讨论了使用GNT的情景，并探讨了在MT中执行GNT的技术挑战，最后讨论了潜在的解决方案，以鼓励朝着更大的包容性发展。

    Gender inclusivity in language technologies has become a prominent research topic. In this study, we explore gender-neutral translation (GNT) as a form of gender inclusivity and a goal to be achieved by machine translation (MT) models, which have been found to perpetuate gender bias and discrimination. Specifically, we focus on translation from English into Italian, a language pair representative of salient gender-related linguistic transfer problems. To define GNT, we review a selection of relevant institutional guidelines for gender-inclusive language, discuss its scenarios of use, and examine the technical challenges of performing GNT in MT, concluding with a discussion of potential solutions to encourage advancements toward greater inclusivity in MT.
    
[^62]: 跨模态注意力一致性正则化用于视觉-语言关系对齐

    Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10549](http://arxiv.org/abs/2212.10549)

    本研究提出了一种跨模态注意力一致性正则化方法，用于视觉-语言关系对齐。通过鼓励语言注意力与视觉注意力的一致性来实现关系级别的对齐，从而提高视觉-语言模型在组合概括性基准测试中的性能。

    

    尽管近年来联合视觉-语言模型的规模不断扩大，但这些模型在诸如Winoground等组合概括性基准测试中仍然存在困难。我们发现，当前视觉-语言模型缺乏的关键组成部分是关系级别的对齐：即能够将文本中的定向语义关系（例如，“草坪中的杯子”）与图像中的空间关系（例如，杯子相对于草坪的位置）进行匹配。为了解决这个问题，我们展示了通过鼓励从“杯子”到“草坪”（捕捉语义关系“在”）的定向语言注意力与从杯子到草坪的定向视觉注意力相匹配来实现关系对齐。通过跨模态注意力软性地识别标记及其对应的对象。我们证明了这种软性关系对齐的概念等同于在由跨模态提供的“基底变换”下实施视觉和语言注意力矩阵之间的一致性。

    Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., "mug in grass") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal a
    
[^63]: 迷你模型适应：通过对齐的浅层训练高效地将预训练模型扩展到新语言上

    Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10503](http://arxiv.org/abs/2212.10503)

    使用迷你模型适应的方法，通过构建浅层迷你模型以及高效训练新的语言特定嵌入向量，实现了将预训练模型扩展到新语言上的快速跨语言传输。

    

    先前的研究表明，通过学习一组新的嵌入向量，并保持transformer主体部分冻结，可以将预训练的遮蔽语言模型（MLM）扩展到新语言。尽管只学习了一小部分参数，但这种方法在计算效率上并不高，因为训练新的嵌入向量需要对整个模型进行完整的前向和反向传播。我们提出了迷你模型适应，一种计算高效的替代方案，通过从大型模型的一小部分参数中构建一个浅层迷你模型。然后可以在迷你模型上高效地训练新的语言特定嵌入向量，并将其插入到对齐的大型模型中进行快速的跨语言传输。我们探索了两种学习迷你模型的方法：MiniJoint，它使用一个具有中间层次上辅助MLM头的单个transformer同时预训练主模型和迷你模型。MiniPost则从常规预训练模型开始，通过提取和冻结几层来构建迷你模型，并学习语言特定的嵌入向量。

    Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a 
    
[^64]: RPN: 一种深度学习中基于词向量的数据增强算法，用于语言理解

    RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.05961](http://arxiv.org/abs/2212.05961)

    RPN是一种基于词向量级别的数据增强算法，通过引入噪声修改原始文本的词嵌入，更好地捕捉自然语言变化，并在自然语言理解任务中表现出优异的性能。

    

    数据增强是机器学习中广泛使用以提高模型性能的技术。然而，现有的自然语言理解数据增强技术可能无法完全捕捉到自然语言的复杂变化，并且在大型数据集中应用起来具有挑战性。本文提出了一种新颖的数据增强技术——随机位置噪声（RPN）算法，它在词向量级别上进行操作。RPN通过根据选定词向量的现有值引入噪声修改原始文本的词嵌入，允许更细粒度的修改并更好地捕捉自然语言变化。与传统的数据增强方法不同，RPN不需要计算图中的梯度来进行虚拟样本更新，使其更容易应用于大型数据集。实验结果表明，在各种自然语言理解任务中，包括情感分析等，RPN始终优于现有数据增强技术。

    Data augmentation is a widely used technique in machine learning to improve model performance. However, existing data augmentation techniques in natural language understanding (NLU) may not fully capture the complexity of natural language variations, and they can be challenging to apply to large datasets. This paper proposes the Random Position Noise (RPN) algorithm, a novel data augmentation technique that operates at the word vector level. RPN modifies the word embeddings of the original text by introducing noise based on the existing values of selected word vectors, allowing for more fine-grained modifications and better capturing natural language variations. Unlike traditional data augmentation methods, RPN does not require gradients in the computational graph during virtual sample updates, making it simpler to apply to large datasets. Experimental results demonstrate that RPN consistently outperforms existing data augmentation techniques across various NLU tasks, including sentime
    
[^65]: 用OPUS-MT实现神经机器翻译民主化

    Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01936](http://arxiv.org/abs/2212.01936)

    本文介绍了OPUS-MT生态系统的发展，包括开放式机器翻译模型和工具的开发，以及它们与最终用户应用程序、开发平台和专业工作流程的整合。通过增加语言覆盖范围和翻译质量，实现了神经机器翻译的民主化。

    

    本文介绍了OPUS生态系统，重点介绍开放式机器翻译模型和工具的开发，以及它们与最终用户应用程序、开发平台和专业工作流程的整合。我们讨论了我们正在进行的增加语言覆盖范围和翻译质量的任务，还描述了正在进行的工作，包括模块化翻译模型的开发和面向常规桌面和小型设备实现实时翻译的速度优化紧凑解决方案。

    This paper presents the OPUS ecosystem with a focus on the development of open machine translation models and tools, and their integration into end-user applications, development platforms and professional workflows. We discuss our on-going mission of increasing language coverage and translation quality, and also describe on-going work on the development of modular translation models and speed-optimized compact solutions for real-time translation on regular desktops and small devices.
    
[^66]: 电子商务网站上的情感分析和意见挖掘

    Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15536](http://arxiv.org/abs/2211.15536)

    本论文研究了电子商务网站上的情感分析和意见挖掘，提出了解决情感极性分类挑战的广泛技术，并进行了句子级分类和评论级分类。

    

    情感分析或意见挖掘有助于说明自然语言处理（NLP）中的短语。情感分析是近年来最重要的主题。本研究的目标是解决情感分析中的情感极性分类挑战。我们提出了一种广泛的技术来对情感对立进行分类，并提供了详细的过程解释。通过分析的结果，进行了句子级分类和评论级分类。最后，我们讨论了未来情感分析研究的计划。

    Sentiment analysis or opinion mining help to illustrate the phrase NLP (Natural Language Processing). Sentiment analysis has been the most significant topic in recent years. The goal of this study is to solve the sentiment polarity classification challenges in sentiment analysis. A broad technique for categorizing sentiment opposition is presented, along with comprehensive process explanations. With the results of the analysis, both sentence-level classification and review-level categorization are conducted. Finally, we discuss our plans for future sentiment analysis research.
    
[^67]: 带属性辨别潜空间的语言去毒化

    Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10329](http://arxiv.org/abs/2210.10329)

    本研究提出了一种使用属性辨别潜空间进行语言去毒化的方法，通过将原始Transformer语言模型的潜空间投影到一个能够通过属性将文本进行良好分离的潜空间上，最小化内存和计算开销，实现了对有毒文本的控制。

    

    基于Transformer的语言模型已在自然语言理解任务上取得了令人瞩目的结果，但它们也可能生成包含侮辱、威胁和亵渎等有毒文本，限制了它们在现实世界中的应用。为了克服这个问题，一些文本生成方法旨在使用额外的语言模型或扰动来去毒化有毒文本。然而，先前的方法需要过多的内存、计算和时间，这在实际应用中成为了严重的瓶颈。为了解决这些限制，我们提出了一种有效 yet 高效的语言去毒化方法，使用一个带属性辨别的潜空间。具体而言，我们通过投影块和属性辨别器，将原始Transformer语言模型的潜空间投影到一个能够通过属性将文本进行良好分离的辨别性潜空间上。这允许语言模型在最小的内存和计算开销下控制文本生成为非有毒的。我们验证了我们的模型，

    Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-
    
[^68]: 连续学习的独占超掩码子网络训练

    Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.10209](http://arxiv.org/abs/2210.10209)

    本研究提出了一种连续学习方法ExSSNeT，通过独占超掩码子网络训练和KNN-based知识传递，解决了固定权重限制和知识积累问题。

    

    连续学习方法关注在避免灾难性遗忘的同时随着时间累积知识。最近，Wortsman等人提出了一种连续学习方法SupSup，该方法使用一个随机初始化的固定基础网络，并为每个新任务找到一个超掩码，以选择性地保留或移除每个权重以产生一个子网络。他们通过不更新网络权重来避免遗忘。虽然没有遗忘，但SupSup的性能不佳，因为固定权重限制了其表征能力。此外，在学习新任务时，模型内部没有知识的积累或传递。因此，我们提出了ExSSNeT（独占超掩码子网络训练），它进行了独有且不重叠的子网络权重训练，避免了后续任务对共享权重的冲突更新，从而提高性能的同时仍然防止遗忘。此外，我们提出了一种基于KNN的知识传递（KKT）方法。

    Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT)
    
[^69]: IsoVec: 控制词向量空间的相对同构性

    IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.05098](http://arxiv.org/abs/2210.05098)

    这项研究介绍了IsoVec，一种控制词向量空间相对同构性的方法，通过在Skip-gram损失函数中加入全局同构度度量，提高了训练后词向量空间的同构性，进而改善了跨语言映射效果。

    

    从单语言词向量空间提取高质量的翻译词典的能力取决于空间的几何相似性——它们的“同构度”。我们解决了跨语言映射出现问题的根本原因：即词向量训练导致底层空间不同构。我们将同构度的全局度量直接合并到Skip-gram损失函数中，成功地增加了训练后词向量空间的相对同构度，并提高了它们映射到共享的跨语言空间的能力。结果是在一般数据条件下，领域不匹配和训练算法不相似情况下改善了双语词汇表归纳的效果。我们在https://github.com/kellymarchisio/isovec 上发布了IsoVec。

    The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces -- their degree of "isomorphism." We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the underlying spaces being non-isomorphic. We incorporate global measures of isomorphism directly into the Skip-gram loss function, successfully increasing the relative isomorphism of trained word embedding spaces and improving their ability to be mapped to a shared cross-lingual space. The result is improved bilingual lexicon induction in general data conditions, under domain mismatch, and with training algorithm dissimilarities. We release IsoVec at https://github.com/kellymarchisio/isovec.
    
[^70]: 《组合性作为词汇对称性》

    Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.12926](http://arxiv.org/abs/2201.12926)

    本文将组合性定义为对数据分布的对称性约束，而不是模型，通过自动发现数据转换并应用于训练数据，提高模型的组合归纳偏置。

    

    在语义解析、指令遵循和问题回答等任务中，标准的深度网络在从小数据集中进行组合泛化时会失败。许多现有方法通过强制实施句子解释的组合过程的模型架构来克服这个限制。本文提出了一个领域通用和模型无关的组合性形式，将其作为数据分布的对称性约束而不是模型。我们证明了，无论何时一个任务可以通过一个组合模型来解决，都存在一个相应的数据增强方案——将示例转换为其他合适示例的过程——可以为解决相同任务的任何训练模型赋予组合归纳偏置。我们描述了一个自动发现这些转换并将其应用于普通神经序列模型训练数据的过程，称为LEXSYM。与现有的组合数据增强过程不同，LEXSYM可以被快速部署。

    In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deplo
    

