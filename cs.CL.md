# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://rss.arxiv.org/abs/2402.01591) | 本文提出了BAT，它结合了双耳声音场景分析模型的空间声音感知能力和大规模语言模型的自然语言推理能力，以复制人类的空间声音推理能力。通过使用合成的双耳音频数据集和基于空间声音的问答数据集进行训练，BAT在空间声音感知和推理方面取得了强大的性能。 |
| [^2] | [Exploring Spatial Schema Intuitions in Large Language and Vision Models](https://rss.arxiv.org/abs/2402.00956) | 通过再现心理语言学实验，研究发现大型语言模型（LLMs）尽管无具身性，却能够有效捕捉到人类对于语言中基本的空间构建块的隐含直觉，这对于理解语言和空间的相互作用具有重要意义。 |
| [^3] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^4] | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) | 逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。 |
| [^5] | [Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction](https://arxiv.org/abs/2403.17540) | 大型语言模型GPT-4在语法错误校正评估中表现优异，与人类判断有很高的相关性，凸显了在评估标准中流畅度的重要性。 |
| [^6] | [MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations](https://arxiv.org/abs/2403.10943) | MIntRec2.0介绍了一个旨在解决多模态意图识别和对话中场外检测挑战的大规模基准数据集。该数据集包含30个细粒度类别的1,245个对话和15,040个样本，其中包括逼真的场外样本，并丰富了发言者信息以支持多方对话研究。 |
| [^7] | [Misinformation is not about Bad Facts: An Analysis of the Production and Consumption of Fringe Content](https://arxiv.org/abs/2403.08391) | 虚假信息不仅仅涉及错误的事实，研究发现在线边缘意识形态通过共识和“事实正确”的内容传播，同时发现了极右用户倾向挑选关注特定主题的新闻，并且可以根据用户的沟通风格识别易于分享错误信息的用户。 |
| [^8] | [SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation](https://arxiv.org/abs/2403.07088) | 提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。 |
| [^9] | [Learning to Use Tools via Cooperative and Interactive Agents](https://arxiv.org/abs/2403.03031) | 提出了一种ConAgents框架，通过合作和互动代理使大型语言模型能够学习使用工具，并引入了迭代校准方法，以解决单个代理执行多样化操作能力有限和自适应纠正错误困难的问题。 |
| [^10] | [Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models](https://arxiv.org/abs/2403.02715) | 该研究通过对LLMs在越南语上进行微调和综合评估，发现微调后的模型在越南语理解和生成方面表现出良好能力，同时指出模型参数数量与性能之间存在着权衡关系，训练或微调数据集的质量是影响LLM性能的关键因素。 |
| [^11] | [Revisiting Meta-evaluation for Grammatical Error Correction](https://arxiv.org/abs/2403.02674) | 该论文提出了SEEDA，一个用于语法错误修正的新数据集，提供了对12种最先进系统进行元评估的校正，通过在句子级别元评估中对粒度进行对齐，提高了相关性。 |
| [^12] | [Accelerating Greedy Coordinate Gradient via Probe Sampling](https://arxiv.org/abs/2403.01251) | 研究引入了一种名为“探查采样”的新算法，通过动态确定草稿模型和目标模型的相似度，来加速贪婪坐标梯度算法，实现高达5.6倍的加速。 |
| [^13] | [Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal](https://arxiv.org/abs/2403.01244) | 提出了一种称为Self-Synthesized Rehearsal（SSR）的框架，利用大型语言模型生成合成实例用于持续学习中的复述，以解决大型语言模型遭受灾难性遗忘的问题。 |
| [^14] | [IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact](https://arxiv.org/abs/2403.01241) | 本研究提出了IntactKV方法，通过保持关键标记的完整性，改善了大型语言模型的量化过程，进一步降低了量化误差的上限，并取得了显著的性能提升。 |
| [^15] | [Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks](https://arxiv.org/abs/2403.01031) | 介绍了一系列阿拉伯多模式大语言模型Peacock，展示了其在视觉推理任务上的出色性能和不断出现的方言潜力，并提出了一个用于评估阿拉伯语相关方面的新基准Henna |
| [^16] | [LoRA Meets Dropout under a Unified Framework](https://arxiv.org/abs/2403.00812) | LoRA是一个轻量级的参数高效微调方法，该论文研究了LoRA与dropout方法在模型定制中的矛盾，重新审视了transformer-specific的dropout方法，并建立了它们之间的数学和经验上的等价性和区别。 |
| [^17] | [Language-guided Skill Learning with Temporal Variational Inference](https://arxiv.org/abs/2402.16354) | 该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。 |
| [^18] | [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187) | 现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。 |
| [^19] | [FuseChat: Knowledge Fusion of Chat Models](https://arxiv.org/abs/2402.16107) | FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。 |
| [^20] | [Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models](https://arxiv.org/abs/2402.15481) | 提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。 |
| [^21] | [Explorations of Self-Repair in Language Models](https://arxiv.org/abs/2402.15390) | 自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。 |
| [^22] | [GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?](https://arxiv.org/abs/2402.15238) | GPT-HateCheck提出了一个框架，通过指导大型语言模型从头开始生成更多样化和现实的功能测试，以解决现有测试案例过于通用简单的问题。 |
| [^23] | [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992) | 本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。 |
| [^24] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^25] | [Unveiling Linguistic Regions in Large Language Models](https://arxiv.org/abs/2402.14700) | 本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。 |
| [^26] | [Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts](https://arxiv.org/abs/2402.14457) | 提出了一种新的标注方案，用于分类条款和条件合同中的不同类型的条款，以支持法律专家快速识别和评估问题，并展示了通过实验证明了对这些类别进行自动分类的可行性。 |
| [^27] | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | 本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能 |
| [^28] | [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847) | 通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。 |
| [^29] | [WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment](https://arxiv.org/abs/2402.12275) | 通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。 |
| [^30] | [A Chinese Dataset for Evaluating the Safeguards in Large Language Models](https://arxiv.org/abs/2402.12193) | 该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。 |
| [^31] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^32] | [SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning](https://arxiv.org/abs/2402.11903) | 提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性 |
| [^33] | [Incremental Sequence Labeling: A Tale of Two Shifts](https://arxiv.org/abs/2402.10447) | 提出了一种名为IS3的框架，旨在解决增量序列标记任务中的E2O和O2E两种重要的语义转变，通过使用知识蒸馏来维持对旧实体的判别能力。 |
| [^34] | [HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs](https://arxiv.org/abs/2402.07309) | 本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。 |
| [^35] | [Limits of Transformer Language Models on Algorithmic Learning](https://arxiv.org/abs/2402.05785) | Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。 |
| [^36] | [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2402.05445) | 本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。 |
| [^37] | [MEMORYLLM: Towards Self-Updatable Large Language Models](https://arxiv.org/abs/2402.04624) | MEMORYLLM是一个自更新的大型语言模型，其中包括Transformer和一个固定大小的内存池，能够有效地整合新知识并保持长期信息保留能力。即使在近百万次内存更新后，MEMORYLLM仍能保持操作的完整性。 |
| [^38] | [Training Language Models to Generate Text with Citations via Fine-grained Rewards](https://arxiv.org/abs/2402.04315) | 本文提出了一种使用细粒度奖励训练语言模型生成高质量引用的有效框架，并在常见的大型语言模型训练策略上进行了实证分析。 |
| [^39] | [ANLS* -- A Universal Document Processing Metric for Generative Large Language Models](https://arxiv.org/abs/2402.03848) | ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。 |
| [^40] | [Isotropy, Clusters, and Classifiers](https://arxiv.org/abs/2402.03191) | 同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。 |
| [^41] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^42] | [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314) | 该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。 |
| [^43] | [Neighboring Perturbations of Knowledge Editing on Large Language Models](https://arxiv.org/abs/2401.17623) | 本文研究大型语言模型上知识编辑对邻近知识的扰动，提出了 additivity 指标以及 Perturbation Evaluation of Appending Knowledge (PEAK) 基准，用于评估附加新知识时邻近知识的扰动程度。 |
| [^44] | [Tradeoffs Between Alignment and Helpfulness in Language Models](https://arxiv.org/abs/2401.16332) | 本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。 |
| [^45] | [SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation](https://arxiv.org/abs/2310.18376) | SQLformer是一个用于文本到SQL翻译的深度自回归查询图生成模型，采用了特定的Transformer架构，并通过结构归纳偏差解决领域泛化和自然语言与SQL查询对齐的难题。 |
| [^46] | [Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View](https://arxiv.org/abs/2310.02124) | 通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。 |
| [^47] | [Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References](https://arxiv.org/abs/2305.15067) | 通过增加参考文献的多样性，本文提出的方法Div-Ref 显著提高了自然语言生成评估的相关性 |
| [^48] | [Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support.](http://arxiv.org/abs/2401.14688) | Taiyi-Diffusion-XL是一个中英双语文本到图像生成模型，通过对CLIP和Stable-Diffusion-XL的能力进行扩展，并通过双语连续预训练来实现。模型通过将常用的汉字整合到CLIP的分词器和嵌入层中，以及使用大型视觉语言模型丰富文本提示，提供了更好的图像标题和高质量的视觉效果。实验证明，该模型在双语图像-文本检索中表现出色。 |
| [^49] | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning.](http://arxiv.org/abs/2401.05268) | AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。 |
| [^50] | [Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion.](http://arxiv.org/abs/2401.02993) | 本文提出了一种计算高效的检索表示融合方法ReFusion，通过将检索表示直接融合到语言模型中，解决了将外部数据库的知识融入非知识密集型任务中的挑战。 |
| [^51] | [Self-Infilling Code Generation.](http://arxiv.org/abs/2311.17972) | 本文介绍了自补代码生成的通用框架，利用自补机制实现了中断和循环机制，使传统解码进程变得非单调。利用中断机制可以推迟生成代码，增强对输出的控制；利用循环机制可以循环更新和同步生成的每个部分。 |
| [^52] | [An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning.](http://arxiv.org/abs/2310.12274) | 提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。 |
| [^53] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^54] | [Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining.](http://arxiv.org/abs/2310.08566) | 通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。 |
| [^55] | [Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding.](http://arxiv.org/abs/2310.07075) | 本文提出了一种无语法错误且具有泛化能力的LLM工具使用方法ToolDec，通过有限状态解码算法消除了工具相关错误，使LLM能够有效选择工具，而无需微调或上下文文档。 |
| [^56] | [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.](http://arxiv.org/abs/2310.06387) | 本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。 |
| [^57] | [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A".](http://arxiv.org/abs/2309.12288) | LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。 |
| [^58] | [Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning.](http://arxiv.org/abs/2309.11489) | Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。 |
| [^59] | [Large Language Model as a User Simulator.](http://arxiv.org/abs/2308.11534) | 本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。 |
| [^60] | [Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures.](http://arxiv.org/abs/2307.05360) | 本文全面评估了ChatGPT在编码算法和数据结构方面的能力，基于最大的编码挑战目录，重点关注Python编程语言和数据结构算法两个基础主题。总结测试中ChatGPT的代码解决问题的准确性、代码质量和运行时错误的性质。 |
| [^61] | [Abugida Normalizer and Parser for Unicode texts.](http://arxiv.org/abs/2306.01743) | 本论文提出了两个针对Indic语言Unicode书写方案中常见及不常见问题的工具库，分别是纠正不一致性的规范化器和Abugida文本的字形解析器，使用这两个工具可以提高400%的速度并显著改善下游任务表现。 |
| [^62] | [Think Before You Act: Decision Transformers with Internal Working Memory.](http://arxiv.org/abs/2305.16338) | 该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。 |
| [^63] | [Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets.](http://arxiv.org/abs/2305.11625) | 本研究针对按代码片段搜索的问题，提出了一个新的基于StackOverflow数据的SearchBySnippet数据集，并开发了一个单编码器模型SnippeR，它在该数据集上具有优异的性能。 |
| [^64] | [Continual Multimodal Knowledge Graph Construction.](http://arxiv.org/abs/2305.08698) | 连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。 |
| [^65] | [Towards Weakly-Supervised Hate Speech Classification Across Datasets.](http://arxiv.org/abs/2305.02637) | 该论文提出使用极度弱的监督方法，只依赖于类别名称而不是注释数据中的类别示例，解决当前仇恨言论识别的研究存在的数据创建策略不系统和不同注释方案问题，并展示了有效性。 |
| [^66] | [Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems.](http://arxiv.org/abs/2304.11090) | 本文提出了一个以模式为导向的负责任AI-by-design参考架构，用于设计基于基础模型的AI系统，重点关注可解释性、公平性、安全性和鲁棒性等关键设计元素。 |
| [^67] | [GesGPT: Speech Gesture Synthesis With Text Parsing from GPT.](http://arxiv.org/abs/2303.13013) | GesGPT 是一种新的语音手势生成方法，它利用大型语言模型 GPT 的语义分析能力，从文本输入中提取与手势相关的信息，并使用手势库和集成模块生成意义丰富的语音手势。 |
| [^68] | [A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification.](http://arxiv.org/abs/2303.12892) | 本研究提出了一个简化的Switch Transformer框架，并从头开始训练，取得了在小型法语临床文本分类任务中比预训练的BERT模型更好的效果，采用Switch Transformer的专家混合机制有助于提高识别准确度，最终在测试集上实现了87％的准确率、87％的精度和86％的召回率。 |

# 详细

[^1]: BAT: 使用大规模语言模型学习关于空间声音的推理能力

    BAT: Learning to Reason about Spatial Sounds with Large Language Models

    [https://rss.arxiv.org/abs/2402.01591](https://rss.arxiv.org/abs/2402.01591)

    本文提出了BAT，它结合了双耳声音场景分析模型的空间声音感知能力和大规模语言模型的自然语言推理能力，以复制人类的空间声音推理能力。通过使用合成的双耳音频数据集和基于空间声音的问答数据集进行训练，BAT在空间声音感知和推理方面取得了强大的性能。

    

    空间声音推理是一种基本的人类技能，它使我们能够根据声音来导航和解释我们的周围环境。本文提出了BAT，它将双耳声音场景分析模型的空间声音感知能力与大规模语言模型（LLM）的自然语言推理能力相结合，以复制这种固有能力。为了解决现有野外空间声音数据集的缺乏，我们使用AudioSet和SoundSpaces 2.0合成了一个双耳音频数据集。接下来，我们开发了一种基于空间声音的问答数据集SpatialSoundQA，提供了一系列QA任务，以训练BAT在空间声音感知和推理的各个方面。BAT的声学前端编码器是一种名为Spatial Audio Spectrogram Transformer（Spatial-AST）的创新空间音频编码器，它本身在声音事件检测、空间定位和距离估计等方面具有强大的性能。通过将Spatial-AST与LLaMA-2 7B集成，

    Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
    
[^2]: 在大型语言和视觉模型中探索空间模式直觉

    Exploring Spatial Schema Intuitions in Large Language and Vision Models

    [https://rss.arxiv.org/abs/2402.00956](https://rss.arxiv.org/abs/2402.00956)

    通过再现心理语言学实验，研究发现大型语言模型（LLMs）尽管无具身性，却能够有效捕捉到人类对于语言中基本的空间构建块的隐含直觉，这对于理解语言和空间的相互作用具有重要意义。

    

    尽管大型语言模型（LLM）在人工智能研究中非常普遍，但关于LLM中的具身性问题还未得到充分探讨，这使它们不同于机器人中具体的具身系统，机器人可以通过感知直接指导行动。我们的研究探索了一个有趣的问题，即LLM是否能够有效地捕捉到人类对于语言中基本的空间构建块的隐含直觉，尽管它们是非具身的。我们运用从早期感知运动经验中发展出的空间认知基础的见解，通过再现三个心理语言学实验来指导我们的探索。令人惊讶的是，模型输出与人类回答之间出现了相关性，揭示了没有与具体经验有实质连接的适应能力。显著的区别包括极化的语言模型响应和视觉语言模型中降低的相关性。这项研究对于深入了解语言和空间的相互作用有着微妙的贡献。

    Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spat
    
[^3]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^4]: LISA：用于高效内存大型语言模型微调的逐层重要性采样

    LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning

    [https://arxiv.org/abs/2403.17919](https://arxiv.org/abs/2403.17919)

    逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。

    

    机器学习领域自大型语言模型（LLMs）首次出现以来取得了令人瞩目的进展，然而它们巨大的内存消耗已成为大规模训练的主要障碍。虽然已经提出了诸如低秩调整（LoRA）之类的参数高效微调技术来缓解这一问题，但在大多数大规模微调设置中，它们的性能仍无法与完整参数训练相匹配。为弥补这一不足，我们研究了LoRA在微调任务中的逐层特性，并观察到不同层之间权重范数的异常偏斜。利用这一关键观察，我们发现了一个令人惊讶简单的训练策略，在记忆成本低于LoRA的情况下，在广泛的设置中优于LoRA和完整参数训练。我们将其命名为Layerwise Importance Sampled AdamW（LISA），这是LoRA的一个有希望的替代方案，应用了

    arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
    
[^5]: 大型语言模型是语法错误校正的最先进评估器

    Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction

    [https://arxiv.org/abs/2403.17540](https://arxiv.org/abs/2403.17540)

    大型语言模型GPT-4在语法错误校正评估中表现优异，与人类判断有很高的相关性，凸显了在评估标准中流畅度的重要性。

    

    大型语言模型（LLMs）据报道在某些任务中胜过现有的自动评估指标，比如文本总结和机器翻译。但是，关于LLMs在语法错误校正（GEC）中作为评估器的研究还不足。本研究通过采用设计的提示来整合先前研究启发的各种评估标准，调查了LLMs在GEC评估中的表现。我们的广泛实验结果表明，GPT-4在与人类判断之间达到了0.662的Kendall等级相关性，超过了所有现有方法。此外，在最近的GEC评估中，我们强调了LLMs规模的重要性，特别强调了流畅度在评估标准中的重要性。

    arXiv:2403.17540v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall's rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.
    
[^6]: MIntRec2.0：用于多模态意图识别和对话中场外检测的大规模基准数据集

    MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations

    [https://arxiv.org/abs/2403.10943](https://arxiv.org/abs/2403.10943)

    MIntRec2.0介绍了一个旨在解决多模态意图识别和对话中场外检测挑战的大规模基准数据集。该数据集包含30个细粒度类别的1,245个对话和15,040个样本，其中包括逼真的场外样本，并丰富了发言者信息以支持多方对话研究。

    

    多模态意图识别面临重大挑战，需要整合来自现实世界背景的非语言形式，以增强对人类意图的理解。现有的基准数据集在规模上受限，并且在处理多轮对话互动中出现的场外样本时存在困难。我们介绍了MIntRec2.0，这是一个用于多方对话中的多模态意图识别的大规模基准数据集。它包含1,245个对话，15,040个样本，每个样本在30个细粒度类别的新意图分类中进行了注释。除了9,304个场内样本外，还包括5,736个出现在多轮上下文中的场外样本，这在现实场景中自然发生。此外，我们还提供了每个话语中发言者的详细信息，丰富了它在多方对话研究中的实用性。

    arXiv:2403.10943v1 Announce Type: cross  Abstract: Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the o
    
[^7]: 虚假信息不是关于错误事实：对边缘内容的生产和消费进行分析

    Misinformation is not about Bad Facts: An Analysis of the Production and Consumption of Fringe Content

    [https://arxiv.org/abs/2403.08391](https://arxiv.org/abs/2403.08391)

    虚假信息不仅仅涉及错误的事实，研究发现在线边缘意识形态通过共识和“事实正确”的内容传播，同时发现了极右用户倾向挑选关注特定主题的新闻，并且可以根据用户的沟通风格识别易于分享错误信息的用户。

    

    如果误传根本不是一个信息问题呢？我们的研究发现，在线边缘意识形态通过一种基于共识和“事实正确”的内容传播。我们发现，具有中间和极右政治倾向的澳大利亚新闻出版商在信息完整性和质量方面居于可比水平；此外，极右翼Twitter用户经常分享来自中间来源的内容。然而，当我们考虑到两个额外因素时，出现了明显差异：1）极右用户对文章的狭隘主题选择，暗示他们只挑选与他们关注特定主题的新闻文章，2）在审查文章写作风格时，中间和极右出版商之间的巨大差异。此外，我们甚至可以根据用户的沟通风格识别易于分享错误信息的用户。这些发现对于合作方面有重要的启示。

    arXiv:2403.08391v1 Announce Type: new  Abstract: What if misinformation is not an information problem at all? Our findings suggest that online fringe ideologies spread through the use of content that is consensus-based and "factually correct". We found that Australian news publishers with both moderate and far-right political leanings contain comparable levels of information completeness and quality; and furthermore, that far-right Twitter users often share from moderate sources. However, a stark difference emerges when we consider two additional factors: 1) the narrow topic selection of articles by far-right users, suggesting that they cherrypick only news articles that engage with specific topics of their concern, and 2) the difference between moderate and far-right publishers when we examine the writing style of their articles. Furthermore, we can even identify users prone to sharing misinformation based on their communication style. These findings have important implications for co
    
[^8]: SPA：面向云端和设备协作的计算友好型Seq2seq个性化生成

    SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation

    [https://arxiv.org/abs/2403.07088](https://arxiv.org/abs/2403.07088)

    提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。

    

    大语言模型(LLMs)表现出色的能力已在各种任务和问答中得到展示。然而，LLMs需要高计算成本和大内存成本。同时，当训练或预测过程包含敏感信息时，LLMs可能会导致隐私泄露。在本文中，我们提出了SPA（Side Plugin Adaption），这是一个轻量级架构，用于快速设备上的推断和在严格的设备计算和内存约束条件下保持隐私。

    arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
    
[^9]: 通过合作和互动代理学习使用工具

    Learning to Use Tools via Cooperative and Interactive Agents

    [https://arxiv.org/abs/2403.03031](https://arxiv.org/abs/2403.03031)

    提出了一种ConAgents框架，通过合作和互动代理使大型语言模型能够学习使用工具，并引入了迭代校准方法，以解决单个代理执行多样化操作能力有限和自适应纠正错误困难的问题。

    

    工具学习使大型语言模型（LLMs）作为代理人能够使用外部工具来扩展其功能。现有方法利用单个基于LLM的代理循环选择和执行工具，然后将结果合并到下一个动作预测中。然而，它们在处理复杂任务时仍然存在潜在的性能下降问题，原因是：（1）单个LLM的固有能力执行多样化操作受限，以及（2）在任务失败时难以自适应地纠正错误。为了缓解这些问题，我们提出了ConAgents，即合作和互动代理框架，将工具学习的工作流模块化为Grounding（基础）、Execution（执行）和Observing（观察）代理。我们还介绍了一种迭代校准（IterCali）方法，使代理能够根据来自工具环境的反馈对自己进行调整。在三个数据集上进行的实验证明了超过

    arXiv:2403.03031v1 Announce Type: new  Abstract: Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the super
    
[^10]: 跨越语言视野：越南大型语言模型的微调和综合评估

    Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models

    [https://arxiv.org/abs/2403.02715](https://arxiv.org/abs/2403.02715)

    该研究通过对LLMs在越南语上进行微调和综合评估，发现微调后的模型在越南语理解和生成方面表现出良好能力，同时指出模型参数数量与性能之间存在着权衡关系，训练或微调数据集的质量是影响LLM性能的关键因素。

    

    最近大型语言模型（LLMs）的进展强调了它们在人工智能发展中的重要性。然而，尽管在多语言数据集上进行了广泛的预训练，但目前开源的LLMs在处理越南语方面的效果有限。这一挑战是由于缺乏专门针对越南语LLM评估的系统化基准数据集和指标。为了缓解这些问题，我们专门为越南语进行了LLM的微调，并开发了一个涵盖10个常见任务和31个指标的综合评估框架。我们的评估结果表明，经过微调的LLMs在越南语方面表现出了增强的理解和生成能力。此外，我们的分析表明，具有更多参数的模型可能会带来更多的偏见和未校准的输出，而影响LLM性能的关键因素是训练或微调数据集的质量。

    arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und
    
[^11]: 重新审视语法错误修正的元评估

    Revisiting Meta-evaluation for Grammatical Error Correction

    [https://arxiv.org/abs/2403.02674](https://arxiv.org/abs/2403.02674)

    该论文提出了SEEDA，一个用于语法错误修正的新数据集，提供了对12种最先进系统进行元评估的校正，通过在句子级别元评估中对粒度进行对齐，提高了相关性。

    

    Metrics在语法错误修正（GEC）中自动评估的基础。其中，元评估依赖于它们与人类判断的相关性。然而，英语GEC中常规的元评估面临一些挑战，包括由于评估粒度不一致而导致的偏见，以及使用传统系统的过时设置。针对这些问题，本文提出了SEEDA，这是一个用于GEC元评估的新数据集，包括校正和两种不同粒度（基于编辑和基于句子）的人类评级。

    arXiv:2403.02674v1 Announce Type: new  Abstract: Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, s
    
[^12]: 通过探查采样加速贪婪坐标梯度

    Accelerating Greedy Coordinate Gradient via Probe Sampling

    [https://arxiv.org/abs/2403.01251](https://arxiv.org/abs/2403.01251)

    研究引入了一种名为“探查采样”的新算法，通过动态确定草稿模型和目标模型的相似度，来加速贪婪坐标梯度算法，实现高达5.6倍的加速。

    

    大型语言模型（LLMs）的安全性已成为一个中心问题，考虑到它们的快速发展和广泛应用。研究表明，贪婪坐标梯度（GCG）在构建包含对抗后缀的提示时非常有效，以破坏被认为是安全的LLMs，但GCG的优化耗时较长，限制了其实用性。为了减少GCG的时间成本并实现对LLMs安全性更全面的研究，在这项工作中，我们研究了一种称为“探查采样”的新算法，以加速GCG算法。该算法的核心是一种机制，动态确定较小草稿模型的预测与目标模型的提示候选预测的相似程度。当目标模型与草稿模型相似时，我们大量依赖于草稿模型来过滤大量潜在提示候选，以减少计算时间。探查采样使用Llam实现高达5.6倍的加速。

    arXiv:2403.01251v1 Announce Type: new  Abstract: Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llam
    
[^13]: 使用自我生成的复述来减轻大型语言模型中的灾难性遗忘

    Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal

    [https://arxiv.org/abs/2403.01244](https://arxiv.org/abs/2403.01244)

    提出了一种称为Self-Synthesized Rehearsal（SSR）的框架，利用大型语言模型生成合成实例用于持续学习中的复述，以解决大型语言模型遭受灾难性遗忘的问题。

    

    大型语言模型（LLMs）在持续学习过程中遭受灾难性遗忘。传统的基于复述的方法依赖于先前的训练数据来保留模型的能力，然而这在现实应用中可能无法实现。为了解决这一挑战，我们提出了一个名为自我生成复述（SSR）的框架，利用LLM生成合成实例进行复述。

    arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe
    
[^14]: IntactKV: 通过保持关键标记完整改进大型语言模型量化

    IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact

    [https://arxiv.org/abs/2403.01241](https://arxiv.org/abs/2403.01241)

    本研究提出了IntactKV方法，通过保持关键标记的完整性，改善了大型语言模型的量化过程，进一步降低了量化误差的上限，并取得了显著的性能提升。

    

    大型语言模型在自然语言处理方面表现出色，但需要大量计算。为了减少这一难题，人们已经探索了各种量化方法，然而这些方法会损害大型语言模型的性能。本文揭示了大型语言模型中一个以前被忽视的异常点类型。这些异常点被发现将大部分注意力分配给输入的初始标记，被称为关键标记，这对于量化的大型语言模型的性能至关重要。鉴于此，我们提出了IntactKV，从完整精度模型中无损地生成关键标记的KV缓存。这种方法简单易行，可以轻松与现有的量化解决方案结合。此外，IntactKV可以被校准为额外的大型语言模型参数，以进一步增强量化的大型语言模型。数学分析还证明了IntactKV有效地降低了量化误差的上限。实证结果表明，IntactKV带来了持续的改进，并实现了无损的仅权重量。

    arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only
    
[^15]: 孔雀：一系列阿拉伯多模式大语言模型及基准

    Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks

    [https://arxiv.org/abs/2403.01031](https://arxiv.org/abs/2403.01031)

    介绍了一系列阿拉伯多模式大语言模型Peacock，展示了其在视觉推理任务上的出色性能和不断出现的方言潜力，并提出了一个用于评估阿拉伯语相关方面的新基准Henna

    

    多模式大语言模型（MLLMs）在需要复杂推理和语言理解的各种任务中已被证明有效。然而，由于除英语以外的其他语言缺乏高质量的多模式资源，MLLMs的成功仍然相对局限于英语环境。这给开发其他语言的可比较模型带来了重大挑战，甚至包括那些拥有庞大说话人口的语言，如阿拉伯语。为了缓解这一挑战，我们引入了一套综合的阿拉伯MLLMs系列，称为\textit{Peacock}，具有强大的视觉和语言能力。通过全面的定性和定量分析，我们展示了我们的模型在各种视觉推理任务上的出色性能，并进一步展示了它们不断出现的方言潜力。此外，我们还介绍了一个名为\textit{Henna}的新基准，专门用于评估MLLM在与阿拉伯语相关的方面。

    arXiv:2403.01031v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic c
    
[^16]: LoRA在统一框架下遇见了Dropout

    LoRA Meets Dropout under a Unified Framework

    [https://arxiv.org/abs/2403.00812](https://arxiv.org/abs/2403.00812)

    LoRA是一个轻量级的参数高效微调方法，该论文研究了LoRA与dropout方法在模型定制中的矛盾，重新审视了transformer-specific的dropout方法，并建立了它们之间的数学和经验上的等价性和区别。

    

    具有显著能力的大型语言模型（LLMs）已成为许多自然语言处理应用中不可或缺的元素，而参数高效微调，特别是LoRA，已经成为模型定制的轻量级方法的流行选择。同时，各种dropout方法最初是为所有参数进行完整微调而设计的，有助于减轻与过多参数冗余相关的过拟合问题。因此，LoRA的可训练参数微不足道与先前dropout方法的有效性之间存在可能的矛盾，这一点之前大多被忽视。为填补这一空白，我们首先确认高效参数的LoRA也容易出现过拟合问题。然后，我们重新审视特定于transformer的dropout方法，从数学和经验上建立它们的等价性和区别。基于这种比较分析，我们引入了一个统一框架进行全面研究，

    arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in
    
[^17]: 通过时间变分推断进行语言引导的技能学习

    Language-guided Skill Learning with Temporal Variational Inference

    [https://arxiv.org/abs/2402.16354](https://arxiv.org/abs/2402.16354)

    该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。

    

    我们提出了一种从专家演示中发现技能的算法。该算法首先利用大型语言模型（LLMs）来提出轨迹的初始分割。随后，一个分层变分推断框架将LLM生成的分割信息纳入其中，通过合并轨迹段来发现可重用的技能。为了进一步控制压缩和可重用性之间的权衡，我们引入了一个基于最小描述长度原则的新辅助目标，帮助引导这种技能发现过程。我们的结果表明，使用我们方法的Agent能够发现有助于加速学习的技能，在BabyAI（一个网格世界导航环境）以及ALFRED（一个家庭模拟环境）的新长期任务中胜过基线技能学习方法。

    arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
    
[^18]: 利用其优势攻击LLM水印

    Attacking LLM Watermarks by Exploiting Their Strengths

    [https://arxiv.org/abs/2402.16187](https://arxiv.org/abs/2402.16187)

    现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。

    

    生成模型的进展使得人工智能生成的文本、代码和图片能够在许多应用中模仿人类生成的内容。水印技术旨在将信息嵌入模型的输出中以验证其来源，对于减少对这些人工智能生成内容的滥用非常有用。然而，现有的水印方案仍然令人意外地容易受到攻击。具体而言，我们展示了现有的LLM水印系统共享的可取特性，例如质量保留、鲁棒性和公开检测API，反过来却使这些系统容易遭受各种攻击。我们在常见水印设计选择方面严格研究潜在攻击，并提出了缓解攻击的最佳实践和防御措施——建立了一套嵌入和检测LLM水印的实用指南。

    arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
    
[^19]: FuseChat：对话模型知识融合

    FuseChat: Knowledge Fusion of Chat Models

    [https://arxiv.org/abs/2402.16107](https://arxiv.org/abs/2402.16107)

    FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。

    

    虽然从头开始训练大型语言模型（LLMs）确实可以导致具有独特能力和优势的模型，但这种方法会产生巨大成本，并可能导致竞争能力的潜在冗余。一种替代策略是将现有的LLMs组合成更强大的LLM，从而减少昂贵的预训练的必要性。但是，由于LLMs的多样化架构，直接参数融合被证明是不可行的。最近，FuseLLM引入了知识融合的概念，通过轻量级的持续训练将多个结构多样的LLM的集体知识转移至目标LLM。在本报告中，我们扩展了FuseLLM框架的可扩展性和灵活性，实现了对话LLM的融合，生成了FuseChat。FuseChat包括两个主要阶段。首先，我们对结构和规模不同的源LLMs进行知识融合

    arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
    
[^20]: 偏见和反复无常：衡量大型语言模型社会歧视的统计框架

    Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models

    [https://arxiv.org/abs/2402.15481](https://arxiv.org/abs/2402.15481)

    提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。

    

    arXiv:2402.15481v1 公告类型: 新的 摘要: 大型语言模型（LLMs）在社会运营中的日益融合加剧了它们对经济、法律、教育和医疗等重要领域决策的影响，引发了公众对这些模型涉及歧视安全和可靠性的担忧。然而，先前的歧视测量框架仅评估LLMs的平均歧视行为，往往由于忽视了一个额外的导致歧视的因素，即LLMs在不同上下文中的预测变化而变得不足。在这项工作中，我们提出了Prejudice-Caprice Framework（PCF），通过考虑LLMs的一贯偏见偏好和在多样上

    arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
    
[^21]: 在语言模型中自修复的探索

    Explorations of Self-Repair in Language Models

    [https://arxiv.org/abs/2402.15390](https://arxiv.org/abs/2402.15390)

    自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。

    

    先前研究狭窄分布的可解释性发现了自修复现象，即如果剥离大型语言模型中的组件，后续组件会改变其行为以进行补偿。我们的工作基于这些过去的文献，展示了当在完整的训练分布上剥离单个注意力头时，自修复存在于各种模型家族和尺寸上。我们进一步表明，在完整的训练分布上，自修复是不完美的，因为头部的原始直接效果并未完全恢复，并且是嘈杂的，因为自修复程度在不同提示之间显著变化（有时超过原始效果）。我们强调了促成自修复的两种不同机制，包括最终LayerNorm缩放因子的变化（可修复直接效果的30%）以及实现反擦除的稀疏神经元集。

    arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
    
[^22]: GPT-HateCheck: LLMs能否为仇恨言论检测编写更好的功能测试？

    GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?

    [https://arxiv.org/abs/2402.15238](https://arxiv.org/abs/2402.15238)

    GPT-HateCheck提出了一个框架，通过指导大型语言模型从头开始生成更多样化和现实的功能测试，以解决现有测试案例过于通用简单的问题。

    

    在线仇恨检测受到数据采样、注释和模型预训练中引入的偏见的影响。因此，仅测量在留存测试数据中所有示例上的平均性能是不足够的。相反，我们必须识别特定模型的弱点，并在其更有可能失败时获得信息。本文提出了一个最近的方向，即HateCheck，这是一个用于在使用模板生成的合成数据上测试精细粒度模型功能的套件，模板的形式为“你只是一个[骂人的词]对我来说”。然而，尽管HateCheck允许获得更详细的诊断见解，但其测试用例通常是通用的，并且具有简单的句子结构，不符合真实世界数据。为解决这一局限性，我们提出了GPT-HateCheck，一个框架，通过指导大型语言模型（LLMs）从头开始生成更多样化和现实的功能测试。我们采用额外的自然语言推理（NLI）模型来验证生成。

    arXiv:2402.15238v1 Announce Type: new  Abstract: Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind "You are just a [slur] to me." However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations
    
[^23]: 小型基准测试：用更少的示例评估LLM

    tinyBenchmarks: evaluating LLMs with fewer examples

    [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)

    本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。

    

    大型语言模型（LLMs）的多功能性导致创建了多种基准测试，彻底测试各种语言模型的能力。这些基准测试包含成千上万个示例，使得评估LLMs非常昂贵。本文研究了减少评估LLMs性能所需的评估次数的策略。例如，我们展示了要准确估计LLMs在MMLU上的性能（一个包含14K个示例的流行多选问答基准测试），只需要在100个精心挑选的示例上评估这个LLMs。我们发布了评估工具和流行基准测试的微型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和微型基准测试足以可靠且高效地重现原始评估结果。

    arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
    
[^24]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^25]: 揭示大型语言模型中的语言区域

    Unveiling Linguistic Regions in Large Language Models

    [https://arxiv.org/abs/2402.14700](https://arxiv.org/abs/2402.14700)

    本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。

    

    大型语言模型(LLMs)已经展示了相当大的跨语言对齐和泛化能力。当前研究主要集中在改善LLMs的跨语言泛化能力上。然而，关于LLMs如何实现跨语言对齐的内在机制仍然缺乏研究。从区域划分的角度出发，本文在LLMs的语言能力上进行了几项调查。我们发现LLMs中的一个核心区域对应于语言能力，大约占总模型参数的1%。通过将参数设置为零来去除这个核心区域，会导致30种不同语言的显著性能下降。此外，这个核心区域表现出显著的维度依赖性，对特定维度上的单个参数的扰动会导致语言能力的丧失。此外，我们发现独特的区域...

    arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions 
    
[^26]: 标注和分类条款和条件合同中的相关条款

    Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts

    [https://arxiv.org/abs/2402.14457](https://arxiv.org/abs/2402.14457)

    提出了一种新的标注方案，用于分类条款和条件合同中的不同类型的条款，以支持法律专家快速识别和评估问题，并展示了通过实验证明了对这些类别进行自动分类的可行性。

    

    在本文中，我们提出了一种新的标注方案，用于分类条款和条件合同中的不同类型的条款，最终目标是支持法律专家快速识别和评估这类法律文件中的问题。为此，我们构建了一小规模的条款和条件合同语料库，并完成了一个包含14个类别的标注方案，最终达成了0.92的标注者间一致性。然后，针对其中的11个类别，我们使用多语言T5和两个基于BERT的LLM的两个微调版本进行了少样本提示的二分类任务实验，其中还包括意大利语的两个模型。我们的实验表明，通过在验证任务中达到从.79到.95的准确率，自动对我们的类别进行分类是可行的。

    arXiv:2402.14457v1 Announce Type: new  Abstract: In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.
    
[^27]: ProSparse: 引入和增强大型语言模型内部激活稀疏性

    ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

    [https://arxiv.org/abs/2402.13516](https://arxiv.org/abs/2402.13516)

    本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能

    

    Activation sparsity指的是激活输出中存在许多弱贡献元素。作为使用ReLU激活函数的模型的普遍属性，已被证明是提高模型推理效率的一种有前途的范例。然而，大多数大型语言模型（LLMs）采用了没有内在激活稀疏性的激活函数（例如GELU和Swish）。一些最近的努力尝试引入ReLU或其变体作为替代激活函数，以帮助LLMs实现激活稀疏性和推理加速，但很少能同时获得高稀疏度和可比较的模型性能。本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动LLMs实现更高的激活稀疏性而不降低模型性能。具体来说，将LLMs的激活函数替换为ReLU后，ProSparse采用渐进稀疏正则化

    arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
    
[^28]: 调整过的语言模型更好的知识学习者

    Instruction-tuned Language Models are Better Knowledge Learners

    [https://arxiv.org/abs/2402.12847](https://arxiv.org/abs/2402.12847)

    通过在持续预训练文档之前暴露LLM到问题-答案对，以便从复杂文档中编码知识，可以更好地适应知识访问方式。

    

    为了使基于大语言模型（LLM）的助手能够有效地适应不断发展的信息需求，必须能够通过持续在新数据上训练来更新它们的事实知识。传统做法涉及在新文档上持续预培训，然后根据问题-答案（QA）对进行指导调整。

    arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
    
[^29]: WorldCoder，一种基于模型的LLM代理：通过编写代码和与环境交互构建世界模型

    WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment

    [https://arxiv.org/abs/2402.12275](https://arxiv.org/abs/2402.12275)

    通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。

    

    我们提出了一种基于模型的代理，通过与环境的交互构建代表其对世界知识的Python程序。该世界模型试图解释其交互，同时对自己能够获得的奖励持乐观态度。我们通过扩展LLM的程序合成工作来实现这一点。我们在网格世界上研究了我们的代理，发现我们的方法在样本效率上比深度强化学习更高，并且在计算效率上比ReAct风格的代理更高效。

    arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
    
[^30]: 用于评估大型语言模型中安全机制的中文数据集

    A Chinese Dataset for Evaluating the Safeguards in Large Language Models

    [https://arxiv.org/abs/2402.12193](https://arxiv.org/abs/2402.12193)

    该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。

    

    许多研究已经证明大型语言模型（LLMs）可能产生有害响应，在LLMs部署时使用户面临意外风险。先前的研究提出了关于LLMs引发风险的综合分类法，以及相应的提示，可用于检查LLMs的安全机制。然而，这些研究几乎完全集中在英语上，其他语言的研究较少。在本文中，我们旨在弥补这一空白。我们首先介绍了一个用于评估中文LLMs安全性的数据集，然后将其扩展到另外两种情景，可用于更好地识别关于有风险提示拒绝的虚阔负面和错误肯定示例。我们进一步针对每种风险类型提出一组细粒度的安全评估标准，促进人工标注和自动评估LLM响应有害性。我们对五个LLM的实验表明，特定于地区的风险...

    arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a
    
[^31]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^32]: SoLA: 为了更好的逻辑推理而对LLM进行求解层调整

    SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning

    [https://arxiv.org/abs/2402.11903](https://arxiv.org/abs/2402.11903)

    提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性

    

    针对大型语言模型（LLMs）在逻辑推理上面临的挑战，先前的努力试图通过工具学习来改变问题求解。虽然在小规模问题上已经取得了进展，但由于规模庞大且表达复杂，解决工业案例仍然困难。在本文中，我们提出了一种新颖的求解层适应（SoLA）方法，我们在LLM中引入了一个求解器作为新层，不同地引导解决方案朝向可满足性。在SoLA中，LLM旨在理解自然语言描述的搜索空间，并识别最高质量的局部解，而求解器层则专注于初始解不满足的约束条件。借助MaxSAT作为桥梁，我们定义了前向和后向传递梯度，使最终模型能够收敛到一个满足的解或证明不可满足性。后门理论确保SoLA能够获得准确性

    arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
    
[^33]: 增量序列标记：两种转变的故事

    Incremental Sequence Labeling: A Tale of Two Shifts

    [https://arxiv.org/abs/2402.10447](https://arxiv.org/abs/2402.10447)

    提出了一种名为IS3的框架，旨在解决增量序列标记任务中的E2O和O2E两种重要的语义转变，通过使用知识蒸馏来维持对旧实体的判别能力。

    

    增量序列标记任务涉及在保留对先前类别知识的同时，随时间不断学习新类别。我们的研究确定了两种重要的语义转变：E2O（模型将旧实体错误标记为非实体）和O2E（模型将非实体或旧实体标记为新实体）。先前的研究主要集中在解决E2O问题上，忽视了O2E问题。这种忽略导致模型在学习过程中对新数据样本进行分类时存在偏见，认为它们属于新类别。为了解决这些挑战，我们提出了一种新颖的框架，即无语义转变的增量顺序标记（IS3）。受到已确定的语义转变（E2O和O2E）的启发，IS3旨在缓解模型中的灾难性遗忘。至于E2O问题，我们使用知识蒸馏来维持模型对旧实体的判别能力。

    arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
    
[^34]: HyperBERT:将混合超图感知层与语言模型用于文本属性超图上的节点分类

    HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs

    [https://arxiv.org/abs/2402.07309](https://arxiv.org/abs/2402.07309)

    本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。

    

    超图通过复杂的拓扑结构标记，表达多个实体之间的高阶相互作用，其中超边扮演重要角色。最近，基于超图的深度学习方法在学习文本属性超图上的节点分类问题中引起了越来越多的研究关注。然而，现有方法往往难以同时捕捉超图结构信息的全部内容和节点属性中的丰富语言属性，这在很大程度上影响了它们的效果和泛化能力。为了克服这些挑战，我们探索了如何通过为节点分类任务进一步增强预训练的BERT模型，引入专门的超图感知层。这些层将高阶结构归纳偏差引入语言模型中，从而提高模型利用超图结构中的高阶上下文信息和文本中的语义信息的能力。

    Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
    
[^35]: Transformer语言模型在算法学习上的限制

    Limits of Transformer Language Models on Algorithmic Learning

    [https://arxiv.org/abs/2402.05785](https://arxiv.org/abs/2402.05785)

    Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。

    

    我们分析了Transformer语言模型在学习离散算法方面的能力。为此，我们引入了两个要求组合多个离散子任务的新任务。我们通过从头开始训练LLaMA模型和在GPT-4和Gemini上提示来衡量学习学习原语的组合。我们观察到，目前最先进的Transformer语言模型的组合能力非常有限，并且在样本规模方面比为新的算法组合重新学习所有子任务效果更差。我们还提出了一个复杂性理论的定理，证明了记忆前馈模型上的梯度下降可以指数级地浪费数据。

    We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
    
[^36]: 准确的LLMs的LoRA-Finetuning量化通过信息保留

    Accurate LoRA-Finetuning Quantization of LLMs via Information Retention

    [https://arxiv.org/abs/2402.05445](https://arxiv.org/abs/2402.05445)

    本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。

    

    将LLMs的LoRA-finetuning量化研究得到准确但紧凑的LLMs以便在资源受限的硬件上部署。然而，现有的方法导致量化的LLMs严重退化，甚至无法从LoRA的调优中获益。本文提出了一种新颖的IR-QLoRA，通过信息保留推动带有LoRA的量化LLMs变得高度准确。所提出的IR-QLoRA主要依赖于两种从统一信息视角派生的技术：（1）基于统计的信息校准量化允许LLMs的量化参数精确保留原始信息；（2）基于调优的信息弹性连接使LoRA利用具有多样信息的弹性表示转换。综合实验证明，在2-4位宽下，IR-QLoRA可以显著提高LLaMA和LLaMA2系列的准确性，例如，4位LLaMA-7B相比于...

    The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
    
[^37]: MEMORYLLM：面向自更新的大型语言模型

    MEMORYLLM: Towards Self-Updatable Large Language Models

    [https://arxiv.org/abs/2402.04624](https://arxiv.org/abs/2402.04624)

    MEMORYLLM是一个自更新的大型语言模型，其中包括Transformer和一个固定大小的内存池，能够有效地整合新知识并保持长期信息保留能力。即使在近百万次内存更新后，MEMORYLLM仍能保持操作的完整性。

    

    现有的大型语言模型（LLMs）在部署后通常保持静态，这可能会使向模型中注入新知识变得困难。我们旨在构建包含相当比例的可自更新参数的模型，使模型能够有效且高效地整合新知识。为此，我们引入了MEMORYLLM，它是一个由Transformer和固定大小的内存池组成的模型，位于Transformer的潜空间内。MEMORYLLM可以通过文本知识进行自我更新并记忆先前注入的知识。我们的评估证明了MEMORYLLM有效地整合新知识的能力，其性能在模型编辑基准上得到了证实。同时，该模型还表现出长期信息保留能力，这在我们自定义的评估和长上下文基准中得到了验证。MEMORYLLM在进行了近百万次内存更新后，没有任何性能下降的迹象，显示出操作的完整性。

    Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.
    
[^38]: 使用细粒度奖励训练语言模型生成带引用文本

    Training Language Models to Generate Text with Citations via Fine-grained Rewards

    [https://arxiv.org/abs/2402.04315](https://arxiv.org/abs/2402.04315)

    本文提出了一种使用细粒度奖励训练语言模型生成高质量引用的有效框架，并在常见的大型语言模型训练策略上进行了实证分析。

    

    最近的大型语言模型（LLMs）在回答用户查询方面非常有用，但容易产生幻觉，并且它们的回答常常缺乏可靠来源的引用。解决这些问题的直观方法是将外部文档的引用作为证据包含在文本中。虽然以前的研究直接促使LLMs生成引用文本，但它们的性能远非令人满意，尤其是对于较小的LLMs。在这项工作中，我们提出了一种有效的训练框架，使用细粒度奖励教授LLMs生成高度支持和相关的引用，同时确保其响应的正确性。我们还对将这些细粒度奖励应用于常见的LLMs训练策略进行了系统分析，证明其相对于传统做法的优势。我们在从ALCE基准测试中获取的问答（QA）数据集上进行了大量实验，并验证了模型的生成能力。

    While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene
    
[^39]: ANLS* -- 一种适用于生成型大语言模型的通用文档处理度量方法

    ANLS* -- A Universal Document Processing Metric for Generative Large Language Models

    [https://arxiv.org/abs/2402.03848](https://arxiv.org/abs/2402.03848)

    ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。

    

    传统上，在文档分类和信息提取等任务中，区分模型一直是主要选择。这些模型做出的预测可以分为有限数量的预定义类别，便于进行二元真假评估，并能直接计算F1分数等指标。然而，生成型大语言模型（GLLMs）的最新进展促使领域发生了转变，因为它们具备了强大的零-shot能力，消除了下游数据集和计算昂贵的微调的需求。然而，评估GLLMs存在挑战，因为对于GLLMs的预测，不能应用于区分模型所使用的二元真假评估方法。本文引入了一种用于生成型模型的新度量方法，称为ANLS*，用于评估各种任务，包括信息提取和分类任务。ANLS*度量方法扩展了现有ANLS度量方法，可作为一种即插即用的替代方案。

    Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
    
[^40]: 同性质，聚类和分类器

    Isotropy, Clusters, and Classifiers

    [https://arxiv.org/abs/2402.03191](https://arxiv.org/abs/2402.03191)

    同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。

    

    最近，关于嵌入空间是否均匀利用所有维度（即是否具有同性质）的问题引起了讨论。有证据支持和反对在嵌入空间中实施同性质。在本文中，我们强调同性质对嵌入空间的要求与聚类的存在不兼容，这也对线性分类目标产生了负面影响。我们通过实验证明了这个事实，并用它来阐明文献中的先前结果。

    Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
    
[^41]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^42]: 通过修正的缩放定律选择大型语言模型进行微调

    Selecting Large Language Model to Fine-tune via Rectified Scaling Law

    [https://arxiv.org/abs/2402.02314](https://arxiv.org/abs/2402.02314)

    该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。

    

    在日益增长的语言模型生态系统中，在众多选项中选择最合适的预训练模型进行微调成为了一个挑战。在资源受限的情况下，微调所有模型然后再进行选择是不现实的。在本文中，我们将这个资源受限的选择任务转化为预测微调性能，并且展示其与缩放定律之间的自然联系。与预训练不同，我们发现微调的缩放曲线不仅包括众所周知的“功率阶段”，还包括以前未被观察到的“预功率阶段”。我们还解释了为什么现有的缩放定律无法理论和实证地捕捉到这种相变现象。为了解决这个问题，我们将“预学习数据大小”概念引入到我们的修正缩放定律中，这克服了理论上的限制，并更好地适应实验结果。通过利用我们的定律，我们提出了一种新颖的语言模型选择算法，可以选择接近最优的模型。

    The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
    
[^43]: 大型语言模型上邻近扰动的知识编辑

    Neighboring Perturbations of Knowledge Editing on Large Language Models

    [https://arxiv.org/abs/2401.17623](https://arxiv.org/abs/2401.17623)

    本文研究大型语言模型上知识编辑对邻近知识的扰动，提出了 additivity 指标以及 Perturbation Evaluation of Appending Knowledge (PEAK) 基准，用于评估附加新知识时邻近知识的扰动程度。

    

    尽管大型语言模型（LLMs）具有卓越的能力，但由于错误或过时的知识，它们容易生成意外的文本。考虑到重新训练LLMs的资源密集型性质，知识编辑的发展呈现出显著增长。然而，目前的方法和评估很少探索编辑对相邻知识的扰动。本文研究了将新知识更新到LLMs中是否扰乱了其中包含的相邻知识。具体而言，我们探讨了将新的答案附加到事实性问题的答案列表中是否会导致原始正确答案的丧失，以及不经意地包含了错误答案。引入了一个加性指标，并构建了一个被称为知识附加扰动评估（PEAK）的基准来评估附加新知识时邻近知识的扰动程度。此外，引入了一个即插即用的机制，用于处理新增知识的效果和扰动。

    Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play 
    
[^44]: 对齐和有用性之间的权衡：语言模型的研究

    Tradeoffs Between Alignment and Helpfulness in Language Models

    [https://arxiv.org/abs/2401.16332](https://arxiv.org/abs/2401.16332)

    本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。

    

    语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望行为和抑制非期望行为，实现人类与语言模型之间的安全交互。通常通过调整模型或插入预设的对齐提示来实现。最近，通过改变训练后的表示来改变模型行为的表示工程方法在对齐语言模型方面表现出了有效性。表示工程在面对对抗攻击和降低社会偏见等对齐导向任务方面取得了增益，但也导致了模型执行基本任务能力的降低。本文研究了增加对齐度和减少模型有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。有趣的是，我们发现，尽管模型的有用性通常会减少

    Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
    
[^45]: SQLformer：深度自回归查询图生成用于文本到SQL翻译

    SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation

    [https://arxiv.org/abs/2310.18376](https://arxiv.org/abs/2310.18376)

    SQLformer是一个用于文本到SQL翻译的深度自回归查询图生成模型，采用了特定的Transformer架构，并通过结构归纳偏差解决领域泛化和自然语言与SQL查询对齐的难题。

    

    近年来，对于文本到SQL翻译的兴趣日益增长，这是将自然语言问题转化为可执行SQL查询的任务。这项技术具有潜在的潜力，可以使数据库中的数据提取民主化。然而，其中一些主要障碍包括领域泛化，即适应以前未见到的数据库，并且将自然语言问题与相应的SQL查询对齐。为了克服这些挑战，我们引入了SQLformer，这是一种针对执行文本到SQL翻译任务而设计的新型Transformer体系结构。我们的模型以自回归的方式预测SQL查询，并在编码器和解码器层中结合结构归纳偏差。这种偏差是由数据库表和列选择引导的，有助于解码器以广度优先搜索的规范顺序生成SQL查询的图形表示。全面的实验说明了现阶段的技术水平

    In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
    
[^46]: 探索LLM代理的协作机制：社会心理学视角

    Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View

    [https://arxiv.org/abs/2310.02124](https://arxiv.org/abs/2310.02124)

    通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。

    

    随着自然语言处理（NLP）系统越来越多地应用于复杂的社会环境中，一个迫切的问题出现了：这些NLP系统能否模仿类人类的协作智能，在由多个大型语言模型（LLMs）组成的多代理社会中？本文通过将实践实验与理论观点相结合，探究当代NLP系统之间的协作机制。我们构建了四个由LLM代理组成的独特“社会”，每个代理以特定的“特质”（随和或过于自信）为特征，并与不同的“思维模式”（辩论或反思）展开协作。通过在三个基准数据集上评估这些多代理社会，我们发现某些协作策略不仅胜过先前顶尖方法，而且优化了效率（使用更少的API令牌）。此外，我们的结果进一步说明LLM代理可以

    arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
    
[^47]: 并非所有评估指标都应受到指责：通过多样化参考文献改进自然语言生成评估

    Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References

    [https://arxiv.org/abs/2305.15067](https://arxiv.org/abs/2305.15067)

    通过增加参考文献的多样性，本文提出的方法Div-Ref 显著提高了自然语言生成评估的相关性

    

    大多数关于自然语言生成（NLG）的研究依赖于具有有限参考文献的评估基准，这可能导致与人类判断的相关性较差。本文提出了一种简单有效的方法，称为Div-Ref，通过丰富参考文献的数量来增强现有的评估基准。我们利用大型语言模型（LLMs）将单个参考文献的表达多样化为多个高质量的表达，以尽可能覆盖参考句的语义空间。我们进行了全面的实验，从经验上证明多样化参考文献的表达可以显著增强自动评估之间的相关性。

    arXiv:2305.15067v2 Announce Type: replace  Abstract: Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation
    
[^48]: Taiyi-Diffusion-XL: 借助大型视觉语言模型支持推进双语文本到图像生成

    Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])

    [http://arxiv.org/abs/2401.14688](http://arxiv.org/abs/2401.14688)

    Taiyi-Diffusion-XL是一个中英双语文本到图像生成模型，通过对CLIP和Stable-Diffusion-XL的能力进行扩展，并通过双语连续预训练来实现。模型通过将常用的汉字整合到CLIP的分词器和嵌入层中，以及使用大型视觉语言模型丰富文本提示，提供了更好的图像标题和高质量的视觉效果。实验证明，该模型在双语图像-文本检索中表现出色。

    

    最近在文本到图像模型中的进展显著提升了图像生成能力，然而在双语或中文语言支持方面仍存在明显的开源模型缺口。为了解决这个需求，我们提出了Taiyi-Diffusion-XL，这是一个新的中英双语文本到图像模型，通过对CLIP和Stable-Diffusion-XL能力的扩展，并通过双语连续预训练的过程来开发。这种方法包括通过将最常用的汉字整合到CLIP的分词器和嵌入层中来扩展词汇量的高效方法，同时还加入了绝对位置编码扩展。此外，我们通过大型视觉语言模型来丰富文本提示，从而提供更好的图像标题和更高的视觉质量。这些增强措施随后应用于下游的文本到图像模型。我们的实证结果表明，开发的CLIP模型在双语图像-文本检索方面表现出色。此外，双语预训练过程中的整合使该模型在中英文场景下具有均衡的表现。

    Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilin
    
[^49]: AUTOACT：通过自主规划实现的自动代理学习

    AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])

    [http://arxiv.org/abs/2401.05268](http://arxiv.org/abs/2401.05268)

    AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。

    

    语言代理在各种复杂任务上取得了相当的性能。尽管在这个领域进行了不断的探索，但现有的语言代理系统仍然面临昂贵、不可重复的数据依赖问题，并且面临将单一模型应用于多个功能的挑战。为此，我们介绍了AutoAct，这是一个自动代理学习框架，不依赖于大规模带注释的数据和来自闭源模型（如GPT-4）的合成轨迹。给定有限的数据和工具库，AutoAct首先自动合成规划轨迹，不需要人类或强闭源模型的任何辅助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动区分，产生一个子代理组来完成任务。我们进行了多种LLMs的广泛实验，结果显示AutoAct在性能上优于或与其相当。

    Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
    
[^50]: 使用计算高效的检索表示融合提高自然语言理解能力

    Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])

    [http://arxiv.org/abs/2401.02993](http://arxiv.org/abs/2401.02993)

    本文提出了一种计算高效的检索表示融合方法ReFusion，通过将检索表示直接融合到语言模型中，解决了将外部数据库的知识融入非知识密集型任务中的挑战。

    

    从外部数据库中获取知识并融入语言模型的检索增强方法在各种知识密集型任务（例如问答和文本生成）中取得了巨大成功。然而，在非知识密集型任务（例如文本分类）中集成检索仍然具有挑战性。现有的研究集中在将检索内容拼接到输入中形成提示性的输入。然而，这种方法要求语言模型具备处理长文本的能力。此外，推断这种拼接数据也会消耗大量的计算资源。为了解决这些问题，本文提出了一种计算高效的检索表示融合方法ReFusion，采用神经架构搜索。主要思想是直接将检索表示与语言模型进行融合。具体地，我们首先提出了一种在线检索模块，重新检索...

    Retrieval-based augmentations that aim to incorporate knowledge from an external database into language models have achieved great success in various knowledge-intensive (KI) tasks, such as question-answering and text generation. However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as text classification, is still challenging. Existing works focus on concatenating retrievals to inputs as context to form the prompt-based inputs. Unfortunately, such methods require language models to have the capability to handle long texts. Besides, inferring such concatenated data would also consume a significant amount of computational resources.  To solve these challenges, we propose \textbf{ReFusion} in this paper, a computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with neural architecture search. The main idea is to directly fuse the retrieval representations into the language models. Specifically, we first propose an online retrieval module that retri
    
[^51]: 自补代码生成

    Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2311.17972](http://arxiv.org/abs/2311.17972)

    本文介绍了自补代码生成的通用框架，利用自补机制实现了中断和循环机制，使传统解码进程变得非单调。利用中断机制可以推迟生成代码，增强对输出的控制；利用循环机制可以循环更新和同步生成的每个部分。

    

    本文介绍了一种自补代码生成的通用框架，它将补充操作融入自回归解码中。我们的方法利用了最近的能够进行填充的代码语言模型可以自动进行填充的观察结果：补充操作旨在根据预定义的前缀和后缀填充中间内容，而自补机制顺序生成这些周围上下文和被填充内容。我们利用这种能力在传统解码中引入了新颖的中断和循环机制，使其进化为非单调过程。中断机制允许推迟生成特定的代码，直到确定的后缀建立，增强对输出的控制。同时，循环机制利用自补和从左到右解码的互补性，可以循环更新和同步每个生成部分。我们进行了大量实验来证明我们的方法的有效性。

    This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
    
[^52]: 一图抵千言：使用多概念提示学习来学习对象级概念

    An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])

    [http://arxiv.org/abs/2310.12274](http://arxiv.org/abs/2310.12274)

    提出了一种多概念提示学习（MCPL）框架，通过同时学习多个新的“词”来解决在单个场景中识别和整合多个对象级概念的挑战。针对词概念相关性准确性问题，提出了注意力掩码、提示对比损失和绑定形容词等三种正则化技术。通过图像生成进行了评估，结果表明该框架能够生成更多样化和合成的图像。

    

    文字反转是一种提示学习方法，它学习一种新的“单词”的嵌入表示图像风格和外观，使其能够整合到自然语言句子中生成新的合成图像。然而，即使对于可获得个别概念的嵌入，识别和整合一个场景中的多个对象级概念仍然面临着显著的挑战，这也得到了我们的实证测试的进一步证实。为了解决这个挑战，我们引入了一个多概念提示学习（MCPL）的框架，可以从一个句子-图像对中同时学习多个新的“词”。为了增强词概念相关性的准确性，我们提出了三种正则化技术：注意力掩码（AttnMask）将学习集中在相关区域；提示对比损失（PromptCL）将不同概念的嵌入分离开来；以及绑定形容词（Bind adj.）将新的“词”与已知词相关联。我们通过图像生成进行评估

    Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
    
[^53]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^54]: 以Transformer为决策者：通过监督预训练实现可证明的上下文强化学习

    Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])

    [http://arxiv.org/abs/2310.08566](http://arxiv.org/abs/2310.08566)

    通过监督预训练，该研究提供了一个理论框架，分析了大型Transformer模型在上下文强化学习中的应用。研究证明，在假设模型可实现的情况下，经过监督预训练的Transformer模型能够模仿专家算法在观察到的轨迹上的条件期望。

    

    在离线强化学习数据集上预训练的大型Transformer模型展示了令人惊叹的上下文强化学习能力，即当它们面对来自未知环境的交互轨迹时，它们能够做出良好的决策。然而，Transformer模型如何进行训练以执行上下文强化学习，在理论上尚未得到很好的理解。特别是，尚不清楚Transformer模型可以在上下文中执行哪些强化学习算法以及离线训练数据中的分布差异如何影响已学习的算法。本文提供了一个理论框架，分析了对上下文强化学习的监督预训练。这包括了两种最近提出的训练方法：算法蒸馏和决策预训练的Transformer模型。首先，在假设模型可实现的情况下，我们证明了经过监督预训练的Transformer模型将模仿专家算法在观察到的轨迹上的条件期望。广义误差的缩放范围与…

    Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
    
[^55]: 通过有限状态解码实现无语法错误和具有泛化能力的LLM工具使用

    Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])

    [http://arxiv.org/abs/2310.07075](http://arxiv.org/abs/2310.07075)

    本文提出了一种无语法错误且具有泛化能力的LLM工具使用方法ToolDec，通过有限状态解码算法消除了工具相关错误，使LLM能够有效选择工具，而无需微调或上下文文档。

    

    大型语言模型(LLMs)已经展示出使用外部工具解决复杂问题的有希望的能力。然而，现有方法要么涉及对工具演示进行微调，这样在没有额外训练的情况下无法推广到新的工具，要么在上下文中提供工具文档，从而限制了工具数量。这两种方法常常产生语法无效的工具调用。在本文中，我们提出了ToolDec，一种有限状态机引导的解码算法，用于工具增强的LLMs。ToolDec通过确保有效的工具名称和类型一致的参数，消除了任何工具增强的LLMs中的工具相关错误。此外，ToolDec使LLM能够仅仅使用它们的名称中包含的信息有效地选择工具，而无需微调或上下文文档。我们在涉及数学函数、知识图谱关系和复杂的现实世界RESTful API的各种任务上评估了多种先前的方法及其ToolDec增强版本。

    Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
    
[^56]: 只需少量上下文示范即可实现越狱和对齐的语言模型

    Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])

    [http://arxiv.org/abs/2310.06387](http://arxiv.org/abs/2310.06387)

    本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。

    

    大规模语言模型（LLM）在各种任务中取得了显著的成功，但对其安全性和生成恶意内容的潜在风险的担忧也浮现出来。本文探讨了在相同上下文学习（ICL）中操纵LLM对齐能力的效果。我们发现，仅通过少量的上下文示范而无需微调，就可以操纵LLM增加或降低越狱概率，即回答恶意提示。基于这些观察，我们提出了用于越狱和对齐语言模型目的的相同上下文攻击（ICA）和相同上下文防御（ICD）方法。ICA通过构造恶意上下文指导模型生成有害输出，而ICD通过拒绝回答有害提示的示范来增强模型的稳健性。我们的实验证明了ICA和ICD在增加或降低对抗越狱攻击成功率方面的有效性。总的来说，我们揭示了ICL在越狱和对齐语言模型领域的潜力。

    Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
    
[^57]: 翻转诅咒: 在大型语言模型中训练的"A是B"无法学习"B是A"

    The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])

    [http://arxiv.org/abs/2309.12288](http://arxiv.org/abs/2309.12288)

    LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。

    

    我们揭示了自回归大型语言模型（LLM）在泛化上的令人惊讶的失败。如果一个模型是基于"A是B"形式的句子进行训练，它不会自动推广到相反的方向"B是A"。这就是翻转诅咒。例如，如果一个模型是基于"Olaf Scholz是德国第九任总理"进行训练的，它不会自动能够回答问题"谁是德国第九任总理？"。此外，正确答案（"Olaf Scholz"）的可能性不会比随机名字更高。因此，模型在逻辑推断上存在基本失败，并且不会推广到它们训练集中的普遍模式（即如果出现"A是B"，则"B是A"更可能出现）。我们通过在虚构的陈述（如"Uriah Hawthorne是'Abyssal Melodies'的作曲家"）上对GPT-3和Llama-1进行微调，并展示它们无法正确回答"谁创作了'Abyssal Melodies'?"来提供翻转诅咒的证据。

    We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
    
[^58]: Text2Reward：针对强化学习的自动生成密集奖励函数的自动化框架

    Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])

    [http://arxiv.org/abs/2309.11489](http://arxiv.org/abs/2309.11489)

    Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。

    

    设计奖励函数是强化学习中长期以来的挑战；它需要专业知识或领域数据，导致开发成本高。为了解决这个问题，我们引入了Text2Reward，一个无需数据的框架，可基于大型语言模型（LLM）自动生成密集奖励函数。给定自然语言描述的目标，Text2Reward生成作为环境紧凑表示的可执行程序的密集奖励函数。与逆强化学习和最近使用LLM编写稀疏奖励代码的工作不同，Text2Reward生成可解释的、自由形式的密集奖励代码，可涵盖各种任务，利用现有软件包，并允许通过人类反馈进行迭代改进。我们在两个机器人操作基准（ManiSkill2，MetaWorld）和两个MuJoCo的运动环境上评估了Text2Reward。在17个操作任务中的13个任务中，使用生成的奖励代码训练的政策实现了类似或更好的性能。

    Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
    
[^59]: 作为用户模拟器的大型语言模型

    Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])

    [http://arxiv.org/abs/2308.11534](http://arxiv.org/abs/2308.11534)

    本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。

    

    闭源ChatGPT的卓越性能引发了对其民主化的努力，借助真实用户和ChatGPT对话的努力取得了显著进展，Vicuna是一个很好的例子。然而，目前的Baize和UltraChat等努力主要依靠ChatGPT根据指令模拟人类行为，而不是真实的人类学习，导致范围有限，多样性减弱，缺乏真正的多轮对话动态。为了解决上述问题，我们创新性地把从真实人机对话中提取的人类问题作为学习目标，并训练一个用户模拟器UserGPT来生成高质量的以人为中心的合成对话数据集RealChat。随后，该数据集训练我们的助手模型ReaLM。实验证明，ReaLM在Vicuna-Bench和MT-Bench中均超过了基准模型。

    The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
    
[^60]: 揭开巨人的真面目：对ChatGPT在编码算法和数据结构方面的熟练程度进行全面评估

    Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])

    [http://arxiv.org/abs/2307.05360](http://arxiv.org/abs/2307.05360)

    本文全面评估了ChatGPT在编码算法和数据结构方面的能力，基于最大的编码挑战目录，重点关注Python编程语言和数据结构算法两个基础主题。总结测试中ChatGPT的代码解决问题的准确性、代码质量和运行时错误的性质。

    

    大型语言模型(LLMs)的转变性影响深刻地重塑了人工智能(AI)技术领域。值得注意的是，ChatGPT在这些模型中有着独特之处，展示出卓越的多轮对话性能，并在多种语言中展示出对编码的熟练程度。在本文中，我们根据迄今为止最大的编码挑战目录对ChatGPT的编码能力进行了全面评估。我们的重点是Python编程语言，以及集中在数据结构和算法上的问题，这两个主题是计算机科学的基础。我们评估ChatGPT解决所提交问题的能力，评估其代码质量以及代码引发的运行时错误的性质。当ChatGPT的代码成功执行但未能解决手头问题时，我们会研究通过的测试案例中的模式，以了解ChatGPT代码中的错误之处。

    The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
    
[^61]: 一种适用于Unicode文本的Abugida规范化器和解析器

    Abugida Normalizer and Parser for Unicode texts. (arXiv:2306.01743v1 [cs.CL])

    [http://arxiv.org/abs/2306.01743](http://arxiv.org/abs/2306.01743)

    本论文提出了两个针对Indic语言Unicode书写方案中常见及不常见问题的工具库，分别是纠正不一致性的规范化器和Abugida文本的字形解析器，使用这两个工具可以提高400%的速度并显著改善下游任务表现。

    

    本文提出了两个库，以解决Indic语言基于Unicode的书写方案的常见和不常见问题。第一个是一个规范化器，通过https://pypi.org/project/bnunicodenormalizer/纠正由编码方案引起的不一致性。第二个是Abugida文本的字形解析器https://pypi.org/project/indicparser/。这两个工具比以前使用的工具更有效率和有效。我们报告了400%的速度提升，并确保不同语言模型基于下游任务表现显著提高。

    This paper proposes two libraries to address common and uncommon issues with Unicode-based writing schemes for Indic languages. The first is a normalizer that corrects inconsistencies caused by the encoding scheme https://pypi.org/project/bnunicodenormalizer/ . The second is a grapheme parser for Abugida text https://pypi.org/project/indicparser/ . Both tools are more efficient and effective than previously used tools. We report 400% increase in speed and ensure significantly better performance for different language model based downstream tasks.
    
[^62]: 深思熟虑：具有内部工作记忆的决策Transformer

    Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])

    [http://arxiv.org/abs/2305.16338](http://arxiv.org/abs/2305.16338)

    该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。

    

    基于大型语言模型（LLM）的决策制定代理已经展示了跨越多个任务的泛化能力。然而，它们的性能依赖于大规模的数据和计算。我们认为，这种低效性源于遗忘现象，即模型通过参数记忆其行为，在训练过程中。因此，新任务的训练可能会降低模型在先前任务上的性能。与LLM的隐式记忆机制不同，人脑利用分布式存储器存储记忆，以有效地管理和组织多种技能，减轻了遗忘现象。因此，我们建议使用内部工作记忆模块来存储、融合和检索不同下游任务的信息。评估结果表明，所提出的方法改善了Atari游戏和元世界物体操作任务的训练效率和泛化能力。此外，我们证明了记忆微调进一步增强了转化决策制定代理对新任务的适应性。

    Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
    
[^63]: 代码搜索：一个新的SearchBySnippet数据集和SnippeR检索模型用于按代码片段搜索

    Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets. (arXiv:2305.11625v1 [cs.CL])

    [http://arxiv.org/abs/2305.11625](http://arxiv.org/abs/2305.11625)

    本研究针对按代码片段搜索的问题，提出了一个新的基于StackOverflow数据的SearchBySnippet数据集，并开发了一个单编码器模型SnippeR，它在该数据集上具有优异的性能。

    

    代码搜索是一个重要的任务，在近年来得到了很多发展。然而，以前的尝试主要考虑通过文本查询搜索代码的问题。我们认为，使用代码片段（可能伴随着回溯）作为查询，并寻找具有修复错误指令和代码示例的答案是一个自然的用例，而现有方法并未涵盖此类情况。此外，现有数据集使用从代码中提取的注释而不是全文描述作为文本，使它们不适用于这种用例。我们提出了一个基于StackOverflow数据实现搜索BySnippet用例的新SearchBySnippet数据集; 在这种情况下，现有体系结构即使在微调之后也不能与最简单的BM25基线相提并论。我们提出了一个新的单编码器模型SnippeR，它在SearchBySnippet数据集上优于几个强基线，结果为0.451 Recall@10; 我们建议将SearchBySnippet数据集和SnippeR作为新的重要基准数据集和模型来推广代码搜索。

    Code search is an important task that has seen many developments in recent years. However, previous attempts have mostly considered the problem of searching for code by a text query. We argue that using a code snippet (and possibly an associated traceback) as a query and looking for answers with bugfixing instructions and code samples is a natural use case that is not covered by existing approaches. Moreover, existing datasets use comments extracted from code rather than full-text descriptions as text, making them unsuitable for this use case. We present a new SearchBySnippet dataset implementing the search-by-code use case based on StackOverflow data; it turns out that in this setting, existing architectures fall short of the simplest BM25 baseline even after fine-tuning. We present a new single encoder model SnippeR that outperforms several strong baselines on the SearchBySnippet dataset with a result of 0.451 Recall@10; we propose the SearchBySnippet dataset and SnippeR as a new imp
    
[^64]: 连续多模态知识图谱构建

    Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08698](http://arxiv.org/abs/2305.08698)

    连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。

    

    多模态知识图谱构建（MKGC）涉及使用多种形式的数据，如文本和图像，创建实体和关系的结构化表示。然而，现有的MKGC模型在处理动态现实场景中新增实体和关系方面面临挑战。目前的连续知识图谱构建设置主要关注从文本数据中提取实体和关系，忽视了其他多模态源。因此，有必要探索连续MKGC的挑战，以解决灾难性遗忘现象，并确保保留从不同形式的数据中提取的过去知识。本研究致力于通过开发终身MKGC基准数据集来研究这个复杂的主题。基于经验发现，当多媒体数据训练时，一些典型的MKGC模型可能在连续设置中意外表现不佳，与那些仅利用文本资源的模型相比。我们以实验证据为基础，总结出以下论点：连续多模态知识图谱构建面临着数据源变化导致的灾难性遗忘问题。

    Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
    
[^65]: 面向跨数据集的弱监督仇恨言论分类

    Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])

    [http://arxiv.org/abs/2305.02637](http://arxiv.org/abs/2305.02637)

    该论文提出使用极度弱的监督方法，只依赖于类别名称而不是注释数据中的类别示例，解决当前仇恨言论识别的研究存在的数据创建策略不系统和不同注释方案问题，并展示了有效性。

    

    如多位学者指出的那样，当前针对仇恨言论（HS）识别的研究特点是不系统的数据创建策略和不同的注释方案。因此，监督学习模型往往对它们未被训练的数据集进行泛化性能差，并且不同HS分类法标记的数据集所训练的模型的性能无法比较。为了解决这个问题，我们提出了一种极度弱的监督方法，只依赖于类别名称而不是注释数据中的类别示例。我们展示了一种最先进的弱监督文本分类模型在各种数据集内和跨数据集的情况下的有效性。此外，我们对HS分类模型通用性较差的原因进行了深入的定量和定性分析。

    As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
    
[^66]: 在ChatGPT时代迈向负责任的人工智能：用于设计基于基础模型的AI系统的参考架构

    Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])

    [http://arxiv.org/abs/2304.11090](http://arxiv.org/abs/2304.11090)

    本文提出了一个以模式为导向的负责任AI-by-design参考架构，用于设计基于基础模型的AI系统，重点关注可解释性、公平性、安全性和鲁棒性等关键设计元素。

    

    ChatGPT、Bard和其他大型语言模型(LLM)聊天机器人的推出在全球范围内引起了巨大关注。基础模型将成为未来大多数AI系统的基础构建块的趋势正在增长。然而，将基础模型纳入AI系统引发了对负责任AI的重大关注，这是由于其黑匣子性质和快速发展的超级智能引起的。此外，基础模型的增长能力最终可能会吞噬AI系统的其他组件，引入架构设计中的运动边界和接口演变挑战。为了应对这些挑战，本文提出了一种以模式为导向的负责任AI-by-design参考架构，用于设计基于基础模型的AI系统。特别地，本文首先呈现了基于基础模型的AI系统在架构演进方面的发展，从"基础模型作为连接器"到"基础模型作为单片机核"。然后，它提出了一个参考架构，包括五个类别的模式，重点关注关键设计元素，例如可解释性、公平性、安全性和鲁棒性。所提出的参考架构为设计负责任的基础模型的AI系统提供了系统化和透明的方法。

    The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
    
[^67]: GesGPT: 基于 GPT 文本解析的语音手势合成

    GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])

    [http://arxiv.org/abs/2303.13013](http://arxiv.org/abs/2303.13013)

    GesGPT 是一种新的语音手势生成方法，它利用大型语言模型 GPT 的语义分析能力，从文本输入中提取与手势相关的信息，并使用手势库和集成模块生成意义丰富的语音手势。

    

    手势合成作为重要的研究领域，旨在生成上下文相关和自然的手势来对应语音或文本输入。尽管基于深度学习的方法取得了显著进展，但它们通常忽略了文本中丰富的语义信息，导致手势不够表达和意义不够丰富。我们提出了 GesGPT，这是一种利用大型语言模型（LLM），如 GPT 的语义分析能力进行手势生成的新方法。通过利用 LLM 在文本分析方面的强大能力，我们设计提示来从文本输入中提取与手势相关的信息。我们的方法包括开发提示原则，将手势生成转化为基于 GPT 的意图分类问题，并利用精心策划的手势库和集成模块生成语义丰富的语音手势。实验结果表明，GesGPT 可以有效地针对语音或文本输入生成上下文相关的、有意义的手势。

    Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates conte
    
[^68]: 用于临床叙述分类的小规模交换变压器和基于NLP的模型

    A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])

    [http://arxiv.org/abs/2303.12892](http://arxiv.org/abs/2303.12892)

    本研究提出了一个简化的Switch Transformer框架，并从头开始训练，取得了在小型法语临床文本分类任务中比预训练的BERT模型更好的效果，采用Switch Transformer的专家混合机制有助于提高识别准确度，最终在测试集上实现了87％的准确率、87％的精度和86％的召回率。

    

    近年来，基于变压器的模型（如交换变压器）在自然语言处理任务中取得了显著的结果。然而，这些模型通常过于复杂并需要大量的预训练，这限制了它们在有限数据的小型临床文本分类任务中的有效性。在本研究中，我们提出了一个简化的Switch Transformer框架，并从头开始在CHU Sainte-Justine医院的小型法语临床文本分类数据集上进行了训练。我们的结果表明，简化的小规模变压器模型优于预训练的BERT模型，包括DistillBERT、CamemBERT、FlauBERT和FrALBERT。此外，使用Switch Transformer的专家混合机制有助于捕获多样的模式；因此，所提出的方法比具有自我注意机制的传统变压器获得更好的结果。最后，我们提出的框架在测试集上实现了87％的准确率，87％的精度和86％的召回率，突显了其在小型临床文本分类任务中的潜力。

    In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\%, precision at 87\%, and recall 
    

