# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Modular Visual Question Answering via Code Generation.](http://arxiv.org/abs/2306.05392) | 论文提出了一种基于代码生成实现模块化视觉问答的方法，无需额外训练，使用预训练的语言模型和视觉模型，可以显著提高VQA准确率。 |
| [^2] | [Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age.](http://arxiv.org/abs/2306.05387) | 本文探讨了儿童诗歌中的话语情感动态，发现随着年龄增长，部分情感强度持续上升，情感基调持续下降，情感变异性、上升速率和恢复速率不断加强，并可为未来情感动态研究和儿童心理健康问题提供借鉴。 |
| [^3] | [Detecting Human Rights Violations on Social Media during Russia-Ukraine War.](http://arxiv.org/abs/2306.05370) | 本研究聚焦于利用来自Telegram平台的数据，通过基于文本的分类器检测人权侵犯的提及情况。研究结果揭示了社交媒体在记录战争中可能存在的人权侵犯方面具有潜在的价值。 |
| [^4] | [The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues.](http://arxiv.org/abs/2306.05360) | 本文介绍了ADAIO团队在BEA-2023共享任务中的系统方案，使用OpenAI GPT-3评估基准模型并在教育对话中生成AI教师回应。通过少量提供提示信息，利用OpenAI的text-davinci-003模型在竞赛中获得第二名，并突出了大型语言模型在AI教师角色中的少量提示学习能力。 |
| [^5] | [Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application.](http://arxiv.org/abs/2306.05323) | 该研究创建了意大利神经精神命名实体识别数据集，并使用巨型语言模型开发出多中心识别模型，整体 F1得分为84.77%。该模型将帮助临床从业者从非结构化的医疗记录中自动提取信息。 |
| [^6] | [KIT's Multilingual Speech Translation System for IWSLT 2023.](http://arxiv.org/abs/2306.05320) | 本文介绍了一个为IWSLT 2023多语言翻译贡献的翻译系统，其重点在于翻译科学会议演讲。用“检索式方法”（kNN-MT）进行有效的适应，该系统采用适配器轻松集成来自数据增强的增量训练数据，并展示级联系统更容易适应特定目标领域的优势。 |
| [^7] | [CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models.](http://arxiv.org/abs/2306.05317) | 本文介绍了CUED在ProbSum 2023的比赛中使用的层次化摘要模型集成方法HESM，通过将不同的精细调整过的Clinical-T5模型的标记级集合加以组合并使用最小贝叶斯风险解码方法，成功地提高了摘要性能，是该共享任务中表现最佳的系统。 |
| [^8] | [Are fairness metric scores enough to assess discrimination biases in machine learning?.](http://arxiv.org/abs/2306.05307) | 本文在机器学习算法在文本数据上对性别歧视的偏见的度量标准的局限性上进行了实验研究。 |
| [^9] | [ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases.](http://arxiv.org/abs/2306.05301) | ToolAlpaca是一个自动生成工具使用语料库，实现紧凑的语言模型参加通用工具学习的框架。 |
| [^10] | [Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training.](http://arxiv.org/abs/2306.05278) | 本文探讨了Few-shot Intent Classification任务的解决方法。相较于传统的在外部资源上连续预训练，本文提出了直接微调预训练语言模型的方法，并通过实验证明其在少量标记数据情况下已经可以取得不错的结果，表明连续预训练并非必要。 |
| [^11] | [Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction.](http://arxiv.org/abs/2306.05276) | 本文对19种基于Transformer的ADE提取模型进行广泛评估，并在具有不同非正式程度的数据集上比较它们的性能。此外，我们使用了成熟的特征重要性技术（SHAP）进一步分析了模型的性能。 |
| [^12] | [Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes.](http://arxiv.org/abs/2306.05270) | 该论文总结了 BioNLP Workshop 2023 发起的 ProbSum 共享任务的数据集、评估指标和基线系统，旨在吸引未来研究努力构建适用于真实世界诊断决策支持应用的自然语言处理模型，以提高患者的医疗保健质量。 |
| [^13] | [Factorized Contrastive Learning: Going Beyond Multi-view Redundancy.](http://arxiv.org/abs/2306.05268) | 本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。 |
| [^14] | [Dealing with Semantic Underspecification in Multimodal NLP.](http://arxiv.org/abs/2306.05240) | 语义未确定性在语言学中很重要，但多模态系统还没有解决好这个问题 |
| [^15] | [Improving Long Context Document-Level Machine Translation.](http://arxiv.org/abs/2306.05183) | 该论文提出了一种新的受限注意力机制来提高长篇文本机器翻译的质量。 |
| [^16] | [M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models.](http://arxiv.org/abs/2306.05179) | M3Exam是一个来源于真实人类考试题目的新型基准测试，用于评估大型语言模型在多语言、多模态和多层次的情境中的普适智能。 |
| [^17] | [RRWKV: Capturing Long-range Dependencies in RWKV.](http://arxiv.org/abs/2306.05176) | 本文介绍了一种新的RRWKV架构，它在保持记忆和计算效率的同时，通过加入回顾能力有效地捕捉长距离依赖关系。 |
| [^18] | [Mapping Brains with Language Models: A Survey.](http://arxiv.org/abs/2306.05126) | 大脑和语言模型激活呈现相似之处，为相关研究提供了基础，但目前仍缺乏明确的结论。 |
| [^19] | [Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework.](http://arxiv.org/abs/2306.05119) | 该论文提出了基于参考校正的细粒度FEC评估框架，并在使用该框架进行实验的过程中发现了不同事实误差类别下的最佳训练模式和现有方法之间的显著性差异。 |
| [^20] | [On Search Strategies for Document-Level Neural Machine Translation.](http://arxiv.org/abs/2306.05116) | 本文探讨了如何在搜索过程中最好地利用文档级神经机器翻译模型，比较了文献中的不同解码方案和作者提出的方案，并发现针对某些语言现象时，常用的解码策略不能够取得较好的性能。 |
| [^21] | [Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media.](http://arxiv.org/abs/2306.05115) | 这篇论文提出了一种使用ChatGPT生成模型解释的方法来改善人工标注的赞助内容检测，以提高标注一致性和准确性，在Instagram数据集上进行的评估结果表明比现有方法有显著的改进。 |
| [^22] | [The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN.](http://arxiv.org/abs/2306.05088) | 本文提出了一种孪生RNN结构用于测量L2-L2交互中语音音质收敛。该模型可以有效捕捉语音收敛和说话者的模仿能力的动态，并能够处理由L1引起的说话者差异。 |
| [^23] | [PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.](http://arxiv.org/abs/2306.05087) | PandaLM是一个评估LLM指令调优的自动基准，它能够区分最优模型，并关注于主观因素。 |
| [^24] | [Enhancing Robustness of AI Offensive Code Generators via Data Augmentation.](http://arxiv.org/abs/2306.05079) | 本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。 |
| [^25] | [DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models.](http://arxiv.org/abs/2306.05076) | 提出了一个新的框架DLAMA-v1，用于筛选来自不同文化背景的事实三元组，解决在多语言模型上使用英语提示更具有一致性的问题。 |
| [^26] | [LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification.](http://arxiv.org/abs/2306.05075) | 本文介绍了一个基于预训练和多任务学习的检测和分类系统，用于解决社交媒体上的性别歧视问题。多任务学习对性别歧视检测和粗粒度性别分类表现出色，而微调则更适用于细粒度分类。 |
| [^27] | [Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization.](http://arxiv.org/abs/2306.05064) | 本文首次提出了一个地球科学领域的大型语言模型K2，并开发了各种资源以进一步促进其在地球科学领域中的研究和应用，包括第一个地球科学教学调音数据集GeoSignal和第一个地球科学基准测试GeoBenchmark。我们使用了完整的方法将预先训练的通用领域LLM LLaMA-7B 模型适应到地球科学领域。 |
| [^28] | [T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification.](http://arxiv.org/abs/2306.04996) | 本文提出了一种翻译-测试的迁移学习方法以用于跨语言文本分类，在神经机器翻译器的帮助下将目标语言翻译成高资源语言来训练文本分类器，并取得了比基线更好的结果。 |
| [^29] | [Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models.](http://arxiv.org/abs/2306.04980) | 本研究介绍了利用预训练语言模型和大型语言模型评估非英语母语人士语音中短语断点的方法，并且实验证明这种方法能够大大降低对标记的训练数据的依赖性，并且性能提高。 |
| [^30] | [Actively Supervised Clustering for Open Relation Extraction.](http://arxiv.org/abs/2306.04968) | 本文提出了一种新的主动监督聚类方法，用于开放式关系抽取，通过交替聚类学习和关系标注，有效提高聚类准确性和关系提取性能。 |
| [^31] | [Leveraging Language Identification to Enhance Code-Mixed Text Classification.](http://arxiv.org/abs/2306.04964) | 本研究提出了一种改进代码混合文本分类的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。我们探索了词级交错和句子后置的语言信息插入方法，以提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。 |
| [^32] | [Open Set Relation Extraction via Unknown-Aware Training.](http://arxiv.org/abs/2306.04950) | 本文提出一种基于未知感知训练的方法，通过动态合成负实例来对模型进行规范化，在不损害已知关系检测的情况下实现了 SOTA 的未知关系检测。 |
| [^33] | [A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics.](http://arxiv.org/abs/2306.04941) | 本文提出一种改进的神经网络模型用于检测语料库中的主题，并提出了一种新的指标用于评估主题的可读性。 |
| [^34] | [InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding.](http://arxiv.org/abs/2306.04933) | 本文提出了一种新的信息论框架，通过最大化提示和其他模型参数之间互信息来优化软提示调整，从而开发了更加高效、准确和稳健的软提示调整方法InfoPrompt。该方法通过两个新型损失函数，发现合适的提示初始化，并从提示令牌中学习足够的任务相关信息，同时鼓励预训练语言模型的输出表示更加关注任务相关信息。 |
| [^35] | [covLLM: Large Language Models for COVID-19 Biomedical Literature.](http://arxiv.org/abs/2306.04926) | 使用大型语言模型开发了一种名为covLLM的工具，用于协助临床医生评估COVID-19文献。covLLM可以汇总和提取相关信息，帮助医生更好地应对COVID-19疫情。 |
| [^36] | [Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning.](http://arxiv.org/abs/2306.04925) | 本文提出了一种新的文本分类方法，使用偏好分类学习辅助数据来提高模型准确性。通过比较输入文本对之间的偏好关系，这种方法能够为模型提供额外的训练信号。 |
| [^37] | [NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal Information Processing.](http://arxiv.org/abs/2306.04903) | NOWJ团队在COLIEE 2023比赛中采用多任务和集成方法提高法律信息处理技术，虽未达最佳结果但为未来改进提供了有价值见解。 |
| [^38] | [In-Context Learning through the Bayesian Prism.](http://arxiv.org/abs/2306.04891) | 这篇论文研究了大型语言模型中的上下文学习现象，并通过实验证据展示了Transformer模型在多种设置下表现出贝叶斯预测器的行为。作者还探讨了上下文学习与贝叶斯学习框架之间的联系，并提出了一个线性回归任务来验证这种联系。 |
| [^39] | [Expanding Scope: Adapting English Adversarial Attacks to Chinese.](http://arxiv.org/abs/2306.04874) | 本文研究将英文的对抗攻击方法适用于中文上，并证明了这些方法可以生成高质量的中文对抗实例。通过关注中文的语言特点，生成的对抗实例可以实现高流畅度和语义一致性，从而可以用来提高中文NLP模型的对抗鲁棒性。 |
| [^40] | [Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts.](http://arxiv.org/abs/2306.04845) | 该论文提出了一种混合超网络方法，通过基于结构路由的专家混合来增强超级网络模型的表达能力，改善了子网络的质量问题和性能差异。 |
| [^41] | [Improving Vietnamese Legal Question--Answering System based on Automatic Data Enrichment.](http://arxiv.org/abs/2306.04841) | 本文针对越南法律问答系统中标记数据稀缺和预训练语言模型有限的问题，实现了一种基于检索的法律问答系统，并提出了一种通过弱标注提高数据质量从而提高语言模型性能的方法。 |
| [^42] | [Data Augmentation for Improving Tail-traffic Robustness in Skill-routing for Dialogue Systems.](http://arxiv.org/abs/2306.04823) | 提出了一种针对技能路由中长尾数据的异构数据增强和训练方法，可以显著提高尾部请求的技能路由性能，同时在头部请求中保持相对较高的准确性。 |
| [^43] | [Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers.](http://arxiv.org/abs/2306.04820) | 本研究探讨了使用不同数据集和大型语言模型在生物医学论文研究方面分类中的作用。结果表明，使用PubMed 200K RCT数据集并不能改善性能。与此同时，尽管GPT-4表现良好，但它并没有超越fine-tuned在CODA-19数据集上的SciBERT模型，这强调了LLMs的重要性。 |
| [^44] | [Privately generating tabular data using language models.](http://arxiv.org/abs/2306.04803) | 通过使用语言模型训练每一行数据作为一个句子并添加差分隐私，可以在多个数据集中生成具有竞争力的合成数据，以实现私人数据生成。 |
| [^45] | [A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises.](http://arxiv.org/abs/2306.04802) | 本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。 |
| [^46] | [Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization.](http://arxiv.org/abs/2306.04787) | 本文提出了一种新的用于无监督多文档抽象摘要生成的Transformer-based方法Absformer，它使用掩码语言建模（MLM）目标进行预训练聚类文档并生成抽象摘要，实验结果显示，它在基准数据集上优于几种最先进的无监督抽象MDS方法。 |
| [^47] | [The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges.](http://arxiv.org/abs/2306.04765) | 本文研究研究聊天机器人的公开部署，发现代理人的抽象拟人化表现影响用户感知，AI可解释性可能影响反馈率，聊天体验的两种水平应有意设计。此研究提供了设计建议和研究方向。 |
| [^48] | [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.](http://arxiv.org/abs/2306.04757) | INSTRUCTEVAL是一个专注于指导调整的大型语言模型评估的综合套件，它采取了全面的方法来评估模型的性能，包括解决问题、写作能力和与人类价值观的一致性等特征。 |
| [^49] | [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.](http://arxiv.org/abs/2306.04751) | 本文探究了指令调优语言模型在一系列开放指令跟随数据集上的最新进展，提供了一组大型指令调优模型，并进行了系统评估。实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。 |
| [^50] | [Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning.](http://arxiv.org/abs/2306.04746) | 该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。 |
| [^51] | [Soft-prompt Tuning for Large Language Models to Evaluate Bias.](http://arxiv.org/abs/2306.04735) | 本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。 |
| [^52] | [Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation.](http://arxiv.org/abs/2306.04724) | 本论文提出了Prompter方法，利用目标领域槽的描述生成动态前缀来进行自适应前缀调整，实现对话状态跟踪领域自适应。通过分析发现，Prompter除了利用槽描述的语义，还能考虑到槽的频率。实验表明，在区分“无”值对话槽方面，Prompter相比基准方法表现更好。 |
| [^53] | [Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts.](http://arxiv.org/abs/2306.04723) | 本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。 |
| [^54] | [Improving Open Language Models by Learning from Organic Interactions.](http://arxiv.org/abs/2306.04707) | BlenderBot 3x是一个更新版本的会话模型，通过参与者的有机对话和反馈数据进行训练，以改进其技能和安全性，技术上通过学习有益的教师避免学习有毒反馈。 |
| [^55] | [ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.04695) | 本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。 |
| [^56] | [Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge.](http://arxiv.org/abs/2306.04657) | 本文提出了一种动态注入常识知识的共情式对话生成方法，使用自适应模块选择常识知识以确保生成的对话回应和说话者情况的一致性，其表现优于基准模型。 |
| [^57] | [M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning.](http://arxiv.org/abs/2306.04387) | M$^3$IT数据集旨在优化开放式视觉语言模型（VLM）与人类指令的对齐，是一个大规模、多模态和多语种的数据集。 |
| [^58] | [GPT Self-Supervision for a Better Data Annotator.](http://arxiv.org/abs/2306.04349) | 本文提出了一种基于GPT自监督学习的数据标注方法，将生成-恢复范式与GPT的一次性学习能力相结合，可显著提高数据标注的覆盖率、准确性和易读性。 |
| [^59] | [Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset.](http://arxiv.org/abs/2306.03030) | 该研究介绍了 CMExam 数据集，这是一个综合的、来自于中国国家医疗执业考试的数据集，为评估大型语言模型提供了一个标准化且客观的方法。在 CMExam 上，GPT-4 表现最好，这表明大型语言模型在医学领域有巨大潜力。 |
| [^60] | [Improving Conversational Recommendation Systems via Counterfactual Data Simulation.](http://arxiv.org/abs/2306.02842) | 本文提出了一种针对对话式推荐系统的反事实数据模拟方法CFCRS，以缓解由于数据不足而导致的训练不足问题。 |
| [^61] | [UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception Detection.](http://arxiv.org/abs/2306.02827) | 统一欺诈语料库为构建跨域泛化的欺诈检测系统提供了支持。 |
| [^62] | [BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models.](http://arxiv.org/abs/2306.01506) | 本文提出了一种语言习得友好型基准，以检验自我监督口语语言模型在儿童词汇和句法经历中的表现，并提出了两个需要解决的挑战：文本和语音之间的差距和干净语音和野外语音之间的差距。 |
| [^63] | [An Empirical Study on Challenging Math Problem Solving with GPT-4.](http://arxiv.org/abs/2306.01337) | 本研究探索使用GPT-4解决更复杂和有挑战性的数学问题，提出了一种名为MathChat的对话式问题求解框架，并在困难高中竞赛问题上进行了评估。 |
| [^64] | [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets.](http://arxiv.org/abs/2305.18486) | 本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。 |
| [^65] | [Whitening-based Contrastive Learning of Sentence Embeddings.](http://arxiv.org/abs/2305.17746) | 本文提出一种基于白化的对比学习方法用于学习句子嵌入，它将对比学习与白化方法结合起来，同时具备更好的一致性和更好的对齐效果。 |
| [^66] | [BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering.](http://arxiv.org/abs/2305.15932) | 本文提出了一种更简单的二分类方法，将下游的多项选择题回答任务转换为二分类任务，根据合理性对所有候选答案进行排名，以实现无监督常识问题回答，相较于现有使用知识图谱的UCR方法，我们的方法更为节省数据。 |
| [^67] | [Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners.](http://arxiv.org/abs/2305.14825) | 本文研究了大型语言模型的内部机制，发现在上下文语境中，语言序列的语义应起到至关重要的作用，与人类符号推理不同，大型语言模型可以在语言序列中建立强连接，并组成一个表面逻辑链。 |
| [^68] | [Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks.](http://arxiv.org/abs/2305.08714) | 本研究评估了多个大型语言模型在日语文本分类中对提示模板的敏感性和鲁棒性，并揭示出即使是高性能的GPT-4模型在这一方面也存在问题。 |
| [^69] | [On the Hidden Mystery of OCR in Large Multimodal Models.](http://arxiv.org/abs/2305.07895) | 本研究全面评估了现有大型多模态模型在文本相关的视觉任务中的表现，结果显示这些模型虽然在语义理解方面表现优异，但对单个字符形状的感知有限，对图像的细粒度特征检测能力也不足，不能与传统领域特定方法相匹配，并仍需进一步探索它们在OCR中的表现。 |
| [^70] | [Asymmetric feature interaction for interpreting model predictions.](http://arxiv.org/abs/2305.07224) | 本文提出了一种解释模型，能够探索深度神经自然语言处理模型推理中的非对称高阶特征交互。在两个情感分类数据集上的实验结果表明，该模型在识别影响特征方面优于现有特征交互归因方法。 |
| [^71] | [BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews.](http://arxiv.org/abs/2305.06595) | BanglaBook 是一个大规模的孟加拉语书评数据集，其中包括 158,065 个样本，针对情感分析分为三个大类，通过使用预训练模型来取代手动构建特征的模型，取得显着的性能优势。 |
| [^72] | [Controlled Text Generation with Natural Language Instructions.](http://arxiv.org/abs/2304.14293) | InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。 |
| [^73] | [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark.](http://arxiv.org/abs/2304.03279) | 本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。 |
| [^74] | [Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer.](http://arxiv.org/abs/2301.06735) | 本文提出一种针对上下文偏差的双阶段上下文过滤方法，将流式输出和预定义上下文单词列表相结合，提高了端对端模型的识别准确率，并加快了推理过程。 |
| [^75] | [Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation.](http://arxiv.org/abs/2301.04907) | 根据 “三思而后语” 行为启发，提出一种两阶段对话代理用于生成情感对话，该代理在情感生成方面优于其他模型，并保持了语义表现。 |
| [^76] | [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning.](http://arxiv.org/abs/2212.10773) | 本文介绍了 MultiInstruct，这是第一个多模态指令调优基准数据集，并探索多种迁移学习策略从大规模的自然语言指令数据集中提高预训练模型的性能。实验结果展示了其在各种未见过的多模态任务中具有强大的零样本表现，以及设计的新的任务完成率指标。 |
| [^77] | [CoRRPUS: Codex-Leveraged Structured Representations for Neurosymbolic Story Understanding.](http://arxiv.org/abs/2212.10754) | 本研究利用Code-LLMs引导符号表示以增强神经符号故事理解，通过CoRRPUS系统和抽象提示程序，在最小的手动工程条件下，击败了当前最先进的结构LLM技术。 |
| [^78] | [Do language models have coherent mental models of everyday things?.](http://arxiv.org/abs/2212.10029) | 语言模型缺乏对日常物品的一致性心理模型，会因此出现荒谬的解决方法。虽然最先进的预训练语言模型具有这些实体的知识碎片，但它们无法为所有实体产生一致且正确的心理模型。语言模型训练可以改善这种情况。 |
| [^79] | [Relational Sentence Embedding for Flexible Semantic Matching.](http://arxiv.org/abs/2212.08802) | 本文提出了一种新的关系句嵌入方法，通过学习相关的关系嵌入来解决句子间复杂语义含义的关系捕捉问题。实验结果表明该方法在多个任务中均具有高效性能。 |
| [^80] | [Explicit Knowledge Transfer for Weakly-Supervised Code Generation.](http://arxiv.org/abs/2211.16740) | 本文提出了显式知识转移 (EKT) 方法，将大型语言模型 (LLMs) 的代码生成能力转移到小型模型，将微调所需昂贵且难以获取的任务特定自然语言代码对改为通过过滤大量 NL-code 对来实现，优于知识蒸馏。 |
| [^81] | [A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models.](http://arxiv.org/abs/2210.12023) | 该研究提出了一种基于因果框架的新方法，用于确定语言模型在数学推理任务中各种因素对输出解决方案的因果影响，研究结果显示GPT-3 Davinci模型（175B）在鲁棒性方面取得了显着改善。 |
| [^82] | [A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT.](http://arxiv.org/abs/2210.11899) | 研究旨在提高方言阿拉伯语的情感翻译。使用半监督方法，利用单语和平行数据来训练NMT系统，以提高在低资源语言中MT系统的准确性。 |
| [^83] | [AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation.](http://arxiv.org/abs/2210.07535) | AutoMoE提出了一种能够在计算约束下设计异构MoE的框架，它在神经机器翻译任务中实现了高效且最先进的性能。 |
| [^84] | [Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework.](http://arxiv.org/abs/2110.15317) | 该论文提出了一种针对文本对抗攻击的框架，通过在嵌入层上持续优化扰动并放大这些扰动，使用遮罩语言模型头对最终扰动的潜在代表进行解码，以获取可能的对抗样本，进行了广泛的评估，并在各种语言任务上取得了制造近乎不可察觉的通用和定向文本对抗样本的最新技术水平。 |

# 详细

[^1]: 通过代码生成来实现模块化视觉问答

    Modular Visual Question Answering via Code Generation. (arXiv:2306.05392v1 [cs.CL])

    [http://arxiv.org/abs/2306.05392](http://arxiv.org/abs/2306.05392)

    论文提出了一种基于代码生成实现模块化视觉问答的方法，无需额外训练，使用预训练的语言模型和视觉模型，可以显著提高VQA准确率。

    

    我们提出了一个框架，将视觉问答问题作为模块化代码生成来实现。与之前的模块化VQA方法相比，我们的方法不需要额外的训练，并依赖于针对图像字幕对预训练的语言模型（LMs）、视觉模型，以及用于上下文学习的50个VQA示例。生成的Python程序通过算术和条件逻辑调用和组合视觉模型的输出。与不采用代码生成的few-shot基线相比，我们的方法可以至少提高COVR数据集上3%的准确性，并在GQA数据集上提高约2%。

    We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by roughly 2% compared to the few-shot baseline that does not employ code generation.
    
[^2]: 儿童诗歌中的话语情感动态：不同年龄段的情感变化

    Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age. (arXiv:2306.05387v1 [cs.CL])

    [http://arxiv.org/abs/2306.05387](http://arxiv.org/abs/2306.05387)

    本文探讨了儿童诗歌中的话语情感动态，发现随着年龄增长，部分情感强度持续上升，情感基调持续下降，情感变异性、上升速率和恢复速率不断加强，并可为未来情感动态研究和儿童心理健康问题提供借鉴。

    

    新兴的心理病理学研究表明，情感状态的变化模式——情感动态——与整体福祉和心理健康有关。最近，有一些工作在追踪情感动态，以便在时间和人群上收集数据。然而，关于情感动态如何随年龄变化，特别是在孩子们的写作中如何确定，仍有一些问题未得到解答。在本文中，我们使用词库和基于机器学习的方法来量化由不同年龄段儿童写的诗歌所确定的情感动态的特征。我们发现，两种方法都指向类似的趋势：某些情感（如愤怒、害怕、喜悦、悲伤、唤醒和支配）的强度随年龄增加而持续增加，而情感的基调随年龄降低而保持稳定。我们还发现随着年龄的增长，情感变异性、上升速率（即情感反应性）和恢复速率（即情感回复力）也在不断提高。我们的结果可以帮助未来情感动态的研究，并帮助确定儿童心理健康问题的可能前兆。

    Emerging psychopathology studies are showing that patterns of changes in emotional state -- emotion dynamics -- are associated with overall well-being and mental health. More recently, there has been some work in tracking emotion dynamics through one's utterances, allowing for data to be collected on a larger scale across time and people. However, several questions about how emotion dynamics change with age, especially in children, and when determined through children's writing, remain unanswered. In this work, we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages. We show that both approaches point to similar trends: consistent increasing intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing valence with age. We also find increasing emotional variability, rise rates (i.e., emotional reactivity), and recov
    
[^3]: 在俄乌战争期间通过社交媒体检测人权侵犯

    Detecting Human Rights Violations on Social Media during Russia-Ukraine War. (arXiv:2306.05370v1 [cs.CY])

    [http://arxiv.org/abs/2306.05370](http://arxiv.org/abs/2306.05370)

    本研究聚焦于利用来自Telegram平台的数据，通过基于文本的分类器检测人权侵犯的提及情况。研究结果揭示了社交媒体在记录战争中可能存在的人权侵犯方面具有潜在的价值。

    

    当今俄乌军事冲突揭示了社交媒体在直接从前线透明地、无拘无束地分享信息方面所起的关键作用。在表达自由受限、信息战争盛行的冲突地区，社交媒体已成为不可或缺的生命线。我们的研究聚焦于分析来自Telegram的数据，这是后苏联地区阅读独立新闻的主要社交媒体平台。我们收集了95个公共Telegram频道的帖子样本数据集，其中包括涵盖政治和战争新闻的帖子，利用这些数据我们可以鉴别出潜在的人权侵犯事件。通过mBERT基于文本的分类器，我们已经分析了Telegram平台上任何有关人权侵犯的提及情况。

    The present-day Russia-Ukraine military conflict has exposed the pivotal role of social media in enabling the transparent and unbridled sharing of information directly from the frontlines. In conflict zones where freedom of expression is constrained and information warfare is pervasive, social media has emerged as an indispensable lifeline. Anonymous social media platforms, as publicly available sources for disseminating war-related information, have the potential to serve as effective instruments for monitoring and documenting Human Rights Violations (HRV). Our research focuses on the analysis of data from Telegram, the leading social media platform for reading independent news in post-Soviet regions. We gathered a dataset of posts sampled from 95 public Telegram channels that cover politics and war news, which we have utilized to identify potential occurrences of HRV. Employing a mBERT-based text classifier, we have conducted an analysis to detect any mentions of HRV in the Telegram 
    
[^4]: BEA-2023共享任务中的ADAIO系统：生成教育对话中AI教师回应

    The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. (arXiv:2306.05360v1 [cs.CL])

    [http://arxiv.org/abs/2306.05360](http://arxiv.org/abs/2306.05360)

    本文介绍了ADAIO团队在BEA-2023共享任务中的系统方案，使用OpenAI GPT-3评估基准模型并在教育对话中生成AI教师回应。通过少量提供提示信息，利用OpenAI的text-davinci-003模型在竞赛中获得第二名，并突出了大型语言模型在AI教师角色中的少量提示学习能力。

    

    本文介绍了ADAIO团队在"Building Educational Applications (BEA) 2023"共享任务中，生成教育对话中AI教师回应的系统方案。本任务旨在评估最先进的生成模型作为AI教师，在学生与教师的对话中产生合适回应的表现。我们使用OpenAI GPT-3评估了各种基准模型，并设计了不同的提示方式来激发OpenAI模型生成教师回应。在竞赛中，我们的系统使用了基于少量提示的方法，采用了OpenAI的text-davinci-003模型，获得了第二名的成绩。结果突出了大型语言模型（尤其是OpenAI的GPT-3）在AI教师角色中的少量提示学习能力。

    This paper presents the ADAIO team's system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI's GPT-3, in the role of AI teachers.
    
[^5]: 巨型语言模型在意大利生物医学信息提取方面的应用：方法论研究和实际应用的多中心实践

    Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])

    [http://arxiv.org/abs/2306.05323](http://arxiv.org/abs/2306.05323)

    该研究创建了意大利神经精神命名实体识别数据集，并使用巨型语言模型开发出多中心识别模型，整体 F1得分为84.77%。该模型将帮助临床从业者从非结构化的医疗记录中自动提取信息。

    

    医院引入计算机化医疗记录有助于减少手写和信息提取等繁琐操作。然而，由于从非结构化文本医疗记录中提取数据需要时间和精力，因此医疗记录中包含的数据仍然被充分利用程度低。自然语言处理的子领域信息提取可以帮助临床从业者克服这一限制，使用自动化文本挖掘流程。在这项工作中，我们创建了意大利神经精神命名实体识别数据集 PsyNIT，并使用它来开发这一任务的巨型语言模型。此外，我们还进行了多个实验，使用三个外部独立数据集来实现有效的多中心模型，整体 F1 得分为 84.77%，精确率为 83.16%，召回率为 86.44%。我们学到的经验是: (i) 一致的注释过程的关键作用和 (ii) 结合经典方法和“少量训练”的 fine-tuning 策略。

    The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
    
[^6]: KIT的多语言演讲翻译系统在IWSLT 2023上的应用

    KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v1 [cs.CL])

    [http://arxiv.org/abs/2306.05320](http://arxiv.org/abs/2306.05320)

    本文介绍了一个为IWSLT 2023多语言翻译贡献的翻译系统，其重点在于翻译科学会议演讲。用“检索式方法”（kNN-MT）进行有效的适应，该系统采用适配器轻松集成来自数据增强的增量训练数据，并展示级联系统更容易适应特定目标领域的优势。

    

    许多现有的语音翻译基准测试都针对高品质录音条件下的以英语为母语的语音，这通常与实际使用情况中的条件不符。在本文中，我们描述了我们为IWSLT 2023多语言轨道设计的语音翻译系统，重点翻译科学会议演讲。测试条件包括口音重的输入语音和术语密集的内容，并且需要翻译成10种资源数量不同的语言。在没有来自目标领域的训练数据的情况下，我们使用了检索式方法（kNN-MT）进行有效的适应（语音翻译+0.8 BLEU）。我们还使用适配器轻松集成来自数据增强的增量训练数据，并展示其与重新训练的性能相匹配。我们观察到，级联系统更容易适应特定目标领域，因为它们是由多个独立模块组成的。我们的级联语音系统远远优于其端到端系统。

    Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system substantially outperforms its end-to-end 
    
[^7]: ProbSum 2023中的CUED：层次化摘要模型集成

    CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models. (arXiv:2306.05317v1 [cs.CL])

    [http://arxiv.org/abs/2306.05317](http://arxiv.org/abs/2306.05317)

    本文介绍了CUED在ProbSum 2023的比赛中使用的层次化摘要模型集成方法HESM，通过将不同的精细调整过的Clinical-T5模型的标记级集合加以组合并使用最小贝叶斯风险解码方法，成功地提高了摘要性能，是该共享任务中表现最佳的系统。

    

    本文考虑在数据有限的情况下对患者医疗进展笔记进行摘要的挑战。在BioNLP Workshop 2023的Problem List Summarization (shared task 1A)中，我们证明了对765个医疗诊所笔记进行了精细调整的Clinical-T5优于其他基线系统，包括抽取式、生成式和零样例系统，从而为医疗笔记摘要提供了合理的基线系统。此外，我们引入了Hierarchical Ensemble of Summarization Models (HESM)，它由不同的精细调整过的Clinical-T5模型的标记级集合组成，然后采用最小贝叶斯风险（MBR）解码。我们的HESM方法大幅提高了摘要性能，并在保留挑战数据上进行评估时，达到了ROUGE-L为32.77，是共享任务排行榜前列的最佳表现系统。

    In this paper, we consider the challenge of summarizing patients' medical progress notes in a limited data setting. For the Problem List Summarization (shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5 fine-tuned to 765 medical clinic notes outperforms other extractive, abstractive and zero-shot baselines, yielding reasonable baseline systems for medical note summarization. Further, we introduce Hierarchical Ensemble of Summarization Models (HESM), consisting of token-level ensembles of diverse fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding. Our HESM approach lead to a considerable summarization performance boost, and when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which was the best-performing system at the top of the shared task leaderboard.
    
[^8]: 机器学习中公平度指标评估歧视偏差是否足够？

    Are fairness metric scores enough to assess discrimination biases in machine learning?. (arXiv:2306.05307v1 [cs.CL])

    [http://arxiv.org/abs/2306.05307](http://arxiv.org/abs/2306.05307)

    本文在机器学习算法在文本数据上对性别歧视的偏见的度量标准的局限性上进行了实验研究。

    

    本文提出了新的实验，揭示了当前用于评估机器学习算法在文本数据上对性别歧视的偏见的度量标准的局限性。我们以Bios数据集为例，学习预测个人的职业。这种预测任务在商业自然语言处理（NLP）应用程序中很常见，例如自动工作推荐。

    This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions. Our experiments sample the Bios dataset and learn more than 200 m
    
[^9]: ToolAlpaca: 通过3000个模拟案例对语言模型进行通用工具学习

    ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (arXiv:2306.05301v1 [cs.CL])

    [http://arxiv.org/abs/2306.05301](http://arxiv.org/abs/2306.05301)

    ToolAlpaca是一个自动生成工具使用语料库，实现紧凑的语言模型参加通用工具学习的框架。

    

    实现大型语言模型有效利用现实工具对于实现具有感知能力的智能至关重要。现有的工具学习方法主要依靠非常大的语言模型，例如GPT-4，在零-shot方式下获得通用的工具使用能力，或利用监督学习在紧凑的模型上训练有限类型的工具。然而，较小的语言模型是否能够在没有特定工具训练的情况下实现通用的工具使用能力尚不确定。本文介绍了ToolAlpaca，这是一个新框架，旨在自动生成工具使用语料库，并在最小人为干预下，在紧凑的语言模型上学习通用的工具使用能力。

    Enabling large language models to effectively utilize real-world tools is crucial for achieving embodied intelligence. Existing approaches to tool learning have primarily relied on either extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or have utilized supervised learning to train limited types of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without specific tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first collects a comprehensive dataset by building a multi-agent simulation environment, which contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subseque
    
[^10]: 重新审视使用PLMs的Few-shot Intent Classification: 直接微调 vs 连续预训练

    Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training. (arXiv:2306.05278v1 [cs.CL])

    [http://arxiv.org/abs/2306.05278](http://arxiv.org/abs/2306.05278)

    本文探讨了Few-shot Intent Classification任务的解决方法。相较于传统的在外部资源上连续预训练，本文提出了直接微调预训练语言模型的方法，并通过实验证明其在少量标记数据情况下已经可以取得不错的结果，表明连续预训练并非必要。

    

    本文考虑Few-shot Intent Classification任务，该任务涉及仅使用少量标记数据训练深度学习模型以基于其基础意图分类话语。解决此问题的当前方法是通过连续预训练，即在外部资源（例如会话语料库、公共意图检测数据集或自然语言理解数据集）上微调预训练语言模型（PLMs），然后使用它们作为话语编码器来训练意图分类器。在本文中，我们展示了连续预训练可能并非必要，因为PLMs在此任务上的过拟合问题可能并不像预期的那样严重。具体来说，我们发现，直接对仅有少量标记示例的PLMs进行微调已经可以产生相当不错的结果，而绩效差距随着标记数据量的增加迅速缩小。为了最大限度地利用有限的标记数据，我们提出了一种新的微调策略，即注意力流控（Attention Flow Control），其允许在不同的预训练层之间动态分配微调的重心。

    We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited a
    
[^11]: 基于Transformer架构的不良药物事件提取方法的广泛评估

    Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])

    [http://arxiv.org/abs/2306.05276](http://arxiv.org/abs/2306.05276)

    本文对19种基于Transformer的ADE提取模型进行广泛评估，并在具有不同非正式程度的数据集上比较它们的性能。此外，我们使用了成熟的特征重要性技术（SHAP）进一步分析了模型的性能。

    

    不良事件（ADE）提取是数字药物监管中的核心任务之一，特别是当应用于非正式文本时。自然语言处理社区使用像BERT这样的大型预训练语言模型来解决这个任务。尽管在文献中使用了大量的基于Transformer的架构，但尚不清楚它们哪一个表现更好以及为什么。因此，在本文中，我们对19种基于Transformer的ADE提取模型进行了广泛评估和分析，并在逐渐增加非正式水平的两个数据集上比较了所有考虑的模型的性能（论坛帖子和推文）。我们还将纯Transformer模型与两个常用的额外处理层（CRF和LSTM）相结合，并分析它们对模型性能的影响。此外，我们使用了一种成熟的特征重要性技术（SHAP）将模型性能与一组特征相关联，以进一步分析模型的性能。

    Adverse Event (ADE) extraction is one of the core tasks in digital pharmacovigilance, especially when applied to informal texts. This task has been addressed by the Natural Language Processing community using large pre-trained language models, such as BERT. Despite the great number of Transformer-based architectures used in the literature, it is unclear which of them has better performances and why. Therefore, in this paper we perform an extensive evaluation and analysis of 19 Transformer-based models for ADE extraction on informal texts. We compare the performance of all the considered models on two datasets with increasing levels of informality (forums posts and tweets). We also combine the purely Transformer-based models with two commonly-used additional processing layers (CRF and LSTM), and analyze their effect on the models performance. Furthermore, we use a well-established feature importance technique (SHAP) to correlate the performance of the models with a set of features that 
    
[^12]: 《2023年ProbSum共享任务概览》：从电子病历进展记录中总结患者活跃诊断和问题的概述。

    Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes. (arXiv:2306.05270v1 [cs.CL])

    [http://arxiv.org/abs/2306.05270](http://arxiv.org/abs/2306.05270)

    该论文总结了 BioNLP Workshop 2023 发起的 ProbSum 共享任务的数据集、评估指标和基线系统，旨在吸引未来研究努力构建适用于真实世界诊断决策支持应用的自然语言处理模型，以提高患者的医疗保健质量。

    

    BioNLP Workshop 2023于2023年1月发起了ProbSum共享任务，旨在吸引未来研究努力构建适用于真实世界诊断决策支持应用的自然语言处理模型。参与者的目标是使用来自重症患者住院期间日常护理记录的输入开发生成诊断和问题列表的模型。八个团队将其最终系统提交到了共享任务排行榜上。本文描述了任务、数据集、评估指标和基线系统，并总结了参与团队尝试的不同方法的技术和评估结果。

    The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant and accurate diagnoses will augment the healthcare providers decision-making process and improve the quality of care for patients. The goal for participants is to develop models that generated a list of diagnoses and problems using input from the daily care notes collected from the hospitalization of critically ill patients. Eight teams submitted their final systems to the shared task leaderboard. In this paper, we describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally, the techniques and results of the evaluation of the different approaches tried by the participating teams are summarized.
    
[^13]: 分解对比学习：超越多视角冗余

    Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])

    [http://arxiv.org/abs/2306.05268](http://arxiv.org/abs/2306.05268)

    本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。

    

    在广泛的多模态任务中，对比学习已成为一种特别吸引人的方法，因为它可以成功地学习具有丰富未标记数据的表示，只需配对信息（例如，图像标题或视频音频对）。这些方法的基础是多视角冗余的假设——跨模态间共享信息对于下游任务是必要且足够的。然而，在许多现实世界的情况下，任务相关信息也包含在跨模态唯一区域中：一种仅存在于一个模态中但与任务仍然相关的信息。如何学习自我监督的多模态表示以捕获与下游任务相关的共享和唯一信息？本文提出了一种新的多模态表示学习方法FactorCL，以超越多视角冗余。FactorCL的基础是三个新的贡献：（1）将任务相关信息分解为共享和唯一表示，（2）限制共享和唯一成分之间的交互，（3）使用因子正则化促进表示学习。

    In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
    
[^14]: 处理多模态NLP中的语义不确定性

    Dealing with Semantic Underspecification in Multimodal NLP. (arXiv:2306.05240v1 [cs.CL])

    [http://arxiv.org/abs/2306.05240](http://arxiv.org/abs/2306.05240)

    语义未确定性在语言学中很重要，但多模态系统还没有解决好这个问题

    

    旨在像人类一样掌握语言的智能系统必须处理其语义未确定性，即语言信号可能仅传达通信所需信息的一部分。标准的NLP模型原则上没有或仅有有限的访问额外信息的能力，而将语言基于其他模态（例如视觉）进行说明的多模态系统自然配备以解决这种现象。然而，我们发现他们难以应对这一问题，这可能成为一个问题。

    Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the gender and number of its referent(s) underspecified. Semantic underspecification is not a bug but a crucial language feature that boosts its storage and processing efficiency. Indeed, human speakers can quickly and effortlessly integrate semantically-underspecified linguistic signals with a wide range of non-linguistic information, e.g., the multimodal context, social or cultural conventions, and shared knowledge. Standard NLP models have, in principle, no or limited access to such extra information, while multimodal systems grounding language into other modalities, such as vision, are naturally equipped to account for this phenomenon. However, we show that they struggle with it, which could ne
    
[^15]: 提高长篇文本机器翻译的质量

    Improving Long Context Document-Level Machine Translation. (arXiv:2306.05183v1 [cs.CL])

    [http://arxiv.org/abs/2306.05183](http://arxiv.org/abs/2306.05183)

    该论文提出了一种新的受限注意力机制来提高长篇文本机器翻译的质量。

    

    对于神经机器翻译（NMT）来说，文本级别的上下文对于提高翻译的一致性、凝聚性、模棱两可输入的翻译以及其他语言现象至关重要。虽然已经有许多关于文档级别 NMT 的相关论文出版，但大多数将系统限制在本地上下文，通常只包括前一两个句子作为更多信息。这可能足以解决一些曖昧性输入，但可能不足以捕捉文档级别信息，例如话题或对话风格。当将上下文大小增加到本地上下文之外时，会面临两个挑战：（i）内存使用将呈指数增长（ii）翻译性能开始降低。我们认为广泛使用的注意机制是这两个问题的原因。因此，我们提出了一种受限的注意力变体，将注意力集中在序列的最相关部分，同时控制对齐权重的总和。

    Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the~memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneou
    
[^16]: M3Exam: 一种多语言、多模态、多层次的基准测试，用于评估大型语言模型

    M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])

    [http://arxiv.org/abs/2306.05179](http://arxiv.org/abs/2306.05179)

    M3Exam是一个来源于真实人类考试题目的新型基准测试，用于评估大型语言模型在多语言、多模态和多层次的情境中的普适智能。

    

    尽管存在着各种针对自然语言处理模型进行评估的基准测试，但我们认为考试更适合评估大型语言模型的普适智能，因为它们囊括了更广泛的能力需求，例如语言理解、领域知识和解决问题的能力。为此，我们引入了 M3Exam，这是一个基于真实和官方人类考试题目的新型基准测试，用于在多语言、多模态和多层次的情境中评估 LLM。M3Exam 具有三个独特特点:（1）多语言性，涵盖多个国家的问题，需要强大的多语言能力和文化知识；（2）多模态，考虑到许多考试问题的多模态性质，以测试模型的多模态理解能力；（3）多层次结构，特别涵盖了三个关键教育阶段的考试，全面评估模型在不同教育水平上的熟练程度。

    Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different lev
    
[^17]: RRWKV：在RWKV中捕捉长距离依赖关系

    RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])

    [http://arxiv.org/abs/2306.05176](http://arxiv.org/abs/2306.05176)

    本文介绍了一种新的RRWKV架构，它在保持记忆和计算效率的同时，通过加入回顾能力有效地捕捉长距离依赖关系。

    

    由于Transformer惊人的点积注意力，它已经成为各种自然语言处理（NLP）任务中的主要架构。最近，Receptance Weighted Key Value（RWKV）架构遵循非Transformer架构，消除了点积注意力的缺点，其中存储和计算复杂度随着序列长度呈二次扩展。尽管RWKV利用了线性张量积注意机制并通过部署时间序列模式实现了并行计算，但与标准Transformer中直接交互获得的完整信息相比，它无法捕捉长距离依赖关系，因为其受限于向后查看先前信息的能力。因此，本文通过将回顾能力纳入RWKV中来设计Retrospected Receptance Weighted Key Value（RRWKV）架构，以有效地吸收信息，同时保持记忆和计算效率。

    Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
    
[^18]: 语言模型对大脑的映射：一项调查研究

    Mapping Brains with Language Models: A Survey. (arXiv:2306.05126v1 [cs.CL])

    [http://arxiv.org/abs/2306.05126](http://arxiv.org/abs/2306.05126)

    大脑和语言模型激活呈现相似之处，为相关研究提供了基础，但目前仍缺乏明确的结论。

    

    多年来，许多研究者似乎都做出了同样的观察：大脑和语言模型的激活呈现一些结构上的相似之处，使得从神经记录和计算语言模型中提取的特征之间存在线性偏差映射。我们尝试评估这一观察已经积累了多少证据，调查了超过30项涵盖10个数据集和8个指标的研究。我们发现，文献中使用的评估方法中一些指标并不保守。目前，尽管积累了大量的证据，但结论仍不确定。然而，与模型大小和质量的相关性为我们提供了谨慎乐观的理由。

    Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how much evidence has been accumulated for this observation, we survey over 30 studies spanning 10 datasets and 8 metrics. How much evidence has been accumulated, and what, if anything, is missing before we can draw conclusions? Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative. We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism.
    
[^19]: 参考文献至上：基于细粒度评估框架的对话摘要事实误差校正基准测试

    Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework. (arXiv:2306.05119v1 [cs.CL])

    [http://arxiv.org/abs/2306.05119](http://arxiv.org/abs/2306.05119)

    该论文提出了基于参考校正的细粒度FEC评估框架，并在使用该框架进行实验的过程中发现了不同事实误差类别下的最佳训练模式和现有方法之间的显著性差异。

    

    在对话摘要中，事实性很重要。对于模型生成的摘要进行事实误差校正（FEC）是提高事实性的一种方式。当前依赖于事实性指标的FEC评估不够可靠和详细。为了解决这个问题，我们首次手动注释了一个包含4000个项目的对话摘要FEC数据集，并提出了基于参考校正的细粒度评估框架FERRANTI，该框架自动评估FEC模型在不同错误类别上的表现。使用这个评估框架，在各种设置下进行了足够的FEC方法实验，并在不同的事实误差类别上找到了最佳的训练模式和现有方法的显著性差异。

    Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually annotate a FEC dataset for dialogue summarization containing 4000 items and propose FERRANTI, a fine-grained evaluation framework based on reference correction that automatically evaluates the performance of FEC models on different error categories. Using this evaluation framework, we conduct sufficient experiments with FEC approaches under a variety of settings and find the best training modes and significant differences in the performance of the existing approaches on different factual error categories.
    
[^20]: 关于文档级神经机器翻译的搜索策略

    On Search Strategies for Document-Level Neural Machine Translation. (arXiv:2306.05116v1 [cs.CL])

    [http://arxiv.org/abs/2306.05116](http://arxiv.org/abs/2306.05116)

    本文探讨了如何在搜索过程中最好地利用文档级神经机器翻译模型，比较了文献中的不同解码方案和作者提出的方案，并发现针对某些语言现象时，常用的解码策略不能够取得较好的性能。

    

    与句子级系统相比，文档级神经机器翻译（NMT）模型能够在一份文件中产生更一致的输出，并且能够更好地解决输入中的歧义。在文档级NMT上已经有许多研究，尤其是着重于修改模型架构或训练策略以更好地适应额外的上下文输入。然而，在大多数研究中，如何通过已训练好的模型执行搜索的问题很少被讨论，有时甚至根本不被提及。本研究旨在回答如何在解码中最好地利用上下文感知翻译模型的问题。我们从最流行的文档级NMT方法开始，比较了文献中的不同解码方案和我们提出的方案。比较中，我们使用了标准自动评价指标以及针对三个标准文档级翻译基准测试的特定语言现象。我们发现，大多数常用的解码策略在针对某些语言现象时并不能取得较好的性能。

    Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies 
    
[^21]: 关闭循环：使用ChatGPT测试生成模型解释以改善社交媒体上赞助内容的人工标注

    Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media. (arXiv:2306.05115v1 [cs.CL])

    [http://arxiv.org/abs/2306.05115](http://arxiv.org/abs/2306.05115)

    这篇论文提出了一种使用ChatGPT生成模型解释的方法来改善人工标注的赞助内容检测，以提高标注一致性和准确性，在Instagram数据集上进行的评估结果表明比现有方法有显著的改进。

    

    全球监管机构通过《不公平商业行为指令》(UCPD)等工具来加强确保透明度社交媒体上的影响者营销，或者根据《联邦贸易委员会法案》第5条进行执行。然而，由于影响力市场的规模庞大，强制实施这些义务一直是一个难题。检测赞助内容的任务旨在实现对此类规定的监控和执行。当前这方面的研究主要将问题框架为一项机器学习任务，着重于开发能够在检测广告时达到高分类性能的模型。这些机器学习任务依赖于人类数据注释来提供基础真实信息。然而，注释者之间的一致性通常很低，导致标签不一致，从而影响模型的可靠性。我们提出了一种新方法，使用语言模型ChatGPT生成模型预测的解释，以提高人工标注者的准确度和一致性，从而改善赞助内容的检测。

    Regulatory bodies worldwide are intensifying their efforts to ensure transparency in influencer marketing on social media through instruments like the Unfair Commercial Practices Directive (UCPD) in the European Union, or Section 5 of the Federal Trade Commission Act. Yet enforcing these obligations has proven to be highly problematic due to the sheer scale of the influencer market. The task of automatically detecting sponsored content aims to enable the monitoring and enforcement of such regulations at scale. Current research in this field primarily frames this problem as a machine learning task, focusing on developing models that achieve high classification performance in detecting ads. These machine learning tasks rely on human data annotation to provide ground truth information. However, agreement between annotators is often low, leading to inconsistent labels that hinder the reliability of models. To improve annotation accuracy and, thus, the detection of sponsored content, we pro
    
[^22]: 对话的艺术：使用孪生RNN测量L2语音中的语音收敛和故意模仿

    The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN. (arXiv:2306.05088v1 [cs.CL])

    [http://arxiv.org/abs/2306.05088](http://arxiv.org/abs/2306.05088)

    本文提出了一种孪生RNN结构用于测量L2-L2交互中语音音质收敛。该模型可以有效捕捉语音收敛和说话者的模仿能力的动态，并能够处理由L1引起的说话者差异。

    

    本文提出了一种孪生递归神经网络（RNN）结构，用于测量L2-L2交互中语音音质的收敛。我们通过添加20名母语为斯洛伐克语的英语学习者来扩展交替阅读任务（ART）数据集。我们训练和测试了该模型，从三个不同的母语组，即意大利语（9对），法语（10对）和斯洛伐克语（10对）中测量L2英语语音的语音收敛。结果表明，孪生RNN模型可以有效捕捉语音收敛和说话者的模仿能力的动态。此外，这是一个文本无关的模型，可以扩展，并能够处理由L1引起的说话者差异。

    Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability.
    
[^23]: PandaLM：LLM指令调优优化的自动评估基准

    PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])

    [http://arxiv.org/abs/2306.05087](http://arxiv.org/abs/2306.05087)

    PandaLM是一个评估LLM指令调优的自动基准，它能够区分最优模型，并关注于主观因素。

    

    由于超参数选择的复杂性和评估调整模型的困难性，LLM（大型语言模型）的指令调优仍然是一项具有挑战性的任务。为确定最佳超参数，需要一个自动的、强大且可靠的评估基准。然而，由于评估准确性和隐私保护的挑战，建立这样一个基准并不是一项简单的任务。为应对这些挑战，我们引入了一款名为PandaLM的评测大型语言模型，该模型经过训练，能够区分出多个LLM中最佳的模型。PandaLM的关注点不仅限于传统评估数据集的客观正确性，还涵盖了诸如相对简洁性、清晰度、遵循说明、全面性和形式性等重要主观因素。为确保PandaLM的可靠性，我们收集了一个多样化的人工注释测试数据集，其中所有上下文都是生成的。

    Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated
    
[^24]: 通过数据增强提升AI攻击性代码生成器的鲁棒性

    Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])

    [http://arxiv.org/abs/2306.05079](http://arxiv.org/abs/2306.05079)

    本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。

    

    本研究提出了一种将扰动添加到安全性代码上下文中的代码描述中的方法，即来自善意开发者的自然语言输入（NL），并分析了扰动如何以及在什么程度上影响AI攻击性代码生成器的性能。我们的实验表明，NL描述中的扰动高度影响代码生成器的性能。为了增强代码生成器的鲁棒性，我们使用该方法执行数据增强，即增加训练数据的变异性和多样性，并证明其对扰动和非扰动的代码描述的有效性。

    In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
    
[^25]: DLAMA: 一个用于为探究预训练语言模型知识筛选多元文化事实的框架

    DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models. (arXiv:2306.05076v1 [cs.CL])

    [http://arxiv.org/abs/2306.05076](http://arxiv.org/abs/2306.05076)

    提出了一个新的框架DLAMA-v1，用于筛选来自不同文化背景的事实三元组，解决在多语言模型上使用英语提示更具有一致性的问题。

    

    已经发布了一些基准数据集来评估预训练语言模型的事实知识。这些基准数据集（例如LAMA和ParaRel）主要是用英语开发的，然后被翻译成新的多语言版本（例如mLAMA和mParaRel）。对这些多语言基准数据集的结果表明，使用英语提示从多语言模型中回忆事实通常比使用非英语提示产生更好且更一致的性能。我们的分析表明，mLAMA偏向于来自西方国家的事实，这可能影响探测模型的公平性。我们提出了一个新的框架，用于筛选来自Wikidata的具有文化多样性的事实三元组。由三对对比文化的实际三元组组成一个新的基准数据集DLAMA-v1，共有20个关系谓词的78259个三元组。这三对包括代表（阿拉伯和西方）、（亚洲和西方）以及（南美洲和西方）的事实。

    A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western)
    
[^26]: LCT-1在SemEval-2023任务10中的应用：预训练和多任务学习用于性别歧视检测和分类

    LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification. (arXiv:2306.05075v1 [cs.CL])

    [http://arxiv.org/abs/2306.05075](http://arxiv.org/abs/2306.05075)

    本文介绍了一个基于预训练和多任务学习的检测和分类系统，用于解决社交媒体上的性别歧视问题。多任务学习对性别歧视检测和粗粒度性别分类表现出色，而微调则更适用于细粒度分类。

    

    性别歧视和性别主义在社交媒体上越来越成为一个问题。虽然在线性别歧视检测方面取得了一些进展，但系统通常是不可解释的。SemEval-2023任务10旨在增加关于性别歧视检测的可解释性，我们的团队参加了所有建议的子任务。我们的系统基于进一步的领域自适应预训练（Gururangan等人，2020）。在具有领域自适应性的基于Transformer的模型的基础上，我们比较了微调和多任务学习，并表明每个子任务需要不同的系统配置。在我们的实验中，多任务学习在性别歧视检测方面表现极佳，并在粗粒度的性别分类方面优于标准微调，而微调则更适用于细粒度分类。

    Misogyny and sexism are growing problems in social media. Advances have been made in online sexism detection but the systems are often uninterpretable. SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at increasing explainability of the sexism detection, and our team participated in all the proposed subtasks. Our system is based on further domain-adaptive pre-training (Gururangan et al., 2020). Building on the Transformer-based models with the domain adaptation, we compare fine-tuning with multi-task learning and show that each subtask requires a different system configuration. In our experiments, multi-task learning performs on par with standard fine-tuning for sexism detection and noticeably better for coarse-grained sexism classification, while fine-tuning is preferable for fine-grained classification.
    
[^27]: 学习地球科学知识理解和利用的基础语言模型

    Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])

    [http://arxiv.org/abs/2306.05064](http://arxiv.org/abs/2306.05064)

    本文首次提出了一个地球科学领域的大型语言模型K2，并开发了各种资源以进一步促进其在地球科学领域中的研究和应用，包括第一个地球科学教学调音数据集GeoSignal和第一个地球科学基准测试GeoBenchmark。我们使用了完整的方法将预先训练的通用领域LLM LLaMA-7B 模型适应到地球科学领域。

    

    大型语言模型(LLM)在自然语言处理的常规领域取得了巨大成功。本文将LLM引入地球科学领域，旨在推进该领域的研究和应用。为此，我们首次提出了地球科学领域的LLM，命名为K2，并开发了一系列资源，以进一步促进LLM在地球科学研究中的应用。例如，我们为LLM提供了第一个地球科学教学调音数据集GeoSignal，旨在将LLM相应与地球科学相关的用户查询对齐。此外，我们还建立了第一个地质科学基准测试GeoBenchmark，以在地球科学环境中评估LLM。在这项工作中，我们尝试使用完整的方法将预先训练的通用领域LLM适应到地球科学领域。具体而言，我们在超过100万篇地球科学文献上进一步训练了LLaMA-7B模型，并利用GeoSignal的监督数据对模型进行微调。此外，我们还分享了一个可以在不同领域中迁移LLM的协议。

    Large language models (LLMs)have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience, with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pretrained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on over 1 million pieces of geoscience literature and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can e
    
[^28]: T3L: 翻译-测试迁移学习用于跨语言文本分类

    T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification. (arXiv:2306.04996v1 [cs.CL])

    [http://arxiv.org/abs/2306.04996](http://arxiv.org/abs/2306.04996)

    本文提出了一种翻译-测试的迁移学习方法以用于跨语言文本分类，在神经机器翻译器的帮助下将目标语言翻译成高资源语言来训练文本分类器，并取得了比基线更好的结果。

    

    跨语言文本分类利用在高资源语言上训练的文本分类器来执行在其他语言上的文本分类，而无需或只需要微调（零/少量关键词跨语言转移）。如今，跨语言文本分类器通常基于预先在多种感兴趣的语言上进行预训练的大规模多语言语言模型（LM）来构建。然而，这些模型在不同的语言和分类任务上的表现有很大的差异，这表明语言建模和分类任务的叠加并不总是有效的。因此，在本文中，我们提议重新审视经典的“翻译-测试”流程，以清晰地分离翻译和分类阶段。所提出的方法将1）神经机器翻译器将目标语言翻译成高资源语言，和2）在高资源语言上训练的文本分类器相结合，但神经机器翻译器生成“软”翻译以保留原始目标语言的特征。我们在十种语言的多样化跨语言文本分类基准测试中评估了我们的方法。结果显示，所提出的方法在零关键词设置下的大部分数据集上胜过领先的基线框架，并且在大多数数据集上，在监督和少量关键词设置下也实现了最先进的性能。

    Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models vary significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic "translate-and-test" pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates "soft" tran
    
[^29]: 利用预训练语言模型和大型语言模型评估非英语母语人士的短语断点

    Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models. (arXiv:2306.04980v1 [cs.CL])

    [http://arxiv.org/abs/2306.04980](http://arxiv.org/abs/2306.04980)

    本研究介绍了利用预训练语言模型和大型语言模型评估非英语母语人士语音中短语断点的方法，并且实验证明这种方法能够大大降低对标记的训练数据的依赖性，并且性能提高。

    

    本研究介绍了利用预训练语言模型（PLMs）和大型语言模型（LLMs）评估非英语母语学习者语音中短语断点的方法。该研究包含两个任务：评估一个语音片段的总体短语断点和对每个可能的短语断点位置进行细粒度评估。为利用NLP模型，语音输入首先被强制对齐到文本，然后被预处理成一个包含单词和短语断点信息的标记序列。为了利用PLMs，我们提出了一个带有处理过的标记的预训练和微调流程。这个过程包括用一个替换破折号检测模块进行预训练，并用文本分类和序列标注进行微调。为了利用LLMs，我们给ChatGPT设计了提示。实验表明，使用PLMs大大降低了对标记的训练数据的依赖性，并且性能得到了提高。同时，我们验证了ChatGPT在这一领域进一步提升的潜力。

    This work introduces approaches to assessing phrase breaks in ESL learners' speech using pre-trained language models (PLMs) and large language models (LLMs). There are two tasks: overall assessment of phrase break for a speech clip and fine-grained assessment of every possible phrase break position. To leverage NLP models, speech input is first force-aligned with texts, and then pre-processed into a token sequence, including words and phrase break information. To utilize PLMs, we propose a pre-training and fine-tuning pipeline with the processed tokens. This process includes pre-training with a replaced break token detection module and fine-tuning with text classification and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The experiments show that with the PLMs, the dependence on labeled training data has been greatly reduced, and the performance has improved. Meanwhile, we verify that ChatGPT, a renowned LLM, has potential for further advancement in this area.
    
[^30]: 主动监督聚类用于开放式关系抽取

    Actively Supervised Clustering for Open Relation Extraction. (arXiv:2306.04968v1 [cs.CL])

    [http://arxiv.org/abs/2306.04968](http://arxiv.org/abs/2306.04968)

    本文提出了一种新的主动监督聚类方法，用于开放式关系抽取，通过交替聚类学习和关系标注，有效提高聚类准确性和关系提取性能。

    

    当前基于聚类的开放式关系抽取(OpenRE)方法通常采用两阶段流程。第一阶段同时学习关系表示和分配。第二阶段手动标记一些实例，并为每个聚类命名关系。然而，无监督目标难以优化模型以获得准确的聚类分配，并且必须提前确定群集数量。在本文中，我们提出了一种新的设置，称为用于OpenRE的主动监督聚类。我们的见解在于聚类学习和关系标注可以交替进行，为聚类提供必要的指导，而不会显著增加人力资源。设置的关键在于选择要标记的实例。我们提出了一种新的策略，适用于动态发现未知关系的聚类。基准数据集上的实验结果显示，我们的方法在聚类准确性和关系提取性能方面优于现有方法。

    Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline. The first stage simultaneously learns relation representations and assignments. The second stage manually labels several instances and thus names the relation for each cluster. However, unsupervised objectives struggle to optimize the model to derive accurate clustering assignments, and the number of clusters has to be supplied in advance. In this paper, we present a novel setting, named actively supervised clustering for OpenRE. Our insight lies in that clustering learning and relation labeling can be alternately performed, providing the necessary guidance for clustering without a significant increase in human effort. The key to the setting is selecting which instances to label. Instead of using classical active labeling strategies designed for fixed known classes, we propose a new strategy, which is applicable to dynamically discover clusters of unknown relations. Experimental resul
    
[^31]: 利用语言识别技术提升代码混合文本分类

    Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])

    [http://arxiv.org/abs/2306.04964](http://arxiv.org/abs/2306.04964)

    本研究提出了一种改进代码混合文本分类的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。我们探索了词级交错和句子后置的语言信息插入方法，以提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。

    

    在同一段文本中使用多种语言叫做代码混合。当前社交媒体平台上，特别是英语和地方语言混合使用的数据越来越多。现有的深度学习模型没有充分利用代码混合文本中的隐性语言信息。本研究旨在通过尝试不同的语言增强方法，提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。我们提出了一种改进代码混合系统的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。在BERT模型中进行语言增强时，我们探索了词级交错和句子后置的语言信息插入方法。我们测试了原始BERT模型和经过代码混合改进的HingBERT在各自基准数据集上的性能。

    The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comp
    
[^32]: 开放集关系抽取——基于未知感知训练

    Open Set Relation Extraction via Unknown-Aware Training. (arXiv:2306.04950v1 [cs.CL])

    [http://arxiv.org/abs/2306.04950](http://arxiv.org/abs/2306.04950)

    本文提出一种基于未知感知训练的方法，通过动态合成负实例来对模型进行规范化，在不损害已知关系检测的情况下实现了 SOTA 的未知关系检测。

    

    现有的监督关系抽取方法在闭集场景下取得了令人瞩目的性能，即在训练和测试过程中关系保持不变。在更现实的开放集场景中，测试集中可能出现未知关系。由于缺乏未知关系的监督信号，一个表现良好的闭集关系抽取器仍然会将其自信地错误归类为已知关系。本文提出了一种未知感知训练方法，通过动态合成负实例来对模型进行规范化。为了促进紧凑的决策边界，需要“困难”的负实例。受到文本敌对攻击的启发，我们改进原始训练实例，自适应地应用小而关键的扰动，从而合成更有可能被模型误分类为已知关系的负实例。实验结果表明，该方法在不损害已知关系检测的情况下实现了 SOTA 的未知关系检测。

    The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, where the relations during both training and testing remain the same. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training method, regularizing the model by dynamically synthesizing negative instances. To facilitate a compact decision boundary, ``difficult'' negative instances are necessary. Inspired by text adversarial attacks, we adaptively apply small but critical perturbations to original training instances and thus synthesizing negative instances that are more likely to be mistaken by the model as known relations. Experimental results show that this method achieves SOTA unknown relation detection without compromising t
    
[^33]: 一种改良的语料主题检测模型及评估主题可读性的新指标

    A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics. (arXiv:2306.04941v1 [cs.CL])

    [http://arxiv.org/abs/2306.04941](http://arxiv.org/abs/2306.04941)

    本文提出一种改进的神经网络模型用于检测语料库中的主题，并提出了一种新的指标用于评估主题的可读性。

    

    本论文提出了一种改良的神经网络模型用于从语料库中检测主题，并且提出了一种新的指标用于评估所检测到的主题。新模型建立在嵌入式主题模型基础上，并加入了一些改进，如文档聚类等。实验表明，该模型在处理各种长度的文档时表现良好。新的指标可以更有效地计算主题可读性，提供了有关所检测到主题的可理解程度的不同信息。

    This paper presents a modified neural model for topic detection from a corpus and proposes a new metric to evaluate the detected topics. The new model builds upon the embedded topic model incorporating some modifications such as document clustering. Numerical experiments suggest that the new model performs favourably regardless of the document's length. The new metric, which can be computed more efficiently than widely-used metrics such as topic coherence, provides variable information regarding the understandability of the detected topics.
    
[^34]: InfoPrompt：用信息论软提示调整自然语言理解的方法

    InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding. (arXiv:2306.04933v1 [cs.CL])

    [http://arxiv.org/abs/2306.04933](http://arxiv.org/abs/2306.04933)

    本文提出了一种新的信息论框架，通过最大化提示和其他模型参数之间互信息来优化软提示调整，从而开发了更加高效、准确和稳健的软提示调整方法InfoPrompt。该方法通过两个新型损失函数，发现合适的提示初始化，并从提示令牌中学习足够的任务相关信息，同时鼓励预训练语言模型的输出表示更加关注任务相关信息。

    

    软提示调整在广泛的少样本任务中取得了卓越的性能。但是，提示调整的性能对初始化的提示非常敏感。本文发现，传统的提示调整方法不能从提示令牌中编码和学习足够的任务相关信息。我们开发了一种信息论框架，将软提示调整形式化为最大化提示和其他模型参数（或编码表示）之间互信息的优化问题。这种新颖的观点有助于我们开发一个更高效、准确和稳健的软提示调整方法InfoPrompt。在这个框架下，我们开发了两个基于互信息的新型损失函数，用于（i）发现下游任务的合适提示初始化，并从提示令牌中学习足够的任务相关信息，（ii）鼓励预训练语言模型的输出表示更加关注任务相关信息。

    Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We also empirically observe that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) discover proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant informa
    
[^35]: covLLM：用于COVID-19生物医学文献的大型语言模型

    covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])

    [http://arxiv.org/abs/2306.04926](http://arxiv.org/abs/2306.04926)

    使用大型语言模型开发了一种名为covLLM的工具，用于协助临床医生评估COVID-19文献。covLLM可以汇总和提取相关信息，帮助医生更好地应对COVID-19疫情。

    

    虽然新冠病毒的研究在不断增加，但COVID-19大流行导致了美国110万人的死亡。这些新发现在转化为临床干预方案方面缓慢，导致患者预后较差和不必要的死亡。其中一种原因是临床医生因患者过多而难以跟上新冠病毒文献的速度。发展一个使用大型语言模型（LLM）评估冠状病毒文献的工具，即神经网络用于自然语言处理，可能是一个解决方案。LLMs可用于汇总和提取用户指定的信息。较大范围和先进的LLMs和预处理的冠状病毒文献数据库提供了通过冠状病毒文献特定LLM（covLLM）协助临床医生评估冠状病毒文献的机会，该工具直接输入研究文章和用户查询以返回答案。在使用COVID-19开放研究数据集（CORD-19）的过程中，我们开发和评估了covLLM，展示了它在总结和从冠状病毒文献中提取信息方面的实用性。

    The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD
    
[^36]: 偏好分类：通过辅助偏好学习改进文本分类器

    Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning. (arXiv:2306.04925v1 [cs.CL])

    [http://arxiv.org/abs/2306.04925](http://arxiv.org/abs/2306.04925)

    本文提出了一种新的文本分类方法，使用偏好分类学习辅助数据来提高模型准确性。通过比较输入文本对之间的偏好关系，这种方法能够为模型提供额外的训练信号。

    

    大量人工标注的基准数据集推动了深度神经网络在各种自然语言处理任务中的成功。为了增强现有基准数据集的效果，收集新的输入输出对通常过于昂贵和具有挑战性，特别是考虑到它们对提高当前模型精度的边际影响。相反，对基准数据集中现有输入文本的附加或补充标注可能是一种有效的额外人工成本支付方式。本文研究了任务特定的输入文本对之间的偏好关系作为这种辅助数据标注的新替代方式。从任务相关的“成对”比较中，辅助偏好学习使模型学习到一种额外的信息性训练信号，这种信号无法通过“实例级”的任务标签来捕捉。为此，我们提出了一种新的多任务学习框架，称为prefer-to-classify （P2C），它可以同时享受实例级和偏好级任务。

    The development of largely human-annotated benchmarks has driven the success of deep neural networks in various NLP tasks. To enhance the effectiveness of existing benchmarks, collecting new additional input-output pairs is often too costly and challenging, particularly considering their marginal impact on improving the current model accuracy. Instead, additional or complementary annotations on the existing input texts in the benchmarks can be preferable as an efficient way to pay the additional human cost. In this paper, we investigate task-specific preferences between pairs of input texts as a new alternative way for such auxiliary data annotation. From 'pair-wise' comparisons with respect to the task, the auxiliary preference learning enables the model to learn an additional informative training signal that cannot be captured with 'instance-wise' task labels. To this end, we propose a novel multi-task learning framework, called prefer-to-classify (P2C), which can enjoy the cooperati
    
[^37]: COLIEE 2023比赛中的NOWJ团队——法律信息处理中的多任务和集成方法

    NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal Information Processing. (arXiv:2306.04903v1 [cs.CL])

    [http://arxiv.org/abs/2306.04903](http://arxiv.org/abs/2306.04903)

    NOWJ团队在COLIEE 2023比赛中采用多任务和集成方法提高法律信息处理技术，虽未达最佳结果但为未来改进提供了有价值见解。

    

    本文介绍了NOWJ团队参加COLIEE 2023比赛所采用的方法，重点是提高法律信息处理技术并将其应用于实际的法律情景中。我们团队着手处理比赛中的四个任务，包括法律案例检索、法律案例蕴含、法规检索和法律文本蕴含。我们采用了最先进的机器学习模型和创新方法，例如BERT、Longformer、BM25排序算法和多任务学习模型。尽管我们的团队没有达到最先进的结果，但我们的发现提供了有价值的见解，并为未来的法律信息处理改进铺平了道路。

    This paper presents the NOWJ team's approach to the COLIEE 2023 Competition, which focuses on advancing legal information processing techniques and applying them to real-world legal scenarios. Our team tackles the four tasks in the competition, which involve legal case retrieval, legal case entailment, statute law retrieval, and legal textual entailment. We employ state-of-the-art machine learning models and innovative approaches, such as BERT, Longformer, BM25-ranking algorithm, and multi-task learning models. Although our team did not achieve state-of-the-art results, our findings provide valuable insights and pave the way for future improvements in legal information processing.
    
[^38]: 基于贝叶斯原理的上下文学习

    In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])

    [http://arxiv.org/abs/2306.04891](http://arxiv.org/abs/2306.04891)

    这篇论文研究了大型语言模型中的上下文学习现象，并通过实验证据展示了Transformer模型在多种设置下表现出贝叶斯预测器的行为。作者还探讨了上下文学习与贝叶斯学习框架之间的联系，并提出了一个线性回归任务来验证这种联系。

    

    上下文学习是大型语言模型中令人惊讶且有用的特性之一。它的工作原理是一个活跃的研究领域。近期，人们设计了一些风格化的类元学习的设置，它们使用语言建模损失函数对来自函数类的输入输出对$(x, f(x))$ 进行训练，并观察模型对同一类中未见过的函数的泛化能力。这一研究线路中的一个主要发现是，对于诸如线性回归等几个问题，训练好的 Transformer 学习了上下文学习算法。然而，导致这种行为的归纳偏差并不清楚。拥有无限的训练数据和计算能力的模型是贝叶斯预测器：它学习了预训练分布。已经证明，高容量的 Transformer 模型在线性回归任务上模拟贝叶斯预测器的行为。在本文中，我们展示了Transformer在多种设置下表现出理想学习者的行为的经验证据，包括外推和求解微分方程。我们探讨了上下文学习和贝叶斯学习框架之间的联系，认为这些模型学习了合理函数的先验概率，而不仅仅是最小化语言建模损失。最后，我们提出了一个简单的线性回归任务来进一步探究这种联系，证明使用真实的贝叶斯先验进行训练的模型比使用固定先验或没有先验训练的模型表现更好。

    In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learne
    
[^39]: 扩大范围：将英文对抗攻击方法适应到中文上的研究

    Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])

    [http://arxiv.org/abs/2306.04874](http://arxiv.org/abs/2306.04874)

    本文研究将英文的对抗攻击方法适用于中文上，并证明了这些方法可以生成高质量的中文对抗实例。通过关注中文的语言特点，生成的对抗实例可以实现高流畅度和语义一致性，从而可以用来提高中文NLP模型的对抗鲁棒性。

    

    最近的研究表明，自然语言处理(NLP)的预测模型容易受到对抗攻击。多数现有的研究着眼于设计攻击方式来评估英语语境下的NLP模型的鲁棒性。然而学术界对其它语言的NLP解决方案需求日益增长。因此，我们自然产生一个问题：当前最先进的对抗攻击方法是否能够泛化到其它语言中？本文研究了如何将在英文环境下的最先进的对抗攻击算法适应到中文上。我们的实验表明，当结合正确的文本分割和语言限制时，先前针对英文NLP的攻击方法也能够在中文中生成高质量的对抗性例子。此外，我们还证明了，通过关注中文的形态和音系，生成的对抗实例可以实现高流畅度和语义一致性，从而可以用来提高中文NLP模型的对抗鲁棒性。

    Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question: whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and semantic consistency by focusing on the Chinese language's morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.
    
[^40]: 混合超网络：通过基于结构路由的专家混合改进共享权重超网络训练

    Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts. (arXiv:2306.04845v1 [cs.CL])

    [http://arxiv.org/abs/2306.04845](http://arxiv.org/abs/2306.04845)

    该论文提出了一种混合超网络方法，通过基于结构路由的专家混合来增强超级网络模型的表达能力，改善了子网络的质量问题和性能差异。

    

    共享权重的超级网络已经成为当前最先进的神经体系结构搜索（NAS）框架中性能评估的关键组成部分。然而，由于权重共享，超级网络直接生成的不同子网络的质量无法保证。在机器翻译和预训练语言建模等NLP任务中，我们观察到，在相同的模型架构下，超级网络与从头开始训练之间存在较大的性能差距。因此，在找到最佳架构后，不能直接使用超级网络，必须进行重新训练。我们提出了混合超网络，这是一种广义的超级网络公式，其中采用了专家混合（MoE）来增强超级网络模型的表达能力，训练开销可以忽略不计。通过这种方式，不同的子网络不是直接共享模型权重，而是通过基于结构的路由机制共享。因此，模型性能得到了改善。

    Weight-sharing supernet has become a vital component for performance estimation in the state-of-the-art (SOTA) neural architecture search (NAS) frameworks. Although supernet can directly generate different subnetworks without retraining, there is no guarantee for the quality of these subnetworks because of weight sharing. In NLP tasks such as machine translation and pre-trained language modeling, we observe that given the same model architecture, there is a large performance gap between supernet and training from scratch. Hence, supernet cannot be directly used and retraining is necessary after finding the optimal architectures.  In this work, we propose mixture-of-supernets, a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model, with negligible training overhead. In this way, different subnetworks do not share the model weights directly, but through an architecture-based routing mechanism. As a result, model 
    
[^41]: 基于自动数据增强的越南法律问答系统的改进

    Improving Vietnamese Legal Question--Answering System based on Automatic Data Enrichment. (arXiv:2306.04841v1 [cs.CL])

    [http://arxiv.org/abs/2306.04841](http://arxiv.org/abs/2306.04841)

    本文针对越南法律问答系统中标记数据稀缺和预训练语言模型有限的问题，实现了一种基于检索的法律问答系统，并提出了一种通过弱标注提高数据质量从而提高语言模型性能的方法。

    

    在法律中进行问题回答是一个具有挑战性的问题，因为法律文件在术语、结构以及时间和逻辑关系方面比普通文本要复杂得多。尤其对于越南这样资源稀少的语言来说，进行法律问答更加困难，因为标记数据稀缺且预训练语言模型仍然有限。本文试图通过实现越南基于检索的文章级法律问答系统，并介绍一种新的方法来提高语言模型的性能，通过弱标注提高数据质量来克服这些限制。我们的假设是，在标记数据有限的情况下，有效的数据增强可以帮助提高整体性能。我们的实验旨在测试多个方面，证明所提出技术的有效性。

    Question answering (QA) in law is a challenging problem because legal documents are much more complicated than normal texts in terms of terminology, structure, and temporal and logical relationships. It is even more difficult to perform legal QA for low-resource languages like Vietnamese where labeled data are rare and pre-trained language models are still limited. In this paper, we try to overcome these limitations by implementing a Vietnamese article-level retrieval-based legal QA system and introduce a novel method to improve the performance of language models by improving data quality through weak labeling. Our hypothesis is that in contexts where labeled data are limited, efficient data enrichment can help increase overall performance. Our experiments are designed to test multiple aspects, which demonstrate the effectiveness of the proposed technique.
    
[^42]: 改进对话系统中尾流量鲁棒性的数据增强技术

    Data Augmentation for Improving Tail-traffic Robustness in Skill-routing for Dialogue Systems. (arXiv:2306.04823v1 [cs.CL])

    [http://arxiv.org/abs/2306.04823](http://arxiv.org/abs/2306.04823)

    提出了一种针对技能路由中长尾数据的异构数据增强和训练方法，可以显著提高尾部请求的技能路由性能，同时在头部请求中保持相对较高的准确性。

    

    大规模对话系统通常依赖于技能路由组件来将用户请求路由到适当的技能和解释中以服务请求。在这样的系统中，代理负责为数千个技能和解释提供服务，级别分布呈长尾分布。因此，为了提高技能路由的鲁棒性，我们提出了一种异构技能路由数据的增强和训练方法，针对长尾数据范围的可靠操作。我们探索了各种有条件的编码器-解码器生成框架，以扰动原始数据字段并创建合成翻译，同时保留用户请求的语义含义。我们的实验表明，所提出的数据增强方法可以显着提高尾部请求的技能路由性能，同时在头部请求中保持相对较高的准确性。

    Large-scale conversational systems typically rely on a skill-routing component to route a user request to an appropriate skill and interpretation to serve the request. In such system, the agent is responsible for serving thousands of skills and interpretations which create a long-tail distribution due to the natural frequency of requests. For example, the samples related to play music might be a thousand times more frequent than those asking for theatre show times. Moreover, inputs used for ML-based skill routing are often a heterogeneous mix of strings, embedding vectors, categorical and scalar features which makes employing augmentation-based long-tail learning approaches challenging. To improve the skill-routing robustness, we propose an augmentation of heterogeneous skill-routing data and training targeted for robust operation in long-tail data regimes. We explore a variety of conditional encoder-decoder generative frameworks to perturb original data fields and create synthetic tra
    
[^43]: 好数据、大数据还是无数据？——在开发生物医学论文研究方面分类器方面比较三种方法

    Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers. (arXiv:2306.04820v1 [cs.CL])

    [http://arxiv.org/abs/2306.04820](http://arxiv.org/abs/2306.04820)

    本研究探讨了使用不同数据集和大型语言模型在生物医学论文研究方面分类中的作用。结果表明，使用PubMed 200K RCT数据集并不能改善性能。与此同时，尽管GPT-4表现良好，但它并没有超越fine-tuned在CODA-19数据集上的SciBERT模型，这强调了LLMs的重要性。

    

    科学出版物的快速增长，特别是在COVID-19大流行期间，强调了需要帮助研究人员有效理解最新进展的工具。了解科学文献的一个关键部分是研究方面分类，它将摘要中的句子分类为背景、目的、方法和发现。在这项研究中，我们研究了不同数据集对众包注释的CODA-19研究方面分类任务模型性能的影响。具体来说，我们探讨了使用大规模的、自动策划的PubMed 200K RCT数据集的潜在优点，并评估了大型语言模型（LLMs）如LLaMA、GPT-3、ChatGPT和GPT-4的效果。我们的结果表明，对于CODA-19任务，使用PubMed 200K RCT数据集并不能改善性能。我们还观察到，尽管GPT-4表现良好，但它并没有超越fine-tuned在CODA-19数据集上的SciBERT模型，这强调了LLMs的重要性。

    The rapid growth of scientific publications, particularly during the COVID-19 pandemic, emphasizes the need for tools to help researchers efficiently comprehend the latest advancements. One essential part of understanding scientific literature is research aspect classification, which categorizes sentences in abstracts to Background, Purpose, Method, and Finding. In this study, we investigate the impact of different datasets on model performance for the crowd-annotated CODA-19 research aspect classification task. Specifically, we explore the potential benefits of using the large, automatically curated PubMed 200K RCT dataset and evaluate the effectiveness of large language models (LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that using the PubMed 200K RCT dataset does not improve performance for the CODA-19 task. We also observe that while GPT-4 performs well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance 
    
[^44]: 使用语言模型私下生成表格数据

    Privately generating tabular data using language models. (arXiv:2306.04803v1 [cs.LG])

    [http://arxiv.org/abs/2306.04803](http://arxiv.org/abs/2306.04803)

    通过使用语言模型训练每一行数据作为一个句子并添加差分隐私，可以在多个数据集中生成具有竞争力的合成数据，以实现私人数据生成。

    

    在以隐私为先的世界中，私下生成合成数据是非常重要的。我们提出并研究了一种简单的方法，将表格中的每一行视为一个句子，并训练差分隐私语言模型。我们展示了这种方法在建模多个数据集的表格数据时取得了具有竞争力的结果，即使是在有利于基于边缘分布的替代方法的小规模情况下也是如此。

    Privately generating synthetic data from a table is an important brick of a privacy-first world. We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy. We show this approach obtains competitive results in modelling tabular data across multiple datasets, even at small scales that favor alternative methods based on marginal distributions.
    
[^45]: 医疗知识图谱综述：资源、应用和前景

    A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])

    [http://arxiv.org/abs/2306.04802](http://arxiv.org/abs/2306.04802)

    本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。

    

    医疗知识图谱(HKGs)已成为组织医学知识的有结构且可解释的有为工具，提供了医学概念及其关系的全面视图。然而，数据异质性和覆盖范围有限等挑战仍然存在，强调了在HKG领域需要进一步研究的必要性。本综述是HKG的第一份综合概述。我们总结了HKG构建的流程和关键技术（即从头开始和通过集成），以及常见的利用方法（即基于模型和非基于模型）。为了为研究人员提供有价值的资源，我们根据它们捕获的数据类型和应用领域（该资源存储于https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase）组织了现有的HKG，并提供了相关的统计信息。在应用部分，我们深入探讨了HKG在各种医疗领域的变革性影响。

    Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
    
[^46]: 基于Transformer的无监督多文档抽象摘要模型

    Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])

    [http://arxiv.org/abs/2306.04787](http://arxiv.org/abs/2306.04787)

    本文提出了一种新的用于无监督多文档抽象摘要生成的Transformer-based方法Absformer，它使用掩码语言建模（MLM）目标进行预训练聚类文档并生成抽象摘要，实验结果显示，它在基准数据集上优于几种最先进的无监督抽象MDS方法。

    

    多文档摘要（MDS）是将多个文档中的文本总结成简洁概括的任务。所生成的摘要通过用少数几句话提供重要内容，可以省去阅读多个文档的时间。抽象MDS旨在使用自然语言生成技术为多个文档生成连贯、流畅的摘要。本文考虑仅有文档而没有摘要的无监督抽象MDS环境，并提出Absformer，这是一种用于无监督抽象摘要生成的新型基于Transformer的方法。我们的方法包括第一步，使用掩码语言建模（MLM）目标作为预训练任务，以训练Transformer编码器，将文档聚类为语义相似的组；第二步，训练一个Transformer解码器，为文档集群生成抽象摘要。据我们所知，这是第一份提出将聚类用于文档组合的无监督抽象MDS方法的工作。我们在基准数据集上的实验结果显示，我们的方法优于几种最先进的无监督抽象MDS方法。

    Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge
    
[^47]: 公开部署研究聊天机器人的人机交互方面：一项用户研究、设计建议和开放挑战

    The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges. (arXiv:2306.04765v1 [cs.AI])

    [http://arxiv.org/abs/2306.04765](http://arxiv.org/abs/2306.04765)

    本文研究研究聊天机器人的公开部署，发现代理人的抽象拟人化表现影响用户感知，AI可解释性可能影响反馈率，聊天体验的两种水平应有意设计。此研究提供了设计建议和研究方向。

    

    公开部署研究聊天机器人是一个涉及必要的风险与收益分析的微妙话题。虽然最近频繁讨论是否负责任地部署此类模型，但对于实现更有效目标的交互范式和设计方法却关注较少。我们通过报告对最近研究聊天机器人进行的混合方法用户研究，力图提出、基于并尝试回答涉及此范围的人机交互问题。我们发现，代理人的抽象拟人化表现对用户的感知有显著影响，提供AI可解释性可能会对反馈率产生影响，而聊天体验的两种水平（故事内和故事外）应有意设计。我们提供设计建议和研究社区进一步关注的领域。

    Publicly deploying research chatbots is a nuanced topic involving necessary risk-benefit analyses. While there have recently been frequent discussions on whether it is responsible to deploy such models, there has been far less focus on the interaction paradigms and design approaches that the resulting interfaces should adopt, in order to achieve their goals more effectively. We aim to pose, ground, and attempt to answer HCI questions involved in this scope, by reporting on a mixed-methods user study conducted on a recent research chatbot. We find that abstract anthropomorphic representation for the agent has a significant effect on user's perception, that offering AI explainability may have an impact on feedback rates, and that two (diegetic and extradiegetic) levels of the chat experience should be intentionally designed. We offer design recommendations and areas of further focus for the research community.
    
[^48]: INSTRUCTEVAL：面向指导调整的大型语言模型的整体评估

    INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])

    [http://arxiv.org/abs/2306.04757](http://arxiv.org/abs/2306.04757)

    INSTRUCTEVAL是一个专注于指导调整的大型语言模型评估的综合套件，它采取了全面的方法来评估模型的性能，包括解决问题、写作能力和与人类价值观的一致性等特征。

    

    指导调整的大型语言模型已经从根本上改变了自然语言处理，已经在诸如对话代理等应用中显示出了巨大的潜力。这些模型，如GPT-4，不仅能够掌握语言，而且可以解决数学、编码、医学和法律等领域的复杂任务。尽管它们具有卓越的能力，但由于许多模型的黑盒性质和缺乏全面的评估研究，对它们的全部潜力仍然缺乏全面的理解。为了解决这些挑战，我们提出了INSTRUCTEVAL，一个更全面的评估套件，专门针对指导调整的大型语言模型。与以往的作品不同，我们的评估包括对模型基于解决问题、写作能力和与人类价值观的一致性的严格评估。我们采取了全面的方法来分析影响模型性能的各种因素，包括预训练基础、指导调整数据和训练。

    Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
    
[^49]: 骆驼能走多远？探索开放资源中指令调优的现状。

    How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])

    [http://arxiv.org/abs/2306.04751](http://arxiv.org/abs/2306.04751)

    本文探究了指令调优语言模型在一系列开放指令跟随数据集上的最新进展，提供了一组大型指令调优模型，并进行了系统评估。实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。

    

    本研究探索了指令调优语言模型在一系列开放指令跟随数据集上的最新进展。尽管最近声称开放模型可以与最先进的专有模型相媲美，但这些声称常常伴随着有限的评估，使得难以全面比较模型并确定各种资源的效用。我们提供了一组大型指令调优模型，大小为6.7B到65B个参数，在12个指令数据集上进行训练，包括手动策划的（例如OpenAssistant）和综合的指令数据集（例如Alpaca），并通过一系列自动、基于模型和基于人的指标对其在事实知识、推理、多语言、编码和开放式指令跟随能力方面进行系统评估。我们进一步介绍了T\"ulu，我们在高质量开放资源组合上微调的表现最佳的指令调优模型组合。我们的实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。

    In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.  Our experiments show that different instru
    
[^50]: 使用大型语言模型注释进行社会科学中的有效下游统计推断: 基于设计的半监督学习

    Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])

    [http://arxiv.org/abs/2306.04746](http://arxiv.org/abs/2306.04746)

    该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。

    

    在计算社会科学（CSS）中，研究人员通过分析文档来解释社会和政治现象。在大多数情况下，CSS研究人员首先获取文档的标签，然后使用可解释的回归分析来解释标签。大型语言模型（LLMs）的最近进展可以通过在规模上便宜地注释文档来降低CSS研究成本，但这些替代标签通常是不完美和有偏的。我们提出了一种新算法，用于使用LLMs的输出进行下游统计分析，同时保证与CSS研究基本相关的统计属性-如渐近无偏性和正确的不确定性量化。我们表明，直接在下游统计分析中使用LLM预测的替代标签会导致实质性偏差和无效置信区间，即使替代准确性高达80-90％。为了解决这个问题，我们基于无偏机器学习提出了基于设计的半监督学习（D-SSL）算法，该算法将LLM注释与有针对性的采样相结合，以实现有效的下游统计推断。我们的方法可以将标签获取的CSS研究成本降低80％，而不影响统计分析的有效性。模拟研究和实际数据示例表明，与直接使用LLM预测标签相比，D-SSL可以将回归估计的准确性提高多达40％。

    In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
    
[^51]: 大型语言模型的软提示调整方法用于评估偏差

    Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])

    [http://arxiv.org/abs/2306.04735](http://arxiv.org/abs/2306.04735)

    本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。

    

    近年来，大型语言模型的提示功能因无需标记数据即可产生良好结果而备受青睐。然而，这需要进行提示调整以获得引导更好模型性能的最佳提示。本文中，我们探讨了在情感分类任务中使用软提示调整来量化大型语言模型（LLMs）如Open Pre-trained Transformers（OPT）和Galactica语言模型中的偏差。由于这些模型是在可能偏向某些人群的真实数据上训练的，因此识别这些潜在问题非常重要。使用软提示来评估偏差给我们带来了额外的优势，可以避免手动设计提示导致的人为偏差注入。我们使用分组公平性（偏差）检查模型对不同敏感属性的偏见，并找到了有趣的偏差模式。由于LLMs已在各种应用中被用于工业中，因此我们对其进行的偏见评估具有实际意义。

    Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
    
[^52]: Prompter:用于对话状态跟踪领域自适应的零样本自适应前缀

    Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation. (arXiv:2306.04724v1 [cs.CL])

    [http://arxiv.org/abs/2306.04724](http://arxiv.org/abs/2306.04724)

    本论文提出了Prompter方法，利用目标领域槽的描述生成动态前缀来进行自适应前缀调整，实现对话状态跟踪领域自适应。通过分析发现，Prompter除了利用槽描述的语义，还能考虑到槽的频率。实验表明，在区分“无”值对话槽方面，Prompter相比基准方法表现更好。

    

    对话状态跟踪领域的挑战是在不使用任何监督数据的情况下使模型适应新域，即零样本领域自适应。参数高效的转移学习（PETL）具有解决此问题的潜力，但尚未应用于零样本场景，因为如何无监督地应用尚不清楚。我们的方法Prompter使用目标领域槽的描述生成动态前缀，并将其连接到每个层的self-attention机制中的键和值上。这允许在零样本中使用前缀调整。Prompter在MultiWOZ和SGD基准测试中表现优异。在生成前缀时，我们的分析发现Prompter不仅利用了槽描述的语义，而且还考虑到了对话中槽一起出现的频率。此外，与基准相比，Prompter的收益在于其能够更好地区分“无”值对话槽。

    A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data, zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly.  Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish "none"-valued dialogue slots, compared against baselines.
    
[^53]: 鲁棒性AI生成文本检测的内部维度估计

    Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])

    [http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723)

    本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。

    

    快速提高的AI生成内容的质量使得很难区分人类和AI生成的文本，这可能会对社会产生不良影响。因此，研究人类文本的不变属性变得越来越重要。本文提出了一种人类文本的不变特征，即给定文本样本嵌入集合下的流形的内部维度。我们展示了自然语言流畅文本的平均内部维度在几个基于字母的语言中约为 $9$，而中文约为 $7$，而每种语言的AI生成文本的平均内部维度较低，差约 $1.5$，并且有明显的统计分离。

    Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
    
[^54]: 通过学习有机交互，改进开放式语言模型

    Improving Open Language Models by Learning from Organic Interactions. (arXiv:2306.04707v1 [cs.CL])

    [http://arxiv.org/abs/2306.04707](http://arxiv.org/abs/2306.04707)

    BlenderBot 3x是一个更新版本的会话模型，通过参与者的有机对话和反馈数据进行训练，以改进其技能和安全性，技术上通过学习有益的教师避免学习有毒反馈。

    

    我们提出了BlenderBot 3x，它是会话模型BlenderBot 3的一个更新版本，现在通过参与者的有机对话和反馈数据进行训练，以改进其技能和安全性。我们公开发布了参与者匿名交互数据，供研究社区使用，以促进进一步的进展。使用有机数据训练模型是具有挑战性的，因为与人们在“野外”的互动包括高质量的对话和反馈，以及对抗性和有毒的行为。我们研究了一些技术，使模型能够从有益的教师学习，同时避免从试图将模型诱导为无用或有毒反应的人中学习。BlenderBot 3x在对话中比BlenderBot 3更受欢迎，并在挑战性情况下显示出更安全的响应。虽然我们目前的模型仍远非完美，但我们相信通过继续使用我们提出的技术以及探索从有机交互中学习的新方法，可以进一步改进。

    We present BlenderBot 3x, an update on the conversational model BlenderBot 3, which is now trained using organic conversation and feedback data from participating users of the system in order to improve both its skills and safety. We are publicly releasing the participating de-identified interaction data for use by the research community, in order to spur further progress. Training models with organic data is challenging because interactions with people "in the wild" include both high quality conversations and feedback, as well as adversarial and toxic behavior. We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses. BlenderBot 3x is both preferred in conversation to BlenderBot 3, and is shown to produce safer responses in challenging situations. While our current models are still far from perfect, we believe further improvement can be achieved by continued use of the techniq
    
[^55]: ConceptBed: 评估文本到图像扩散模型的概念学习能力

    ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])

    [http://arxiv.org/abs/2306.04695](http://arxiv.org/abs/2306.04695)

    本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。

    

    理解视觉概念并从图像中复制和组合这些概念的能力是计算机视觉的一个核心目标。最近文本到图像（T2I）模型的进展使得通过学习大量图像及其描述来生成高清晰度和逼真的图像质量成为可能。然而，评估T2I模型的重点在于照片般的真实感和有限的视觉理解定性量度。为了量化T2I模型在学习和合成新的视觉概念方面的能力，我们引入了ConceptBed，一个包含284个独特视觉概念、5K个独特概念组合和33K个组合文本提示的大规模数据集。除了数据集，我们提出了一个评估指标Concept Confidence Deviation（CCD），它利用oracle概念分类器的置信度来衡量T2I生成器生成的概念与地面真实图像中包含的概念之间的对齐度。我们评估的视觉概念是对象或者...

    The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
    
[^56]: 动态注入常识知识提升共情对话生成

    Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge. (arXiv:2306.04657v1 [cs.CL])

    [http://arxiv.org/abs/2306.04657](http://arxiv.org/abs/2306.04657)

    本文提出了一种动态注入常识知识的共情式对话生成方法，使用自适应模块选择常识知识以确保生成的对话回应和说话者情况的一致性，其表现优于基准模型。

    

    在共情对话中，个体表达对他人的共情。以往的工作主要依靠说话人的情感生成共情式回应。此外，外部的常识知识也可用于增强系统对语境的理解。然而，常识知识库中包含各种关系，可能导致对话系统的混淆。因此，情感、生成的回应和说话者的情境信息之间存在不一致性。因此，我们提出了一种新的方法，用于共情式回应生成，其中包括一个自适应模块用于常识知识选择，以确保所生成的共情式回应和说话者的情况之间的一致性。选择的知识用于调整生成的响应的常识认知和共情表达。实验结果表明，我们的方法明显优于基准模型。

    In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker's emotion. Besides, external commonsense knowledge has been applied to enhance the system's understandings of the speaker's situation. However, given an event, commonsense knowledge base contains various relations, potentially leading to confusion for the dialogue system. Consequently, inconsistencies arise among the emotion, generated response and speaker's contextual information. To this end, we propose a novel approach for empathetic response generation, which incorporates an adaptive module for commonsense knowledge selection to ensure consistency between the generated empathetic responses and the speaker's situation. This selected knowledge is used to refine the commonsense cognition and empathy expression for generated responses. Experimental results show that our approach significantly outperforms baseline mod
    
[^57]: M$^3$IT: 多模态多语种指令调整的大规模数据集

    M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v1 [cs.CV])

    [http://arxiv.org/abs/2306.04387](http://arxiv.org/abs/2306.04387)

    M$^3$IT数据集旨在优化开放式视觉语言模型（VLM）与人类指令的对齐，是一个大规模、多模态和多语种的数据集。

    

    指令调整已经显著推进了像ChatGPT这样的大型语言模型（LLMs），使它们能够在各种任务中与人类指令保持一致。然而，由于高质量指令数据集的稀缺性，开放式视觉语言模型（VLM）的进展一直受到限制。为了解决这个挑战，并促进视觉语言领域的研究，我们介绍了多模态多语言指令调整（M$^3$IT）数据集，旨在优化VLM与人类指令的对齐。我们的M$^3$IT数据集包括40个精心策划的数据集，包括240万个实例和400个手动编写的任务指令，格式化为视觉到文本结构。重要任务被翻译成80种语言，使用先进的翻译系统，确保更广泛的可访问性。M$^3$IT在任务覆盖范围、指令数量和实例规模方面超过了以前的数据集。此外，我们开发了Ying-VLM，它是在我们的M$^3$IT数据集上训练的VLM模型，展示了其潜在的有效性。

    Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potenti
    
[^58]: GPT自监督学习在数据标注中的应用

    GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v1 [cs.CL])

    [http://arxiv.org/abs/2306.04349](http://arxiv.org/abs/2306.04349)

    本文提出了一种基于GPT自监督学习的数据标注方法，将生成-恢复范式与GPT的一次性学习能力相结合，可显著提高数据标注的覆盖率、准确性和易读性。

    

    在各个领域中，将数据注释为简洁的摘要是一个巨大的挑战，常常需要人类专家投入大量时间和专业知识。尽管现有的大型语言模型在注释任务中的使用已经有所尝试，但仍存在诸如不适用于无标签数据、缺乏自监督方法、缺乏对复杂结构化数据的关注等重要问题。本文提出了一种基于GPT自监督学习的数据标注方法。该方法采用生成-恢复范式，利用生成预训练转换器（GPT）中的一次性学习能力。该方法包括一次性调整阶段和生成阶段。在一次性调整阶段，我们从支持集中抽取数据作为GPT生成文本摘要的一部分，然后用该摘要恢复原始数据。恢复数据与原始数据的对齐分数用于微调GPT权重。在生成阶段，我们使用相同基于GPT的方法为无标签数据生成新的文本摘要。在两个不同的数据集上进行实验表明，我们的方法在覆盖率、准确性和易读性方面具有显著的优势，比现有的标注方法更加有效。

    The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method. This method embodies a generating-recovering paradigm that leverages the capabilities of one-shot learning capabilities in Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and or
    
[^59]: 基于 CMExam 的大型语言模型评测——一份综合的中国医学考试数据集

    Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset. (arXiv:2306.03030v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03030](http://arxiv.org/abs/2306.03030)

    该研究介绍了 CMExam 数据集，这是一个综合的、来自于中国国家医疗执业考试的数据集，为评估大型语言模型提供了一个标准化且客观的方法。在 CMExam 上，GPT-4 表现最好，这表明大型语言模型在医学领域有巨大潜力。

    

    最近大型语言模型 (LLM) 的进步已经改变了问答领域，然而，由于缺乏标准化和全面性的数据集，在医学领域对 LLM 进行评估具有挑战性。为了弥补这一空白，我们介绍了 CMExam，它来自中国国家医疗执业考试，由60,000多个选择题和模型推理解释的答案解析构成，可进行标准化和客观化的评估。为了深入分析 LLM，我们邀请医学专业人士对五个额外的问题逐个进行标注，包括疾病组、临床科室、医学学科、能力领域和难度级别。除了数据集外，我们还在 CMExam 上对代表性的 LLM 和 QA 算法进行了彻底的实验。结果表明，GPT-4 的准确度最高，为61.6％，加权 F1 分数为0.617。这些结果突显了大型语言模型在医学领域的巨大潜力以及标准化数据集对评估其性能的重要性。

    Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great 
    
[^60]: 通过反事实数据模拟提高对话式推荐系统

    Improving Conversational Recommendation Systems via Counterfactual Data Simulation. (arXiv:2306.02842v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2306.02842](http://arxiv.org/abs/2306.02842)

    本文提出了一种针对对话式推荐系统的反事实数据模拟方法CFCRS，以缓解由于数据不足而导致的训练不足问题。

    

    对话式推荐系统（CRSs）旨在通过自然语言对话提供推荐服务。虽然已经有多种方法用于开发有能力的CRSs，但通常需要足够的训练数据进行训练。由于难以注释面向推荐的对话数据集，现有的CRSs方法通常因训练数据的稀缺而受到不足的训练数据的问题。为了解决这个问题，在本文中，我们提出了一个名为CFCRS的CRS的反事实数据模拟方法，以减缓CRSs中数据不足的问题。我们的方法是基于反事实数据增强框架开发的，该框架逐步将重写到真实对话中的用户偏好中，而不干扰整个对话流程。为了开发我们的方法，我们通过涉及对话的实体来对用户偏好进行表征并组织对话流程，并设计了多阶段方法。

    Conversational recommender systems (CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data. To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage 
    
[^61]: UNIDECOR: 用于跨语料欺诈检测的统一欺诈语料库

    UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception Detection. (arXiv:2306.02827v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02827](http://arxiv.org/abs/2306.02827)

    统一欺诈语料库为构建跨域泛化的欺诈检测系统提供了支持。

    

    认知语言学、心理学和法医学研究了口头欺骗的许多方面，旨在理解行为模式、鉴别虚假证言和检测在线交流中的欺诈行为。不同研究领域对动机的不同选择导致欺诈概念的不同，使得比较模型和为特定语言构建强大的欺诈检测系统变得困难。本文通过调查包括社交媒体评论、法庭证言、特定话题上的意见陈述和在线策略游戏中的欺骗对话等不同领域的可用英语欺诈数据集，将这些数据集统一到一个语料库中。基于这个资源，我们进行了欺诈语言提示的交叉语料建模实验和相关性分析，结果显示用统一的语料库进行训练可实现跨域泛化。

    Verbal deception has been studied in psychology, forensics, and computational linguistics for a variety of reasons, like understanding behaviour patterns, identifying false testimonies, and detecting deception in online communication. Varying motivations across research fields lead to differences in the domain choices to study and in the conceptualization of deception, making it hard to compare models and build robust deception detection systems for a given language. With this paper, we improve this situation by surveying available English deception datasets which include domains like social media reviews, court testimonials, opinion statements on specific topics, and deceptive dialogues from online strategy games. We consolidate these datasets into a single unified corpus. Based on this resource, we conduct a correlation analysis of linguistic cues of deception across datasets to understand the differences and perform cross-corpus modeling experiments which show that a cross-domain ge
    
[^62]: BabySLM: 自我监督口语语言模型的语言习得友好型基准

    BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models. (arXiv:2306.01506v1 [cs.CL])

    [http://arxiv.org/abs/2306.01506](http://arxiv.org/abs/2306.01506)

    本文提出了一种语言习得友好型基准，以检验自我监督口语语言模型在儿童词汇和句法经历中的表现，并提出了两个需要解决的挑战：文本和语音之间的差距和干净语音和野外语音之间的差距。

    

    已经证明，学习语音表示的自我监督技术能够从听到的语音中发展出语言能力，而无需人类标签。为了充分发挥这些方法的潜力并进一步了解婴儿学习语言的方式，模拟必须紧密模仿现实情况，通过在开发上符合儿童语言经验典型词汇库和对应测试集进行基准测试。为此，我们提出了一种用于检测在词汇和句法层面上的口语语言模型的语言习得友好型基准。本文介绍了此基准，并总结了一系列实验，证明其有用性。此外，我们还强调了需要解决的两个挑战：填补文本和语音之间以及干净语音和野外语音之间的差距。

    Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.
    
[^63]: 基于GPT-4的复杂数学问题求解的实证研究

    An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v1 [cs.CL])

    [http://arxiv.org/abs/2306.01337](http://arxiv.org/abs/2306.01337)

    本研究探索使用GPT-4解决更复杂和有挑战性的数学问题，提出了一种名为MathChat的对话式问题求解框架，并在困难高中竞赛问题上进行了评估。

    

    使用大型语言模型（LLM）来解决数学问题是一项有趣的研究，考虑到在各种科学和工程领域中用自然语言表达的数学问题的丰富性。虽然之前有几项工作研究了使用LLM解决初等数学问题，但本研究探索了使用GPT-4解决更复杂和有挑战性的数学问题的前沿。我们评估了使用GPT-4的各种方法。其中一些是从现有工作中改编而来的，其中一个是MathChat，这是本研究新提出的一种对话式问题求解框架。我们在来自MATH数据集的困难高中竞赛问题上进行评估，表明了所提出的对话式方法的优势。

    Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. While several prior works have investigated solving elementary mathematics using LLMs, this work explores the frontier of using GPT-4 for solving more complex and challenging math problems. We evaluate various ways of using GPT-4. Some of them are adapted from existing work, and one is \MathChat, a conversational problem-solving framework newly proposed in this work. We perform the evaluation on difficult high school competition problems from the MATH dataset, which shows the advantage of the proposed conversational approach.
    
[^64]: 基准数据集上 ChatGPT 的系统研究和全面评估

    A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])

    [http://arxiv.org/abs/2305.18486](http://arxiv.org/abs/2305.18486)

    本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。

    

    最近，如 ChatGPT 这样的大型语言模型（LLM）的开发引起了很多关注。然而，由于难以将该模型生成的产出与基本事实进行比较，因此其在基准学术数据集上的评估仍未充分探索。本文旨在对 ChatGPT 在包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务中的表现进行彻底评估。具体而言，我们在 140 个任务中评估了 ChatGPT，并分析了其在这些数据集中生成的 255K 次响应，这使我们的工作成为了在 NLP 基准测试中对 ChatGPT 进行的最大评估。简而言之，我们的研究旨在验证 ChatGPT 在各种任务中的优势和弱点，并为使用 LLM 的未来研究提供见解。我们还报告了一种新的迸发能力，即遵循多个查询指令。

    The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
    
[^65]: 基于白化的对比学习句子嵌入

    Whitening-based Contrastive Learning of Sentence Embeddings. (arXiv:2305.17746v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17746](http://arxiv.org/abs/2305.17746)

    本文提出一种基于白化的对比学习方法用于学习句子嵌入，它将对比学习与白化方法结合起来，同时具备更好的一致性和更好的对齐效果。

    

    本文提出了一种基于白化的对比学习方法用于学习句子嵌入（WhitenedCSE），它将对比学习与一种新颖的洗牌组白化结合起来。通常，对比学习拉近一个样本的扭曲（即正样本）并将负样本远离，从而促进特征空间的对齐和一致性。"推"操作的另一种流行替代方案是白化特征空间，它散布所有样本以实现一致性。由于白化和对比学习相对于一致性具有大量冗余，它们通常单独使用且不容易共同工作。本文首次将白化集成到对比学习方案中，并促进了两个好处。1) 更好的一致性。我们发现这两种方法不完全冗余，实际上由于不同的一致性机制，它们有一些互补性。2) 更好的对齐。

    This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the "pushing'' operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We rand
    
[^66]: BUCA：一种用于无监督常识问题回答的二分类方法

    BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])

    [http://arxiv.org/abs/2305.15932](http://arxiv.org/abs/2305.15932)

    本文提出了一种更简单的二分类方法，将下游的多项选择题回答任务转换为二分类任务，根据合理性对所有候选答案进行排名，以实现无监督常识问题回答，相较于现有使用知识图谱的UCR方法，我们的方法更为节省数据。

    

    随着常识推理数据集的构建变得越来越昂贵且在范围上不可避免地受限，无监督的常识推理(UCR)变得越来越流行。UCR的一种流行方法是利用外部知识将语言模型进行微调(例如，知识图谱)，但这通常需要大量的训练样例。在本文中，我们提出将下游的多项选择题回答任务转换为一个更简单的二分类任务，通过对所有候选答案的合理性进行排名来完成。为了训练模型，我们将知识图谱三元组转换为合理和不合理的文本。广泛的实验结果显示了我们的方法在各种多项选择问题回答基准测试中的有效性。此外，与使用KG的现有UCR方法相比，我们的方法更节省数据。我们的代码可在https://github.com/probe2/BUCA上获取。

    Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.
    
[^67]: 大型语言模型是上下文语义推理器而不是符号推理器

    Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners. (arXiv:2305.14825v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14825](http://arxiv.org/abs/2305.14825)

    本文研究了大型语言模型的内部机制，发现在上下文语境中，语言序列的语义应起到至关重要的作用，与人类符号推理不同，大型语言模型可以在语言序列中建立强连接，并组成一个表面逻辑链。

    

    近年来，大型语言模型(Large Language Models, LLMs)的出现引起了自然语言和机器学习界的极大兴趣，其出色的应用性能备受推崇。然而，LLMs在上下文语境下的推理能力背后的机制仍然不清楚。本文提出我们的假设：在推理过程中，语言序列中学习到的"语义"发挥了至关重要的作用。与人类的符号推理过程不同，LLMs的语义表示可以在语言序列中建立强连接，因此组成一个表面逻辑链。为了测试我们的假设，我们将语义从语言推理过程中分离出来，并评估了三种推理能力：演绎、归纳和拟合。我们的发现表明，在上下文语境中，语义对LLMs的推理能力起着至关重要的作用，当语义与常识一致时，LLMs的表现更佳，但在解决符号或反常识推理问题上会出现困难。

    The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense re
    
[^68]: 大型语言模型在日语文本分类中对提示模板的敏感性和鲁棒性研究

    Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks. (arXiv:2305.08714v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08714](http://arxiv.org/abs/2305.08714)

    本研究评估了多个大型语言模型在日语文本分类中对提示模板的敏感性和鲁棒性，并揭示出即使是高性能的GPT-4模型在这一方面也存在问题。

    

    近年来，预训练语言模型和大型语言模型的先进发展主导了提示工程相关研究的显著增长。然而，这一领域存在一个关键问题：这些模型对于提示模板的敏感性和鲁棒性不足，特别是在日语这样的较少研究的语言中。本文通过全面评估几个代表性的大型语言模型（LLMs）和一个广泛使用的预训练模型（PLM）来探讨这个问题。我们使用一组基准数据集对这些模型进行了审查，旨在评估和分析当前多语言模型在这种情况下的性能表现。我们的实验结果揭示了惊人的差异。一个简单的提示模板句子结构的修改导致GPT-4的准确率从49.21下降到了25.44。这一观察结果强调了即使是高性能的GPT-4模型也存在这一问题。

    Prompt engineering relevance research has seen a notable surge in recent years, primarily driven by advancements in pre-trained language models and large language models. However, a critical issue has been identified within this domain: the inadequate of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM). These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model 
    
[^69]: 关于大型多模态模型中OCR的隐秘之谜

    On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])

    [http://arxiv.org/abs/2305.07895](http://arxiv.org/abs/2305.07895)

    本研究全面评估了现有大型多模态模型在文本相关的视觉任务中的表现，结果显示这些模型虽然在语义理解方面表现优异，但对单个字符形状的感知有限，对图像的细粒度特征检测能力也不足，不能与传统领域特定方法相匹配，并仍需进一步探索它们在OCR中的表现。

    

    近来，大型模型在自然语言处理和多模态视觉语言学习中扮演着支配性的角色。关于它们在文本相关的视觉任务中有效性的探索仍不够。我们对现有公开可用的多模态模型进行了全面的研究，评估了它们在文本识别、基于文本的视觉问答和关键信息提取方面的表现。我们的研究结果揭示了这些模型的优劣势，它们主要依赖于语义理解来识别单词，并表现出较差的对单个字符形状的感知。它们对文本长度漠不关心，在检测图像的细粒度特征方面具有有限的能力。因此，这些结果表明，即使当前最强大的大型多模态模型也无法与传统文本任务的领域特定方法相匹配，并在更复杂的任务中面临更大的挑战。最重要的是，本研究展示的基线结果揭示了大型多模态模型中OCR的隐秘之谜，仍需要进一步探索。

    Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting fine-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study
    
[^70]: 面向模型预测解释的非对称特征交互

    Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])

    [http://arxiv.org/abs/2305.07224](http://arxiv.org/abs/2305.07224)

    本文提出了一种解释模型，能够探索深度神经自然语言处理模型推理中的非对称高阶特征交互。在两个情感分类数据集上的实验结果表明，该模型在识别影响特征方面优于现有特征交互归因方法。

    

    在自然语言处理领域，深度神经网络能够模拟上下文之间的复杂交互，并在一系列自然语言处理任务上取得了令人瞩目的成果。先前有关特征交互归因的研究主要集中在对称交互的研究上，它只能解释单个词汇组合后对模型预测的附加影响，而无法捕捉导致模型预测的非对称影响。在本文中，我们提出了一个非对称特征交互解释模型，旨在探索深度神经自然语言处理模型推理中的非对称高阶特征交互。通过表示我们的解释为一个有向交互图，我们实验验证了该图的可解释性，能够发现非对称特征交互作用。在两个情感分类数据集上的实验结果表明，我们的模型在识别影响特征方面优于现有特征交互归因方法。

    In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential featu
    
[^71]: BanglaBook: 一种用于情感分析的大规模孟加拉语书评数据集

    BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v1 [cs.CL])

    [http://arxiv.org/abs/2305.06595](http://arxiv.org/abs/2305.06595)

    BanglaBook 是一个大规模的孟加拉语书评数据集，其中包括 158,065 个样本，针对情感分析分为三个大类，通过使用预训练模型来取代手动构建特征的模型，取得显着的性能优势。

    

    消费者情感分析可以通过评论表达提供有关产品质量的丰富见解。尽管情感分析的研究在许多流行语言中得到了广泛探索，但由于缺乏相关数据和跨领域适应性，相对较少关注孟加拉语言。为了解决这个限制，我们提出了 BanglaBook，这是一个大规模的孟加拉语书评数据集，包括 158,065 个样本，分为三个大类：积极、消极和中性。我们对数据集进行了详细的统计分析，并使用一系列机器学习模型建立了基线，包括 SVM、LSTM 和 Bangla-BERT。我们的研究结果表明，与依赖手动构建特征的模型相比，预训练模型具有显着的性能优势，强调了在此领域需要额外的培训资源。此外，我们通过检查情感错误分类来进行深入的错误分析，并提供有关孟加拉情感分析性质的进一步见解。

    The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews consisting of 158,065 samples classified into three broad categories: positive, negative, and neutral. We provide a detailed statistical analysis of the dataset and employ a range of machine learning models to establish baselines including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial performance advantage of pre-trained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain. Additionally, we conduct an in-depth error analysis by examining se
    
[^72]: 用自然语言指令控制文本生成

    Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])

    [http://arxiv.org/abs/2304.14293](http://arxiv.org/abs/2304.14293)

    InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。

    

    大型语言模型可以产生流畅的文本，并能根据自然语言指令解决各种任务，无需特定的训练。然而，控制它们的生成以满足不同应用程序所需的各种约束条件是非常困难的。本文提供了一个带约束调节的文本生成框架——InstructCTG，该框架通过基于自然语言描述和约束演示来纳入不同的约束条件。我们首先通过一系列现成的NLP工具和简单的启发式方法来提取自然文本的潜在约束条件。此外，我们将这些约束条件转化为自然语言指令，以形成弱监督的训练数据。通过添加自然语言约束描述和少量演示，我们对预训练语言模型进行了微调，以纳入各种类型的约束条件。与现有基于搜索或得分的方法相比，InstructCTG 更加有效。

    Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
    
[^73]: 奖励是否合理？在 MACHIAVELLI 基准测试中衡量奖励与道德行为之间的权衡

    Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])

    [http://arxiv.org/abs/2304.03279](http://arxiv.org/abs/2304.03279)

    本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。

    

    传统上，人工智能代理被训练成最大化奖励，这可能会激励追求权力和欺骗行为，类似于语言模型中的下一个标记预测可能会激励有害行为。那么代理是否自然而然地学会了马基雅维利行为？我们如何在 GPT-4 等通用模型中衡量这些行为呢？为回答这些问题，我们引入了 MACHIAVELLI 基准测试，该测试涵盖了超过一百万个多样化的情景，重点关注社会决策制定，用于衡量人工代理是否表现出马基雅维利行为。我们数学化了数十种有害行为，并使用我们的注释来评估代理倾向于追求权力，造成功能不良和违反伦理的倾向。我们观察到最大化奖励和行为的道德性之间存在一些紧张关系。为了改善这种权衡，我们研究了基于语言模型的方法，以使代理趋向于采取更少的有害行为。我们的结果显示，MACHIAVELLI 是评估人工代理马基雅维利行为水平的有用基准测试。

    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
    
[^74]: 统一流式和非流式变换器中的上下文偏差双阶段上下文过滤

    Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer. (arXiv:2301.06735v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.06735](http://arxiv.org/abs/2301.06735)

    本文提出一种针对上下文偏差的双阶段上下文过滤方法，将流式输出和预定义上下文单词列表相结合，提高了端对端模型的识别准确率，并加快了推理过程。

    

    对于一个端对端的自动语音识别系统来说，很难识别像实体这样在训练数据中出现不频繁的单词。缓解这个问题的一种常用方法是将上下文信息输入到声学模型中。之前的研究已经证明了一个紧凑而准确的上下文列表可以显著提高性能。本文提出了一种高效的方法，为统一的流式/非流式的端对端模型得到一个高质量的上下文列表。具体来说，我们利用基于电话级别的流式输出来首先过滤预定义的上下文单词列表，然后将其融合到非因果编码器和解码器中生成最终的识别结果。我们的方法提高了上下文ASR系统的准确性并加快了推理过程。在两个数据集上的实验证明，相比基线系统，CERR提高了20%以上。同时，当上下文单词列表的大小超过6000时，我们的系统的RTF可以稳定在0.15左右。

    It is difficult for an E2E ASR system to recognize words such as entities appearing infrequently in the training data. A widely used method to mitigate this issue is feeding contextual information into the acoustic model. Previous works have proven that a compact and accurate contextual list can boost the performance significantly. In this paper, we propose an efficient approach to obtain a high quality contextual list for a unified streaming/non-streaming based E2E model. Specifically, we make use of the phone-level streaming output to first filter the predefined contextual word list then fuse it into non-casual encoder and decoder to generate the final recognition results. Our approach improve the accuracy of the contextual ASR system and speed up the inference process. Experiments on two datasets demonstrates over 20% CERR comparing to the baseline system. Meanwile, the RTF of our system can be stabilized within 0.15 when the size of the contextual word list grows over 6000.
    
[^75]: Think Twice：一种人类化的两阶段对话代理用于生成情感响应

    Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation. (arXiv:2301.04907v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.04907](http://arxiv.org/abs/2301.04907)

    根据 “三思而后语” 行为启发，提出一种两阶段对话代理用于生成情感对话，该代理在情感生成方面优于其他模型，并保持了语义表现。

    

    针对人类化的对话系统，目前情感对话方法采用统一的神经网络联合模型情感和语义。这种策略由于情感和语义之间的相互限制往往会产生安全的响应，并且需要罕见的情感标注大规模对话语料库。受到人类对话中“三思而后语”的行为启发，我们提出了一种用于生成情感对话的两阶段对话代理。首先，一个没有使用情感标注对话语料库训练的对话模型生成符合上下文语义的原型响应。其次，第一阶段原型将通过一个可控的情感优化器与共情假设进行修改。在DailyDialog和EmpatheticDialogues数据集上的实验结果表明，我们提出的对话代理在情感生成方面优于比较模型，并在自动和人类评估中保持了语义表现。

    Towards human-like dialogue systems, current emotional dialogue approaches jointly model emotion and semantics with a unified neural network. This strategy tends to generate safe responses due to the mutual restriction between emotion and semantics, and requires rare emotion-annotated large-scale dialogue corpus. Inspired by the "think twice" behavior in human dialogue, we propose a two-stage conversational agent for the generation of emotional dialogue. Firstly, a dialogue model trained without the emotion-annotated dialogue corpus generates a prototype response that meets the contextual semantics. Secondly, the first-stage prototype is modified by a controllable emotion refiner with the empathy hypothesis. Experimental results on the DailyDialog and EmpatheticDialogues datasets demonstrate that the proposed conversational outperforms the comparison models in emotion generation and maintains the semantic performance in automatic and human evaluations.
    
[^76]: MultiInstruct: 通过指令调优来改善多模态零样本学习

    MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. (arXiv:2212.10773v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10773](http://arxiv.org/abs/2212.10773)

    本文介绍了 MultiInstruct，这是第一个多模态指令调优基准数据集，并探索多种迁移学习策略从大规模的自然语言指令数据集中提高预训练模型的性能。实验结果展示了其在各种未见过的多模态任务中具有强大的零样本表现，以及设计的新的任务完成率指标。

    

    指令调优是一种新的学习范式，它在指令指定的任务上对预训练的语言模型进行微调，在各种自然语言处理任务上展现了有希望的零样本表现。然而，它尚未被用于视觉和多模态任务。本文介绍了 MultiInstruct，这是第一个多模态指令调优基准数据集，包含 47 个不同的多模态任务，涵盖了 11 个广泛的类别。每个任务至少设计有 5,000 个实例（输入-输出对）来自现有的开源数据集和 5 个专家编写的指令。我们选取 OFA 作为多模态指令调优的基础预训练模型，并探索多种迁移学习策略，以利用大规模的自然语言指令数据集。实验结果展示了其在各种未见过的多模态任务中具有强大的零样本表现，以及从纯文本指令中获得迁移学习的好处。我们还设计了一种新的任务完成率指标来评估零样本性能，度量模型在仅有指令的情况下完成任务的能力。

    Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it's still not explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 47 diverse multimodal tasks covering 11 broad categories. Each task is designed at least with 5,000 instances (input-out pairs) from existing open-source datasets and 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to improve its performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate its strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from text-only instructions. We also design a ne
    
[^77]: CoRRPUS: 利用Codex的结构化表示增强神经符号故事理解

    CoRRPUS: Codex-Leveraged Structured Representations for Neurosymbolic Story Understanding. (arXiv:2212.10754v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10754](http://arxiv.org/abs/2212.10754)

    本研究利用Code-LLMs引导符号表示以增强神经符号故事理解，通过CoRRPUS系统和抽象提示程序，在最小的手动工程条件下，击败了当前最先进的结构LLM技术。

    

    随着所有自然语言生成/理解任务的神经符号技术的飞速发展，故事生成和理解也得到了极大的改进。本研究利用最先进的Code-LLMs，如Codex，引导符号方法的使用，以跟踪故事状态并帮助故事理解。我们展示了我们的CoRRPUS系统和抽象提示程序如何在最小的手动工程条件下，击败了当前最先进的结构LLM技术，完成了预先存在的故事理解任务（bAbI task 2 和 Re^3）。我们希望本研究能够凸显符号表示和专业提示对LLMs的重要性，因为这些模型需要一些手工优化。

    Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some g
    
[^78]: 语言模型是否具有日常物品的一致性心理模型？

    Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10029](http://arxiv.org/abs/2212.10029)

    语言模型缺乏对日常物品的一致性心理模型，会因此出现荒谬的解决方法。虽然最先进的预训练语言模型具有这些实体的知识碎片，但它们无法为所有实体产生一致且正确的心理模型。语言模型训练可以改善这种情况。

    

    当人们想到像“鸡蛋”这样的日常用品时，通常会有一个与之相关联的心理图像。这种常识性知识有助于我们理解这些日常用品的工作原理以及如何与它们交互。然而，如果系统对这样的日常用品没有一致的图像，比如认为鸡蛋黄包围着壳，那么它可能不得不采取荒谬的方法，比如试图把鸡蛋黄刮下壳放入平底锅中煎煮。语言模型是否具有这种日常用品的一致性心理模型？为了调查这个问题，我们提出了一个基准数据集，包括100种日常用品、它们的部件以及这些部件之间的关系。我们观察到，像GPT-3和Macaw这样的最先进的预训练语言模型具有这些实体的知识碎片，但它们无法为所有实体产生一致且正确的心理模型。我们还发现，对这个基准数据集进行语言模型训练可以提高它们在某些方面的性能。

    When people think of everyday things like an "egg," they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consist
    
[^79]: 关于灵活语义匹配的关系句嵌入

    Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08802](http://arxiv.org/abs/2212.08802)

    本文提出了一种新的关系句嵌入方法，通过学习相关的关系嵌入来解决句子间复杂语义含义的关系捕捉问题。实验结果表明该方法在多个任务中均具有高效性能。

    

    我们提出了一种新的关系句嵌入 (RSE) 范式，以进一步发掘句子嵌入的潜力。先前的工作主要是基于嵌入距离建模句子间的相似度。由于表达的复杂语义含义，句子对可能具有各种关系类型，包括但不限于蕴涵、释义和问答。对于现有的嵌入方法来捕捉这种关系信息提出了挑战。我们通过学习相关的关系嵌入来解决这个问题。具体地，我们通过一个预训练的共生中心编码器，对源句子应用关系转换操作来推断对应的目标句子。通过学习的嵌入，可以计算出细粒度的关系相似度分数。我们在19个数据集上对我们的方法进行了基准测试，涵盖了广泛的任务范围，包括语义文本相似性、转移和领域特定任务。实验结果表明，我们的方法是有效的。

    We present Relational Sentence Embedding (RSE), a new paradigm to further discover the potential of sentence embeddings. Prior work mainly models the similarity between sentences based on their embedding distance. Because of the complex semantic meanings conveyed, sentence pairs can have various relation types, including but not limited to entailment, paraphrasing, and question-answer. It poses challenges to existing embedding methods to capture such relational information. We handle the problem by learning associated relational embeddings. Specifically, a relation-wise translation operation is applied to the source sentence to infer the corresponding target sentence with a pre-trained Siamese-based encoder. The fine-grained relational similarity scores can be computed from learned embeddings. We benchmark our method on 19 datasets covering a wide range of tasks, including semantic textual similarity, transfer, and domain-specific tasks. Experimental results show that our method is eff
    
[^80]: 弱监督条件下的显式知识转移用于代码生成

    Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.16740](http://arxiv.org/abs/2211.16740)

    本文提出了显式知识转移 (EKT) 方法，将大型语言模型 (LLMs) 的代码生成能力转移到小型模型，将微调所需昂贵且难以获取的任务特定自然语言代码对改为通过过滤大量 NL-code 对来实现，优于知识蒸馏。

    

    大型语言模型 (LLMs) 可以通过少量的学习获取强大的代码生成能力。相比之下，小型模型仍需要监督微调才能实现良好的性能。这种微调需要大量的任务特定的自然语言代码对，昂贵且难以获取。本文尝试利用弱监督数据将 LLM 的代码生成能力转移到小型模型。我们提出了显式知识转移 (EKT)，它使用教师 LLM 的少量样本能力来创建 NL-code 对，然后我们过滤出正确的对并对学生模型进行微调。我们评估了 EKT 在从 GSM8k 数据集中生成数学问题代码解时的任务上。我们发现，EKT 不仅比使用专家迭代训练的方法产生更好的性能，而且优于知识蒸馏，另一种知识转移形式。使用 GPT-J 作为教师模型，采用 EKT 对 GPT-Neo 1.3B 模型进行训练可获得 12.4% 的性能提升。

    Large language models (LLMs) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an LLM to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that we then filter for correctness and fine-tune the student on. We evaluate EKT on the task of generating code solutions to math word problems from the GSM8k dataset. We find that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%
    
[^81]: 一种量化语言模型数学推理鲁棒性的因果框架

    A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12023](http://arxiv.org/abs/2210.12023)

    该研究提出了一种基于因果框架的新方法，用于确定语言模型在数学推理任务中各种因素对输出解决方案的因果影响，研究结果显示GPT-3 Davinci模型（175B）在鲁棒性方面取得了显着改善。

    

    最近，语言模型在解决困难数学推理问题方面取得了一些令人印象深刻的成果，同时，这些模型的鲁棒性也备受质疑。最近的研究表明，当生成解决方案时，模型可能会依赖于问题描述中的浅层模式。我们提出了一个新的框架，建立在行为测试的思想基础上，它能够确定输入中各种因素，例如问题文本的表面形式、操作数和数学运算符对输出解决方案的因果影响。通过在直观推理过程中描述因果图，将行为分析根据鲁棒性和对输入空间的直接干预敏感性研究语言模型的行为。我们将这个框架应用于数学题的测试中。我们的分析显示，鲁棒性似乎并不会随着模型大小的增加而不断改善，但与较小的模型相比，GPT-3 Davinci模型（175B）在鲁棒性方面取得了显着改善。我们提供了一种系统而可解释的方法来理解语言模型在数学推理任务中的行为。

    We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramati
    
[^82]: 一种半监督方法以更好地翻译方言阿拉伯语的情感

    A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT. (arXiv:2210.11899v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.11899](http://arxiv.org/abs/2210.11899)

    研究旨在提高方言阿拉伯语的情感翻译。使用半监督方法，利用单语和平行数据来训练NMT系统，以提高在低资源语言中MT系统的准确性。

    

    在线世界中，机器翻译系统被广泛用于翻译用户生成的文本（如评论、推文和社交媒体帖子），其中主要信息通常是作者对文本主题的积极或消极态度。然而，MT系统在某些低资源语言中仍然缺乏准确性，并且有时会出现重大的翻译错误，完全翻转目标单词或短语的情感极性，从而传递错误的情感信息。这在不遵循通用词汇和语法标准的文本中特别明显，例如在线平台上使用的阿拉伯方言（DA）。在这项研究中，我们旨在改善用方言版本的阿拉伯语撰写的UGT到英语中的情感翻译。鉴于UGT领域的DA-EN黄金标准并行数据稀缺，我们引入了一种半监督方法，利用单语和平行数据来训练NMT系统。

    In the online world, Machine Translation (MT) systems are extensively used to translate User-Generated Text (UGT) such as reviews, tweets, and social media posts, where the main message is often the author's positive or negative attitude towards the topic of the text. However, MT systems still lack accuracy in some low-resource languages and sometimes make critical translation errors that completely flip the sentiment polarity of the target word or phrase and hence delivers a wrong affect message. This is particularly noticeable in texts that do not follow common lexico-grammatical standards such as the dialectical Arabic (DA) used on online platforms. In this research, we aim to improve the translation of sentiment in UGT written in the dialectical versions of the Arabic language to English. Given the scarcity of gold-standard parallel data for DA-EN in the UGT domain, we introduce a semi-supervised approach that exploits both monolingual and parallel data for training an NMT system i
    
[^83]: AutoMoE：自适应计算的异构专家混合体，在神经机器翻译中实现高效。

    AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07535](http://arxiv.org/abs/2210.07535)

    AutoMoE提出了一种能够在计算约束下设计异构MoE的框架，它在神经机器翻译任务中实现了高效且最先进的性能。

    

    在神经机器翻译（NMT）任务中，专家混合体（MoE）模型获得了最先进的性能。MoE的现有工作主要考虑同质设计，其中相同数量的相同大小的专家均匀地放置在整个网络中。此外，现有的MoE工作没有考虑计算约束（例如FLOPs、延迟）来指导其设计。为此，我们开发了AutoMoE--一个在计算约束下设计异构MoE的框架。AutoMoE利用神经架构搜索（NAS）来获取高效的稀疏MoE子变压器，具有4倍推理速度优化（CPU）和FLOPs减少，相对于手动设计的Transformer，在NMT基准数据集上实现了BLEU分数的平稳性。采用密集和稀疏激活的Transformer模块的异构搜索空间（例如有多少专家？在哪里放置它们？它们的大小应该是多少？）允许更好地探索模型设计空间，最大限度地提高资源利用率。

    Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows
    
[^84]: 桥接计算机视觉与自然语言处理间的鸿沟！一种基于梯度的文本对抗攻击框架。

    Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.15317](http://arxiv.org/abs/2110.15317)

    该论文提出了一种针对文本对抗攻击的框架，通过在嵌入层上持续优化扰动并放大这些扰动，使用遮罩语言模型头对最终扰动的潜在代表进行解码，以获取可能的对抗样本，进行了广泛的评估，并在各种语言任务上取得了制造近乎不可察觉的通用和定向文本对抗样本的最新技术水平。

    

    尽管深度学习在各种任务上取得了最近的成功，但在小扰动的对抗样本上仍表现不佳。优化类的对抗攻击方法虽已在计算机视觉领域得到了很好的研究，但由于文本的离散性质，直接将这些方法应用于自然语言处理是不切实际的。为了解决这个问题，我们提出了一个统一的框架，将现有的视觉域优化类对抗攻击方法扩展到文本对抗样本的制造上。在这个框架中，我们通过在嵌入层上持续优化扰动，并在前向传播过程中放大这些扰动。随后，使用遮罩语言模型头对最终扰动的潜在代表进行解码，以获取可能的对抗样本。在本文中，我们使用了一种名为文本投影梯度下降（T-PGD）的攻击算法来实例化我们的框架。我们发现即使在文本领域中常见的使用代理梯度信息的情况下，我们的算法也具有很好的效果。此外，我们对各种语言任务进行了广泛的评估，包括情感分析、文本分类和命名实体识别。实验结果表明，我们提出的框架实现了制造近乎不可察觉的通用和定向文本对抗样本的最新技术水平。

    Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision domain to craft textual adversarial samples. In this framework, continuously optimized perturbations are added to the embedding layer and amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a masked language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with an attack algorithm named Textual Projected Gradient Descent (T-PGD). We find our algorithm effective even using proxy gradient informati
    

