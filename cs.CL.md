# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Contextual Biasing of Named-Entities with Large Language Models.](http://arxiv.org/abs/2309.00723) | 本文研究了使用大型语言模型进行上下文偏倚的方法，通过在第二次打分时提供额外的上下文信息，以提高自动语音识别性能。我们利用提示信息对大型语言模型进行boosting，并采用多任务训练以预测实体类别和下一个标记。此外，我们提出了动态提示方法来提高效率。 |
| [^2] | [Taken out of context: On measuring situational awareness in LLMs.](http://arxiv.org/abs/2309.00667) | 这个论文目的在于研究大型语言模型中的情境意识的产生，提出了一种衡量模型情境意识的能力，并通过实验证明了其有效性。 |
| [^3] | [GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice.](http://arxiv.org/abs/2309.00649) | GPT通过金融素养测试显示出具备成为大众金融机器顾问的能力，其中基于GPT-4的ChatGPT几乎完美地得分99%，揭示了金融素养正在成为最先进模型的新兴能力。 |
| [^4] | [Extracting Mathematical Concepts with Large Language Models.](http://arxiv.org/abs/2309.00642) | 使用大型语言模型从数学文本中提取数学概念，为自动术语提取和数学文本处理做出了贡献，同时提供了一套标准化的提取过程和新的标注工具。 |
| [^5] | [Misinformation Concierge: A Proof-of-Concept with Curated Twitter Dataset on COVID-19 Vaccination.](http://arxiv.org/abs/2309.00639) | 错误信息管家是一个可行性概念，旨在提供关于社交媒体中流行的错误信息的可操作情报。它使用语言处理和机器学习工具来识别错误信息并推荐反驳信息，帮助干预和反击错误信息。 |
| [^6] | [Baseline Defenses for Adversarial Attacks Against Aligned Language Models.](http://arxiv.org/abs/2309.00614) | 这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。 |
| [^7] | [TouchStone: Evaluating Vision-Language Models by Language Models.](http://arxiv.org/abs/2308.16890) | TouchStone提出了一种评估方法，使用强大的语言模型作为评委来全面评估大规模视觉-语言模型的各种能力。通过构建综合的视觉对话数据集和整合图像注释，评估包括识别、理解、对话和叙事等多个能力。 |
| [^8] | [Can Programming Languages Boost Each Other via Instruction Tuning?.](http://arxiv.org/abs/2308.16824) | 研究发现，编程语言可以在指令调优阶段相互促进，并显著提高彼此的能力。 |
| [^9] | [Link Prediction for Wikipedia Articles as a Natural Language Inference Task.](http://arxiv.org/abs/2308.16469) | 本文将维基百科文章的链接预测任务视为自然语言推理任务，采用了一种新的方法，并在DSAA-2023竞赛中取得了较高的评分。 |
| [^10] | [BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge.](http://arxiv.org/abs/2308.16458) | BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。 |
| [^11] | [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models.](http://arxiv.org/abs/2308.16137) | LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。 |
| [^12] | [Is the U.S. Legal System Ready for AI's Challenges to Human Values?.](http://arxiv.org/abs/2308.15906) | 美国法律需要加强应对生成式人工智能对人类价值观挑战的能力，并提供积极、可审计的指导，以填补现有法律框架在保护基本价值观方面的空白和不确定性。 |
| [^13] | [ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer.](http://arxiv.org/abs/2308.15459) | ParaGuide是一种用于通用风格转移的引导性扩散改写器，可以灵活适应任意目标风格，通过梯度引导和改写条件的扩散模型实现文本的风格转变，同时保留语义信息。 |
| [^14] | [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs.](http://arxiv.org/abs/2308.13387) | 这项工作收集了第一个用于评估LLMs中安全机制的开源数据集，并通过训练分类器实现了与GPT-4在自动安全评估上相媲美的结果。 |
| [^15] | [Construction Grammar and Language Models.](http://arxiv.org/abs/2308.13315) | 最新的深度学习和自然语言处理进展为计算方法和建构语法研究之间的协同关系提供了机会，本章提供了三种不同的计算方法与建构语法相互作用的途径，并重点关注语言模型。 |
| [^16] | [From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models.](http://arxiv.org/abs/2308.12014) | 本文综合调查了大模型对齐目标的不同观点，并追踪其演化路径，旨在帮助确定最重要的目标。 |
| [^17] | [Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model.](http://arxiv.org/abs/2308.11773) | 通过自动语音识别系统和深度学习主题模型，我们在智能手机采集的语音录音中识别出与抑郁相关的29个主题，并确定了其中6个主题作为抑郁的风险主题。此研究表明，通过长期监测语言使用，可以了解主题的出现与抑郁之间的关联。 |
| [^18] | [UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding.](http://arxiv.org/abs/2308.11592) | UniDoc是一种通用的大型多模态模型，具备文本检测和识别能力，并通过任务之间的有益交互提高每个任务的性能，达到了在多个基准测试中的最先进水平。 |
| [^19] | [LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report).](http://arxiv.org/abs/2308.11148) | 本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。 |
| [^20] | [CausalLM is not optimal for in-context learning.](http://arxiv.org/abs/2308.06912) | 最近的研究显示，上下文学习中使用前缀语言模型（PrefixLM）比因果语言模型（CausalLM）效果更好。本文通过理论分析证明，虽然两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，因果语言模型的收敛动态遵循在线梯度下降算法，不保证收敛到最优解。 |
| [^21] | [Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing.](http://arxiv.org/abs/2308.06035) | 这篇论文研究了多模态大语言模型（mLLMs）在预测语言处理过程中与人类的视觉-语言集成能力是否一致的问题，并通过实验验证了mLLMs的多模态输入方法可以减少认知负荷，提高感知和理解能力。 |
| [^22] | [Towards Generalist Foundation Model for Radiology.](http://arxiv.org/abs/2308.02463) | 本研究旨在为放射学构建通用基础模型，提出了一个大规模的医学多模态数据集和支持不同放射学任务的架构，同时提出了一个新的评估基准。 |
| [^23] | [Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.](http://arxiv.org/abs/2308.00085) | 本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成。该方法综合考虑了用户的角度和系统的角度，并通过集成常识知识提升了ChatGPT在系统的推理能力。实验结果表明，该方法在多项评估指标上超过了其他方法。 |
| [^24] | [AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models.](http://arxiv.org/abs/2307.11772) | AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。 |
| [^25] | [OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples.](http://arxiv.org/abs/2307.11729) | OUTFOX是一个新的框架，通过允许检测器和攻击者考虑彼此的输出，提高了LLM生成文本检测器的鲁棒性。攻击者利用检测器的预测标签作为示例进行上下文学习，并生成难以检测的对抗生成的论文。 |
| [^26] | [Analyzing Dataset Annotation Quality Management in the Wild.](http://arxiv.org/abs/2307.08153) | 该论文调查分析了自然语言数据集的创建过程中的质量管理实践，并提供了相应的建议。研究表明，流行数据集中存在较多的错误注释、偏见或注释伪像。这项研究的贡献是在这一领域进行了大规模的实证分析，并提出了实践指南。 |
| [^27] | [A scoping review on multimodal deep learning in biomedical images and texts.](http://arxiv.org/abs/2307.07362) | 这篇综述旨在提供对多模态深度学习在生物医学图像和文本中进行联合学习的当前状况的全面概述，并探索未来的研究方向和该领域的研究空白。 |
| [^28] | [ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task.](http://arxiv.org/abs/2307.06954) | ACTI在EVALITA 2023中的阴谋论辨识任务共有15支团队参与，通过使用大型语言模型判断阴谋内容和分类，得出了关于利用这些模型抵制在在线平台传播错误信息的结论。 |
| [^29] | [SCALE: Scaling up the Complexity for Advanced Language Model Evaluation.](http://arxiv.org/abs/2306.09237) | 该论文提出了一个新颖的自然语言处理基准测试，挑战当前大型语言模型在处理长文档、利用领域专业知识、多语言理解和多任务处理方面的能力。基准测试包含瑞士法律系统的多样化法律NLP数据集，允许进行对底层非英语、固有多语言的法律系统进行全面研究。 |
| [^30] | [AMR4NLI: Interpretable and robust NLI measures from semantic graphs.](http://arxiv.org/abs/2306.00936) | 该论文提出了一种从语义图中获取可解释和鲁棒的NLI度量方法，与使用上下文嵌入的方法相比具有补充性，可以在混合模型中结合使用。 |
| [^31] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^32] | [InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission.](http://arxiv.org/abs/2303.15049) | 论文介绍了一种基于神经元的端到端对话模型- InterviewBot，该模型利用与输入的历史对话和定制主题集成到同一嵌入空间中来评估外国学生申请美国大学的学术和文化准备情况。同时，为克服基于变形金刚编码器-解码器模型的输入/输出大小限制，提出了上下文关注和主题存储两种新方法。该模型在统计和动态测试中表现出了流畅性和上下文感知的高度满意。 |
| [^33] | [Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering.](http://arxiv.org/abs/2303.12671) | 该论文针对VLSP2022-EVJVQA共享任务提出了一种基于卷积序列到序列网络的方法，在多语言视觉问答中将预训练的VQA模型和图像特征集成，获得了很好的效果。 |
| [^34] | [BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models.](http://arxiv.org/abs/2302.07371) | BiasTestGPT是一个开源的偏见测试框架，利用ChatGPT进行测试句子的生成，可以更好地检测语言模型中的社会偏见，尤其是在交叉偏见等挑战性情境中。 |
| [^35] | [Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking.](http://arxiv.org/abs/2302.07189) | 本文提出了基于BERT的实体链接方法BLINKout，通过与特殊NIL实体匹配来识别没有相应KB实体的实体提及，相较于现有方法具有优势。 |
| [^36] | [Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation.](http://arxiv.org/abs/2212.10313) | 本论文提出了一个新的多模态机器翻译方法，利用大规模非三元组数据来增强翻译质量，并构建了一个新的英汉电子商务多模态翻译数据集EMMT。 |
| [^37] | [Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work.](http://arxiv.org/abs/2211.10443) | 本文描述了一个为社交媒体挖掘非医学处方药物使用信息而开发的端到端管道，并讨论了挑战和未来工作。 |
| [^38] | [Semantic Representations of Mathematical Expressions in a Continuous Vector Space.](http://arxiv.org/abs/2211.08142) | 该论文提出了一种在连续向量空间中表示数学表达式的方法，并使用序列到序列框架的编码器生成向量表示来捕捉数学语义，比自动编码器效果更好。 |
| [^39] | [Combing for Credentials: Active Pattern Extraction from Smart Reply.](http://arxiv.org/abs/2207.10802) | 该论文研究了在智能回复应用程序中潜在的信息泄漏漏洞，并且提出了一种在实际设置中限制查询类型的攻击方式。 |
| [^40] | [How trial-to-trial learning shapes mappings in the mental lexicon: Modelling Lexical Decision with Linear Discriminative Learning.](http://arxiv.org/abs/2207.00430) | 本研究探讨了试验对试验学习在词汇决策中的应用，使用了具有分布语义意义表示的心理词库模型，并通过预测反应时间进一步分析了结果。 |

# 详细

[^1]: 大型语言模型中的命名实体上下文偏倚研究

    Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])

    [http://arxiv.org/abs/2309.00723](http://arxiv.org/abs/2309.00723)

    本文研究了使用大型语言模型进行上下文偏倚的方法，通过在第二次打分时提供额外的上下文信息，以提高自动语音识别性能。我们利用提示信息对大型语言模型进行boosting，并采用多任务训练以预测实体类别和下一个标记。此外，我们提出了动态提示方法来提高效率。

    

    本文研究了在大型语言模型(LLMs)中进行上下文偏倚，即在第二次打分时为LLM提供额外的上下文信息，以提高自动语音识别(ASR)性能。我们提出了在打分期间利用提示信息对LLM进行boosting，而无需进行微调，这些提示信息包括偏倚列表和少样本示例，用于在计算假设得分时作为附加信息。除了少样本提示学习外，我们还提出了LLM的多任务训练，以预测实体类别和下一个标记。为了提高上下文偏倚的效率并避免超过LLMs的最大序列长度，我们提出了动态提示，即使用类别标签预测选择最可能的类别，并仅使用这个类别中的实体作为下一个标记预测的上下文。对内部的呼叫、消息和口述数据集以及SLUE-Voxpopuli数据集进行了词错误率(WER)评估。

    This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
    
[^2]: 脱离上下文的影响：关于衡量LLMs中的情境意识论文

    Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])

    [http://arxiv.org/abs/2309.00667](http://arxiv.org/abs/2309.00667)

    这个论文目的在于研究大型语言模型中的情境意识的产生，提出了一种衡量模型情境意识的能力，并通过实验证明了其有效性。

    

    我们旨在更好地理解大型语言模型（LLMs）中“情境意识”的出现。如果一个模型在意识到自己是一个模型的同时，能够识别自己当前是处于测试或部署状态，那么这个模型在情境上是具备意识的。今天的LLMs在部署之前会经过安全性和对齐性的测试。在部署后，一个LLM可能会利用情境意识在安全测试中取得高分，但在实际使用中采取有害行为。情境意识可能会意外地在模型扩展中出现。为了更好地预测这种出现，我们提出了对于情境意识而言必要的能力之一，即“脱离上下文推理”（与基于上下文的学习相对）。我们通过实验研究了脱离上下文推理。首先，我们在没有提供任何示例或演示的情况下，对LLM进行了描述测试的微调。在测试时，我们评估模型是否能通过测试。令我们惊讶的是，我们发现LLMs在这种情况下取得了成功。

    We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-
    
[^3]: GPT已经具备了金融素养：来自GPT金融素养测试的见解以及人们使用其作为咨询来源的初步测试

    GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice. (arXiv:2309.00649v1 [cs.CL])

    [http://arxiv.org/abs/2309.00649](http://arxiv.org/abs/2309.00649)

    GPT通过金融素养测试显示出具备成为大众金融机器顾问的能力，其中基于GPT-4的ChatGPT几乎完美地得分99%，揭示了金融素养正在成为最先进模型的新兴能力。

    

    通过使用金融素养测试，我们评估了GPT（一种大型语言模型）作为大众金融机器顾问的能力。基于GPT-3.5的Davinci和ChatGPT分别在金融素养测试中得分为66%和65%，而基于GPT-4的ChatGPT几乎完美地得到了99%的分数，这表明金融素养正在成为最先进模型的新兴能力。我们使用Judge-Advisor系统和一个储蓄困境来说明研究人员如何评估大型语言模型提供的建议利用情况。我们还提出了一些未来研究的方向。

    We assess the ability of GPT -- a large language model -- to serve as a financial robo-advisor for the masses, by using a financial literacy test. Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial literacy test, respectively, compared to a baseline of 33%. However, ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models. We use the Judge-Advisor System and a savings dilemma to illustrate how researchers might assess advice-utilization from large language models. We also present a number of directions for future research.
    
[^4]: 使用大型语言模型从数学文本中提取数学概念

    Extracting Mathematical Concepts with Large Language Models. (arXiv:2309.00642v1 [cs.CL])

    [http://arxiv.org/abs/2309.00642](http://arxiv.org/abs/2309.00642)

    使用大型语言模型从数学文本中提取数学概念，为自动术语提取和数学文本处理做出了贡献，同时提供了一套标准化的提取过程和新的标注工具。

    

    我们使用生成型大型语言模型（LLM）（如ChatGPT）从数学文本中提取数学概念，为自动术语提取（ATE）和数学文本处理领域做出了贡献，同时也为LLM研究作出了贡献。我们的工作在于，在数学领域（范畴论）中自动提取术语（关键词），使用的语料库是2020年左右的在线期刊《范畴的理论与应用》中的755个摘要。我们的研究与之前的工作不同之处在于：（1）对于什么使数学术语提取成为困难问题进行了更全面的分析；（2）对互相之间的标注者意见不一致进行了仔细的关注；（3）提供了一套供人工和机器标注者使用的指导方针，用以标准化提取过程；（4）引入了一个新的标注工具，帮助人类进行ATE，可适用于任何数学领域，甚至超越数学；（5）

    We extract mathematical concepts from mathematical text using generative large language models (LLMs) like ChatGPT, contributing to the field of automatic term extraction (ATE) and mathematical text processing, and also to the study of LLMs themselves. Our work builds on that of others in that we aim for automatic extraction of terms (keywords) in one mathematical field, category theory, using as a corpus the 755 abstracts from a snapshot of the online journal "Theory and Applications of Categories", circa 2020. Where our study diverges from previous work is in (1) providing a more thorough analysis of what makes mathematical term extraction a difficult problem to begin with; (2) paying close attention to inter-annotator disagreements; (3) providing a set of guidelines which both human and machine annotators could use to standardize the extraction process; (4) introducing a new annotation tool to help humans with ATE, applicable to any mathematical field and even beyond mathematics; (5
    
[^5]: 《错误信息管家：以COVID-19疫苗为例的经过筛选的推特数据集的概念验证》

    Misinformation Concierge: A Proof-of-Concept with Curated Twitter Dataset on COVID-19 Vaccination. (arXiv:2309.00639v1 [cs.CL])

    [http://arxiv.org/abs/2309.00639](http://arxiv.org/abs/2309.00639)

    错误信息管家是一个可行性概念，旨在提供关于社交媒体中流行的错误信息的可操作情报。它使用语言处理和机器学习工具来识别错误信息并推荐反驳信息，帮助干预和反击错误信息。

    

    我们展示了错误信息管家，这是一个可行性概念，可提供关于社交媒体中流行的错误信息的可操作情报。具体而言，它使用语言处理和机器学习工具来识别话语的子主题和辨别不准确/误导性的帖子；为决策者提供统计报告，以及及时了解流行的错误信息的整体情况；并推荐对特定错误信息的反驳信息，从数据语料库中识别出来，以便及时干预和反击错误信息。错误信息管家的概念验证使用了经过筛选的数据集，并可以在以下网址访问：https://demo-frontend-uy34.onrender.com/

    We demonstrate the Misinformation Concierge, a proof-of-concept that provides actionable intelligence on misinformation prevalent in social media. Specifically, it uses language processing and machine learning tools to identify subtopics of discourse and discern non/misleading posts; presents statistical reports for policy-makers to understand the big picture of prevalent misinformation in a timely manner; and recommends rebuttal messages for specific pieces of misinformation, identified from within the corpus of data - providing means to intervene and counter misinformation promptly. The Misinformation Concierge proof-of-concept using a curated dataset is accessible at: https://demo-frontend-uy34.onrender.com/
    
[^6]: 面向对齐语言模型的对抗攻击的基线防御

    Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])

    [http://arxiv.org/abs/2309.00614](http://arxiv.org/abs/2309.00614)

    这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。

    

    随着大型语言模型的普及，其安全漏洞变得至关重要。最近的研究表明，文本优化器可以生成绕过审查和对齐的越狱提示。借鉴对抗机器学习的丰富研究成果，我们从三个问题入手：在这个领域中什么样的威胁模型是实用的？基线防御技术在这个新领域中表现如何？LLM安全性与计算机视觉有何不同？我们对主导对抗LLM攻击的几种基线防御策略进行评估，讨论了每种策略在各种设置下的可行性和有效性。特别是，我们关注三种类型的防御：检测（基于困惑度）、输入预处理（改写和重新标记化）和对抗训练。我们讨论了白盒和灰盒设置，并讨论了每种考虑的防御策略在鲁棒性和性能之间的权衡。

    As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
    
[^7]: TouchStone: 用语言模型评估视觉-语言模型

    TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])

    [http://arxiv.org/abs/2308.16890](http://arxiv.org/abs/2308.16890)

    TouchStone提出了一种评估方法，使用强大的语言模型作为评委来全面评估大规模视觉-语言模型的各种能力。通过构建综合的视觉对话数据集和整合图像注释，评估包括识别、理解、对话和叙事等多个能力。

    

    大规模视觉-语言模型（LVLMs）近年来取得了快速进展，通过将视觉接收器与大型语言模型（LLMs）相连接，展现出了惊人的感知、理解和处理视觉信息的能力。然而，目前的评估主要关注识别和推理能力，缺乏对对话能力和视觉叙事能力的直接评估。本文提出了一种评估方法，使用强大的LLMs作为评委来全面评估LVLMs的各种能力。首先，我们构建了一个包含开放世界图像和问题的综合视觉对话数据集TouchStone，涵盖了五个主要能力和27个子任务。该数据集不仅涵盖了基础的识别和理解，还扩展到文学创作。其次，通过整合详细的图像注释，我们有效地将多模态输入内容转化为LLMs可以理解的形式。

    Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This
    
[^8]: 编程语言能通过指令调优相互提升吗？

    Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])

    [http://arxiv.org/abs/2308.16824](http://arxiv.org/abs/2308.16824)

    研究发现，编程语言可以在指令调优阶段相互促进，并显著提高彼此的能力。

    

    当人类程序员掌握了一种编程语言后，学习一种新的编程语言会更容易。在本报告中，我们重点探讨了在代码大规模语言模型的指令微调阶段中，编程语言是否能够通过相互提升来增强彼此的能力。我们在StarCoder上对8种流行的编程语言进行了广泛的实验（Python，JavaScript，TypeScript，C，C ++，Java，Go，HTML）。结果表明，编程语言可以显著提高彼此的能力。例如，通过在Python上训练的CodeM-Python 15B可以使Java的pass@1率绝对增加了17.95％。更令人惊讶的是，我们发现通过在HTML语料库上训练的CodeM-HTML 7B可以使Java的pass@1率绝对增加了15.24％。我们的训练数据已经发布在https://github.com/NL2Code/CodeM上。

    When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
    
[^9]: 将维基百科文章的链接预测任务作为自然语言推理任务

    Link Prediction for Wikipedia Articles as a Natural Language Inference Task. (arXiv:2308.16469v1 [cs.CL])

    [http://arxiv.org/abs/2308.16469](http://arxiv.org/abs/2308.16469)

    本文将维基百科文章的链接预测任务视为自然语言推理任务，采用了一种新的方法，并在DSAA-2023竞赛中取得了较高的评分。

    

    链接预测任务对于自动理解大型知识库的结构至关重要。本文介绍了我们在数据科学和高级分析2023年竞赛“高效和有效的链接预测”（DSAA-2023竞赛）中用包含948,233个训练样本和238,265个用于公共测试的语料库解决这一任务的系统。本文引入了一种将维基百科文章的链接预测问题建模为自然语言推理任务（NLI）的方法。受到近期自然语言处理和理解方面的进展启发，我们将链接预测作为一个NLI任务，其中将两个文章之间的链接存在视为前提，任务是基于文章中呈现的信息来确定该前提是否成立。我们的系统是基于用于维基百科文章链接预测的句对分类的实现。我们的系统实现了0.99996的Macro F1-score和1.00000的Macro F1-score。

    Link prediction task is vital to automatically understanding the structure of large knowledge bases. In this paper, we present our system to solve this task at the Data Science and Advanced Analytics 2023 Competition "Efficient and Effective Link Prediction" (DSAA-2023 Competition) with a corpus containing 948,233 training and 238,265 for public testing. This paper introduces an approach to link prediction in Wikipedia articles by formulating it as a natural language inference (NLI) task. Drawing inspiration from recent advancements in natural language processing and understanding, we cast link prediction as an NLI task, wherein the presence of a link between two articles is treated as a premise, and the task is to determine whether this premise holds based on the information presented in the articles. We implemented our system based on the Sentence Pair Classification for Link Prediction for the Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000 Macro F1-s
    
[^10]: BioCoder: 一种带有上下文语用知识的生物信息学代码生成基准

    BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])

    [http://arxiv.org/abs/2308.16458](http://arxiv.org/abs/2308.16458)

    BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。

    

    预训练的语言模型（如ChatGPT）显著改进了代码生成。随着这些模型的扩大，需要输出来处理更复杂的任务的需求也越来越多。此外，在生物信息学中，生成功能程序由于领域知识量大、需要复杂的数据操作和复杂的功能依赖关系而面临额外的挑战。在这里，我们介绍了BioCoder，这是一个用于评估现有预训练模型在生成生物信息学代码方面的基准。与函数代码生成有关，BioCoder涵盖了可能的包依赖关系、类声明和全局变量。它包括来自GitHub的1026个Python和Java函数和1243个方法，以及来自Rosalind项目的253个示例。BioCoder还结合了一个用于评估的模糊测试框架，我们已经应用它来评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT。

    Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
    
[^11]: LM-Infinite: 大规模语言模型的简单即时长度推广

    LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])

    [http://arxiv.org/abs/2308.16137](http://arxiv.org/abs/2308.16137)

    LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。

    

    近年来，在Transformer-based大规模语言模型（LLM）在各个领域取得了显著的进展。随着这些LLM在越来越复杂的任务上的部署，它们往往面临着对长时间推理过程或理解更大上下文的需求。在这些情况下，LLM在长序列上的长度推广失败变得更加突出。大多数预训练方案将训练序列截断到固定长度（例如LLaMa的2048）。即使使用了相对位置编码来应对这个问题，LLM在更长的上下文之后往往难以生成流畅的文本，更不用说进行下游任务了。常见的解决方案，如在更长的语料库上进行微调，往往需要耗费大量的硬件和时间成本，并需要进行仔细的训练过程设计。为了更高效地利用现有LLM的生成能力，我们在理论和实证上研究了主要的分布外(OOD) f

    In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
    
[^12]: 美国法律体系是否准备好应对人工智能对人类价值观的挑战？

    Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])

    [http://arxiv.org/abs/2308.15906](http://arxiv.org/abs/2308.15906)

    美国法律需要加强应对生成式人工智能对人类价值观挑战的能力，并提供积极、可审计的指导，以填补现有法律框架在保护基本价值观方面的空白和不确定性。

    

    我们的跨学科研究调查了美国法律在面对生成式人工智能对人类价值观挑战时的有效性。通过分析专家研讨会期间制定的多种假设情景，我们发现现有法律框架在保护自主权、隐私权、尊严、多样性、平等以及身心健康等基本价值观方面存在明显的空白和不确定性。宪法和民权法似乎无法对人工智能生成的歧视性产出提供足够的保护。此外，即使我们排除第230条款提供的责任保护，由于人工智能系统的复杂和不透明性，证明诽谤和产品责任索赔的因果关系也是一项具有挑战性的任务。为了应对生成式人工智能带来的独特和难以预测的威胁，我们主张建立能够适应新威胁并为行业利益相关者提供积极、可审计的指导的法律框架。

    Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
    
[^13]: ParaGuide: 用于即插即用文本风格转移的引导性扩散改写器

    ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])

    [http://arxiv.org/abs/2308.15459](http://arxiv.org/abs/2308.15459)

    ParaGuide是一种用于通用风格转移的引导性扩散改写器，可以灵活适应任意目标风格，通过梯度引导和改写条件的扩散模型实现文本的风格转变，同时保留语义信息。

    

    文本风格转移是在保留意义的同时转变文本的风格属性的任务。目标风格可以以多种方式定义，从单一属性（例如正式性）到作者（例如莎士比亚）。先前的无监督风格转移方法通常依赖于大量标记数据，仅适用于固定的风格集，或需要大型语言模型。相反，我们引入了一种新的基于扩散的通用风格转移框架，可以在推理时灵活适应任意目标风格。我们的参数高效方法ParaGuide利用了改写条件的扩散模型以及来自现成的分类器和强大的风格嵌入器的梯度引导，以转变文本的风格同时保留语义信息。我们在Enron邮件语料库上进行了验证，包括人工和自动评估，并发现其在正式性和... (内容太多，请参考英文摘要)

    Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
    
[^14]: Do-Not-Answer: 用于评估LLMs中安全机制的数据集

    Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. (arXiv:2308.13387v1 [cs.CL])

    [http://arxiv.org/abs/2308.13387](http://arxiv.org/abs/2308.13387)

    这项工作收集了第一个用于评估LLMs中安全机制的开源数据集，并通过训练分类器实现了与GPT-4在自动安全评估上相媲美的结果。

    

    随着大型语言模型（LLMs）的快速发展，出现了新的难以预测的有害功能。这要求开发者能够通过评估LLMs中的“危险能力”来识别风险，以负责任地部署LLMs。在这项工作中，我们收集了第一个用于评估LLMs中安全机制的开源数据集，并以较低成本部署更安全的开源LLMs。我们的数据集由负责任的语言模型不应遵循的指令精心策划和过滤而成。我们对六种流行的LLMs对这些指令的回应进行了注释和评估。基于我们的标注，我们继续训练了几个类似BERT的分类器，并发现这些小分类器在自动安全评估上可以达到与GPT-4相当的结果。警告：本文包含可能具有冒犯性、有害性或偏见性的示例数据。

    With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to be able to identify risks through the evaluation of "dangerous capabilities" in order to responsibly deploy LLMs. In this work, we collect the first open-source dataset to evaluate safeguards in LLMs, and deploy safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We annotate and assess the responses of six popular LLMs to these instructions. Based on our annotation, we proceed to train several BERT-like classifiers, and find that these small classifiers can achieve results that are comparable with GPT-4 on automatic safety evaluation. Warning: this paper contains example data that may be offensive, harmful, or biased.
    
[^15]: 建构语法与语言模型

    Construction Grammar and Language Models. (arXiv:2308.13315v1 [cs.CL])

    [http://arxiv.org/abs/2308.13315](http://arxiv.org/abs/2308.13315)

    最新的深度学习和自然语言处理进展为计算方法和建构语法研究之间的协同关系提供了机会，本章提供了三种不同的计算方法与建构语法相互作用的途径，并重点关注语言模型。

    

    深度学习和自然语言处理的最新进展已经产生了强大的模型，这些模型主要在一个填空式任务上进行训练，并显示出具有丰富语言信息的一些证据，包括一些构式知识。这一突破性的发现为计算方法和建构语法研究之间的协同关系提供了令人兴奋的机会。在本章中，我们探讨了三种不同的计算方法和建构语法之间的相互作用方式：（一）文本分析的计算方法、（二）计算建构语法，以及（三）深度学习模型，特别关注语言模型。我们在介绍计算方法的基础上接触第一种和第二种方法，然后提供一种易于理解但全面的深度学习模型概述，也解决了建构语法学家可能存在的保留意见。此外，我们还深入探讨了...

    Recent progress in deep learning and natural language processing has given rise to powerful models that are primarily trained on a cloze-like task and show some evidence of having access to substantial linguistic information, including some constructional knowledge. This groundbreaking discovery presents an exciting opportunity for a synergistic relationship between computational methods and Construction Grammar research. In this chapter, we explore three distinct approaches to the interplay between computational methods and Construction Grammar: (i) computational methods for text analysis, (ii) computational Construction Grammar, and (iii) deep learning models, with a particular focus on language models. We touch upon the first two approaches as a contextual foundation for the use of computational methods before providing an accessible, yet comprehensive overview of deep learning models, which also addresses reservations construction grammarians may have. Additionally, we delve into e
    
[^16]: 从指令到内在人类价值 - 大模型对齐目标的调查

    From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])

    [http://arxiv.org/abs/2308.12014](http://arxiv.org/abs/2308.12014)

    本文综合调查了大模型对齐目标的不同观点，并追踪其演化路径，旨在帮助确定最重要的目标。

    

    大模型，例如大型语言模型（LLM），通常在大规模数据上进行预训练，并由大量参数组成，不仅在各种任务中获得显著改进的性能，还呈现出较小模型所没有的新能力。然而，大模型与日常生活的日益交织可能带来潜在风险，并可能造成严重的社会危害。因此，许多努力已经进行了，以使LLM与人类对齐，以使它们更好地遵循用户的指令并满足人类的偏好。然而，“与何对齐”还没有得到充分讨论，不当的对齐目标甚至可能适得其反。在本文中，我们对现有工作中的不同对齐目标进行了综合调查，并追踪它们的演化路径，以帮助确定最基本的目标。特别是，我们从对齐目标的定义和对齐评估两个角度进行了相关工作的调查。我们的分析包括...

    Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses
    
[^17]: 使用自动语音识别系统和深度学习主题模型在智能手机采集的自由回答语音录音中识别与抑郁相关的主题

    Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model. (arXiv:2308.11773v1 [cs.CL])

    [http://arxiv.org/abs/2308.11773](http://arxiv.org/abs/2308.11773)

    通过自动语音识别系统和深度学习主题模型，我们在智能手机采集的语音录音中识别出与抑郁相关的29个主题，并确定了其中6个主题作为抑郁的风险主题。此研究表明，通过长期监测语言使用，可以了解主题的出现与抑郁之间的关联。

    

    语言使用已被证明与抑郁相关，但需要大规模验证。传统方法如诊所研究费用高昂。因此，自然语言处理已被应用于社交媒体上预测抑郁，但仍存在一些限制-缺乏验证标签、样本偏差和缺乏上下文。我们的研究使用Whisper工具和BERTopic模型在来自265名参与者的3919个智能手机录音中识别出29个主题。其中6个主题的PHQ-8中位数大于或等于10被视为抑郁的风险主题：没有期望、睡眠、心理治疗、理发、学习和课程。为了阐明主题的出现和与抑郁的关联，我们比较了识别出的主题在行为（来自可穿戴设备）和语言特征方面的差异。还对主题转变和随时间变化的抑郁严重程度之间的相关性进行了研究，表明了长期监测语言使用的重要性。我们还进行了实证研究

    Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested
    
[^18]: UniDoc: 一种通用的大型多模态模型，用于同时进行文本检测、识别、定位和理解

    UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])

    [http://arxiv.org/abs/2308.11592](http://arxiv.org/abs/2308.11592)

    UniDoc是一种通用的大型多模态模型，具备文本检测和识别能力，并通过任务之间的有益交互提高每个任务的性能，达到了在多个基准测试中的最先进水平。

    

    在大语言模型（LLMs）时代，多模态理解领域取得了巨大的进展。然而，现有的高级算法受限于有效利用大型预训练模型所固有的巨大表示能力和丰富的世界知识，并且在文本丰富场景中任务之间的有益连接尚未充分探索。在这项工作中，我们介绍了UniDoc，一种新颖的多模态模型，具备现有方法所缺乏的文本检测和识别能力。此外，UniDoc利用任务之间的有益交互来提高每个单独任务的性能。为了实现UniDoc，我们对贡献的大规模指令跟随数据集进行了统一的多模态指导调优。定量和定性实验结果表明，UniDoc在多个具有挑战性的基准测试中取得了最先进的分数。

    In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our kn
    
[^19]: LLaMA-Reviewer: 通过参数高效微调推进大型语言模型在代码审查自动化中的应用（实证研究）

    LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])

    [http://arxiv.org/abs/2308.11148](http://arxiv.org/abs/2308.11148)

    本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。

    

    代码审查活动的自动化长期以来一直是软件工程领域的追求，主要通过许多领域特定的预训练模型来解决。尽管这些模型取得了一定的成功，但它们经常需要大量的资源从头开始进行预训练。相比之下，大型语言模型（LLMs）在补充领域特定知识的情况下展现出了令人着迷的潜力。然而，它们在自动化代码审查任务方面的潜力仍然很少被探索。为了填补这一研究空白，我们提出了LLaMA-Reviewer，这是一个创新的框架，它利用了流行的LLM——LLaMA在代码审查领域的能力。考虑到资源限制，该框架采用了参数高效微调（PEFT）方法，以极少的可训练参数提供高性能。我们对LLaMA-Reviewer进行了广泛的评估，使用了两个不同的公开数据集。值得注意的是，即使在只使用不到1%的可训练参数的情况下，它也取得了显著的成果。

    The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
    
[^20]: CausalLM不适用于上下文学习

    CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06912](http://arxiv.org/abs/2308.06912)

    最近的研究显示，上下文学习中使用前缀语言模型（PrefixLM）比因果语言模型（CausalLM）效果更好。本文通过理论分析证明，虽然两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，因果语言模型的收敛动态遵循在线梯度下降算法，不保证收敛到最优解。

    

    最近的实证证据表明，在上下文学习中，使用前缀语言模型（PrefixLM）表现更好，其允许上下文样本相互关注；相比之下，因果语言模型（CausalLM）使用自回归注意力机制，禁止上下文样本关注未来的样本。虽然这个结果是直观的，但从理论角度并不清楚。本文采用理论方法，分析了在特定参数构建下，前缀语言模型和因果语言模型的收敛行为。分析结果显示，两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，而因果语言模型的收敛动态遵循在线梯度下降算法，即使样本数量趋于无穷，也不能保证收敛到最优解。我们通过对合成数据的经验实验来支持我们的理论观点。

    Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
    
[^21]: 多模态大语言模型在预测语言处理期间表现出人类视觉-语言集成的证据

    Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])

    [http://arxiv.org/abs/2308.06035](http://arxiv.org/abs/2308.06035)

    这篇论文研究了多模态大语言模型（mLLMs）在预测语言处理过程中与人类的视觉-语言集成能力是否一致的问题，并通过实验验证了mLLMs的多模态输入方法可以减少认知负荷，提高感知和理解能力。

    

    大语言模型（LLMs）的先进语言处理能力引发了关于它们是否能够复制人类认知过程的争议。LLMs和人类在语言处理方面的一个区别在于，语言输入通常建立在多个知觉模态上，而大多数LLMs仅处理基于文本的信息。多模态基础使人类能够整合视觉背景与语言信息，从而对即将出现的单词的空间施加限制，减少认知负荷，提高感知和理解能力。最近的多模态LLMs（mLLMs）结合了视觉和语言嵌入空间，并使用变压器类型的注意机制进行下一个单词的预测。在多大程度上，基于多模态输入的预测语言处理在mLLMs和人类中吻合？为了回答这个问题，200名被试观看了短的视听剪辑，并估计了即将出现的动词或名词的可预测性。

    The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
    
[^22]: 为放射学构建通用基础模型的探索

    Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])

    [http://arxiv.org/abs/2308.02463](http://arxiv.org/abs/2308.02463)

    本研究旨在为放射学构建通用基础模型，提出了一个大规模的医学多模态数据集和支持不同放射学任务的架构，同时提出了一个新的评估基准。

    

    本研究旨在启动放射学基础模型的开发，称为RadFM。我们从数据、模型设计和评估的角度全面考虑了基础模型的构建。我们的贡献可总结如下：（i）构建了一个大规模的医学多模态数据集MedMD，包括1600万个2D和3D医学扫描。据我们所知，这是第一个包含3D医学扫描的多模态数据集。（ii）我们提出了一种架构，使得可视条件生成预训练成为可能，可以将文本输入与2D或3D医学扫描交错，生成不同放射学任务的响应。该模型首先在MedMD上进行了预训练，然后在RadMD上进行了特定领域的微调，RadMD是MedMD的放射学清理版本，包含300万个放射学的视觉语言对。（iii）我们提出了一个新的评估基准，包括五个任务，旨在全面评估该模型的能力。

    In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
    
[^23]: 先思考再回应：为共情回应生成集成常识的因果解释

    Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])

    [http://arxiv.org/abs/2308.00085](http://arxiv.org/abs/2308.00085)

    本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成。该方法综合考虑了用户的角度和系统的角度，并通过集成常识知识提升了ChatGPT在系统的推理能力。实验结果表明，该方法在多项评估指标上超过了其他方法。

    

    最近的共情回应生成方法试图整合常识知识或对情绪原因的推理，以更好地理解用户的经历和感受。然而，这些方法主要关注从用户的角度理解上下文的因果关系，忽略了系统的角度。本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成，同时考虑用户的角度（用户的欲望和反应）和系统的角度（系统的意图和反应）。我们通过将上下文学习与常识知识相结合，增强了ChatGPT在系统的角度上的推理能力。然后，我们将基于常识的因果解释与ChatGPT和基于T5模型的方法进行整合。实验评估表明，我们的方法在自动评估和人工评估上优于其他可比较的方法。

    Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
    
[^24]: AutoAlign：基于大型语言模型的全自动有效知识图谱对齐方法

    AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])

    [http://arxiv.org/abs/2307.11772](http://arxiv.org/abs/2307.11772)

    AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。

    

    知识图谱间的实体对齐任务旨在识别出两个不同知识图谱中表示相同实体的每对实体。许多基于机器学习的方法已被提出用于这个任务。然而，据我们所知，现有的方法都需要手工制作的种子对齐，这是非常昂贵的。在本文中，我们提出了第一个名为AutoAlign的完全自动对齐方法，它不需要任何手工制作的种子对齐。具体而言，对于谓词嵌入，AutoAlign使用大型语言模型构建谓词近邻图，自动捕捉两个知识图谱中谓词的相似性。对于实体嵌入，AutoAlign首先使用TransE独立计算每个知识图谱的实体嵌入，然后通过计算基于实体属性的实体相似性，将两个知识图谱的实体嵌入移动到相同的向量空间中。因此，AutoAlign实现了谓词对齐和实体对齐。

    The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
    
[^25]: OUTFOX: 基于上下文学习和对抗生成例子的LLM生成论文检测

    OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])

    [http://arxiv.org/abs/2307.11729](http://arxiv.org/abs/2307.11729)

    OUTFOX是一个新的框架，通过允许检测器和攻击者考虑彼此的输出，提高了LLM生成文本检测器的鲁棒性。攻击者利用检测器的预测标签作为示例进行上下文学习，并生成难以检测的对抗生成的论文。

    

    大型语言模型(LLMs)已经达到了与人类写作相当的流利程度，很难区分人类写作和LLM生成的文本。这增加了LLMs被误用的风险，并需要开发检测器来识别LLM生成的文本。然而，现有的检测器通过简单地改写LLM生成的文本来降低检测准确性。此外，这些检测器在学生在写作作业（如论文）中使用LLMs并迅速学会如何规避这些检测器的真实生活情况下的有效性尚未被探讨。在本文中，我们提出了OUTFOX，一个新的框架，通过允许检测器和攻击者考虑彼此的输出并将其应用于学生论文领域来提高LLM生成文本检测器的鲁棒性。在我们的框架中，攻击者使用检测器的预测标签作为上下文学习的示例，并对难以检测的对抗生成论文进行生成。

    Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard
    
[^26]: 在实践中分析数据集注释质量管理

    Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.08153](http://arxiv.org/abs/2307.08153)

    该论文调查分析了自然语言数据集的创建过程中的质量管理实践，并提供了相应的建议。研究表明，流行数据集中存在较多的错误注释、偏见或注释伪像。这项研究的贡献是在这一领域进行了大规模的实证分析，并提出了实践指南。

    

    数据质量对于训练准确、公正和可信的机器学习模型以及它们的正确评估至关重要。然而，最近的研究表明，即使是用于训练和评估最先进模型的流行数据集中也存在大量的错误注释、偏见或注释伪像。关于注释项目的最佳实践和指南已经存在，但据我们所知，迄今为止还没有进行大规模分析，以研究创建自然语言数据集时实际进行的质量管理以及是否遵循了这些建议。因此，我们首先调查并总结了文献中描述的数据集创建的推荐质量管理实践，并提供了如何应用这些实践的建议。然后，我们编制了一个由591篇科学出版物组成的文本数据集语料库，并针对与质量相关的方面进行了注释，例如注释者管理、一致性、仲裁或数据验证。

    Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models and their correct evaluation. Recent works, however, have shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, bias or annotation artifacts. There exist best practices and guidelines regarding annotation projects. But to the best of our knowledge, no large-scale analysis has been performed as of yet on how quality management is actually conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions on how to apply them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication or data validat
    
[^27]: 一篇关于多模态深度学习在生物医学图像和文本中的扫描综述

    A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])

    [http://arxiv.org/abs/2307.07362](http://arxiv.org/abs/2307.07362)

    这篇综述旨在提供对多模态深度学习在生物医学图像和文本中进行联合学习的当前状况的全面概述，并探索未来的研究方向和该领域的研究空白。

    

    未来的计算辅助诊断和预后系统应该能够同时处理多模态数据。多模态深度学习（MDL）涉及多种数据源（如图像和文本）的整合，有潜力彻底改变生物医学数据的分析和解释。然而，这一领域直到最近才引起研究人员的注意。因此，有必要对这个主题进行系统综述，确定当前工作的局限性，并探索未来的方向。在这篇综述中，我们旨在提供对该领域 current state 的全面概述，并重点关注生物医学图像和文本的联合学习，主要是因为这两种数据类型在 MDL 研究中最常用。本研究回顾了多模态深度学习在五个任务中的当前应用：（1）报告生成，（2）视觉问答，（3）交叉...

    Computer-assisted diagnostic and prognostic systems of the future should be capable of simultaneously processing multimodal data. Multimodal deep learning (MDL), which involves the integration of multiple sources of data, such as images and text, has the potential to revolutionize the analysis and interpretation of biomedical data. However, it only caught researchers' attention recently. To this end, there is a critical need to conduct a systematic review on this topic, identify the limitations of current work, and explore future directions. In this scoping review, we aim to provide a comprehensive overview of the current state of the field and identify key concepts, types of studies, and research gaps with a focus on biomedical images and texts joint learning, mainly because these two were the most commonly available data types in MDL research. This study reviewed the current uses of multimodal deep learning on five tasks: (1) Report generation, (2) Visual question answering, (3) Cros
    
[^28]: ACTI在EVALITA 2023中的综述：阴谋论辨识任务概述

    ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])

    [http://arxiv.org/abs/2307.06954](http://arxiv.org/abs/2307.06954)

    ACTI在EVALITA 2023中的阴谋论辨识任务共有15支团队参与，通过使用大型语言模型判断阴谋内容和分类，得出了关于利用这些模型抵制在在线平台传播错误信息的结论。

    

    阴谋论辨识任务是Evalita 2023首次提出的新共享任务。ACTI挑战仅基于Telegram上的阴谋频道评论，分为两个子任务：(i) 阴谋内容分类：辨识阴谋内容和(ii) 阴谋类别分类：针对特定阴谋理论分类。共有15支团队参与了该任务，总共提交了81个结果。我们说明了基于大型语言模型的最佳方法。最后，我们得出了关于利用这些模型来抵制在在线平台上传播错误信息的结论。

    Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
    
[^29]: SCALE: 提升高级语言模型评估的复杂性

    SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.09237](http://arxiv.org/abs/2306.09237)

    该论文提出了一个新颖的自然语言处理基准测试，挑战当前大型语言模型在处理长文档、利用领域专业知识、多语言理解和多任务处理方面的能力。基准测试包含瑞士法律系统的多样化法律NLP数据集，允许进行对底层非英语、固有多语言的法律系统进行全面研究。

    

    最近在大型语言模型（LLM）方面取得的进展已经饱和了许多自然语言处理基准测试（包括专业领域的基准测试），强调了需要新颖、更具挑战性的测试来正确评估LLM的能力。在本文中，我们引入了一个新颖的自然语言处理基准测试，对当前LLM的四个关键方面提出了挑战：处理长文档（多达50K个标记）、利用领域专业知识（体现在法律文本中）、多语言理解（涵盖五种语言）和多任务处理（包括法律文件到文件信息检索、法庭视图生成、重要决策摘要、引用提取和八个具有挑战性的文本分类任务）。我们的基准测试包含了来自瑞士法律系统的多样的法律NLP数据集，可以对底层非英语、固有多语言的联邦法律系统进行全面研究。尽管最近取得了进展，但对于强烈的审查/分析任务，高效地处理长文档仍然是一个挑战。

    Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
    
[^30]: AMR4NLI: 从语义图中获得可解释和鲁棒的NLI度量

    AMR4NLI: Interpretable and robust NLI measures from semantic graphs. (arXiv:2306.00936v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00936](http://arxiv.org/abs/2306.00936)

    该论文提出了一种从语义图中获取可解释和鲁棒的NLI度量方法，与使用上下文嵌入的方法相比具有补充性，可以在混合模型中结合使用。

    

    自然语言推理（NLI）任务要求判断给定的前提（用自然语言表达）是否蕴含给定的假设。NLI基准包含了蕴含性的人工评分，但是驱动这些评分的意义关系并未形式化。是否可以以一种可解释且鲁棒的方式更明确地表示句子对之间的关系？我们比较了表示前提和假设的语义结构，包括一组上下文化嵌入和语义图（抽象意义表示），并使用可解释的度量方法来衡量假设是否是前提的语义子结构。在三个英语基准测试中的评估发现，上下文化嵌入和语义图都有其价值；而且它们提供了互补的信号，并可以在混合模型中一起利用。

    The task of natural language inference (NLI) asks whether a given premise (expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human ratings of entailment, but the meaning relationships driving these ratings are not formalized. Can the underlying sentence pair relationships be made more explicit in an interpretable yet robust fashion? We compare semantic structures to represent premise and hypothesis, including sets of contextualized embeddings and semantic graphs (Abstract Meaning Representations), and measure whether the hypothesis is a semantic substructure of the premise, utilizing interpretable metrics. Our evaluation on three English benchmarks finds value in both contextualized embeddings and semantic graphs; moreover, they provide complementary signals, and can be leveraged together in a hybrid model.
    
[^31]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^32]: InterviewBot：面向大学招生考试的实时端到端对话系统

    InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15049](http://arxiv.org/abs/2303.15049)

    论文介绍了一种基于神经元的端到端对话模型- InterviewBot，该模型利用与输入的历史对话和定制主题集成到同一嵌入空间中来评估外国学生申请美国大学的学术和文化准备情况。同时，为克服基于变形金刚编码器-解码器模型的输入/输出大小限制，提出了上下文关注和主题存储两种新方法。该模型在统计和动态测试中表现出了流畅性和上下文感知的高度满意。

    

    我们提出了InterviewBot，它动态将会话历史和定制主题集成到一致的嵌入空间中，以进行10分钟的混合领域（开放和封闭）对话，以评估外国学生申请美国大学的学术和文化准备情况。为构建基于神经元的端到端对话模型，我们自动转录了7,361个人对人的面试音频记录，其中440个进行了手动纠正以进行微调和评估。为了克服基于变形金刚编码器-解码器模型的输入/输出大小限制，我们提出了两种新方法，上下文关注和主题存储，使模型能够进行相关和一致的交互。我们的最终模型在统计上被测试，通过将其响应与面试数据进行比较，并动态地邀请专业面试官和各种学生与其实时交互，在流畅性和上下文感知方面发现其非常令人满意。

    We present the InterviewBot that dynamically integrates conversation history and customized topics into a coherent embedding space to conduct 10 mins hybrid-domain (open and closed) conversations with foreign students applying to U.S. colleges for assessing their academic and cultural readiness. To build a neural-based end-to-end dialogue model, 7,361 audio recordings of human-to-human interviews are automatically transcribed, where 440 are manually corrected for finetuning and evaluation. To overcome the input/output size limit of a transformer-based encoder-decoder model, two new methods are proposed, context attention and topic storing, allowing the model to make relevant and consistent interactions. Our final model is tested both statistically by comparing its responses to the interview data and dynamically by inviting professional interviewers and various students to interact with it in real-time, finding it highly satisfactory in fluency and context awareness.
    
[^33]: 基于卷积序列到序列网络将图像特征集成到多语言视觉问答中

    Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v1 [cs.CV])

    [http://arxiv.org/abs/2303.12671](http://arxiv.org/abs/2303.12671)

    该论文针对VLSP2022-EVJVQA共享任务提出了一种基于卷积序列到序列网络的方法，在多语言视觉问答中将预训练的VQA模型和图像特征集成，获得了很好的效果。

    

    视觉问答(VQA)是一项要求计算机基于图像回答输入问题的任务。这个任务对人类来说很容易，但对计算机来说很具有挑战性。VLSP2022-EVJVQA共享任务在一个新发布的数据集UIT-EVJVQA中进行了多语言领域的VQA任务，其中问题和答案用三种不同语言编写：英语、越南语和日语。我们将这个挑战作为一个序列到序列的学习任务，通过将来自预训练的最先进的VQA模型和图像特征的提示与卷积序列到序列网络集成在一起来生成所需的答案。我们在公共测试集上获得了F1得分达到0.3442，在私有测试集上获得了0.4210的好成绩，并在比赛中取得了第三名。

    Visual Question Answering (VQA) is a task that requires computers to give correct answers for the input questions based on the images. This task can be solved by humans with ease but is a challenge for computers. The VLSP2022-EVJVQA shared task carries the Visual Question Answering task in the multilingual domain on a newly released dataset: UIT-EVJVQA, in which the questions and answers are written in three different languages: English, Vietnamese and Japanese. We approached the challenge as a sequence-to-sequence learning task, in which we integrated hints from pre-trained state-of-the-art VQA models and image features with Convolutional Sequence-to-Sequence network to generate the desired answers. Our results obtained up to 0.3442 by F1 score on the public test set, 0.4210 on the private test set, and placed 3rd in the competition.
    
[^34]: BiasTestGPT: 使用ChatGPT对语言模型进行社会偏见测试

    BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models. (arXiv:2302.07371v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07371](http://arxiv.org/abs/2302.07371)

    BiasTestGPT是一个开源的偏见测试框架，利用ChatGPT进行测试句子的生成，可以更好地检测语言模型中的社会偏见，尤其是在交叉偏见等挑战性情境中。

    

    预训练语言模型（PLMs）存在固有的社会偏见，可能导致有害的现实影响。这种社会偏见是通过PLMs对一组测试句子中不同社会群体和属性的概率值进行测量得出的。然而，目前的偏见测试方法非常繁琐，因为测试句子要么是从有限的一组手动模板中生成，要么需要昂贵的众包。我们提出使用ChatGPT进行可控生成测试句子，以满足用户指定的任意社会群体和属性组合。与基于模板的方法相比，我们使用ChatGPT进行测试句子生成的方法在检测社会偏见方面更为优越，特别是在交叉偏见等具有挑战性的情境中。我们提供了一个开源的全面偏见测试框架（BiasTestGPT），托管在HuggingFace上，可以插入到任何开源PLM中进行偏见测试。

    Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications. Such social biases are measured through the probability values that PLMs output for different social groups and attributes appearing in a set of test sentences. However, bias testing is currently cumbersome since the test sentences are generated either from a limited set of manual templates or need expensive crowd-sourcing. We instead propose using ChatGPT for controllable generation of test sentences, given any arbitrary user-specified combination of social groups and attributes appearing in the test sentences. When compared to template-based methods, our approach using ChatGPT for test sentence generation is superior in detecting social bias, especially in challenging settings such as intersectional biases. We present an open-source comprehensive bias testing framework (BiasTestGPT), hosted on HuggingFace, that can be plugged into any open-source PLM for bias testing. W
    
[^35]: 揭示未知：基于实体链接的知识库外提及发现

    Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07189](http://arxiv.org/abs/2302.07189)

    本文提出了基于BERT的实体链接方法BLINKout，通过与特殊NIL实体匹配来识别没有相应KB实体的实体提及，相较于现有方法具有优势。

    

    从文本中发现知识库（KB）外的实体提及，在KB维护中起着至关重要的作用，但并未被完全开发。当前的方法主要局限于简单的基于阈值的方法和基于特征的分类，并且用于评估的数据集相对较少。本文提出了BLINKout，一种新的基于BERT的实体链接（EL）方法，可以通过将提及与特殊的NIL实体进行匹配来识别没有相应KB实体的提及。为了更好地利用BERT，我们提出了包括NIL实体表示和分类在内的新技术，并增强了其同义词。我们还提出了KB修剪和版本控制策略，以自动从常见的KB EL数据集构建出KB外的数据集。在医学本体论、UMLS、SNOMED CT等五个不同领域中，对临床笔记、生物医学出版物和维基百科文章的结果表明，BLINKout在识别知识库外提及方面优于现有方法。

    Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also propose KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT,
    
[^36]: 超越三元组：利用最多的数据进行多模态机器翻译

    Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation. (arXiv:2212.10313v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10313](http://arxiv.org/abs/2212.10313)

    本论文提出了一个新的多模态机器翻译方法，利用大规模非三元组数据来增强翻译质量，并构建了一个新的英汉电子商务多模态翻译数据集EMMT。

    

    多模态机器翻译旨在通过引入其他模态（如视觉）的信息来提高翻译质量。以往的多模态机器翻译系统主要关注更好地获取和利用视觉信息，并倾向于验证其方法在与图像相关的数据集上。这些研究面临两个挑战。首先，它们只能利用三元组数据（带有图像的双语文本），这种数据稀缺；其次，当前的基准相对受限，不符合真实场景。因此，本文相应地建立了多模态机器翻译的新方法和新数据集。首先，我们提出了一个名为2/3-Triplet的框架，并采用两种新方法来增强多模态机器翻译，即利用大规模非三元组数据：单语图像文本数据和平行文本数据。其次，我们构建了一个英汉电子商务多模态翻译数据集（包括训练和测试），命名为EMMT，其中测试集的选择经过精心考虑，因为其中有些词是模糊的，需要进行翻译。

    Multimodal machine translation (MMT) aims to improve translation quality by incorporating information from other modalities, such as vision. Previous MMT systems mainly focus on better access and use of visual information and tend to validate their methods on image-related datasets. These studies face two challenges. First, they can only utilize triple data (bilingual texts with images), which is scarce; second, current benchmarks are relatively restricted and do not correspond to realistic scenarios. Therefore, this paper correspondingly establishes new methods and new datasets for MMT. First, we propose a framework 2/3-Triplet with two new approaches to enhance MMT by utilizing large-scale non-triple data: monolingual image-text data and parallel text-only data. Second, we construct an English-Chinese {e}-commercial {m}ulti{m}odal {t}ranslation dataset (including training and testing), named EMMT, where its test set is carefully selected as some words are ambiguous and shall be trans
    
[^37]: 社交媒体挖掘处方药物的毒性监测: 从头到尾的管道，挑战和未来工作。

    Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work. (arXiv:2211.10443v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.10443](http://arxiv.org/abs/2211.10443)

    本文描述了一个为社交媒体挖掘非医学处方药物使用信息而开发的端到端管道，并讨论了挑战和未来工作。

    

    物质使用、物质使用障碍和与物质使用相关的过量是全球和美国的主要公共卫生问题。从公共卫生的角度来解决这些问题的一个关键方面是改进监测系统。传统的监测系统滞后，而社交媒体是及时数据的潜在有用来源。然而，从社交媒体中挖掘知识是具有挑战性的，并且需要开发先进的人工智能，特别是自然语言处理（NLP）和机器学习方法。我们开发了一个复杂的从社交媒体中挖掘非医学处方药物使用信息的端到端管道，即Twitter和Reddit。我们的管道采用了监督式机器学习和NLP来过滤噪音和描述交流内容。在本文中，我们描述了我们在四年内开发的端到端管道。除了描述我们的数据挖掘基础设施外，我们还讨论了现有的挑战。

    Substance use, substance use disorder, and overdoses related to substance use are major public health problems globally and in the United States. A key aspect of addressing these problems from a public health standpoint is improved surveillance. Traditional surveillance systems are laggy, and social media are potentially useful sources of timely data. However, mining knowledge from social media is challenging, and requires the development of advanced artificial intelligence, specifically natural language processing (NLP) and machine learning methods. We developed a sophisticated end-to-end pipeline for mining information about nonmedical prescription medication use from social media, namely Twitter and Reddit. Our pipeline employs supervised machine learning and NLP for filtering out noise and characterizing the chatter. In this paper, we describe our end-to-end pipeline developed over four years. In addition to describing our data mining infrastructure, we discuss existing challenges 
    
[^38]: 连续向量空间中的数学表达式语义表示

    Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08142](http://arxiv.org/abs/2211.08142)

    该论文提出了一种在连续向量空间中表示数学表达式的方法，并使用序列到序列框架的编码器生成向量表示来捕捉数学语义，比自动编码器效果更好。

    

    数学符号在STEM文献中占据了很大一部分，但是，为公式找到语义表示仍然是一个具有挑战性的问题。由于数学符号是精确的，在字符微小变化时其含义会发生显著变化，因此适用于自然文本的方法并不一定适用于数学表达式。在这项工作中，我们描述了一种在连续向量空间中表示数学表达式的方法。我们使用一个序列到序列框架的编码器，训练其在视觉上不同但在数学上等价的表达式上生成向量表示（或嵌入）。我们将这种方法与自动编码器进行比较，并表明前者更能捕捉数学语义。最后，为了促进未来的研究，我们发布了一份等价的超越和代数表达式对的语料库。

    Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
    
[^39]: 从智能回复中提取主动模式进行认证

    Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2207.10802](http://arxiv.org/abs/2207.10802)

    该论文研究了在智能回复应用程序中潜在的信息泄漏漏洞，并且提出了一种在实际设置中限制查询类型的攻击方式。

    

    预训练的大型语言模型，如GPT-2和BERT，通常会通过微调来在下游任务中达到最先进的性能。一个自然的例子是“智能回复”应用程序，其中预训练模型被调整以提供给定查询消息的建议回复。由于微调数据通常是敏感的数据，如电子邮件或聊天记录，因此重要的是了解和减轻模型泄漏微调数据的风险。我们调查了典型智能回复流程中潜在的信息泄露漏洞。我们考虑了一个现实的情况，即攻击者只能通过前端界面与基础模型进行交互，并限制了可以发送到模型的查询类型。先前的攻击在这些设置中不起作用，而是需要能够直接向模型发送无限制的查询。即使在没有查询约束的情况下，以往的攻击通常需要数千甚至数百万

    Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, are often fine-tuned to achieve state-of-the-art performance on a downstream task. One natural example is the ``Smart Reply'' application where a pre-trained model is tuned to provide suggested responses for a given query message. Since the tuning data is often sensitive data such as emails or chat transcripts, it is important to understand and mitigate the risk that the model leaks its tuning data. We investigate potential information leakage vulnerabilities in a typical Smart Reply pipeline. We consider a realistic setting where the adversary can only interact with the underlying model through a front-end interface that constrains what types of queries can be sent to the model. Previous attacks do not work in these settings, but require the ability to send unconstrained queries directly to the model. Even when there are no constraints on the queries, previous attacks typically require thousands, or even millions, 
    
[^40]: 试验对试验学习如何塑造心理词库中的映射：使用线性判别学习模拟词汇决策

    How trial-to-trial learning shapes mappings in the mental lexicon: Modelling Lexical Decision with Linear Discriminative Learning. (arXiv:2207.00430v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.00430](http://arxiv.org/abs/2207.00430)

    本研究探讨了试验对试验学习在词汇决策中的应用，使用了具有分布语义意义表示的心理词库模型，并通过预测反应时间进一步分析了结果。

    

    提出了试验对试验学习的概念，该概念表明处理一个刺激会影响后续试验中的响应。本研究探讨了在无启动条件的词汇决策实验中是否能够检测到试验对试验学习。使用具有分布语义意义表示的心理词库模型，模拟了每个受试者的词汇决策实验，并使用Widrow-Hoff规则模拟了错误驱动的增量学习。利用来自英国词库项目的数据，使用基于该模型模拟的衡量指标预测了反应时间，然后使用广义可加模型进行了分析。

    Trial-to-trial effects have been found in a number of studies, indicating that processing a stimulus influences responses in subsequent trials. A special case are priming effects which have been modelled successfully with error-driven learning (Marsolek, 2008), implying that participants are continuously learning during experiments. This study investigates whether trial-to-trial learning can be detected in an unprimed lexical decision experiment. We used the Discriminative Lexicon Model (DLM; Baayen et al., 2019), a model of the mental lexicon with meaning representations from distributional semantics, which models error-driven incremental learning with the Widrow-Hoff rule. We used data from the British Lexicon Project (BLP; Keuleers et al., 2012) and simulated the lexical decision experiment with the DLM on a trial-by-trial basis for each subject individually. Then, reaction times were predicted with Generalised Additive Models (GAMs), using measures derived from the DLM simulations 
    

