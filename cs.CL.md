# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](https://arxiv.org/abs/2402.05935) | 本论文介绍了SPHINX-X，一种扩展的多模态大型语言模型系列。通过改进架构和训练效率，我们成功构建了一系列参数大小和多语言能力不同的MLLMs，与数据和参数规模有强相关性。 |
| [^2] | [Driving Everywhere with Large Language Model Policy Adaptation](https://arxiv.org/abs/2402.05932) | 本文介绍了LLaDA，一种使用大型语言模型的工具，使得驾驶员和自动驾驶车辆能够在各地驾驶，通过将任务和运动计划调整到新位置的交通规则。通过广泛的用户研究，证明了LLaDA指导在解决意外情况和适应AV运动计划策略方面的有效性。 |
| [^3] | [WebLINX: Real-World Website Navigation with Multi-Turn Dialogue](https://arxiv.org/abs/2402.05930) | 本论文提出了对话式网站导航的问题，并设计了一个 WEBLINX 基准测试，用于训练和评估代理。为了解决大量信息的处理瓶颈，文中提出了一个受检索启发的模型。实验结果表明，该模型能够在多种场景下复制人类行为的能力。 |
| [^4] | [On the Convergence of Zeroth-Order Federated Tuning in Large Language Models](https://arxiv.org/abs/2402.05926) | 我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。 |
| [^5] | [Efficient Stagewise Pretraining via Progressive Subnetworks](https://arxiv.org/abs/2402.05913) | 通过渐进子网络训练，该方法实现了高效的分阶段预训练，避免了早期阶段无法评估完整模型和模型质量下降等问题。 |
| [^6] | [FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs](https://arxiv.org/abs/2402.05904) | FACT-GPT是一个利用LLMs自动化主张匹配的系统，通过训练在合成数据集上，它可以识别与先前被揭穿的主张相一致、相矛盾或无关的社交媒体内容，并在识别相关主张方面的准确度与更大模型相当。 |
| [^7] | [CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion](https://arxiv.org/abs/2402.05889) | 该论文提出了一种名为CREMA的高效且模块化的模态融合框架，用于将任意新的模态注入视频推理。通过利用预训练模型增强多种信息模态，并引入查询转换器和融合模块，实现了灵活且有效的多模态组合推理。 |
| [^8] | [Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking](https://arxiv.org/abs/2402.05880) | LLM驱动的对话式搜索系统增加了选择性曝光，且支持用户观点的有偏见LLM加剧了这种偏差。 |
| [^9] | [PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868) | PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。 |
| [^10] | [Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs](https://arxiv.org/abs/2402.05864) | 提出了一种名为Permute-and-Flip（PF）解码器，其具有最佳的鲁棒性和质量-鲁棒性的 tradeoff，且比采样方法更好。还设计了一种针对PF解码器的水印方案，能够保持样本的分布不变，并实现任意低的假阳性率和高的召回率。实验证明PF解码器在困惑度方面明显优于朴素采样，为LLM解码提供了一种有希望的新方法。 |
| [^11] | [How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis](https://arxiv.org/abs/2402.05863) | 本文研究了LLM代理之间的谈判能力，开发了NegotiationArena框架用于评估和探索LLM代理的谈判能力。实验结果表明，LLM代理可以通过运用特定的行为策略显著提高谈判结果。 |
| [^12] | [Is it Possible to Edit Large Language Models Robustly?](https://arxiv.org/abs/2402.05827) | 本研究旨在了解大型语言模型的稳健编辑方法的优势和局限性，从而促进对交流型人工智能的稳健、现实应用。 |
| [^13] | [Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model](https://arxiv.org/abs/2402.05819) | 本文提出了PW-HuBERT框架，它通过将伪词级目标整合到训练过程中，从视觉语音模型中提取目标，从而在口语理解任务中展现出了优越性能。 |
| [^14] | [Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models](https://arxiv.org/abs/2402.05813) | 本研究提出了一种新方法，在语言模型中实现了精确和选择性的遗忘，以解决神经模型意外保留个人或敏感数据的问题。此外，还提出了两个创新的评估指标，旨在衡量敏感信息消除的效果。为了强化遗忘框架，还提出了一种有效的标注敏感范围的方法。 |
| [^15] | [FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension](https://arxiv.org/abs/2402.05812) | 本文介绍了FAQ-Gen，一个利用文本转文本转换模型开发的自动化系统，用于生成特定领域的常见问题解答。该系统通过使用自我策划的算法来优化信息表示和问题-答案排名，提高了FAQ的准确性和相关性。定性人类评估表明生成的FAQs构造良好且可推广至不同领域。 |
| [^16] | [Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2402.05808) | 本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。 |
| [^17] | [Phonetically rich corpus construction for a low-resourced language](https://arxiv.org/abs/2402.05794) | 本文提出了一种用于资源有限语言巴西葡萄牙语的音标丰富语料库构建方法，包括收集文本数据集、基于三音分布的句子选择算法和根据声学-发音语音特征进行新的音素分类。 |
| [^18] | [Limits of Transformer Language Models on Algorithmic Learning](https://arxiv.org/abs/2402.05785) | Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。 |
| [^19] | [Text-to-Code Generation with Modality-relative Pre-training](https://arxiv.org/abs/2402.05783) | 本论文研究了如何根据不同的模态调整和表示序列标记，以进一步提高文本到代码生成的效果。 |
| [^20] | [Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images](https://arxiv.org/abs/2402.05779) | 本研究使用了一个新的平行图像数据集，通过查询大型视觉语言模型，观察到在性别和种族维度上存在显著的偏见。 |
| [^21] | [SpiRit-LM: Interleaved Spoken and Written Language Model](https://arxiv.org/abs/2402.05755) | SPIRIT-LM是一个基于预训练文本语言模型的多模态语言模型，通过将文本和语音连续训练，实现了口语和书面语言的混合模型。它展示了文本模型的语义能力和语音模型的表现能力。此外，SPIRIT-LM还能以少量样本的方式学习新任务。 |
| [^22] | [TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation](https://arxiv.org/abs/2402.05733) | TimeArena是一个具有时间感知的模拟环境，为语言代理提供了更好的多任务处理能力。实验结果显示即使是最先进的语言模型也无法达到人类的多任务效率。 |
| [^23] | [Unified Speech-Text Pretraining for Spoken Dialog Modeling](https://arxiv.org/abs/2402.05706) | 本研究提出了一个名为统一口语对话模型（USDM）的广泛语音文本模型框架，用于生成与输入语音相关的连贯口语回复。通过使用多步骤的语音文本推理方式和广义语音文本预训练方案，该方法能够有效捕捉跨模态语义，并生成自然流畅的口语回复。 |
| [^24] | [Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation](https://arxiv.org/abs/2402.05699) | 本文提出了一个通过社交场景模拟来自对齐大型语言模型的方法，以减轻其被滥用造成的潜在不良影响。通过一个名为MATRIX的虚拟排练空间，LLM可以在回答查询前考虑社交后果，并通过MATRIX-simulated数据的微调，保持对人类价值的遵从和推理速度的平衡。实验证明，在温和假设下，带有MATRIX的LLM胜过了宪法AI。 |
| [^25] | [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672) | 这项技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，包括三种不同大小的模型和一种新的以指令为导向的嵌入模型。模型在推理效率和嵌入质量上取得了平衡，性能与同等大小的最先进的仅英文模型相当。 |
| [^26] | [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668) | 对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。 |
| [^27] | [Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations](https://arxiv.org/abs/2402.05629) | 该论文研究了长文生成中的事实性问题，发现大型语言模型在生成段落时可能会由于实体模糊而将可验证的事实组合成非事实的段落。现有的事实准确度评估方法无法正确评估这些非事实段落，作者提出了一种增强的度量指标来应对这个问题。 |
| [^28] | [Efficient Models for the Detection of Hate, Abuse and Profanity](https://arxiv.org/abs/2402.05624) | 这篇论文提出了针对仇恨、辱骂和亵渎检测的高效模型，因为大型语言模型在训练过程中可能学习到这些负面内容并生成不合适的文本。 |
| [^29] | [Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings](https://arxiv.org/abs/2402.05617) | 该论文调查了基于深度学习的计算机工作市场分析中的技能提取和分类方法。通过全面概述深度学习方法、数据集和术语，以及对公开可获取的数据集的分类，填补了这一领域的研究空白。 |
| [^30] | [Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks](https://arxiv.org/abs/2402.05616) | 预训练的小型生成式语言模型可以作为序列型任务的通用学习框架，通过指令微调可以在化学信息学任务中实现接近最先进结果。 |
| [^31] | [AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers](https://arxiv.org/abs/2402.05602) | AttnLRP是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，并具有与单一反向传播相似的计算效率的方法。它通过扩展逐层相关传递归因方法以处理注意力层来解决了黑盒Transformer模型的归因问题，具有超越现有方法的准确性和理解潜在表示的能力。 |
| [^32] | [SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels](https://arxiv.org/abs/2402.05591) | 本文提出了SoftEDA方法，通过使用软标签在增强数据上，从而解决了基于规则的文本数据增强方法可能破坏文本原始含义的问题，实验证明了该方法的有效性。 |
| [^33] | [AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes](https://arxiv.org/abs/2402.05584) | 本文提出了将AutoAugment方法应用于解决文本数据增强中的语义损害问题，实验证明该方法可以加强现有的增强方法并提升预训练语言模型的性能。 |
| [^34] | [Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models](https://arxiv.org/abs/2402.05581) | 该论文提出了一种通过使用跨语言模型的预训练向量表示来确定音频录音之间的抽象程度的方法。通过进行ABX测试，该研究证实了从具有不同语言和非语言特征的录音提取的表示沿着相同的维度有所不同。这一方法是无监督的，可以作为获取大规模数据集中音频表征的有效途径。 |
| [^35] | [Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study](https://arxiv.org/abs/2402.05571) | 该研究旨在开发高效的机器学习模型，用于对饮食紊乱相关的推文进行分类。研究发现，基于转换器的双向编码器表示在四个类别中实现了最高的F1分数，并表明基于转换器的模型较传统技术更为优越，但需要更多的计算资源。 |
| [^36] | [Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset](https://arxiv.org/abs/2402.05547) | 本研究介绍了“ChatCoach”，一个集成人工智能与人类医生合作的框架，在交流医疗辅导中利用大型语言模型，提供模拟环境和实时反馈，以帮助医学学员提高沟通技巧。 |
| [^37] | [Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data](https://arxiv.org/abs/2402.05545) | 本文介绍了使用合成数据构建命名实体识别模型的方法，该模型通过语音转文字转录数据提取地址部分。通过利用SlovakBERT模型，并强调在合成数据中模拟口语语言的可变性，我们评估了仅使用合成数据训练的NER模型的性能。 |
| [^38] | [Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts](https://arxiv.org/abs/2402.05536) | 这项研究提出了一种新颖的方法，通过结合知识图谱和深度学习，借助实体识别和链接技术以及上下文化嵌入，增强了机器学习模型对社交媒体帖子的分类能力。该方法主要应用于健康领域的饮食障碍识别，为早期诊断提供帮助。 |
| [^39] | [NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning](https://arxiv.org/abs/2402.05515) | NoisyICL通过在模型参数中引入噪音，提高了上下文学习的性能和校准性，实验结果显示NoisyICL可以产生更准确、更公平的预测。 |
| [^40] | [GPTs Are Multilingual Annotators for Sequence Generation Tasks](https://arxiv.org/abs/2402.05512) | 本研究提出了一种利用大型语言模型进行自动注释的方法，具有费用效益高和适用于低资源语言注释的优点，同时构建了一个图像字幕数据集并开放了源代码。 |
| [^41] | [Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia](https://arxiv.org/abs/2402.05467) | 本文介绍了一种名为RIPPLE的快速优化方法，该方法通过潜意识利用和模仿动作的思想，解决了通过越狱提示绕过安全措施的问题。 |
| [^42] | [It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition](https://arxiv.org/abs/2402.05457) | 本论文通过将声学信息融入大型语言模型（LLM）中，提出了一种名为不确定性感知动态融合（UADF）的后期融合解决方案，以克服生成性错误纠正（GER）中存在的数据不确定性问题，并应用于自动语音识别（ASR）任务。通过在自回归解码过程中实施UADF方法，在LLM决策的标记级分析和校准的基础上，动态地融合声学信息，从而提高了ASR的准确性。 |
| [^43] | [Large Language Models for Psycholinguistic Plausibility Pretesting](https://arxiv.org/abs/2402.05455) | 本研究探讨了使用语言模型(LMs)生成语言材料的合理性判断，并发现GPT-4的合理性判断与人类判断高度相关。这意味着LMs可以替代人类进行预测试，从而在心理语言学中具有重要的应用潜力。 |
| [^44] | [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2402.05445) | 本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。 |
| [^45] | [Improving Agent Interactions in Virtual Environments with Language Models](https://arxiv.org/abs/2402.05440) | 这项研究通过利用语言模型在虚拟环境中改进智能体的交互，实现了对任务的理解和沟通能力的提升，并在实验中取得了显著的改进。 |
| [^46] | [GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study](https://arxiv.org/abs/2402.05435) | 本研究通过使用结构化叙事提示，验证了GPT-4生成的叙述在传达生活事件方面的有效性。研究结果表明，大多数叙述能够足够传达提示的意图。同时，通过机器学习模型的训练和验证，可以自动识别有效和无效的叙述。 |
| [^47] | [Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes](https://arxiv.org/abs/2402.05406) | 本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。 |
| [^48] | [In-Context Principle Learning from Mistakes](https://arxiv.org/abs/2402.05403) | 本文提出了一种新的学习方法LEAP，通过让模型从少量输入-输出示例中犯错误，然后反思并学习准则，从而提升模型在各种任务上的表现。 |
| [^49] | [Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models](https://arxiv.org/abs/2402.05376) | 本论文提出了一种在大型语言模型中使用进化算法引导的零样本思维链推理的方法。通过动态生成多样的推理方式，并通过重写操作增强模型对问题的理解，我们的方法在十个推理数据集上展现出了卓越的性能。 |
| [^50] | [CIC: A framework for Culturally-aware Image Captioning](https://arxiv.org/abs/2402.05374) | CIC是一种面向文化感知图像字幕的框架，通过结合视觉问答和大型语言模型，它能够生成能描述图像中文化元素的详细字幕。 |
| [^51] | [Noise Contrastive Alignment of Language Models with Explicit Rewards](https://arxiv.org/abs/2402.05369) | 本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。 |
| [^52] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^53] | [Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs](https://arxiv.org/abs/2402.05318) | 本文探讨了信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用，LLMs的整合标志着用户与信息系统交互方式的范式转变。 |
| [^54] | [Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection](https://arxiv.org/abs/2402.05294) | 本文首次分析了多模态联邦学习中的模态不一致性的影响，并揭示了其与参与客户端之间的数据异质性的联系。通过使用不考虑不一致性的信息融合机制和模态插值网络，在解决模态不一致性问题方面取得了一定的成果。 |
| [^55] | [TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing](https://arxiv.org/abs/2402.05282) | 本文提出了一种TreeForm的标注方案和评估方法，旨在解决表单文档解析的端到端模型开发和评估的问题。 |
| [^56] | [VerAs: Verify then Assess STEM Lab Reports](https://arxiv.org/abs/2402.05224) | VerAs是一个端到端的神经架构，用于验证和评估STEM实验报告。它通过利用多个维度的分析评估标准，以及针对学生提供详细反馈，帮助他们提高科学写作技巧。 |
| [^57] | [The Effect of Sampling Temperature on Problem Solving in Large Language Models](https://arxiv.org/abs/2402.05201) | 这项研究实证研究了采样温度对大型语言模型在解题中的影响，结果显示在0.0至1.0的温度范围内，LLM性能对解题任务没有显著影响。 |
| [^58] | [Are LLMs Ready for Real-World Materials Discovery?](https://arxiv.org/abs/2402.05200) | LLMs在材料科学中的应用受限，无法实现实际应用。我们提出了基于材料科学知识和假设测试的MatSci-LLMs框架，并描述了关键的材料科学信息提取挑战。 |
| [^59] | [$\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space](https://arxiv.org/abs/2402.05195) | $\lambda$-ECLIPSE通过利用CLIP潜空间，实现了多概念个性化文本到图像的扩散模型。相比于传统方法，它减小了训练资源需求，并提供了更一致、高质量的图像生成结果。 |
| [^60] | [InCoRo: In-Context Learning for Robotics Control with Feedback Loops](https://arxiv.org/abs/2402.05188) | 本文提出了InCoRo系统，使用经典的机器人反馈循环，通过LLM控制器、场景理解单元和机器人的协同工作，实现对动态环境中机器人控制的上下文学习。该系统能够持续分析环境状态并提供适应性执行命令，使机器人能够适应环境变化并纠正控制器错误。 |
| [^61] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^62] | [ApiQ: Finetuning of 2-Bit Quantized Large Language Model](https://arxiv.org/abs/2402.05147) | 这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。 |
| [^63] | [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://arxiv.org/abs/2402.05140) | 本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。 |
| [^64] | [SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark](https://arxiv.org/abs/2402.05138) | SceMQA是一种科学类大学入学级多模态问题回答的基准，填补了现有基准中被忽视的教育阶段的空白。它包含核心科学科目，融合了多项选择和自由回答的格式，并提供详细的问题解析和答案解释。该基准还通过相同背景但问题不同的方式，促进了对推理能力更全面和准确的评估。 |
| [^65] | [LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K](https://arxiv.org/abs/2402.05136) | LV-Eval是一个具有五个长度级别的长上下文基准测试，支持256k上下文长度，并具有混淆事实、关键词和短语替换以及基于关键词回忆的度量设计等关键技术，旨在减少知识泄漏和提供更客观的评估。 |
| [^66] | [CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation](https://arxiv.org/abs/2402.05135) | CADReN是一个上下文锚点驱动的关系网络，用于可控的跨图节点重要性估计。它通过引入上下文锚点机制，考虑知识图谱中的结构和语义特征，实现了更好的性能，包括零-shot预测能力，并开源了两个新的数据集RIC200和WK1K。 |
| [^67] | [Personalized Language Modeling from Personalized Human Feedback](https://arxiv.org/abs/2402.05133) | 该论文提出了一个个性化语言模型的方法，通过在于用户的反馈数据中引入个性化特征来解决强化学习框架在多样化用户偏好下存在的问题。 |
| [^68] | [TexShape: Information Theoretic Sentence Embedding for Language Models](https://arxiv.org/abs/2402.05132) | 这项研究提出了一种名为TexShape的信息论句子嵌入模型，通过使用互信息的经验估计来优化文本表示，可用于数据压缩和敏感信息过滤，提升隐私和公平性。 |
| [^69] | [Financial Report Chunking for Effective Retrieval Augmented Generation](https://arxiv.org/abs/2402.05131) | 本文提出了一种扩展的方法来切块财务报告，通过根据文档的结构元素组件进行切块，从而实现更有效的检索增强生成。这种方法可以优化切块大小，而无需调整，并提供了对整体上下文和准确性的评估以及对问答任务性能的影响。 |
| [^70] | [LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System](https://arxiv.org/abs/2402.05130) | LB-KBQA是一种基于大语言模型和BERT的基于知识的问答系统，通过生成式人工智能的帮助，能够提高意图识别的性能和解决语言多样性的问题。 |
| [^71] | [Best Practices for Text Annotation with Large Language Models](https://arxiv.org/abs/2402.05129) | 大型语言模型的广泛使用在文本标注领域带来了许多挑战，这篇论文提出了一套可靠、可重复、符合伦理要求的使用标准和最佳实践，以解决相关问题。 |
| [^72] | [Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation](https://arxiv.org/abs/2402.05128) | 本论文通过引入检索增强生成（RAG）技术和利用迁移学习来处理长文本和提升推理能力，为教科书问答任务带来了显著的改进。 |
| [^73] | [Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering](https://arxiv.org/abs/2402.05127) | 本研究提出了一种新的抑郁症检测和治疗范式，使用先进的大型语言模型，经过特定提示微调以诊断、解释和建议治疗干预。同时还介绍了一个丰富的数据库，以提供个性化的治疗建议。此方法与患者进行共情对话管理，有效支持抑郁症患者。 |
| [^74] | [Graph Neural Network and NER-Based Text Summarization](https://arxiv.org/abs/2402.05126) | 这个项目使用图神经网络和命名实体识别系统，提出了一种创新的文本摘要方法。图神经网络能够捕获并处理文本信息中的关系数据，而命名实体识别系统通过识别和强调关键实体来保持摘要的重点。 |
| [^75] | [Zero-Shot Clinical Trial Patient Matching with LLMs](https://arxiv.org/abs/2402.05125) | 本研究基于LLMs开发了一个零样本临床试验患者匹配系统，可以高效评估患者是否符合入选标准，并通过优化提示策略和检索流程提高了数据和成本效率。 |
| [^76] | [A Survey on Data Selection for LLM Instruction Tuning](https://arxiv.org/abs/2402.05123) | 本综述对LLM指导调优的数据选择进行了全面调查。研究发现，数据集的质量在指导调优过程中比数量更为重要，因此许多研究致力于探索从指导数据集中选择高质量子集的方法。课题呈现了一种新的分类体系、介绍了最近的研究进展并详细评估了这些方法。 |
| [^77] | [History of generative Artificial Intelligence (AI) chatbots: past, present, and future development](https://arxiv.org/abs/2402.05122) | 本研究回顾了生成式人工智能聊天机器人的发展历程，从最初的基本规则系统到如今由人工智能驱动的高级对话机器人。研究突出了关键创新，如聊天机器人ELIZA和ALICE，并探讨了影响演进的重要里程碑，以及Transformer模型的最新进展。 |
| [^78] | [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121) | 该调查综述了大型语言模型在表格处理中的应用，包括传统的表格问题回答和事实验证，以及新兴的表格操作和高级表格数据分析。还讨论了LLMs的最新范例，特别关注了指导调整、提示和基于代理的方法。 |
| [^79] | [More Agents Is All You Need](https://arxiv.org/abs/2402.05120) | 大型语言模型的性能与代理数量成比例，通过简单的采样和投票方法可以进一步增强性能，这种方法与现有的复杂方法正交。 |
| [^80] | [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119) | 本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。 |
| [^81] | [Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature](https://arxiv.org/abs/2402.05116) | 本研究旨在通过文本挖掘方法评估ChatGPT和Google Bard生成的内容与生物医学文献之间的相似性。实验结果显示，在余弦文档相似性方面，ChatGPT表现优于Google Bard。 |
| [^82] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^83] | [Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models](https://arxiv.org/abs/2402.04614) | 本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。 |
| [^84] | [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916) | 本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。 |
| [^85] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^86] | [GIRT-Model: Automated Generation of Issue Report Templates](https://arxiv.org/abs/2402.02632) | 本研究介绍了GIRT-模型，这是一个基于开发者指示自动生成问题报告模板的助理语言模型。通过在GitHub仓库中构建的数据集进行指导调整，GIRT-模型在IRT生成方面表现显著优于其他通用语言模型。 |
| [^87] | [APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation](https://arxiv.org/abs/2402.01697) | APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets. |
| [^88] | [ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models](https://arxiv.org/abs/2402.00794) | 本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。 |
| [^89] | [Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model](https://arxiv.org/abs/2402.00746) | 提出了一个创新的框架，健康-LLM，通过大规模特征提取和医学知识权衡评分，实现了个性化的检索增强疾病预测模型。这种方法通过整合健康报告，调整特征权重，以及利用语言模型和专家见解提高预测准确性，与传统健康管理方法相比具有明显优势。 |
| [^90] | [Robust Knowledge Extraction from Large Language Models using Social Choice Theory](https://arxiv.org/abs/2312.14877) | 本研究提出使用排名查询和社会选择理论的方法来提高大型语言模型（LLMs）查询的鲁棒性，特别是在高风险领域如医学中。我们通过实证评估验证了我们方法的鲁棒性和其他有趣属性。 |
| [^91] | [Social Learning: Towards Collaborative Learning with Large Language Models](https://arxiv.org/abs/2312.11441) | 本论文在大型语言模型的背景下引入了“社会学习”的框架，通过自然语言相互共享知识，提出了两种知识传递方法，并证明了这些方法的可行性和效果。 |
| [^92] | [An Analysis of Dialogue Repair in Voice Assistants](https://arxiv.org/abs/2311.03952) | 本研究通过分析与谷歌助手和Siri的交互，探讨了虚拟助手与用户之间的对话修复中交互语言的重要性。研究发现助手生成了几种修复策略，但无法复制人类类似的修复策略，如“嗯？”。英语和西班牙语用户可接受性调查显示了用户对修复策略的偏好和助手使用的差异。这些结果揭示了人机交互中交互语言与人际交互之间的不平等，强调了对交互语言影响的进一步研究的必要性。 |
| [^93] | [Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency](https://arxiv.org/abs/2311.02772) | 本文展示了将语音转换器作为音频模型的编码器，可以显著提高预训练模型的效率。此外，我们发现只使用自注意力也能实现类似的效果，尤其与低位权重量化技术结合使用时效果更好。这一发现有助于防止错误在量化模块之间传播。 |
| [^94] | [TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings](https://arxiv.org/abs/2310.14450) | 本研究提出了一种通过对比学习以及利用未标记新闻文章数据集来训练主题无关和主题感知嵌入的方法，在立场检测中取得了最先进的性能，并在公开数据集上达到了0.771的F1分数。 |
| [^95] | [Trainable Transformer in Transformer](https://arxiv.org/abs/2307.01189) | 这篇论文介绍了一种名为Transformer in Transformer (TinT)的高效构造方式，它可以让Transformer在推理过程中模拟和微调复杂的内部模型，同时使用创新的近似技术大幅减少了模型参数和内存开销。 |
| [^96] | [Investigating Agency of LLMs in Human-AI Collaboration Tasks](https://arxiv.org/abs/2305.12815) | 本论文探讨了LLMs在人工智能协作任务中的代理力，开发了一种用于衡量和管理LLMs代理力的特征框架，并通过新的数据集验证了该方法的有效性。 |
| [^97] | [PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs](https://arxiv.org/abs/2305.12392) | 提出了一个名为"PiVe"的框架，通过迭代验证来提升LLMs的基于图的生成能力。实验结果表明，PiVe方法在三个基于图的数据集上取得了一致的改善，并且验证模块可以作为数据增强工具帮助提高结果质量。 |
| [^98] | [Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa](https://arxiv.org/abs/2302.10786) | 这项研究扩展了AI教辅Kwame，面向西非科学教育，并将其作为一个网络应用程序进行了实际部署和评估。结果显示，在实际环境中，Kwame for Science取得了很高的准确率，并且获得了广泛的用户和问题提问量。 |
| [^99] | [Towards Uncertainty-Aware Language Agent.](http://arxiv.org/abs/2401.14016) | UALA是一个使用不确定性量化来进行代理和外部世界交互的框架，相比于其他方法，它在多个任务和语言模型尺寸下表现出显著的性能改进，并且在对外部世界的依赖性方面更低。 |
| [^100] | [Self-Rewarding Language Models.](http://arxiv.org/abs/2401.10020) | 该论文提出了自奖励语言模型的概念，通过LLM作为评判者，使用语言模型自己提供训练过程中的奖励。研究表明，该方法不仅可以提高指令遵循能力，还可以为自己提供高质量的奖励。通过对Llama 2 70B模型的三次迭代微调，结果在AlpacaEval 2.0排行榜上超过了其他现有系统。这项工作为实现能够不断自我改进的模型开辟了新的可能性。 |
| [^101] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^102] | [Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning.](http://arxiv.org/abs/2310.03249) | 本研究提出了一种新的基准测试PPNL，评估大型语言模型的空间-时间推理能力。实验结果显示，少样本的GPT-4在空间推理方面表现良好，但仍有待改进。 |
| [^103] | [In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations.](http://arxiv.org/abs/2310.00313) | 本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。 |
| [^104] | [Automatic Prompt Rewriting for Personalized Text Generation.](http://arxiv.org/abs/2310.00152) | 这项研究提出了一种自动修订个性化文本生成提示的新方法，在大型语言模型无法微调的情况下，通过改进输入文本的方式实现个性化文本生成。 |
| [^105] | [Teaching Text-to-Image Models to Communicate.](http://arxiv.org/abs/2309.15516) | 本文提出了一种针对对话生成图像的高效方法，通过微调预训练的文本到图像模型，实现在给定对话背景下生成一致逼真的图像。 |
| [^106] | [Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs.](http://arxiv.org/abs/2309.07311) | 本文通过对掩码语言模型中的语法习得进行案例研究，发现在训练的一个短暂窗口内，模型突然获得了语法注意结构(SAS)，并伴随着损失的陡峭下降。SAS对随后习得语言能力起到了重要的促进作用。 |
| [^107] | [How (Not) to Use Sociodemographic Information for Subjective NLP Tasks.](http://arxiv.org/abs/2309.07034) | 该论文研究了如何使用社会人口统计信息在主观NLP任务中，发现社会人口提示技术在某些任务上有效，但也存在一些限制和挑战。 |
| [^108] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^109] | [S2vNTM: Semi-supervised vMF Neural Topic Modeling.](http://arxiv.org/abs/2307.04804) | S2vNTM是一种半监督的vMF神经主题建模方法，通过利用关键词的模式来识别潜在的主题，并优化主题关键词集的质量，提高了分类准确度，并且速度至少比基线模型快两倍。 |
| [^110] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |

# 详细

[^1]: SPHINX-X: 扩展数据和参数用于一系列多模态大型语言模型

    SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models

    [https://arxiv.org/abs/2402.05935](https://arxiv.org/abs/2402.05935)

    本论文介绍了SPHINX-X，一种扩展的多模态大型语言模型系列。通过改进架构和训练效率，我们成功构建了一系列参数大小和多语言能力不同的MLLMs，与数据和参数规模有强相关性。

    

    我们提出SPHINX-X，一种基于SPHINX开发的广泛多模态大型语言模型（MLLM）系列。为了改善架构和训练效率，我们通过移除冗余的视觉编码器、绕过完全填充的子图像，并将多阶段训练简化成为一阶段的全集合模式，修改了SPHINX框架。为了充分发挥MLLM的潜力，我们组装了一个综合的跨语言、跨视觉和视觉-语言任务的多领域、多模态的数据集，涵盖了公开可用的资源。我们进一步使用我们的OCR密集和Mark数据集丰富这个收集，扩展了多样性和普适性。通过对不同基础LLM进行训练，包括TinyLlama1.1B、InternLM2-7B、LLaMA2-13B和Mixtral8x7B，我们获得了一系列参数大小和多语言能力变化的MLLMs。全面的基准测试揭示了多模态性能与数据和参数规模之间的强相关性。

    We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
    
[^2]: 通过大型语言模型政策适应在各处驾驶

    Driving Everywhere with Large Language Model Policy Adaptation

    [https://arxiv.org/abs/2402.05932](https://arxiv.org/abs/2402.05932)

    本文介绍了LLaDA，一种使用大型语言模型的工具，使得驾驶员和自动驾驶车辆能够在各地驾驶，通过将任务和运动计划调整到新位置的交通规则。通过广泛的用户研究，证明了LLaDA指导在解决意外情况和适应AV运动计划策略方面的有效性。

    

    自动驾驶中，将驾驶行为适应新环境、习俗和法律是一个长期存在的问题，这限制了自动驾驶车辆(AVs)的广泛部署。本文介绍了LLaDA，一种简单而强大的工具，可以使人类驾驶员和自动驾驶车辆都能在各处行驶，通过将任务和运动计划调整到新位置的交通规则。LLaDA利用大型语言模型(LLMs)在解释本地驾驶手册中的交通规则时具有令人印象深刻的零-shot泛化能力。通过广泛的用户研究，我们展示了LLaDA的指导在解决现实世界中的意外情况时的有用性。我们还展示了LLaDA在真实世界数据集中调整AV运动计划策略的能力；LLaDA在所有指标上优于基线规划方法。请访问我们的网站了解更多详细信息：https://boyiliee.github.io/llada.

    Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.
    
[^3]: WebLINX: 多轮对话下的真实世界网站导航

    WebLINX: Real-World Website Navigation with Multi-Turn Dialogue

    [https://arxiv.org/abs/2402.05930](https://arxiv.org/abs/2402.05930)

    本论文提出了对话式网站导航的问题，并设计了一个 WEBLINX 基准测试，用于训练和评估代理。为了解决大量信息的处理瓶颈，文中提出了一个受检索启发的模型。实验结果表明，该模型能够在多种场景下复制人类行为的能力。

    

    我们提出了对话式网站导航的问题，其中数字代理控制着一个网页浏览器，并按照用户的指令以多轮对话的方式解决真实世界任务。为了支持这个问题，我们引入了 WEBLINX - 一个100K交互的大规模基准测试，在2300个专家演示中进行了对话式网站导航的测试。我们的基准涵盖了150多个真实世界网站上的广泛模式，可以用于在不同场景下训练和评估代理。由于存在大量信息，大型语言模型 (LLMs) 无法实时处理整个网页。为了解决这个瓶颈，我们设计了一个受检索启发的模型，通过排名相关元素来高效地修剪 HTML 页面。我们使用选定的元素，以及屏幕截图和操作历史记录，评估各种模型在导航网页时复制人类行为的能力。我们的实验从小型纯文本模型到专有的多模态 LLMs 进行了测试。

    We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
    
[^4]: 关于大规模语言模型中零阶联邦调整的收敛性

    On the Convergence of Zeroth-Order Federated Tuning in Large Language Models

    [https://arxiv.org/abs/2402.05926](https://arxiv.org/abs/2402.05926)

    我们的研究提出了一种名为FedMeZO的方法，以在零阶联邦学习和大规模语言模型之间实现内存高效的调整。我们的理论和实证证据表明FedMeZO不仅收敛速度快于传统的一阶方法，而且在个性化的联邦策略中具有关键的作用。

    

    联邦学习（FL）和大规模语言模型（LLM）的融合为隐私保护的自然语言处理带来了新时代。然而，精调LLM所需的强大内存要求在部署到边缘设备时会面临重大挑战，因为这些设备的计算资源有限。为了解决这个问题，我们在联邦环境中探索了内存高效的零阶优化的全新整合，我们称之为FedMeZO。我们的研究是第一个在LLM背景下考察FedMeZO的理论基础的研究，涉及到大参数空间对优化行为的影响、收敛性的建立以及为个性化的联邦策略确定关键参数的问题。我们广泛的实证证据支持了这个理论，表明FedMeZO不仅比传统的一阶方法（如SGD）收敛更快，而且明显...

    The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
    
[^5]: 通过渐进子网络实现高效的分阶段预训练

    Efficient Stagewise Pretraining via Progressive Subnetworks

    [https://arxiv.org/abs/2402.05913](https://arxiv.org/abs/2402.05913)

    通过渐进子网络训练，该方法实现了高效的分阶段预训练，避免了早期阶段无法评估完整模型和模型质量下降等问题。

    

    最近大型语言模型的发展引起了人们对高效预训练方法的关注。最近的一个有效范例是进行分阶段训练，即在训练过程中逐渐增加模型的大小（例如逐渐叠加（Reddi等人，2023年））。虽然资源和墙钟时间的节省很吸引人，但它也有局限性，特别是在早期阶段无法评估完整的模型，并且由于初始阶段模型容量较小而导致模型质量下降。在这项工作中，我们提出了一种替代性框架，即渐进子网络训练，在整个训练过程中保持完整的模型，但每个步骤只训练模型中的子网络。我们专注于这个框架的一个简单实例，即随机路径训练（RaPTr），它在每个步骤中只训练一条子路径，逐渐增加路径长度。RaPTr在BERT和UL2语言模型的预训练损失方面取得了更好的效果，同时只需要2

    Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 2
    
[^6]: FACT-GPT: 通过与LLMs进行主张匹配来增强事实核查

    FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs

    [https://arxiv.org/abs/2402.05904](https://arxiv.org/abs/2402.05904)

    FACT-GPT是一个利用LLMs自动化主张匹配的系统，通过训练在合成数据集上，它可以识别与先前被揭穿的主张相一致、相矛盾或无关的社交媒体内容，并在识别相关主张方面的准确度与更大模型相当。

    

    我们的社会面临着猖獗的虚假信息，危害公共健康和信任。为了应对这一社会挑战，我们引入了FACT-GPT，这是一个利用大型语言模型（LLMs）来自动化事实核查中的主张匹配阶段的系统。FACT-GPT在合成数据集上训练，可以识别与先前被揭穿的主张相一致、相矛盾或无关的社交媒体内容。我们的评估结果显示，我们专门设计的LLMs在识别相关主张方面的准确度与更大模型相当，几乎能够模拟人类判断。这项研究为高效的主张匹配提供了自动化解决方案，展示了LLMs在支持事实核查员方面的潜力，并为该领域的进一步研究提供了宝贵资源。

    Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field.
    
[^7]: CREMA: 通过有效的模块化适应和融合进行多模态组合视频推理

    CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion

    [https://arxiv.org/abs/2402.05889](https://arxiv.org/abs/2402.05889)

    该论文提出了一种名为CREMA的高效且模块化的模态融合框架，用于将任意新的模态注入视频推理。通过利用预训练模型增强多种信息模态，并引入查询转换器和融合模块，实现了灵活且有效的多模态组合推理。

    

    尽管在多模态组合推理方法方面取得了令人瞩目的进展，但由于处理固定模态输入并更新许多模型参数，仍然存在灵活性和效率方面的限制。本文解决了这些关键挑战，提出了CREMA，一种用于将任何新的模态注入视频推理的高效且模块化的模态融合框架。我们首先利用现有的预训练模型从给定的视频中增强多种信息模态（如光流、3D点云、音频），而无需额外的人工注释。接下来，我们引入了一个查询转换器，该转换器与每个可以访问的模态相关联，并具有多个参数高效的模块。它将多种模态特征投影到LLM令牌嵌入空间，使模型能够整合不同的数据类型以进行响应生成。此外，我们提出了一个融合模块，用于压缩多模态查询，在LLM中保持计算效率的同时进行融合组合。

    Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
    
[^8]: 生成性回音室？LLM驱动的搜索系统对多样化信息搜索的影响

    Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking

    [https://arxiv.org/abs/2402.05880](https://arxiv.org/abs/2402.05880)

    LLM驱动的对话式搜索系统增加了选择性曝光，且支持用户观点的有偏见LLM加剧了这种偏差。

    

    数亿人已经使用过大型语言模型（LLM）驱动的对话式搜索系统，并且相信这些系统相比传统搜索带来了许多好处。然而，虽然几十年的研究和公共讨论都调查了搜索系统在增加选择性曝光和产生回音室方面的风险，即限制接触多样化意见并导致意见偏执，但对于LLM驱动的对话式搜索的这种风险知之甚少。我们进行了两个实验来研究：1）LLM驱动的对话式搜索相较于传统搜索是否以及如何增加选择性曝光；2）具有支持或挑战用户观点的意见偏见的LLM如何改变这种影响。总体而言，我们发现参与者在LLM驱动的对话式搜索中更倾向于进行偏见的信息查询，并且支持他们观点的有偏见的LLM加剧了这种偏差。这些结果呈现了重要的意义。

    Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implicatio
    
[^9]: PromptCrypt: 使用表情符号对大型语言模型进行安全通信的提示加密

    PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models

    [https://arxiv.org/abs/2402.05868](https://arxiv.org/abs/2402.05868)

    PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。

    

    基于云的大型语言模型（LLM）如ChatGPT在日常操作中变得越来越重要，成为各种应用程序中的重要工具。虽然这些模型在可访问性和功能性方面带来了重大好处，但它们也引入了重要的隐私问题：在云基础架构中传输和存储用户数据会产生重大的数据泄露和未经授权访问敏感信息的风险；即使数据的传输和存储被加密，LLM服务提供商仍然知道数据的真实内容，从而阻止个人或实体放心使用此类LLM服务。为了解决这些问题，本文提出了一种简单但有效的机制PromptCrypt来保护用户隐私。它使用表情符号对用户输入进行加密，然后将其发送到LLM，有效地使其对人类或LLM的检查无法理解，同时保留原始提示的意图，从而确保用户隐私。

    Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
    
[^10]: Permute-and-Flip：一种具有最佳鲁棒性和可加水印的LLMs解码器

    Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs

    [https://arxiv.org/abs/2402.05864](https://arxiv.org/abs/2402.05864)

    提出了一种名为Permute-and-Flip（PF）解码器，其具有最佳的鲁棒性和质量-鲁棒性的 tradeoff，且比采样方法更好。还设计了一种针对PF解码器的水印方案，能够保持样本的分布不变，并实现任意低的假阳性率和高的召回率。实验证明PF解码器在困惑度方面明显优于朴素采样，为LLM解码提供了一种有希望的新方法。

    

    在本文中，我们提出了一种名为Permute-and-Flip（PF）解码器的新解码方法。它具有与标准采样解码器相似的鲁棒性特性，但在质量和鲁棒性的 tradeoff 上证明比采样方法更好，且永远不会差于任何其他解码器。同时，我们还设计了一种类似于Aaronson的Gumbel水印的加密水印方案，但是针对PF解码器而自然量身定制。该水印方案不改变样本的分布，同时允许任意低的假阳性率和高的召回率，只要生成的文本具有高熵。我们的实验证明，PF解码器（及其带有水印的对应物）在困惑度方面明显优于朴素采样（及其带有Gumbel水印的对应物），同时保持相同的鲁棒性（和可检测性），因此为LLM解码提供了一个有希望的新方法。代码可在https://github.com/XuandongZhao/pf-decoding找到。

    In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
    
[^11]: LLMs能够进行良好的谈判吗？NegotiationArena平台与分析

    How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis

    [https://arxiv.org/abs/2402.05863](https://arxiv.org/abs/2402.05863)

    本文研究了LLM代理之间的谈判能力，开发了NegotiationArena框架用于评估和探索LLM代理的谈判能力。实验结果表明，LLM代理可以通过运用特定的行为策略显著提高谈判结果。

    

    谈判是社会交往的基础；人们谈判从汽车价格到如何共享共同资源的一切。随着对使用大型语言模型（LLMs）代表人类用户行动的兴趣不断增长，这些LLM代理也需要具备谈判能力。在本文中，我们研究了LLMs之间的谈判能力。我们开发了NegotiationArena：一个灵活的评估和探索LLM代理谈判能力的框架。我们在NegotiationArena中实施了三种类型的场景，以评估LLMs在分配共享资源（终极博弈）、聚合资源（交易博弈）和买卖商品（价格谈判）方面的行为。每个场景都允许LLM代理之间进行多轮灵活对话，以进行更复杂的谈判。有趣的是，通过采用某些行为策略，LLM代理可以显著提高谈判结果。例如，通过假装处境困顿和绝望，LLM代理可以增加谈判成果。

    Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LL
    
[^12]: 是否可以稳健地编辑大型语言模型？

    Is it Possible to Edit Large Language Models Robustly?

    [https://arxiv.org/abs/2402.05827](https://arxiv.org/abs/2402.05827)

    本研究旨在了解大型语言模型的稳健编辑方法的优势和局限性，从而促进对交流型人工智能的稳健、现实应用。

    

    大型语言模型（LLM）在构建能模仿人类行为的交流型人工智能方面发挥了关键作用，但也面临着高效定制的挑战。为了解决这个挑战，最近的研究涉及到了模型编辑的领域，通过操纵语言模型的特定记忆并改变相关的语言生成来进行编辑。然而，模型编辑的稳健性仍然是一个悬而未决的问题。本研究旨在了解编辑方法的优势和局限性，从而促进对交流型人工智能的稳健、现实应用。具体而言，我们进行了广泛的分析以回答三个关键的研究问题。Q1：编辑后的LLM是否能在现实情境中一致地表现出类似于交流型人工智能的行为？Q2：改写提示在多大程度上导致LLM偏离编辑的知识记忆？Q3：哪些知识特征与编辑的性能和稳健性相关？我们的实验结果揭示了显著的差异。

    Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity 
    
[^13]: 将自我监督的语音模型与视觉语音模型生成的伪词级目标相结合

    Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model

    [https://arxiv.org/abs/2402.05819](https://arxiv.org/abs/2402.05819)

    本文提出了PW-HuBERT框架，它通过将伪词级目标整合到训练过程中，从视觉语音模型中提取目标，从而在口语理解任务中展现出了优越性能。

    

    最近自我监督的语音模型在许多下游任务中取得了显著的改进。然而，这些模型主要集中在帧级训练目标上，在需要语义理解的口语理解任务中可能不足够。现有的工作通常依赖于额外的语音-文本数据作为中间目标，这在实际环境中成本高昂。为了解决这个挑战，我们提出了Pseudo-Word HuBERT（PW-HuBERT）框架，该框架将伪词级目标整合到训练过程中，其中目标是从视觉语音模型中提取的，因此消除了对语音-文本配对数据的需求。我们在四个口语理解基准测试中的实验结果表明了我们模型在捕捉语义信息方面的优越性。

    Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information.
    
[^14]: 选择性遗忘：推进语言模型中的机器遗忘技术和评估

    Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models

    [https://arxiv.org/abs/2402.05813](https://arxiv.org/abs/2402.05813)

    本研究提出了一种新方法，在语言模型中实现了精确和选择性的遗忘，以解决神经模型意外保留个人或敏感数据的问题。此外，还提出了两个创新的评估指标，旨在衡量敏感信息消除的效果。为了强化遗忘框架，还提出了一种有效的标注敏感范围的方法。

    

    本研究旨在研究机器遗忘（MU），这是一个致力于解决神经模型意外保留个人或敏感数据的问题的新兴领域。在这里，引入了一种新方法，实现精确和选择性遗忘语言模型中的信息。与以往完全相反的训练目标的方法不同，这种方法旨在减轻对语言模型性能的负面影响，特别是在生成任务中。此外，提出了两个创新的评估指标：敏感信息提取可能性（S-EL）和敏感信息存储准确性（S-MA），旨在衡量敏感信息消除的有效性。为了加强遗忘框架，提出了一种有效的标注敏感范围的方法，涵盖了在线和离线策略。在线选择机制利用语言概率得分确保计算效率，而离线策略则利用基于距离的过滤器。

    The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline
    
[^15]: FAQ-Gen：一个自动生成特定领域常见问题解答的自动化系统，用于辅助理解内容

    FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension

    [https://arxiv.org/abs/2402.05812](https://arxiv.org/abs/2402.05812)

    本文介绍了FAQ-Gen，一个利用文本转文本转换模型开发的自动化系统，用于生成特定领域的常见问题解答。该系统通过使用自我策划的算法来优化信息表示和问题-答案排名，提高了FAQ的准确性和相关性。定性人类评估表明生成的FAQs构造良好且可推广至不同领域。

    

    常见问题解答（FAQ）是指关于特定内容的最常见询问。它们通过简化主题和通过简明扼要地呈现信息来增强理解，作为内容理解的辅助工具。在本文中，我们通过开发一个端到端系统利用文本转文本转换模型来解决FAQ生成作为一个明确定义的自然语言处理（NLP）任务。我们提供了一份涵盖传统问答系统的文献综述，强调它们在直接应用于FAQ生成任务时的局限性。我们提出了一个系统，能够根据特定领域的文本内容构建FAQs，提高其准确性和相关性。我们利用自我策划的算法来获得提供给输入的信息的最佳表示，并对问题-答案对进行排序以最大化人类理解。定性人类评估显示生成的FAQs构造良好且可推广至不同领域。

    Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and re
    
[^16]: 通过反向课程强化学习训练大型语言模型进行推理

    Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

    [https://arxiv.org/abs/2402.05808](https://arxiv.org/abs/2402.05808)

    本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。

    

    在本文中，我们提出了R$^3$：通过反向课程强化学习（RL）进行推理的学习方法，该方法只使用结果监督来实现大型语言模型的过程监督的好处。将RL应用于复杂推理的核心挑战是确定一系列行动，以获得正向奖励并提供适当的优化监督。结果监督为最终结果提供了稀疏奖励，而不识别错误位置，而过程监督提供了逐步奖励，但需要大量手动注释。R$^3$通过学习正确演示来克服这些限制。具体而言，R$^3$将推理的起始状态从演示的结束滑动到开始，从而在所有阶段都促进了更容易的模型探索。因此，R$^3$建立了一个逐步的课程，使结果监督能够提供阶段级信号并精确定位错误。

    In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
    
[^17]: 用于资源有限语言的音标丰富语料库构建

    Phonetically rich corpus construction for a low-resourced language

    [https://arxiv.org/abs/2402.05794](https://arxiv.org/abs/2402.05794)

    本文提出了一种用于资源有限语言巴西葡萄牙语的音标丰富语料库构建方法，包括收集文本数据集、基于三音分布的句子选择算法和根据声学-发音语音特征进行新的音素分类。

    

    语音技术依赖于捕捉说话人的声音变异性和获取全面的语言信息。文本提示和句子选择方法已经在文献中被提出，以组成适度的音标数据，被称为音标丰富的语料库。然而，对于资源有限的语言而言，它们仍然对声学建模不足。因此，本文提出了一种新的方法，并概述了为资源有限的巴西葡萄牙语创建具有广泛音标覆盖的语料库所需的方法论方面。我们的方法包括从文本数据集收集到基于三音分布的句子选择算法。此外，我们提出了一种新的音素分类方法，根据声学-发音语音特征，因为不同的三音的绝对数量或低概率三音并不能保证对每种可能的组合进行适当的表示。

    Speech technologies rely on capturing a speaker's voice variability while obtaining comprehensive language information. Textual prompts and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \textit{corpus}. However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources. Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \textit{corpus} with broad phonetic coverage for a low-resourced language, Brazilian Portuguese. Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution. Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination. Usin
    
[^18]: Transformer语言模型在算法学习上的限制

    Limits of Transformer Language Models on Algorithmic Learning

    [https://arxiv.org/abs/2402.05785](https://arxiv.org/abs/2402.05785)

    Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。

    

    我们分析了Transformer语言模型在学习离散算法方面的能力。为此，我们引入了两个要求组合多个离散子任务的新任务。我们通过从头开始训练LLaMA模型和在GPT-4和Gemini上提示来衡量学习学习原语的组合。我们观察到，目前最先进的Transformer语言模型的组合能力非常有限，并且在样本规模方面比为新的算法组合重新学习所有子任务效果更差。我们还提出了一个复杂性理论的定理，证明了记忆前馈模型上的梯度下降可以指数级地浪费数据。

    We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
    
[^19]: 使用模态相对预训练的方法生成文本到代码的转换

    Text-to-Code Generation with Modality-relative Pre-training

    [https://arxiv.org/abs/2402.05783](https://arxiv.org/abs/2402.05783)

    本论文研究了如何根据不同的模态调整和表示序列标记，以进一步提高文本到代码生成的效果。

    

    最近，大型预训练语言模型被广泛应用于编程语言任务，并取得了巨大成功，通常通过进一步预先训练严格自然语言模型，其中训练序列通常包含自然语言和（线性化的）编程语言。这种方法有效地将序列的两种模态映射到同一嵌入空间中。然而，编程语言关键词（例如“while”）往往具有非常严格的定义语义。因此，从自然语言用法进行的迁移学习对其代码应用可能并不一定有益，反之亦然。在本研究中，我们假设已经预先训练好的语言模型，探讨了如何根据它们所属的模态不同来调整和表示序列标记，并最终有益于下游任务。我们在模态相对训练目标的进一步模型预训练中尝试了在模态之间分离嵌入空间的方法。

    Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectiv
    
[^20]: 使用一个新颖的平行图像数据集，研究大型视觉语言模型中的性别和种族偏见

    Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images

    [https://arxiv.org/abs/2402.05779](https://arxiv.org/abs/2402.05779)

    本研究使用了一个新的平行图像数据集，通过查询大型视觉语言模型，观察到在性别和种族维度上存在显著的偏见。

    

    在最近大语言模型(LLMs)和相应的对话模型的进步后，出现了一波新的大型视觉语言模型(LVLMs)。这样的模型可以将图像作为输入，并执行视觉问答、图像字幕、故事生成等任务。在这里，我们在这些系统中考察潜在的性别和种族偏见，基于输入图像中人物的被感知特征。为了实现这一目标，我们提出了一个新的数据集PAIRS（日常情景下的平行图像）。PAIRS数据集包含一组人工智能生成的人物图像，这些图像在背景和视觉内容方面非常相似，但在性别（男性、女性）和种族（黑人、白人）维度上有所不同。通过使用这样的图像查询LVLMs，我们观察到根据人物的被感知性别或种族，响应有明显差异。

    Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.
    
[^21]: SpiRit-LM: 交织的口语和书面语言模型

    SpiRit-LM: Interleaved Spoken and Written Language Model

    [https://arxiv.org/abs/2402.05755](https://arxiv.org/abs/2402.05755)

    SPIRIT-LM是一个基于预训练文本语言模型的多模态语言模型，通过将文本和语音连续训练，实现了口语和书面语言的混合模型。它展示了文本模型的语义能力和语音模型的表现能力。此外，SPIRIT-LM还能以少量样本的方式学习新任务。

    

    我们引入了SPIRIT-LM，这是一个基于文本和语音自由混合的多模态语言模型。我们的模型基于预训练的文本语言模型，并通过连续在文本和语音单元上进行训练将其扩展到语音模态。语音和文本序列被连接为一组单词，并使用一个小型自动筛选的语音-文本平行语料库来进行词级交织的训练方法。SPIRIT-LM有两个版本：一个是使用语音语义单元的BASE版本，另一个是在语义单元之外还使用了音高和风格单元来模拟表现力的EXPRESSIVE版本。对于这两个版本，文本是用子词BPE标记编码的。结果模型展示了文本模型的语义能力和语音模型的表现能力。此外，我们还证明了SPIRIT-LM能够在跨模态（即ASR、TTS、语音分类）中以少量样本的方式学习新任务。

    We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).
    
[^22]: TimeArena：在具有时间感知的模拟中塑造高效的多任务语言代理

    TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation

    [https://arxiv.org/abs/2402.05733](https://arxiv.org/abs/2402.05733)

    TimeArena是一个具有时间感知的模拟环境，为语言代理提供了更好的多任务处理能力。实验结果显示即使是最先进的语言模型也无法达到人类的多任务效率。

    

    尽管通过大规模语言模型（LLM）模拟人类行为取得了显著进展，但当前的文本模拟在时间概念上仍然不足。为此，我们介绍了TimeArena，一个新颖的文本模拟环境，它融入了复杂的时间动态和约束，更好地反映了现实生活中的计划场景。在TimeArena中，代理被要求尽快完成多个任务，允许并行处理以节省时间。我们实现了动作之间的依赖关系、每个动作的时间持续和代理及环境中的对象的占有情况。TimeArena基于烹饪、家庭活动和实验室工作中的30个真实任务。我们使用TimeArena对各种最先进的LLM模型进行了大量实验证明，即使是最强大的模型，如GPT-4，在有效的多任务处理方面仍然落后于人类，凸显了改进时间感知的需要。

    Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activities, and laboratory work. We conduct extensive experiments with various state-of-the-art LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the dev
    
[^23]: 面向口语对话建模的统一语音文本预训练方法

    Unified Speech-Text Pretraining for Spoken Dialog Modeling

    [https://arxiv.org/abs/2402.05706](https://arxiv.org/abs/2402.05706)

    本研究提出了一个名为统一口语对话模型（USDM）的广泛语音文本模型框架，用于生成与输入语音相关的连贯口语回复。通过使用多步骤的语音文本推理方式和广义语音文本预训练方案，该方法能够有效捕捉跨模态语义，并生成自然流畅的口语回复。

    

    近期的研究表明，扩展大型语言模型（LLM）以直接理解和合成语音具有良好的结果，但用于口语对话建模的基于LLM的策略仍然难以实现，需要进一步研究。本文提出了一个广泛的语音文本LLM框架，命名为统一口语对话模型（USDM），以在不依赖于自动语音识别（ASR）或文本到语音（TTS）解决方案的情况下生成与给定输入语音相关的连贯口语回复和有机的韵律特征。我们的方法采用了一种多步骤的语音文本推理方式，利用了底层LLM所展示的推理链能力。我们还提出了一种广义的语音文本预训练方案，有助于捕捉跨模态语义。自动和人工评估结果表明，所提出的方法能够有效生成自然流畅的口语回复，并且优于之前的和级联的基线模型。详细的比较研究

    While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative s
    
[^24]: 通过基于垄断对话的社交场景模拟实现大型语言模型的自对齐

    Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation

    [https://arxiv.org/abs/2402.05699](https://arxiv.org/abs/2402.05699)

    本文提出了一个通过社交场景模拟来自对齐大型语言模型的方法，以减轻其被滥用造成的潜在不良影响。通过一个名为MATRIX的虚拟排练空间，LLM可以在回答查询前考虑社交后果，并通过MATRIX-simulated数据的微调，保持对人类价值的遵从和推理速度的平衡。实验证明，在温和假设下，带有MATRIX的LLM胜过了宪法AI。

    

    将大型语言模型(LLMs)与人类价值对齐，以减轻其被滥用造成的潜在不良影响，具有重要意义。本文借鉴社会学的见解，即认识到所有各方的关切是塑造人类价值观的关键因素，提出了一种自对齐LLMs的新方向：社交场景模拟。为此，我们提出了一个名为MATRIX的创新社交场景模拟器，它可以模拟用户输入查询周围的现实场景，使LLM在回答前能够考虑社交后果。MATRIX类似于一个“垄断对话”下的虚拟排练空间，LLM在其中扮演与查询相关的多个角色并进行自我实践。为了引入这种对齐能力，我们使用MATRIX模拟数据对LLM进行微调，确保其在不影响推理速度的情况下符合人类价值观。理论上，我们证明了在温和假设下，带有MATRIX的LLM胜过了宪法AI。最后，大量实验证实了我们的方法在多个任务上都取得了最佳性能。

    Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
    
[^25]: 多语言E5文本嵌入：一项技术报告

    Multilingual E5 Text Embeddings: A Technical Report

    [https://arxiv.org/abs/2402.05672](https://arxiv.org/abs/2402.05672)

    这项技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，包括三种不同大小的模型和一种新的以指令为导向的嵌入模型。模型在推理效率和嵌入质量上取得了平衡，性能与同等大小的最先进的仅英文模型相当。

    

    本技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，该模型于2023年中期发布。提供了三种不同大小（小/基础/大）的嵌入模型，平衡了推理效率和嵌入质量。训练过程遵循英文E5模型的配方，涉及10亿个多语言文本对的对比预训练，然后在一系列标记数据集上进行微调。此外，我们还引入了一种新的以指令为导向的嵌入模型，性能与相似大小的最先进的仅英文模型相当。有关模型发布的信息可以在https://github.com/microsoft/unilm/tree/master/e5找到。

    This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .
    
[^26]: 对LLMs的越狱攻击的综合评估

    Comprehensive Assessment of Jailbreak Attacks Against LLMs

    [https://arxiv.org/abs/2402.05668](https://arxiv.org/abs/2402.05668)

    对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。

    

    对大型语言模型（LLMs）的滥用引起了广泛关注。为了解决这个问题，已经采取了安全措施以确保LLMs符合社会伦理。然而，最近的研究发现了一种绕过LLMs安全措施的不稳定漏洞，被称为越狱攻击。通过应用技术，如角色扮演场景、对抗性样本或对安全目标的微妙破坏作为提示，LLMs可以产生不适当甚至有害的回应。虽然研究人员已经研究了几种越狱攻击的类别，但他们都是孤立地进行的。为了填补这个空白，我们提出了对各种越狱攻击方法的首次大规模测量。我们集中在来自四个类别的13种尖端越狱方法、16种违规类别的160个问题以及六种流行的LLMs上。我们广泛的实验结果表明，优化的越狱提示始终能够达到最高的攻击成功率，并表现出...

    Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
    
[^27]: 合并事实，塑造谬误：评估长文生成中聚合事实性主张的矛盾性质

    Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations

    [https://arxiv.org/abs/2402.05629](https://arxiv.org/abs/2402.05629)

    该论文研究了长文生成中的事实性问题，发现大型语言模型在生成段落时可能会由于实体模糊而将可验证的事实组合成非事实的段落。现有的事实准确度评估方法无法正确评估这些非事实段落，作者提出了一种增强的度量指标来应对这个问题。

    

    大型语言模型（LLMs）产生的长文生成物包含了一系列事实和非事实的主张，这使得评估事实性变得困难。为了以更精细的方式评估长文生成物的事实准确性，先前的研究提出将长文生成物分解为多个可验证的事实并独立验证这些事实。生成物的事实性是所有事实中可验证事实的比例。这些方法假设结合了事实主张形成了一个事实性段落。本文展示了这一假设可能因为实体模糊而被违反。我们展示了LLMs可以生成包含可验证事实的段落，但由于实体模糊，这些事实被结合形成了一个非事实的段落。我们进一步揭示了现有的事实准确度度量指标，包括FActScore和引用回顾，无法正确评估这些非事实段落的事实性。为了解决这个问题，我们引入了一种增强度量指标，D-FActScore，作为一个具体的解决方案。

    Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifi
    
[^28]: 针对仇恨、辱骂和亵渎检测的高效模型

    Efficient Models for the Detection of Hate, Abuse and Profanity

    [https://arxiv.org/abs/2402.05624](https://arxiv.org/abs/2402.05624)

    这篇论文提出了针对仇恨、辱骂和亵渎检测的高效模型，因为大型语言模型在训练过程中可能学习到这些负面内容并生成不合适的文本。

    

    大型语言模型(LLM)是许多自然语言处理(NLP)任务的基石，如情感分析、文档分类、命名实体识别、问答、摘要等。LLM通常在来自网络的数据上进行训练。这些数据容易包含仇恨、辱骂和亵渎(HAP)内容。由于LLM在训练过程中接触到HAP内容，模型会学习到并生成带有仇恨或亵渎内容。例如，当使用HuggingFace(Transformers库的开源RoBERTa模型(具体来说，是RoBERTa基础模型))来替换句子`I do not know that Persian people are that MASK`中的掩码标记时，它返回得分最高的词为`stupid`。这在文明对话中是不可接受的。文本中的仇恨、辱骂和亵渎的检测是创建文明和没有偏见的LLM的重要组成部分。

    Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only f
    
[^29]: 基于深度学习的计算机工作市场分析：对技能提取和分类从招聘信息的调查

    Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings

    [https://arxiv.org/abs/2402.05617](https://arxiv.org/abs/2402.05617)

    该论文调查了基于深度学习的计算机工作市场分析中的技能提取和分类方法。通过全面概述深度学习方法、数据集和术语，以及对公开可获取的数据集的分类，填补了这一领域的研究空白。

    

    最近几年，自然语言处理（NLP）取得了显著进展，使得计算机工作市场分析领域取得了快速发展。在该应用领域中，核心任务是从招聘信息中提取和分类技能。由于其快速增长和跨学科性质，目前尚缺乏对这一兴起领域的详尽评估。本调查旨在通过提供关于NLP驱动的技能提取和分类的深度学习方法、数据集和术语的全面概述来填补这一空白。我们对公开可获取的数据集进行了全面分类，解决了数据集创建和特征的信息缺乏的问题。最后，对术语的重点关注解决了目前对于重要概念（如硬技能和软技能）以及与技能提取和分类相关的术语缺乏一致定义的问题。

    Recent years have brought significant advances to Natural Language Processing (NLP), which enabled fast progress in the field of computational job market analysis. Core tasks in this application domain are skill extraction and classification from job postings. Because of its quick growth and its interdisciplinary nature, there is no exhaustive assessment of this emerging field. This survey aims to fill this gap by providing a comprehensive overview of deep learning methodologies, datasets, and terminologies specific to NLP-driven skill extraction and classification. Our comprehensive cataloging of publicly available datasets addresses the lack of consolidated information on dataset creation and characteristics. Finally, the focus on terminology addresses the current lack of consistent definitions for important concepts, such as hard and soft skills, and terms relating to skill extraction and classification.
    
[^30]: 预训练的生成式语言模型作为序列型任务的通用学习框架

    Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks

    [https://arxiv.org/abs/2402.05616](https://arxiv.org/abs/2402.05616)

    预训练的小型生成式语言模型可以作为序列型任务的通用学习框架，通过指令微调可以在化学信息学任务中实现接近最先进结果。

    

    我们提出了一个观点，即具有数百万参数的小型预训练基础生成式语言模型可以用作序列型任务的通用学习框架。我们的提议解决了从头开始训练神经网络和语言模型所面临的计算资源、技能需求和时间限制等挑战。此外，我们的方法专注于创建小型且高度专业化的模型，能够准确执行基于任务模型无法完成的挑战性任务。我们证明了使用125M、350M和1.3B参数的预训练基础语言模型进行指令微调，使用10,000到1,000,000个指令示例可以在具有挑战性的化学信息学任务上实现接近最先进结果。我们还展示了连续的语言模型微调时期对改进结果的作用，以及数据格式和预训练基础语言模型选择对指令微调成功的重要性。

    We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.
    
[^31]: AttnLRP: 注意力感知的逐层相关传递用于Transformer

    AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers

    [https://arxiv.org/abs/2402.05602](https://arxiv.org/abs/2402.05602)

    AttnLRP是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，并具有与单一反向传播相似的计算效率的方法。它通过扩展逐层相关传递归因方法以处理注意力层来解决了黑盒Transformer模型的归因问题，具有超越现有方法的准确性和理解潜在表示的能力。

    

    大型语言模型容易产生偏见的预测和幻象，这突显了理解其模型内部推理过程的重要性。然而，实现对整个黑盒Transformer模型的准确归因并保持计算效率是一个尚未解决的挑战。通过扩展逐层相关传递归因方法以处理注意力层，我们有效地解决了这些挑战。虽然存在部分解决方案，但我们的方法是首个能够忠实且全面地归因Transformer模型的输入和潜在表示，同时计算效率与单一反向传播相似。通过对Llama 2、Flan-T5和Vision Transformer架构上与现有方法的广泛评估，我们证明了我们提出的方法在准确性方面超过了其他方法，并能够理解潜在表示，为概念打开了大门。

    Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
    
[^32]: SoftEDA: 用软标签重新思考基于规则的数据增强

    SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels

    [https://arxiv.org/abs/2402.05591](https://arxiv.org/abs/2402.05591)

    本文提出了SoftEDA方法，通过使用软标签在增强数据上，从而解决了基于规则的文本数据增强方法可能破坏文本原始含义的问题，实验证明了该方法的有效性。

    

    基于规则的文本数据增强因其简单性而被广泛应用于自然语言处理任务。然而，这种方法可能会破坏文本的原始含义，最终影响模型的性能。为了克服这个限制，我们提出了一种将软标签应用于增强数据的简单技术。我们进行了七个不同的分类任务的实验，并经验性地证明了我们提出的方法的有效性。我们已经公开了我们的源代码以便复现。

    Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility.
    
[^33]: AutoAugment是你需要的：在低资源环境下增强基于规则的数据增强方法

    AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes

    [https://arxiv.org/abs/2402.05584](https://arxiv.org/abs/2402.05584)

    本文提出了将AutoAugment方法应用于解决文本数据增强中的语义损害问题，实验证明该方法可以加强现有的增强方法并提升预训练语言模型的性能。

    

    文本数据增强是一个复杂的问题，因为句子的离散性质。尽管基于规则的增强方法因其简单性而在实际应用中被广泛采用，但它们可能导致潜在的语义损害。以往的研究者提出了使用软标签（softEDA）进行简单数据增强，并采用标签平滑来减轻这个问题。然而，针对每个模型和数据集找到最佳因子是具有挑战性的，因此在实际应用中使用softEDA仍然困难。在本文中，我们提出了将AutoAugment应用于解决这个问题的方法。实验结果表明，所提出的方法可以提升现有的增强方法，并且基于规则的方法可以增强先进的预训练语言模型。我们提供源代码。

    Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models. We offer the source code.
    
[^34]: 使用大规模跨语言模型在不同维度上建立音频录音之间的亲密程度

    Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models

    [https://arxiv.org/abs/2402.05581](https://arxiv.org/abs/2402.05581)

    该论文提出了一种通过使用跨语言模型的预训练向量表示来确定音频录音之间的抽象程度的方法。通过进行ABX测试，该研究证实了从具有不同语言和非语言特征的录音提取的表示沿着相同的维度有所不同。这一方法是无监督的，可以作为获取大规模数据集中音频表征的有效途径。

    

    在低资源语言研究的高度约束背景下，我们通过使用预训练模型中的语音向量表示来确定它们相对于音频信号的抽象程度。我们提出了一种新的无监督方法，利用具有精心策划的元数据的音频录音进行ABX测试，以了解表示中存在的信息类型。ABX测试确定多语言语音模型计算的表示是否编码了给定的特征。我们设计了三个实验：一个关于房间声学特性，一个关于语言体裁，一个关于语音特征。结果证实，来自具有不同语言/非语言特性的录音的表示沿着相同的线路有所不同。将更多的音频信号嵌入一个向量中可以更好地区分非语言特征，而较短的片段可以更好地区分分段信息。该方法完全无监督，可能是一种从大规模数据集中获取音频表征的有效方法。

    In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully unsupervised, potentially op
    
[^35]: 传统机器学习模型和基于双向编码器表示的转换器(BERT)的自动分类饮食紊乱推文：算法开发和验证研究

    Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study

    [https://arxiv.org/abs/2402.05571](https://arxiv.org/abs/2402.05571)

    该研究旨在开发高效的机器学习模型，用于对饮食紊乱相关的推文进行分类。研究发现，基于转换器的双向编码器表示在四个类别中实现了最高的F1分数，并表明基于转换器的模型较传统技术更为优越，但需要更多的计算资源。

    

    背景：饮食紊乱日益普遍，社交网络提供了宝贵的信息。目标：我们的目标是确定有效的机器学习模型，用于对与饮食紊乱相关的推文进行分类。方法：在三个月内，我们收集了关于饮食紊乱的推文。我们对2,000条推文进行了标记，标记了：(1)由饮食紊乱患者撰写的推文，(2)宣传饮食紊乱的推文，(3)信息性，和(4)科学内容。使用传统机器学习和深度学习模型进行分类，并评估准确度、F1分数和计算时间。结果：从1,058,957条收集到的推文中，基于转换器的双向编码器表示在所有四个类别中实现了最高的F1分数(71.1%-86.4%)。结论：基于转换器的模型在分类与饮食紊乱相关的推文方面表现优于传统技术，但它们需要更多的计算资源。

    Background: Eating disorders are increasingly prevalent, and social networks offer valuable information.   Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders.   Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time.   Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories.   Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.
    
[^36]: 在交流医疗辅导上对大型语言模型进行基准测试：一种新的系统和数据集

    Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset

    [https://arxiv.org/abs/2402.05547](https://arxiv.org/abs/2402.05547)

    本研究介绍了“ChatCoach”，一个集成人工智能与人类医生合作的框架，在交流医疗辅导中利用大型语言模型，提供模拟环境和实时反馈，以帮助医学学员提高沟通技巧。

    

    在医疗保健领域，自然语言处理（NLP）的传统应用主要集中在以患者为中心的服务上，增强患者互动和护理交付，例如医学对话系统。然而，NLP在帮助经验不丰富的医生，特别是在交流医疗辅导等领域的潜力仍然很少被探索。我们引入了“ChatCoach”，一个集成的人工智能合作框架。在这个框架内，一个患者代理和一个辅导代理共同支持医学学员在会诊过程中练习医学沟通技巧。与传统的对话系统不同，ChatCoach提供了一个模拟环境，医生可以在其中与患者代理进行医学对话。同时，辅导代理会提供实时反馈给医生。为了构建ChatCoach系统，我们开发了一个数据集，并集成了ChatGPT和Llama2等大型语言模型，旨在评估它们在交流医疗辅导方面的效果。

    Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess 
    
[^37]: 使用合成数据进行语音转文字中地址识别的命名实体识别

    Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data

    [https://arxiv.org/abs/2402.05545](https://arxiv.org/abs/2402.05545)

    本文介绍了使用合成数据构建命名实体识别模型的方法，该模型通过语音转文字转录数据提取地址部分。通过利用SlovakBERT模型，并强调在合成数据中模拟口语语言的可变性，我们评估了仅使用合成数据训练的NER模型的性能。

    

    本文介绍了一种在Bidirectional Encoder Representations from Transformations（BERT）架构上建立的命名实体识别（NER）模型的方法，特别是利用了SlovakBERT模型。该NER模型从语音转文字转录数据中提取地址部分。由于真实数据的稀缺性，使用GPT API生成了一个合成数据集。强调在这个人工数据中模拟口语语言的可变性的重要性。使用仅基于合成数据训练的NER模型，对小规模真实测试数据集进行了评估。

    This paper introduces an approach for building a Named Entity Recognition (NER) model built upon a Bidirectional Encoder Representations from Transformers (BERT) architecture, specifically utilizing the SlovakBERT model. This NER model extracts address parts from data acquired from speech-to-text transcriptions. Due to scarcity of real data, a synthetic dataset using GPT API was generated. The importance of mimicking spoken language variability in this artificial data is emphasized. The performance of our NER model, trained solely on synthetic data, is evaluated using small real test dataset.
    
[^38]: 为加强社交媒体帖子中的饮食障碍检测，赋能机器学习模型的语境知识

    Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts

    [https://arxiv.org/abs/2402.05536](https://arxiv.org/abs/2402.05536)

    这项研究提出了一种新颖的方法，通过结合知识图谱和深度学习，借助实体识别和链接技术以及上下文化嵌入，增强了机器学习模型对社交媒体帖子的分类能力。该方法主要应用于健康领域的饮食障碍识别，为早期诊断提供帮助。

    

    社交网络在信息共享方面至关重要，尤其在健康领域用于讨论疾病和治疗。然而，这些平台上的帖子往往是简短的文本，给人工智能在理解上带来了挑战。我们介绍了一种创新的混合方法，结合社区维护的知识图谱（如Wikidata）和深度学习，以增强社交媒体帖子的分类。该方法利用先进的实体识别器和链接器（如Falcon 2.0）将短文帖子中的实体连接到知识图谱。然后使用知识图谱嵌入（KGEs）和上下文化词嵌入（如BERT）来创建丰富的、基于语境的帖子表示。我们的重点是健康领域，特别是识别与饮食障碍相关的帖子（如厌食症、暴食症），以帮助医疗服务提供者进行早期诊断。我们在一个包含2,000条关于饮食障碍的推文的数据集上测试了我们的方法，发现合并单词嵌入和上下文化嵌入可以显著提高检测性能。

    Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts.   Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word em
    
[^39]: NoisyICL: 一点噪音在模型参数中提高了上下文学习的性能、

    NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning

    [https://arxiv.org/abs/2402.05515](https://arxiv.org/abs/2402.05515)

    NoisyICL通过在模型参数中引入噪音，提高了上下文学习的性能和校准性，实验结果显示NoisyICL可以产生更准确、更公平的预测。

    

    上下文学习 (ICL) 在高先验偏差和不可信任的置信度的影响下，表现不佳且校准不足。以往的一些工作通过使用庞大的数据集和计算成本对语言模型进行微调以改善 ICL 的性能。在本文中，我们提出了 NoisyICL，通过随机噪音扰动模型参数来努力提高性能和校准性。我们在2个模型和12个下游数据集上的实验表明，NoisyICL可以帮助ICL产生更准确的预测。进一步的分析表明，NoisyICL使得模型能够提供更公平的预测，同时置信度更可信。因此，我们认为NoisyICL是ICL的一种有效校准方法。我们的实验代码已上传至Github。

    In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.
    
[^40]: GPT对于序列生成任务具有多语言注释功能

    GPTs Are Multilingual Annotators for Sequence Generation Tasks

    [https://arxiv.org/abs/2402.05512](https://arxiv.org/abs/2402.05512)

    本研究提出了一种利用大型语言模型进行自动注释的方法，具有费用效益高和适用于低资源语言注释的优点，同时构建了一个图像字幕数据集并开放了源代码。

    

    数据注释是构建新数据集的关键步骤。然而，传统的通过众包进行数据注释的方法既耗时又昂贵。此外，当处理低资源语言时，由于众包工作者的语言池差异，这个过程的复杂性增加。为了解决这些问题，本研究提出了一种利用大型语言模型进行自动注释的方法，最近的研究表明这些模型具有出色的性能。通过我们的实验，我们证明了所提出的方法不仅费用效益高，而且适用于低资源语言注释。此外，我们使用我们的方法构建了一个图像字幕数据集，并致力于将此数据集开放给未来的研究。我们已经开放了我们的源代码，以便进一步研究和可复现性。

    Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.
    
[^41]: 快速优化LLM越狱方法：通过潜意识利用和模仿动作

    Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia

    [https://arxiv.org/abs/2402.05467](https://arxiv.org/abs/2402.05467)

    本文介绍了一种名为RIPPLE的快速优化方法，该方法通过潜意识利用和模仿动作的思想，解决了通过越狱提示绕过安全措施的问题。

    

    大型语言模型(LLMs)在各个领域得到了广泛应用，通过其非凡的推理和理解能力改变了人类生活。随着它们在敏感任务中的增加使用，安全问题引起了广泛关注。人们已经做出了大量努力，以确保LLMs与人类道德原则相一致，以确保其安全部署。尽管有潜力，但最近的研究表明，对齐的LLMs容易受到专门的越狱提示的影响，这些提示绕过安全措施，引发暴力和有害内容。当代LLMs的离散本质和庞大规模使得自动生成多样化、高效和强效的越狱提示面临重大挑战，这是一个持续的障碍。在本文中，我们介绍了一种名为RIPPLE（基于潜意识利用和模仿动作的快速优化）的新型基于优化的方法，该方法受到了两个心理学概念的启发：潜意识和模仿动作。

    Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the p
    
[^42]: 永远不嫌晚：将声学信息融入大型语言模型以进行自动语音识别

    It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition

    [https://arxiv.org/abs/2402.05457](https://arxiv.org/abs/2402.05457)

    本论文通过将声学信息融入大型语言模型（LLM）中，提出了一种名为不确定性感知动态融合（UADF）的后期融合解决方案，以克服生成性错误纠正（GER）中存在的数据不确定性问题，并应用于自动语音识别（ASR）任务。通过在自回归解码过程中实施UADF方法，在LLM决策的标记级分析和校准的基础上，动态地融合声学信息，从而提高了ASR的准确性。

    

    最近的研究成功地表明，大型语言模型（LLM）可以成功用于在自动语音识别（ASR）输出之上进行生成性错误纠正（GER）。具体而言，LLM被用于对由ASR系统生成的N最佳假设列表进行直接映射，生成预测的输出转录。然而，尽管其有效性，GER引入了额外的数据不确定性，因为LLM在训练时没有考虑到语音信号中可用的声学信息。在这项工作中，我们旨在通过一种名为不确定性感知动态融合（UADF）的新型后期融合解决方案，通过注入声学信息以生成预测转录来克服这种限制。UADF是一种多模态融合方法，实现在自回归解码过程中，并分为两个阶段：（i）它首先分析和校准标记级LLM决策，然后（ii）它动态地吸收声学信息。

    Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the aco
    
[^43]: 大型语言模型用于心理语言学合理性预测

    Large Language Models for Psycholinguistic Plausibility Pretesting

    [https://arxiv.org/abs/2402.05455](https://arxiv.org/abs/2402.05455)

    本研究探讨了使用语言模型(LMs)生成语言材料的合理性判断，并发现GPT-4的合理性判断与人类判断高度相关。这意味着LMs可以替代人类进行预测试，从而在心理语言学中具有重要的应用潜力。

    

    在心理语言学中，创建受控材料对于确保研究结果仅归因于预期操作而不受外部因素影响至关重要。为了实现这一目标，心理语言学家通常会对语言材料进行预测试，其中一种常见的预测试是向人类评估者征求关于特定句子的合理性判断。在本文中，我们研究了是否可以利用语言模型(LMs)生成这些合理性判断。我们研究了多种不同语言结构的LMs，并评估它们的合理性判断与人类判断之间是否相关。我们发现GPT-4的合理性判断与我们研究的不同结构的人类判断高度相关，而其他LMs在常用的句法结构上与人类的判断具有良好的相关性。然后，我们测试了这种相关性是否意味着可以用LMs代替人类进行预测试。我们发现，当粗粒度的合理性判断进行时，LMs的结果与人类评估者的结果高度相关，并且LMs的合理性判断可以代替人类进行合理性预测试。

    In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgeme
    
[^44]: 准确的LLMs的LoRA-Finetuning量化通过信息保留

    Accurate LoRA-Finetuning Quantization of LLMs via Information Retention

    [https://arxiv.org/abs/2402.05445](https://arxiv.org/abs/2402.05445)

    本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。

    

    将LLMs的LoRA-finetuning量化研究得到准确但紧凑的LLMs以便在资源受限的硬件上部署。然而，现有的方法导致量化的LLMs严重退化，甚至无法从LoRA的调优中获益。本文提出了一种新颖的IR-QLoRA，通过信息保留推动带有LoRA的量化LLMs变得高度准确。所提出的IR-QLoRA主要依赖于两种从统一信息视角派生的技术：（1）基于统计的信息校准量化允许LLMs的量化参数精确保留原始信息；（2）基于调优的信息弹性连接使LoRA利用具有多样信息的弹性表示转换。综合实验证明，在2-4位宽下，IR-QLoRA可以显著提高LLaMA和LLaMA2系列的准确性，例如，4位LLaMA-7B相比于...

    The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the 
    
[^45]: 用语言模型改善虚拟环境中的智能体交互

    Improving Agent Interactions in Virtual Environments with Language Models

    [https://arxiv.org/abs/2402.05440](https://arxiv.org/abs/2402.05440)

    这项研究通过利用语言模型在虚拟环境中改进智能体的交互，实现了对任务的理解和沟通能力的提升，并在实验中取得了显著的改进。

    

    为了有效的人类辅助，增强AI系统的沟通技巧需要系统主动探索特定情况并适当交互。本研究以Minecraft数据集中的集体建立任务为重点，利用语言建模来通过最先进的方法增强任务理解能力。这些模型注重多模态理解和面向任务的对话理解任务，揭示了它们的解释和响应能力。我们的实验结果表明，与现有方法相比，我们取得了显著的改进，为将来在这个领域的研究指明了一个有希望的方向。

    Enhancing AI systems with efficient communication skills for effective human assistance necessitates proactive initiatives from the system side to discern specific circumstances and interact aptly. This research focuses on a collective building assignment in the Minecraft dataset, employing language modeling to enhance task understanding through state-of-the-art methods. These models focus on grounding multi-modal understanding and task-oriented dialogue comprehension tasks, providing insights into their interpretative and responsive capabilities. Our experimental results showcase a substantial improvement over existing methods, indicating a promising direction for future research in this domain.
    
[^46]: GPT-4使用结构化叙事提示生成生活事件的叙述：一项验证研究

    GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study

    [https://arxiv.org/abs/2402.05435](https://arxiv.org/abs/2402.05435)

    本研究通过使用结构化叙事提示，验证了GPT-4生成的叙述在传达生活事件方面的有效性。研究结果表明，大多数叙述能够足够传达提示的意图。同时，通过机器学习模型的训练和验证，可以自动识别有效和无效的叙述。

    

    大型语言模型在生成各种叙述方面发挥重要作用，促进了对其在叙述形式中传达生活事件效果的系统探索。本研究利用零-shot结构化叙事提示，使用OpenAI的GPT-4生成了24,000个叙述。从这个数据集中，我们手动分类了2,880个叙述，并评估它们在传达出生、死亡、招聘和解雇事件方面的有效性。令人惊讶的是，87.43%的叙述足够传达结构化提示的意图。为了自动识别有效和无效的叙述，我们对分类数据集训练和验证了九个机器学习模型。利用这些模型，我们扩展了对剩余21,120个叙述的分类预测分析。所有的机器学习模型在将有效的叙述分类为有效方面表现出色，但在同时将无效的叙述分类为无效方面存在挑战。我们的研究结果不仅推进了这一领域的发展，还提供了自动识别有效叙述的有益信息。

    Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
    
[^47]: 现在所有人都修剪：仅使用前向传递的LLM结构化修剪

    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes

    [https://arxiv.org/abs/2402.05406](https://arxiv.org/abs/2402.05406)

    本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。

    

    鉴于非专业从业者和最富有资源的机构之间的硬件差距，尺寸不断增长的LLM变得越来越难以使用。虽然提出了许多方法来压缩LLM，以使其资源消耗可管理，但这些方法本身往往耗费资源，使其目标用户群无法接触到。在这项工作中，我们探讨了仅使用前向传递的LLM结构化修剪问题。我们希望让从业者能够修剪模型，使其规模大到硬件仅有足够的内存来运行推理。我们开发了Bonsai，这是一种无梯度、扰动修剪方法，能够生成小、快和准确的修剪模型。我们观察到，Bonsai生成的修剪模型（i）优于更昂贵的梯度-based结构化修剪方法生成的模型，并且（ii）与半结构化修剪模型相比，速度快一倍且准确性相当。

    Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
    
[^48]: 从错误中学习的上下文准则学习

    In-Context Principle Learning from Mistakes

    [https://arxiv.org/abs/2402.05403](https://arxiv.org/abs/2402.05403)

    本文提出了一种新的学习方法LEAP，通过让模型从少量输入-输出示例中犯错误，然后反思并学习准则，从而提升模型在各种任务上的表现。

    

    上下文学习（ICL，也称为少样本提示）已成为将LLMs适应下游任务的标准方法，通过从少量的输入-输出示例中学习。然而，所有基于ICL的方法只从正确的输入-输出对中学习。在本文中，我们重新审视这一范例，通过从少给定的输入-输出示例中学习更多内容。我们引入了学习准则（LEAP）：首先，我们有意诱使模型在这些少量示例中犯错误；然后，我们反思这些错误，并从中学习显式的任务特定“准则”，这些准则有助于解决类似的问题并避免常见的错误；最后，我们使用原始的少样本示例和这些学到的通用准则来提示模型回答未见过的测试问题。我们在包括多跳问题回答（Hotpot QA）、文本问题回答（DROP）、Big-Bench困难推理和数学问题（GSM8K和MATH）在内的多个基准测试上评估了LEAP；在所有这些基准测试中，LEAP都有所改进。

    In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest 
    
[^49]: 在大型语言模型中使用进化算法引导的零样本思维链推理

    Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models

    [https://arxiv.org/abs/2402.05376](https://arxiv.org/abs/2402.05376)

    本论文提出了一种在大型语言模型中使用进化算法引导的零样本思维链推理的方法。通过动态生成多样的推理方式，并通过重写操作增强模型对问题的理解，我们的方法在十个推理数据集上展现出了卓越的性能。

    

    大型语言模型（LLM）通过应用零样本思维链推理展示了出色的性能，并展现了令人印象深刻的推理能力。然而，由于预训练阶段中句子前缀的演化性质，现有的零样本思维链推理方法无法在所有任务实例上都采用相同的推理方式，可能不够优化。在本文中，我们引入了一种新的零样本推理方法，利用进化算法动态生成LLM的多样推理方式。我们的方法涉及初始化两个思维链推理方式，基于LLM进行进化操作以生成多样集合，并利用LLM选择适合给定问题的思维链推理方式。此外，通过选定的思维链推理方式引导的重写操作增强LLM对问题的理解。通过对十个推理数据集进行大量实验，证明了我们方法的卓越性能。

    Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of 
    
[^50]: CIC：一种面向文化感知图像字幕的框架

    CIC: A framework for Culturally-aware Image Captioning

    [https://arxiv.org/abs/2402.05374](https://arxiv.org/abs/2402.05374)

    CIC是一种面向文化感知图像字幕的框架，通过结合视觉问答和大型语言模型，它能够生成能描述图像中文化元素的详细字幕。

    

    图像字幕通过使用视觉-语言预训练模型（VLPs）如BLIP从图像生成描述性句子，这种方法已经取得了很大的改进。然而，当前的方法缺乏对图像中所描绘的文化元素（例如亚洲文化群体的传统服装）生成详细描述性字幕的能力。在本文中，我们提出了一种新的框架，\textbf{面向文化感知图像字幕（CIC）}，该框架能够从代表不同文化的图像中生成字幕并描述文化元素。受到将视觉模态和大型语言模型（LLMs）通过适当的提示进行组合的方法的启发，我们的框架（1）根据图像中的文化类别生成问题，（2）利用生成的问题从视觉问答（VQA）中提取文化视觉元素，（3）使用带有提示的LLMs生成文化感知字幕。我们在4个不同大学的45名参与者上进行了人工评估。

    Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
    
[^51]: 以显式奖励的噪声对比对齐语言模型

    Noise Contrastive Alignment of Language Models with Explicit Rewards

    [https://arxiv.org/abs/2402.05369](https://arxiv.org/abs/2402.05369)

    本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。

    

    用户意图通常被形式化为需要在微调语言模型时最大化的评估奖励。现有的对齐方法，如直接优化偏好（DPO），主要适用于隐含定义而非明确给定奖励的两两偏好数据。在本文中，我们引入了一个通用的LM对齐框架，利用噪声对比估计（NCE）来解决明确注释有标量评估的奖励数据处理的差距。我们的框架包括两个并行算法，NCA和InfoNCA，两者都能从奖励数据和偏好数据中直接提取LM策略。值得注意的是，我们证明了DPO损失是我们提出的InfoNCA目标在两两偏好设置下的特殊情况，从而集成和扩展了当前的对齐理论。通过对比NCA和InfoNCA，我们展示了InfoNCA和DPO如何在不同响应对于单个指令的相对可能性上进行调整。

    User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
    
[^52]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^53]: 航行知识之海：利用LLMs进行行星级答案检索

    Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs

    [https://arxiv.org/abs/2402.05318](https://arxiv.org/abs/2402.05318)

    本文探讨了信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用，LLMs的整合标志着用户与信息系统交互方式的范式转变。

    

    信息检索是一个快速发展的信息检索领域，其特点是从基本的超链接导航到复杂的算法驱动搜索引擎的不断改进。本文旨在全面介绍信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用。LLMs在响应检索和索引领域的整合标志着用户与信息系统交互方式的范式转变。这种范式转变是由像GPT-4这样的大型语言模型的整合驱动的，它们能够理解和生成类似于人类的文本，从而能够提供更直接和情境相关的答案给用户查询。通过这种探索，我们试图阐明技术里程碑。

    Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones 
    
[^54]: 检验医疗视觉和基于语言的疾病检测的多模态联邦学习中的模态不一致性

    Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection

    [https://arxiv.org/abs/2402.05294](https://arxiv.org/abs/2402.05294)

    本文首次分析了多模态联邦学习中的模态不一致性的影响，并揭示了其与参与客户端之间的数据异质性的联系。通过使用不考虑不一致性的信息融合机制和模态插值网络，在解决模态不一致性问题方面取得了一定的成果。

    

    多模态联邦学习（MMFL）利用每个客户端中的多个模态构建比其单模态对应物更强大的联邦学习（FL）模型。然而，不同客户端缺失模态的影响，也称为模态不一致性，一直被大大忽视。本文首次分析了模态不一致性的影响，并揭示了其与参与客户端之间的数据异质性的联系。我们特别检查了不一致的MMFL与单模态和多模态客户端相比是否更有益于单模态FL。此外，我们还研究了解决这个问题的三个潜在途径。首先，我们研究了各种自注意机制对于不考虑不一致性的信息融合在MMFL中的有效性。其次，我们在多模态客户端中引入了一个预先训练的模态插值网络（MIN）来解决单模态客户端中的模态翻译问题，并研究其减轻缺失模态问题的潜力。第三，我们...

    Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly
    
[^55]: TreeForm: 表单文档解析的端到端标注与评估

    TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing

    [https://arxiv.org/abs/2402.05282](https://arxiv.org/abs/2402.05282)

    本文提出了一种TreeForm的标注方案和评估方法，旨在解决表单文档解析的端到端模型开发和评估的问题。

    

    由于文档的高度结构化特性以及高度变化的样式和内容，具有视觉丰富的表单理解（VRFU）提出了一个复杂的研究问题。当前的标注方案将表单理解分解，并省略了关键的层次结构，使得开发和评估端到端模型变得困难。在本文中，我们提出了一种新的F1度量方法来评估表单解析器，并描述了一种适用于VRFU的新的内容无关的基于树形的标注方案：TreeForm。我们提供了将以前的标注方案转换为TreeForm结构的方法，并使用修改后的标准化树编辑距离评估TreeForm的预测结果。我们基于FUNSD和XFUND数据集分别得到了端到端性能度量和TreeForm编辑距离的初步基线结果，分别为61.5和26.4。我们希望TreeForm能够促进对类似表单文档的复杂性进行更深入的标注、建模和评估的研究。

    Visually Rich Form Understanding (VRFU) poses a complex research problem due to the documents' highly structured nature and yet highly variable style and content. Current annotation schemes decompose form understanding and omit key hierarchical structure, making development and evaluation of end-to-end models difficult. In this paper, we propose a novel F1 metric to evaluate form parsers and describe a new content-agnostic, tree-based annotation scheme for VRFU: TreeForm. We provide methods to convert previous annotation schemes into TreeForm structures and evaluate TreeForm predictions using a modified version of the normalized tree-edit distance. We present initial baselines for our end-to-end performance metric and the TreeForm edit distance, averaged over the FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm encourages deeper research in annotating, modeling, and evaluating the complexities of form-like documents.
    
[^56]: VerAs: 验证然后评估STEM实验报告

    VerAs: Verify then Assess STEM Lab Reports

    [https://arxiv.org/abs/2402.05224](https://arxiv.org/abs/2402.05224)

    VerAs是一个端到端的神经架构，用于验证和评估STEM实验报告。它通过利用多个维度的分析评估标准，以及针对学生提供详细反馈，帮助他们提高科学写作技巧。

    

    随着STEM教育对批判性思维能力的日益关注，科学写作在注重探究技能的课程中发挥着越来越重要的作用。最近发布的一份数据集是基于一套探究型物理课程的两组大学水平的实验报告，依赖于利用多个维度的分析评估标准，指定学科知识和优秀解释的一般组成部分。每个分析维度都以6分制进行评估，以提供详细反馈，帮助学生提高科学写作技巧。手动评估可能较慢，并且在大班中对所有学生进行一致性校准可能很困难。尽管在STEM学科的开放性问题的自动评估上已经有很多工作，但在实验报告等长篇写作中的工作要少得多。我们提出了一个端到端的神经架构，其中包括独立的验证器和评估模块，灵感来源于开放领域问题回答的方法。

    With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
    
[^57]: 采样温度对大型语言模型在解题中的影响

    The Effect of Sampling Temperature on Problem Solving in Large Language Models

    [https://arxiv.org/abs/2402.05201](https://arxiv.org/abs/2402.05201)

    这项研究实证研究了采样温度对大型语言模型在解题中的影响，结果显示在0.0至1.0的温度范围内，LLM性能对解题任务没有显著影响。

    

    在这项研究中，我们通过实证研究调查了采样温度对大型语言模型（LLMs）在各种解题任务中的性能影响。我们通过从标准LLM基准中随机抽取问题，创建了一个多项选择问题（MCQA）考试。然后，我们使用了四种常见的LLM以及五种提示引擎技术来解决MCQA问题，同时将采样温度从0.0增加到1.0。尽管有关的报道与之相反，我们的实证结果表明，在0.0至1.0的温度范围内，LLM性能在解题任务中的变化没有统计学上显著的影响。此外，这些结果似乎不受LLM、提示引擎技术或问题领域的影响。所有代码、数据和补充资料都可以在GitHub上找到：https://github.com/matthewrenze/jhu-llm-temperature。

    In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
    
[^58]: LLMs是否准备好应用于真实世界的材料发现？

    Are LLMs Ready for Real-World Materials Discovery?

    [https://arxiv.org/abs/2402.05200](https://arxiv.org/abs/2402.05200)

    LLMs在材料科学中的应用受限，无法实现实际应用。我们提出了基于材料科学知识和假设测试的MatSci-LLMs框架，并描述了关键的材料科学信息提取挑战。

    

    大型语言模型（LLMs）为材料科学中的强大语言处理工具提供了令人兴奋的可能性，加快了材料研究的进展。然而，LLMs在实际应用中仍存在不足，无法成为实用的材料科学工具。本文通过展示LLMs在材料科学中的相关失败案例，揭示了LLMs在理解和推理复杂、相互关联的材料科学知识方面的现有限制。鉴于这些缺点，我们提出了一种开发基于材料科学知识和假设生成与测试的材料科学LLMs（MatSci-LLMs）的框架。实现高性能的MatSci-LLMs的路径在很大程度上取决于建立高质量的多模态数据集，这些数据集来自科学文献，其中存在各种信息提取挑战。因此，我们描述了关键的材料科学信息提取挑战。

    Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
    
[^59]: $\lambda$-ECLIPSE: 通过利用CLIP潜空间，基于多概念个性化文本到图像扩散模型

    $\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space

    [https://arxiv.org/abs/2402.05195](https://arxiv.org/abs/2402.05195)

    $\lambda$-ECLIPSE通过利用CLIP潜空间，实现了多概念个性化文本到图像的扩散模型。相比于传统方法，它减小了训练资源需求，并提供了更一致、高质量的图像生成结果。

    

    尽管个性化文本到图像(P-T2I)生成模型取得了近期的进展，但基于主题的T2I仍然具有挑战性。主要的瓶颈包括：1) 需要大量的训练资源，2) 超参数敏感性导致不一致的输出，以及3) 平衡新的视觉概念和构图对齐的复杂性。我们重新阐述了T2I扩散模型的核心理念，以解决上述限制。主要地，当代的基于主题的T2I方法依赖于潜空间扩散模型(LDMs)，通过交叉注意力层实现T2I映射。虽然LDMs提供了明显的优势，但P-T2I方法对这些扩散模型的潜空间的依赖显著增加了资源需求，导致结果不一致，并需要多次迭代才能得到一个所需的图像。最近，ECLIPSE展示了一种更具资源效率的训练UnCLIP-based T2I模型的路径，避免了需要扩散的需求。

    Despite the recent advances in personalized text-to-image (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffu
    
[^60]: InCoRo：带有反馈循环的上下文学习用于机器人控制

    InCoRo: In-Context Learning for Robotics Control with Feedback Loops

    [https://arxiv.org/abs/2402.05188](https://arxiv.org/abs/2402.05188)

    本文提出了InCoRo系统，使用经典的机器人反馈循环，通过LLM控制器、场景理解单元和机器人的协同工作，实现对动态环境中机器人控制的上下文学习。该系统能够持续分析环境状态并提供适应性执行命令，使机器人能够适应环境变化并纠正控制器错误。

    

    机器人技术的一个挑战是使机器人具备足够强大的推理能力，能够在动态环境中执行复杂任务。最近的LLM进展将它们定位为简单推理任务的首选工具，激发了Liang等人的开创性工作[35]，该工作使用LLM将自然语言命令转化为机器人单位的低级静态执行计划。在机器人系统中使用LLM将其泛化能力提升到一个新的水平，实现了对新任务的零样本泛化。本文将这项先前工作扩展到了动态环境。我们提出了InCoRo，一个使用经典的机器人反馈循环的系统，由LLM控制器、场景理解单元和机器人组成。我们的系统持续分析环境状态并提供适应性执行命令，使机器人能够适应不断变化的环境条件并纠正控制器错误。我们的系统不需要任何迭代设计。

    One of the challenges in robotics is to enable robotic units with the reasoning capability that would be robust enough to execute complex tasks in dynamic environments. Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al. [35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units. Using LLMs inside robotics systems brings their generalization to a new level, enabling zero-shot generalization to new tasks. This paper extends this prior work to dynamic environments. We propose InCoRo, a system that uses a classical robotic feedback loop composed of an LLM controller, a scene understanding unit, and a robot. Our system continuously analyzes the state of the environment and provides adapted execution commands, enabling the robot to adjust to changing environmental conditions and correcting for controller errors. Our system does not require any iterativ
    
[^61]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^62]: ApiQ：2位量化大型语言模型的微调

    ApiQ: Finetuning of 2-Bit Quantized Large Language Model

    [https://arxiv.org/abs/2402.05147](https://arxiv.org/abs/2402.05147)

    这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。

    

    随着大型语言模型的增大，内存高效的模型微调近年来备受关注，主要是由于GPU内存限制和这些方法与完全微调的可比结果所带来的约束。尽管有了进展，如QLoRA这样的内存高效微调策略在不同位宽的量化和多样化任务中表现不一致。这种不一致主要来自于量化过程对保留知识的有害影响，导致灾难性遗忘，削弱了预训练模型在微调中的利用。在这项工作中，我们引入了一种名为ApiQ的新型量化框架，旨在通过同时初始化LoRA组件和量化LLM的权重来恢复量化损失的信息。这种方法确保了原始LLM的激活精度的维持，同时减轻了误差的传播。

    Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
    
[^63]: Tag-LLM: 将通用的LLM应用于专业领域的再利用

    Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains

    [https://arxiv.org/abs/2402.05140](https://arxiv.org/abs/2402.05140)

    本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。

    

    大型语言模型（LLMs）在理解和生成自然语言方面表现出了非凡的能力。然而，在专门领域中，如物理学和生物医学科学这样的预训练语料库中未充分涵盖的领域，它们的能力下降。本文探讨了如何将通用LLMs重新用于专业领域的有效任务解决方案。我们介绍了一种新颖的、与模型无关的框架，用于学习自定义的输入标签，这些标签被参数化为连续向量并附加到LLMs的嵌入层，以对LLMs进行条件约束。我们设计了两种类型的输入标签：领域标签用于限定专业表示（例如化学式）并提供领域相关的上下文；功能标签用于表示特定的功能（例如预测分子性质）并压缩功能解决指令。我们使用辅助数据和领域知识开发了一个包括三个阶段的学习这些标签的协议。通过明确将任务领域与任务功能分离，我们的方法能够改善在专业领域中的任务求解能力。

    Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
    
[^64]: SceMQA：一种科学类大学入学级多模态问题回答基准

    SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark

    [https://arxiv.org/abs/2402.05138](https://arxiv.org/abs/2402.05138)

    SceMQA是一种科学类大学入学级多模态问题回答的基准，填补了现有基准中被忽视的教育阶段的空白。它包含核心科学科目，融合了多项选择和自由回答的格式，并提供详细的问题解析和答案解释。该基准还通过相同背景但问题不同的方式，促进了对推理能力更全面和准确的评估。

    

    本文介绍了SceMQA，这是一种面向大学入学级科学类多模态问题回答的新型基准。它填补了现有基准中常常被忽视的关键教育阶段，涵盖了高中到大学预科的水平。SceMQA专注于核心科学科目，包括数学、物理学、化学和生物学。它融合了多项选择和自由回答的格式，确保对AI模型的能力进行全面评估。此外，我们的基准为每个问题提供了具体的知识点和详细的答案解释。SceMQA还独特地提供了相同背景但问题不同的问题，以促进对推理能力进行更全面和准确的评估。在实验中，我们对开源和闭源的最新多模态大语言模型（MLLMs）进行了评估，同时考虑了不同的实验设置。结果表明，在开发科学类大学入学级多模态问题回答方面，需要进一步的研究和发展。

    The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developi
    
[^65]: LV-Eval:一个平衡的长上下文基准测试，具有5个长度级别，最多可达256K

    LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K

    [https://arxiv.org/abs/2402.05136](https://arxiv.org/abs/2402.05136)

    LV-Eval是一个具有五个长度级别的长上下文基准测试，支持256k上下文长度，并具有混淆事实、关键词和短语替换以及基于关键词回忆的度量设计等关键技术，旨在减少知识泄漏和提供更客观的评估。

    

    最先进的大型语言模型（LLMs）现在声称支持的上下文长度可以达到256k甚至更多。相比之下，主流基准测试的平均上下文长度不足（5k-21k），并且它们容易出现知识泄漏和不准确的评估指标，导致评估结果偏见。本文介绍了LV-Eval，一个具有五个长度级别（16k，32k，64k，128k和256k）的具有挑战性的长上下文基准测试，最多可达256k个单词。LV-Eval包含两个主要任务，单跳问答和多跳问答，包含11个双语数据集。LV-Eval的设计融合了三个关键技术，即混淆事实插入、关键词和短语替换以及基于关键词回忆的度量设计。LV-Eval的优点包括对不同上下文长度的可控评估、具有混淆事实的具有挑战性的测试实例、减少的知识泄漏以及更客观的评估。我们在LV-Eval上评估了10个LLMs，并进行了消融研究

    State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies o
    
[^66]: CADReN: 上下文锚点驱动的关系网络用于可控跨图节点重要性估计

    CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation

    [https://arxiv.org/abs/2402.05135](https://arxiv.org/abs/2402.05135)

    CADReN是一个上下文锚点驱动的关系网络，用于可控的跨图节点重要性估计。它通过引入上下文锚点机制，考虑知识图谱中的结构和语义特征，实现了更好的性能，包括零-shot预测能力，并开源了两个新的数据集RIC200和WK1K。

    

    节点重要性估计(NIE)对于通过检索增强生成将外部信息整合到大型语言模型中至关重要。传统方法侧重于静态的单一图特征，在新图和用户特定要求方面缺乏适应性。我们提出的CADReN通过引入上下文锚点(CA)机制来解决这些限制。该方法使网络能够相对于CA评估节点的重要性，考虑知识图谱(KGs)中的结构和语义特征。广泛的实验表明，CADReN在跨图NIE任务中取得了更好的性能，并具有零-shot预测能力。CADReN还被证明在单一图NIE任务上与以前的模型性能相匹配。此外，我们还引入并开源了两个新数据集RIC200和WK1K，专门用于跨图NIE研究，为这个领域的未来发展提供了宝贵的资源。

    Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.
    
[^67]: 个性化语言模型基于个性化人类反馈

    Personalized Language Modeling from Personalized Human Feedback

    [https://arxiv.org/abs/2402.05133](https://arxiv.org/abs/2402.05133)

    该论文提出了一个个性化语言模型的方法，通过在于用户的反馈数据中引入个性化特征来解决强化学习框架在多样化用户偏好下存在的问题。

    

    从个性化人类反馈中进行强化学习（RLHF）是目前主流的框架，用于调整大型语言模型以更好地符合人类偏好。然而，在这个框架下开发的算法的基本前提在用户偏好多样化的情况下可能会出现问题。在本文中，我们旨在通过开发个性化语言模型的方法来解决这个问题。我们首先正式介绍了从个性化人类反馈中学习的任务，并解释了为什么在这种情况下普通的RLHF可能会存在问题。然后，我们提出了一个通用的个性化-RLHF（P-RLHF）框架，需要同时学习用户模型和语言（或奖励）模型。用户模型接收用户信息并输出用户表示。其结构编码了我们对反馈数据中用户偏好的假设。我们为个性化奖励建模和个性化直接偏好优化开发了新的学习目标。

    Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
    
[^68]: TexShape:信息论句子嵌入用于语言模型

    TexShape: Information Theoretic Sentence Embedding for Language Models

    [https://arxiv.org/abs/2402.05132](https://arxiv.org/abs/2402.05132)

    这项研究提出了一种名为TexShape的信息论句子嵌入模型，通过使用互信息的经验估计来优化文本表示，可用于数据压缩和敏感信息过滤，提升隐私和公平性。

    

    随着数据量的指数增长和数据密集应用的出现，尤其是在机器学习领域，与资源利用、隐私和公平性相关的问题变得越来越重要。本文关注数据的文本领域，并通过信息论的视角解决了将句子编码为其优化表示的挑战。具体而言，我们使用Donsker-Varadhan定义的Kullback-Leibler散度的经验估计值来计算互信息。我们的方法利用这种估计来训练一种信息论句子嵌入模型，称为TexShape，用于（基于任务的）数据压缩或过滤敏感信息，从而增强隐私和公平性。在本研究中，我们使用基准语言模型进行初步文本表示，并用神经网络进行信息理论压缩和互信息估计。我们的实验表明，我们的方法显著进展。

    With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advanceme
    
[^69]: 有效检索增强生成的财务报告切块

    Financial Report Chunking for Effective Retrieval Augmented Generation

    [https://arxiv.org/abs/2402.05131](https://arxiv.org/abs/2402.05131)

    本文提出了一种扩展的方法来切块财务报告，通过根据文档的结构元素组件进行切块，从而实现更有效的检索增强生成。这种方法可以优化切块大小，而无需调整，并提供了对整体上下文和准确性的评估以及对问答任务性能的影响。

    

    切块信息是检索增强生成(RAG)的关键步骤。目前的研究主要集中在段落级切块上。这种方法将所有文本都视为平等的，并忽略了文档结构中包含的信息。我们提出了一种扩展的方法，通过不仅仅将文档切块到段落级别，而是根据文档的结构元素组件来切块。将文档分解为这些组成元素可以创建一种新的文档切块方式，可以得到最佳的切块大小，无需调整。我们引入了一种新颖的框架，评估根据由文档理解模型注释的元素类型进行切块如何对所检索信息的整体上下文和准确性贡献。我们还演示了这种方法对RAG辅助问答任务性能的影响。我们的研究包括对各种元素类型的全面分析，它们在有效信息检索中的作用以及它们对其产生的影响。

    Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the 
    
[^70]: LB-KBQA: 基于大语言模型和BERT的基于知识的问答系统

    LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System

    [https://arxiv.org/abs/2402.05130](https://arxiv.org/abs/2402.05130)

    LB-KBQA是一种基于大语言模型和BERT的基于知识的问答系统，通过生成式人工智能的帮助，能够提高意图识别的性能和解决语言多样性的问题。

    

    生成式人工智能（AI）因其新兴的能力而赋予了各个领域的力量，其中一个典型的应用领域是大语言模型（LLMs）。与传统的基于AI的方法相比，生成式AI的典型应用领域之一是大语言模型（LLMs），并且在自然语言理解能力方面，LLM的能力得到了显著提高。自然语言理解能力一直以来都是基于知识的问答系统意图识别性能的障碍，这源自语言多样性和新出现的意图。传统的基于AI的意图识别方法可以分为基于语义解析的方法和基于模型的方法。然而，这两种方法都在意图识别方面受到有限的资源限制。为了解决这个问题，我们提出了一种基于大语言模型（LLM）和BERT的新型KBQA系统（LB-KBQA）。在生成式AI的帮助下，我们提出的方法可以检测到……（略）

    Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
    
[^71]: 大型语言模型文本标注的最佳实践

    Best Practices for Text Annotation with Large Language Models

    [https://arxiv.org/abs/2402.05129](https://arxiv.org/abs/2402.05129)

    大型语言模型的广泛使用在文本标注领域带来了许多挑战，这篇论文提出了一套可靠、可重复、符合伦理要求的使用标准和最佳实践，以解决相关问题。

    

    大型语言模型（LLMs）开启了文本标注的新时代，由于其易用性、高准确性和相对较低的成本，导致最近几个月使用这种模型的增长迅猛。然而，这个领域的快速发展意味着基于LLM的标注成为了一种学术界的“野西”，缺乏建立的实践和标准导致了对研究的质量和有效性的担忧。研究人员警告说，LLM的显而易见的简单性可能会导致误导，因为它们容易受到偏见、误解和不可靠的结果的影响。本文旨在认识到LLM的变革潜力，提出了一套可靠、可重复和符合伦理要求的使用标准和最佳实践。这些指南涵盖了关键领域，如模型选择、提示工程、结构化提示、提示稳定性分析、严格的模型验证以及伦理和法律意识的考虑。本文强调了对全面的验证和透明度的需求。

    Large Language Models (LLMs) have ushered in a new era of text annotation, as their ease-of-use, high accuracy, and relatively low costs have meant that their use has exploded in recent months. However, the rapid growth of the field has meant that LLM-based annotation has become something of an academic Wild West: the lack of established practices and standards has led to concerns about the quality and validity of research. Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results. Recognizing the transformative potential of LLMs, this paper proposes a comprehensive set of standards and best practices for their reliable, reproducible, and ethical use. These guidelines span critical areas such as model selection, prompt engineering, structured prompting, prompt stability analysis, rigorous model validation, and the consideration of ethical and legal implications. The paper emphasizes the need fo
    
[^72]: 用大型语言模型和检索增强生成提升教科书问答任务

    Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation

    [https://arxiv.org/abs/2402.05128](https://arxiv.org/abs/2402.05128)

    本论文通过引入检索增强生成（RAG）技术和利用迁移学习来处理长文本和提升推理能力，为教科书问答任务带来了显著的改进。

    

    教科书问答（TQA）是人工智能中的一项具有挑战性的任务，由于上下文和多模式数据的复杂性。尽管以前的研究在任务上取得了显著的进展，但仍存在一些限制，包括模型推理能力不足和无法捕捉长文本中的上下文信息。大型语言模型（LLMs）的引入革命了人工智能领域，然而，直接应用LLMs经常会导致不准确的答案。本文提出了一种方法来处理TQA中领域外情景，即概念分布在不同课程中，该方法结合了检索增强生成（RAG）技术和迁移学习来处理长文本并提升推理能力。通过对LLM模型Llama-2进行监督微调并加入RAG，我们的架构优于基线，在验证集上的准确度提高了4.12%，在测试集上提高了9.84%。

    Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
    
[^73]: Illuminate：一种使用可解释分析和积极治疗的新方法进行抑郁症检测

    Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering

    [https://arxiv.org/abs/2402.05127](https://arxiv.org/abs/2402.05127)

    本研究提出了一种新的抑郁症检测和治疗范式，使用先进的大型语言模型，经过特定提示微调以诊断、解释和建议治疗干预。同时还介绍了一个丰富的数据库，以提供个性化的治疗建议。此方法与患者进行共情对话管理，有效支持抑郁症患者。

    

    本文介绍了一种使用先进的大型语言模型（LLMs）：Generative Pre-trained Transformer 4（GPT-4）、Llama 2 chat和Gemini的抑郁症检测和治疗新范式。这些LLMs通过特定的提示进行微调，以诊断、解释和建议抑郁症的治疗干预。一种独特的少样本提示方法增强了模型根据DSM-5标准分析和解释抑郁症状的能力。在交互阶段，模型采用共情对话管理，利用PsychDB和认知行为疗法（CBT）指南等资源，与患有重度抑郁症的个体进行支持性互动。此外，研究还介绍了Illuminate数据库，其中包含各种CBT模块，可帮助个性化的治疗建议。该研究使用F1分数、精确度、召回率、余弦相似度和召回率导向的差错进行LLM性能评估。

    This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for
    
[^74]: 图神经网络和基于NER的文本摘要

    Graph Neural Network and NER-Based Text Summarization

    [https://arxiv.org/abs/2402.05126](https://arxiv.org/abs/2402.05126)

    这个项目使用图神经网络和命名实体识别系统，提出了一种创新的文本摘要方法。图神经网络能够捕获并处理文本信息中的关系数据，而命名实体识别系统通过识别和强调关键实体来保持摘要的重点。

    

    在当今时代信息和数据的丰富性下，人们或者机器几乎不可能逐行查看所有的数据。通常的做法是试图从行中快速浏览并保留绝对重要的信息，这在更正式的术语中称为摘要。文本摘要是一项重要的任务，旨在将冗长的文档或文章压缩成较短、连贯的表达方式，同时保留核心信息和意义。本项目引入了一种创新的文本摘要方法，利用了图神经网络（GNNs）和命名实体识别（NER）系统的能力。GNNs以其优秀的捕获和处理文本信息中的关系数据的能力，能够理解大型文档中的复杂结构。同时，NER系统通过识别和强调关键实体，确保摘要过程保持专注于重点内容。

    With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization. Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the 
    
[^75]: 零样本临床试验患者匹配与LLMs

    Zero-Shot Clinical Trial Patient Matching with LLMs

    [https://arxiv.org/abs/2402.05125](https://arxiv.org/abs/2402.05125)

    本研究基于LLMs开发了一个零样本临床试验患者匹配系统，可以高效评估患者是否符合入选标准，并通过优化提示策略和检索流程提高了数据和成本效率。

    

    将患者与临床试验匹配是推出新药的关键难题。目前，识别符合试验入选标准的患者是高度手动的，每位患者需花费长达1小时。然而，自动筛选具有挑战性，因为它需要理解非结构化的临床文本。大型语言模型（LLMs）提供了一个有望的解决方案。在这项工作中，我们探索了它们在试验匹配中的应用。首先，我们设计了一个基于LLM的系统，可以在给定一个患者的病史作为非结构化的临床文本时，评估该患者是否符合一组包含标准（也以自由文本形式指定）。我们的零样本系统在n2c2 2018队列选择基准测试中取得了最先进的得分。其次，我们通过识别一种提示策略，改善了我们方法的数据和成本效率，该策略与现状相比可以将患者匹配时间和成本降低一个数量级，并且开发了一个两阶段的检索流程，减少了匹配消除的次数。

    Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
    
[^76]: LLM指导调优的数据选择综述

    A Survey on Data Selection for LLM Instruction Tuning

    [https://arxiv.org/abs/2402.05123](https://arxiv.org/abs/2402.05123)

    本综述对LLM指导调优的数据选择进行了全面调查。研究发现，数据集的质量在指导调优过程中比数量更为重要，因此许多研究致力于探索从指导数据集中选择高质量子集的方法。课题呈现了一种新的分类体系、介绍了最近的研究进展并详细评估了这些方法。

    

    指导调优是训练大型语言模型（LLM）的关键步骤，如何提高指导调优的效果已经引起了增加的关注。现有研究表明，在LLM的指导调优过程中，数据集的质量比数量更为重要。因此，最近许多研究致力于探索从指导数据集中选择高质量子集的方法，旨在降低训练成本并改善LLM的指导能力。本文对LLM指导调优的数据选择进行了综述。首先，介绍了广泛使用的指导数据集。然后，提出了一种新的数据选择方法分类体系，并详细介绍了最近的研究进展，还详细阐述了数据选择方法的评估策略和结果。最后，强调了该任务的开放挑战和新的前景。

    Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.
    
[^77]: 生成式人工智能聊天机器人的历史：过去、现在和未来发展

    History of generative Artificial Intelligence (AI) chatbots: past, present, and future development

    [https://arxiv.org/abs/2402.05122](https://arxiv.org/abs/2402.05122)

    本研究回顾了生成式人工智能聊天机器人的发展历程，从最初的基本规则系统到如今由人工智能驱动的高级对话机器人。研究突出了关键创新，如聊天机器人ELIZA和ALICE，并探讨了影响演进的重要里程碑，以及Transformer模型的最新进展。

    

    本研究全面深入地回顾了聊天机器人技术的发展，从最初依赖规则的基本系统到如今由人工智能驱动的高级对话机器人。该论文跨越了几十年的时间，探讨了推动聊天机器人演进的重要里程碑、创新和范式转变。回顾起源于1906年的基本统计模型，到上世纪60年代和70年代的早期聊天机器人，例如ELIZA和ALICE，研究追踪了导致如今高级对话机器人（如ChatGPT和Google Bard）的关键创新。该研究综合了学术文献和行业资料的见解，突出了图灵测试的引入、CALO等具有影响力的项目以及最近的基于Transformer的模型。展望未来，本文强调了自然语言处理和机器学习如何被集成到现代聊天机器人中的更高级的能力。

    This research provides an in-depth comprehensive review of the progress of chatbot technology over time, from the initial basic systems relying on rules to today's advanced conversational bots powered by artificial intelligence. Spanning many decades, the paper explores the major milestones, innovations, and paradigm shifts that have driven the evolution of chatbots. Looking back at the very basic statistical model in 1906 via the early chatbots, such as ELIZA and ALICE in the 1960s and 1970s, the study traces key innovations leading to today's advanced conversational agents, such as ChatGPT and Google Bard. The study synthesizes insights from academic literature and industry sources to highlight crucial milestones, including the introduction of Turing tests, influential projects such as CALO, and recent transformer-based models. Tracing the path forward, the paper highlights how natural language processing and machine learning have been integrated into modern chatbots for more sophist
    
[^78]: 大型语言模型在表格处理中的应用：一项调查

    Large Language Model for Table Processing: A Survey

    [https://arxiv.org/abs/2402.05121](https://arxiv.org/abs/2402.05121)

    该调查综述了大型语言模型在表格处理中的应用，包括传统的表格问题回答和事实验证，以及新兴的表格操作和高级表格数据分析。还讨论了LLMs的最新范例，特别关注了指导调整、提示和基于代理的方法。

    

    表格通常是二维结构化的，用于存储大量数据，在数据库查询、电子表格计算和从网络表格生成报告等日常活动中起着重要作用。利用大型语言模型（LLMs）自动化这些以表格为中心的任务可以带来重大的公众利益，引起了学术界和工业界的兴趣。该调查对表格任务进行了广泛的概述，不仅涵盖传统领域如表格问题回答（Table QA）和事实验证，还包括最近强调的方面，如表格操作和高级表格数据分析。此外，它还超越了早期的预训练和微调小型语言模型的策略，包括LLM使用中的最新范例。重点是LLMs领域内的指导调整、提示和基于代理的方法。最后，我们重点介绍了几个挑战，涵盖私有部署、高效推断和 LLMS 发展等方面。

    Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the developmen
    
[^79]: 更多的代理就是你所需要的

    More Agents Is All You Need

    [https://arxiv.org/abs/2402.05120](https://arxiv.org/abs/2402.05120)

    大型语言模型的性能与代理数量成比例，通过简单的采样和投票方法可以进一步增强性能，这种方法与现有的复杂方法正交。

    

    我们发现，仅通过一种采样和投票的方法，大型语言模型(Large Language Models, LLMs)的性能与实例化的代理数量成比例。此外，这种方法对已有的复杂方法进一步增强LLMs是正交的，而增强的程度与任务的困难程度相关。我们进行了广泛的实验，验证了我们的发现，并研究了能够促进其发生的属性。我们的代码公开在以下网址: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}

    We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
    
[^80]: 研究指令调整的局限性

    A Closer Look at the Limitations of Instruction Tuning

    [https://arxiv.org/abs/2402.05119](https://arxiv.org/abs/2402.05119)

    本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。

    

    指令调整（IT）是使用指令-回应对来训练大型语言模型（LLM）的过程，已成为将基础预训练LLM转化为开放领域对话代理的主要方法。虽然IT取得了显著的成功并广泛应用，但其局限性和不足仍未得到充分探讨。本文通过严格的实验和对LLM通过IT发生的变化的深入分析，揭示了IT的多种局限性。特别是，我们发现：（1）IT无法增强LLM的知识或技能。LoRA微调仅限于学习回应的启动和样式令牌，而全参数微调会导致知识退化。（2）从具有知识来源的IT数据集复制回应模式会导致回应质量下降。（3）全参数微调通过不准确地从IT数据集中获取概念上相似实例的标记，增加了错误生成的情况。

    Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
    
[^81]: 量化相似性：使用文本挖掘方法评估ChatGPT和Google Bard生成内容与生物医学文献的关联性

    Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature

    [https://arxiv.org/abs/2402.05116](https://arxiv.org/abs/2402.05116)

    本研究旨在通过文本挖掘方法评估ChatGPT和Google Bard生成的内容与生物医学文献之间的相似性。实验结果显示，在余弦文档相似性方面，ChatGPT表现优于Google Bard。

    

    背景：在大语言模型（LLMs）的支持下，生成式人工智能工具的出现展示了强大的生成内容能力。到目前为止，评估通过所谓的提示工程生成的内容的有用性已经成为一个有趣的研究问题。目标：通过提示工程的平均值，我们评估这些内容与科学家产生的真实文献的相似性和接近程度。方法：在这个探索性分析中，（1）我们通过提示工程来生成ChatGPT和Google Bard的临床内容，以便与文献对应内容进行比较，（2）我们通过比较所生成内容与生物医学文献对应内容的相似性来评估它们之间的相似性。我们的方法是使用文本挖掘方法比较文档和相关的二元组，并使用网络分析来评估术语的中心性。结果：实验表明，ChatGPT在余弦文档相似性方面表现优于Google Bard（38%对34%），

    Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), 
    
[^82]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^83]: 信实性与可信度: 关于大型语言模型解释的(不)可靠性

    Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models

    [https://arxiv.org/abs/2402.04614](https://arxiv.org/abs/2402.04614)

    本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。

    

    大型语言模型(LLMs)被部署为几种自然语言处理（NLP）应用的强大工具。最近的研究显示，现代LLMs可以生成自我解释（SEs），这些SEs揭示了它们解释其行为的中间推理步骤。由于其对话性和可信度的特点，自我解释已广泛应用。然而，我们对其信实性了解甚少。在本研究中，我们讨论了LLMs生成的SEs中信实性和可信度之间的二分法。我们认为，虽然LLMs擅长生成可信的解释-对人类用户来说似乎逻辑和连贯-但这些解释未必与LLMs的推理过程相一致，引发对其信实性的担忧。我们强调，当前趋势是为了用户友好界面的需求而增加解释的可信度，可能会以降低解释的信实性为代价。

    Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness
    
[^84]: 大型语言模型能否检测社交媒体上的谣言？

    Can Large Language Models Detect Rumors on Social Media?

    [https://arxiv.org/abs/2402.03916](https://arxiv.org/abs/2402.03916)

    本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。

    

    在这项工作中，我们研究了使用大型语言模型（LLMs）在社交媒体上进行谣言检测。然而，LLMs在推理整个传播信息时面临挑战，因为该信息包含新闻内容和大量评论，LLMs可能无法集中关注复杂传播信息中的关键线索，并且在面对大量和冗余信息时难以进行推理。因此，我们提出了一种基于LLMs增强的谣言检测（LeRuD）方法，在其中设计提示来教导LLMs关注新闻和评论中的重要线索，并将整个传播信息分解为传播链以减轻LLMs的负担。我们在Twitter和微博数据集上进行了大量实验证明，LeRuD的性能优于几种最先进的谣言检测模型，提升了2.4％至7.6％。同时，通过应用LLMs，LeRuD无需进行训练数据，并且在少样本或零样本情况下展现出更具有潜力的谣言检测能力。

    In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
    
[^85]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^86]: GIRT-模型：自动生成问题报告模板

    GIRT-Model: Automated Generation of Issue Report Templates

    [https://arxiv.org/abs/2402.02632](https://arxiv.org/abs/2402.02632)

    本研究介绍了GIRT-模型，这是一个基于开发者指示自动生成问题报告模板的助理语言模型。通过在GitHub仓库中构建的数据集进行指导调整，GIRT-模型在IRT生成方面表现显著优于其他通用语言模型。

    

    GitHub和GitLab等平台引入问题报告模板（IRT）以提高问题管理效率并更好地与开发者期望对齐。然而，大多数仓库并未广泛采用这些模板，并且目前没有可用的工具来辅助开发者生成模板。在这项工作中，我们介绍了GIRT-模型，这是一个基于开发者关于结构和必需字段的指示自动生成IRT的助理语言模型。我们创建了GIRT-Instruct，这是一个包含指示和IRT对的数据集，其中IRT来自GitHub存储库。我们使用GIRT-Instruct来指导调整T5-base模型以创建GIRT-模型。在我们的实验中，GIRT-模型在IRT生成方面表现优于通用语言模型（带有不同参数大小的T5和Flan-T5），在ROUGE、BLEU、METEOR和人工评估上得分显著更高。此外，我们分析了GIRT-模型在用户体验中的有效性。

    Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model. In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user st
    
[^87]: APT-Pipe: 用于社交计算数据标注的自动提示调整工具

    APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation

    [https://arxiv.org/abs/2402.01697](https://arxiv.org/abs/2402.01697)

    APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.

    

    最近的研究突出了像ChatGPT这样的LLM应用在社交计算文本标注中的潜力。然而，已经人们已经知道性能取决于输入提示的质量。为了解决这个问题，已经有了大量的研究来探索提示调整的技术和指南，试图改善提示的质量。然而，这些方法往往依赖于手工努力和对正在标注的数据集的先前知识。为了解决这个限制，我们提出了一个自动化的提示调整流水线APT-Pipe。APT-Pipe旨在自动调整提示，以提高ChatGPT在任何给定数据集上的文本分类性能。我们实现了APT-Pipe，并在12个不同的文本分类数据集上进行了测试。我们发现APT-Pipe调整的提示有助于ChatGPT在12个实验数据集中有9个获得更高的加权F1分数，平均改进了7.01％。我们进一步突出了APT-Pipe作为一个框架的灵活性。

    Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
    
[^88]: ReAGent: 一个面向生成语言模型的模型无关特征归因方法

    ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

    [https://arxiv.org/abs/2402.00794](https://arxiv.org/abs/2402.00794)

    本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。

    

    特征归因方法（FAs），如梯度和注意力机制，被广泛应用于确定所有输入特征对模型预测的重要性。现有自然语言处理领域的研究主要集中在为仅有编码器的语言模型（LMs）开发和测试FAs，用于分类任务。然而，尚不清楚在文本生成上是否可以使用这些FAs来处理仅有解码器的模型，因为模型架构和任务设置之间存在固有差异。此外，先前的研究已经证明，没有一个通用的FA适用于所有模型和任务。这使得针对大型LMs选择FA计算上非常昂贵，因为输入重要性的推导通常需要多个前向和反向传递，包括可能是限制性的梯度计算。为了解决这些问题，我们提出了一种面向生成LMs的模型无关FA，称为递归归因生成器（ReAGent）。

    Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
    
[^89]: 健康-LLM：个性化检索增强的疾病预测模型

    Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model

    [https://arxiv.org/abs/2402.00746](https://arxiv.org/abs/2402.00746)

    提出了一个创新的框架，健康-LLM，通过大规模特征提取和医学知识权衡评分，实现了个性化的检索增强疾病预测模型。这种方法通过整合健康报告，调整特征权重，以及利用语言模型和专家见解提高预测准确性，与传统健康管理方法相比具有明显优势。

    

    在卫生保健领域，人工智能（AI）极大地推进了智能医疗技术的发展。然而，传统智能医疗受限于静态数据和统一标准，无法完全与个体情况集成，同时也面临其他挑战。为此，我们提出了一种创新的框架，命名为健康-LLM，将大规模特征提取和医学知识权衡评分相结合。与传统健康管理方法相比，我们的方法具有三个主要优势。首先，我们的方法将健康报告整合到大模型中，提供详细的任务信息。其次，我们使用专业的医学专业知识调整健康特征的权重得分。第三，我们使用半自动特征提取框架增强语言模型的分析能力，并整合专家见解以提高疾病预测的准确性。

    Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
    
[^90]: 使用社会选择理论从大型语言模型中提取稳健知识

    Robust Knowledge Extraction from Large Language Models using Social Choice Theory

    [https://arxiv.org/abs/2312.14877](https://arxiv.org/abs/2312.14877)

    本研究提出使用排名查询和社会选择理论的方法来提高大型语言模型（LLMs）查询的鲁棒性，特别是在高风险领域如医学中。我们通过实证评估验证了我们方法的鲁棒性和其他有趣属性。

    

    大型语言模型(LLMs)可以支持很多应用，如对话代理、创意写作或一般查询回答。然而，在高风险领域（如医学）中，它们不适用于查询回答，因为当多次提示相同查询时，它们通常不具有鲁棒性 - 结果可能不同。为了提高LLM查询的鲁棒性，我们提出使用排名查询，并使用社会选择理论的方法来汇总查询结果。我们研究了诊断场景中的排名查询，如医学和故障诊断，并讨论了如何应用文献中的Partial Borda Choice函数来合并多个查询结果。我们还讨论了我们设置中的一些其他有趣属性，并通过实证评估了我们方法的鲁棒性。

    Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.
    
[^91]: 社会学习：朝着与大型语言模型的协作学习

    Social Learning: Towards Collaborative Learning with Large Language Models

    [https://arxiv.org/abs/2312.11441](https://arxiv.org/abs/2312.11441)

    本论文在大型语言模型的背景下引入了“社会学习”的框架，通过自然语言相互共享知识，提出了两种知识传递方法，并证明了这些方法的可行性和效果。

    

    我们在大型语言模型（LLMs）的背景下引入了“社会学习”的框架，在隐私保护的前提下，模型通过使用自然语言相互共享知识。我们提出并评估了两种在LLMs之间进行知识传递的方法。在第一种情况下，我们允许模型生成抽象提示以便教授任务。在第二种方法中，模型通过生成合成示例来传递知识。我们跨多个数据集评估了这些方法，并以记忆化作为隐私损失的代理进行量化。这些受到社会学习启发的技术取得了有希望的结果，原始数据的记忆化程度较低。特别是，我们展示了使用这些方法的性能与使用原始标签和提示的结果相媲美。我们的工作证明了社会学习在LLMs中的可行性，建立了基线方法，并突出了几个未开发的领域供未来研究使用。

    We introduce the framework of "social learning" in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.
    
[^92]: 语音助手对话修复的分析

    An Analysis of Dialogue Repair in Voice Assistants

    [https://arxiv.org/abs/2311.03952](https://arxiv.org/abs/2311.03952)

    本研究通过分析与谷歌助手和Siri的交互，探讨了虚拟助手与用户之间的对话修复中交互语言的重要性。研究发现助手生成了几种修复策略，但无法复制人类类似的修复策略，如“嗯？”。英语和西班牙语用户可接受性调查显示了用户对修复策略的偏好和助手使用的差异。这些结果揭示了人机交互中交互语言与人际交互之间的不平等，强调了对交互语言影响的进一步研究的必要性。

    

    口语对话系统通过实时响应查询改变了人机交互，但是用户与系统之间的误解仍然存在。本研究通过分析与谷歌助手和Siri的交互，探讨虚拟助手与用户之间的对话修复中交互语言的重要性，重点关注了在人际交互中常见的对话修复策略“嗯？”的使用和回应。研究结果揭示了助手生成的几种修复策略，但无法复制人类类似的修复策略，如“嗯？”。英语和西班牙语用户可接受性调查显示了用户对修复策略的偏好和助手使用的差异，在这两种调查语言中都存在相似性和差异性。这些结果揭示了人机交互中交互语言与人际交互之间的不平等，强调了对交互语言影响的进一步研究的必要性。

    Spoken dialogue systems have transformed human-machine interaction by providing real-time responses to queries. However, misunderstandings between the user and system persist. This study explores the significance of interactional language in dialogue repair between virtual assistants and users by analyzing interactions with Google Assistant and Siri, focusing on their utilization and response to the other-initiated repair strategy "huh?" prevalent in human-human interaction. Findings reveal several assistant-generated strategies but an inability to replicate human-like repair strategies such as "huh?". English and Spanish user acceptability surveys show differences in users' repair strategy preferences and assistant usage, with both similarities and disparities among the two surveyed languages. These results shed light on inequalities between interactional language in human-human interaction and human-machine interaction, underscoring the need for further research on the impact of inte
    
[^93]: 注意力还是卷积：用于推断效率的转换器编码器在音频语言模型中的应用

    Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency

    [https://arxiv.org/abs/2311.02772](https://arxiv.org/abs/2311.02772)

    本文展示了将语音转换器作为音频模型的编码器，可以显著提高预训练模型的效率。此外，我们发现只使用自注意力也能实现类似的效果，尤其与低位权重量化技术结合使用时效果更好。这一发现有助于防止错误在量化模块之间传播。

    

    在本文中，我们展示了一个简单的自监督预训练音频模型可以达到与更复杂的预训练模型（具有语音转换器编码器）相当的推断效率。这些语音转换器结合了卷积模块和自注意力模块，实现了在ASR方面的最先进性能和最高效率。我们首先展示了将这些语音转换器作为编码器显著提高了预训练音频模型的效率。然而，我们的研究表明，仅使用先进的自注意力也可以实现可比较的效率。我们证明了这种更简单的方法特别有利于低位权重量化技术来提高效率。我们假设这可以防止在最近的语音转换器中混合量化卷积和量化自注意力模块时在不同量化模块之间传播错误。

    In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.
    
[^94]: TATA: 通过主题无关和主题感知的嵌入进行立场检测

    TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings

    [https://arxiv.org/abs/2310.14450](https://arxiv.org/abs/2310.14450)

    本研究提出了一种通过对比学习以及利用未标记新闻文章数据集来训练主题无关和主题感知嵌入的方法，在立场检测中取得了最先进的性能，并在公开数据集上达到了0.771的F1分数。

    

    立场检测对于理解互联网上不同的态度和信仰很重要。然而，鉴于一篇文章对给定主题的立场往往高度依赖于该主题，建立一个能推广到未知主题的立场检测模型是困难的。在这项工作中，我们提出使用对比学习以及涵盖了各种不同主题的未标记新闻文章数据集来训练用于下游立场检测的主题无关（TAG）和主题感知（TAW）的嵌入。将这些嵌入组合在我们的完整TATA模型中，我们在几个公开的立场检测数据集上实现了最先进的性能（在Zero-shot VAST数据集上达到0.771的F1分数）。我们在https://github.com/hanshanley/tata发布了我们的代码和数据。

    Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 $F_1$-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata.
    
[^95]: 可训练的Transformer in Transformer

    Trainable Transformer in Transformer

    [https://arxiv.org/abs/2307.01189](https://arxiv.org/abs/2307.01189)

    这篇论文介绍了一种名为Transformer in Transformer (TinT)的高效构造方式，它可以让Transformer在推理过程中模拟和微调复杂的内部模型，同时使用创新的近似技术大幅减少了模型参数和内存开销。

    

    最近的研究将大型预训练语言模型中的上下文学习能力归因于在推理过程中隐式模拟和微调内部模型（如线性或2层MLP）。然而，这种构造需要大量的内存开销，使得模拟更复杂的内部模型变得困难。在这项工作中，我们提出了一种高效的构造方式，称为Transformer in Transformer（简称TinT），它允许一个Transformer在推理过程中模拟和微调复杂的内部模型（如预训练语言模型）。特别是，我们引入了创新的近似技术，使得一个拥有不到20亿参数的TinT模型能够在单次前向传递中模拟和微调一个拥有1.25亿参数的Transformer模型。TinT适用于许多常见的Transformer变体，其设计思路还改进了Transformer中简单模型的效率。我们进行了端到端实验来验证...

    Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validat
    
[^96]: 探究LLMs在人工智能协作任务中的代理力

    Investigating Agency of LLMs in Human-AI Collaboration Tasks

    [https://arxiv.org/abs/2305.12815](https://arxiv.org/abs/2305.12815)

    本论文探讨了LLMs在人工智能协作任务中的代理力，开发了一种用于衡量和管理LLMs代理力的特征框架，并通过新的数据集验证了该方法的有效性。

    

    代理力是人类互动和协作的核心能力。虽然LLMs被开发成模拟人类行为并作为类人代理使用，但对这些模型应具有的代理力的关注却不足，以便主动管理互动和协作的方向。在本文中，我们研究了代理力作为LLMs的一种理想功能的性质，以及如何衡量和管理代理力。我们借鉴社会认知理论，开发了一种特征框架，通过对话表达代理力，表示您打算做什么（意向性），激发您的意图（动机），对意向有自信（自我效能），并能够自我调整（自我调节）。我们收集了一个新的数据集，包含908个对人对话片段的注释，用于标记代理力特征。利用这个数据集，我们开发了衡量LLMs代理力的方法。

    Agency, the capacity to proactively shape events, is central to how humans interact and collaborate. While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration. In this paper, we investigate Agency as a desirable function of LLMs, and how it can be measured and managed. We build on social-cognitive theory to develop a framework of features through which Agency is expressed in dialogue - indicating what you intend to do (Intentionality), motivating your intentions (Motivation), having self-belief in intentions (Self-Efficacy), and being able to self-adjust (Self-Regulation). We collect a new dataset of 83 human-human collaborative interior design conversations containing 908 conversational snippets annotated for Agency features. Using this dataset, we develop methods for measuring Agency of LLMs. Autom
    
[^97]: PiVe：通过迭代验证提升LLMs的基于图的生成能力的提示方法

    PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs

    [https://arxiv.org/abs/2305.12392](https://arxiv.org/abs/2305.12392)

    提出了一个名为"PiVe"的框架，通过迭代验证来提升LLMs的基于图的生成能力。实验结果表明，PiVe方法在三个基于图的数据集上取得了一致的改善，并且验证模块可以作为数据增强工具帮助提高结果质量。

    

    大型语言模型(LLMs)在解决各种不同领域的自然语言任务方面展示了强大的能力。由于LLMs的训练目标和预训练数据，LLMs对涉及结构化数据生成的任务并不非常适用。我们提出了一个名为"PiVe"的框架，通过迭代验证来提升LLMs的基于图的生成能力。我们展示了如何训练一个小型语言模型作为LLMs的输出的验证模块(例如ChatGPT，GPT-4)，通过精细的纠正指令来迭代改进其性能。我们还展示了验证模块如何在离线环境中应用迭代校正，以获得更经济高效的文本到图形生成任务解决方案。在三个基于图的数据集上的实验结果表明，通过PiVe的方法能够持续改善结果。此外，我们创建了GenWiki-HIQ数据集，并强调验证模块可以作为数据增强工具，帮助提高自动生成的结果质量。

    Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatical
    
[^98]: 面向西非科学教育的AI教辅Kwame的现实世界部署和评估

    Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa

    [https://arxiv.org/abs/2302.10786](https://arxiv.org/abs/2302.10786)

    这项研究扩展了AI教辅Kwame，面向西非科学教育，并将其作为一个网络应用程序进行了实际部署和评估。结果显示，在实际环境中，Kwame for Science取得了很高的准确率，并且获得了广泛的用户和问题提问量。

    

    非洲教师与学生的比例高，这限制了学生们获取教育问题解答等学习支持的机会。本研究将面向编码教育的AI教辅Kwame扩展为面向科学教育，并将其部署为一个网络应用程序。Kwame for Science通过提供来自精选知识来源的段落以及基于西非高级中学证书考试（WASSCE）的综合科学科目的相关过去的国家考试问题的答案来回答学生们的问题。此外，学生们还可以查看过去的国家考试问题及其答案，并通过我们开发的主题检测模型进行按年份、问题类型和主题的自动分类（91%非加权平均召回率）。我们在实际环境中部署了Kwame for Science超过8个月，有来自32个国家（其中15个在非洲）的750个用户，共提出了1.5K的问题。我们的评估结果显示，87.2%的前三名问题准确率（n=109）。

    Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109
    
[^99]: 面向不确定性感知的语言智能体

    Towards Uncertainty-Aware Language Agent. (arXiv:2401.14016v1 [cs.CL])

    [http://arxiv.org/abs/2401.14016](http://arxiv.org/abs/2401.14016)

    UALA是一个使用不确定性量化来进行代理和外部世界交互的框架，相比于其他方法，它在多个任务和语言模型尺寸下表现出显著的性能改进，并且在对外部世界的依赖性方面更低。

    

    虽然语言智能体通过将大型语言模型置于更多功能的设计核心以及与外部世界的动态交互中取得了令人期待的成功，但现有方法在这些交互过程中忽视了不确定性的概念。我们提出了一种称为不确定性感知的语言智能体（UALA）的框架，该框架使用不确定性量化来编排代理和外部世界之间的交互。与其他知名对手（如ReAct）相比，我们在3个代表性任务（HotpotQA，StrategyQA，MMLU）和各种LLM尺寸上进行了广泛的实验，结果表明UALA在性能方面有显著的改进，同时对外部世界的依赖性显著降低（即，减少了工具调用和标记数）。我们的分析提供了各种见解，包括与代理微调相比，UALA的巨大潜力，并强调在语言模型的口头置信度作为不确定性的代理时的不可靠性。

    While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.
    
[^100]: 自奖励语言模型

    Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])

    [http://arxiv.org/abs/2401.10020](http://arxiv.org/abs/2401.10020)

    该论文提出了自奖励语言模型的概念，通过LLM作为评判者，使用语言模型自己提供训练过程中的奖励。研究表明，该方法不仅可以提高指令遵循能力，还可以为自己提供高质量的奖励。通过对Llama 2 70B模型的三次迭代微调，结果在AlpacaEval 2.0排行榜上超过了其他现有系统。这项工作为实现能够不断自我改进的模型开辟了新的可能性。

    

    我们假设要实现超人级的智能体，未来的模型需要超人级的反馈，以提供足够的训练信号。目前的方法通常是从人类偏好中训练奖励模型，这可能会受到人类表现水平的限制，而且这些独立的冻结奖励模型在LLM训练过程中无法学习改进。在这项工作中，我们研究了自奖励语言模型，其中语言模型本身通过LLM作为评判者的提示在训练过程中提供自己的奖励。我们表明，在迭代DPO训练中，不仅指令遵循能力得到了提高，而且能够为自己提供高质量的奖励。通过对Llama 2 70B进行我们方法的三次迭代的微调，得到的模型在AlpacaEval 2.0排行榜上胜过许多现有系统，包括Claude 2、Gemini Pro和GPT-4 0613。虽然这只是一项初步研究，但这项工作为可能实现能够不断自我改进的模型打开了大门。

    We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
    
[^101]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^102]: 大型语言模型能成为好的路径规划器吗？对空间-时间推理进行的基准测试和调查

    Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])

    [http://arxiv.org/abs/2310.03249](http://arxiv.org/abs/2310.03249)

    本研究提出了一种新的基准测试PPNL，评估大型语言模型的空间-时间推理能力。实验结果显示，少样本的GPT-4在空间推理方面表现良好，但仍有待改进。

    

    大型语言模型（LLM）在各种任务中取得了显著的成功，但在需要长期规划和空间推理的场景中仍然面临限制。为了促进这一研究方向，本文提出了一种新的基准测试，称为自然语言路径规划（PPNL）。我们的基准测试通过制定需要LLM导航到目标位置并避开障碍物和遵守约束条件的“路径规划”任务，评估LLM的空间-时间推理能力。利用这个基准测试，我们系统地调查了包括GPT-4在内的LLM，使用不同的少样本提示方法和各种规模的BART和T5进行微调。实验结果表明，在提示LLM进行推理和交互行动时，少样本的GPT-4在空间推理方面有希望，但仍无法进行长期时间推理。相比之下，经过微调的LLM取得了较好的结果。

    Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\textbf{P}$ath $\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage ($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieve
    
[^103]: 大型语言模型中的上下文学习: 对表示的神经科学启发式分析

    In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00313](http://arxiv.org/abs/2310.00313)

    本研究通过神经科学启发的技术，研究了大型语言模型中上下文学习的机制，通过调查嵌入和注意力的变化，揭示了这种改进背后的潜在变化，并提出了用于参数化探测和注意力比率分析的新方法。

    

    通过利用输入中的特定任务示例，大型语言模型（LLMs）通过上下文学习（ICL）展现了卓越的性能提升。然而，这种改进背后的机制仍然难以理解。本研究中，我们调查了Llama-2 70B和Vicuna 13B中的嵌入和注意力表示。具体而言，我们研究了上下文学习后嵌入和注意力的变化以及这些变化如何调解行为的改进。我们采用了受神经科学启发的技术，如表示相似性分析（RSA），并提出了参数化探测和注意力比率分析（ARA，衡量关注相关与无关信息的比率）的新方法。我们设计了三个具有条件之间先验关系的任务：阅读理解，线性回归和对抗提示注入。我们提出了关于任务表示中预期相似性的假设，以研究嵌入和注意力中的潜在变化。

    Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti
    
[^104]: 个性化文本生成的自动提示重写

    Automatic Prompt Rewriting for Personalized Text Generation. (arXiv:2310.00152v1 [cs.CL])

    [http://arxiv.org/abs/2310.00152](http://arxiv.org/abs/2310.00152)

    这项研究提出了一种自动修订个性化文本生成提示的新方法，在大型语言模型无法微调的情况下，通过改进输入文本的方式实现个性化文本生成。

    

    在大型语言模型（LLMs）的帮助下，个性化文本生成已成为一个快速增长的研究方向。大多数现有研究集中在为特定领域设计专门的模型，或者需要微调LLMs以生成个性化文本。我们考虑了一个典型情景，在这种情况下，生成个性化输出的大型语言模型是冻结的，只能通过API进行访问。在这个限制下，唯一能做的就是改进发送给LLM的输入文本（即文本提示），这个过程通常是手动完成的。在本文中，我们提出了一种新颖的方法，用于自动修订个性化文本生成的提示。所提出的方法采用了一个训练范式，将监督学习（SL）和

    Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and 
    
[^105]: 教授文本到图像模型进行交流

    Teaching Text-to-Image Models to Communicate. (arXiv:2309.15516v1 [cs.CL])

    [http://arxiv.org/abs/2309.15516](http://arxiv.org/abs/2309.15516)

    本文提出了一种针对对话生成图像的高效方法，通过微调预训练的文本到图像模型，实现在给定对话背景下生成一致逼真的图像。

    

    在文本到图像生成的研究中，各种工作已经得到广泛研究。虽然现有模型在文本到图像生成方面表现良好，但是在直接应用于对话生成图像时存在重大挑战。在本文中，我们首先突出了一个新的问题：对话到图像生成，即在给定对话背景的情况下，模型应该生成一个与指定对话内容一致的逼真图像作为回应。为了解决这个问题，我们提出了一种无需中间转换的高效方法，该方法最大程度地提取对话中包含的语义信息。考虑到对话结构的特点，我们在对话中的每个说话回合之前放置分割标记，以区分不同的发言者。然后，我们对预训练的文本到图像模型进行微调，使其能够根据处理后的对话背景生成图像。经过微调后，我们的方法可以生成与处理后对话环境相一致的图像。

    Various works have been extensively studied in the research of text-to-image generation. Although existing models perform well in text-to-image generation, there are significant challenges when directly employing them to generate images in dialogs. In this paper, we first highlight a new problem: dialog-to-image generation, that is, given the dialog context, the model should generate a realistic image which is consistent with the specified conversation as response. To tackle the problem, we propose an efficient approach for dialog-to-image generation without any intermediate translation, which maximizes the extraction of the semantic information contained in the dialog. Considering the characteristics of dialog structure, we put segment token before each sentence in a turn of a dialog to differentiate different speakers. Then, we fine-tune pre-trained text-to-image models to enable them to generate images conditioning on processed dialog context. After fine-tuning, our approach can con
    
[^106]: 损失突然下降：语法习得、相变和MLM中的简化偏差

    Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])

    [http://arxiv.org/abs/2309.07311](http://arxiv.org/abs/2309.07311)

    本文通过对掩码语言模型中的语法习得进行案例研究，发现在训练的一个短暂窗口内，模型突然获得了语法注意结构(SAS)，并伴随着损失的陡峭下降。SAS对随后习得语言能力起到了重要的促进作用。

    

    自然语言处理(NLP)中的大多数可解释性研究侧重于理解完全训练模型的行为和特征。然而，通过观察训练过程的轨迹，可能才能获得对模型行为的某些洞察。在本文中，我们通过对掩码语言模型(MLMs)中的语法习得进行案例研究，展示了如何通过分析训练过程中可解释性的演化来加深我们对新兴行为的理解。具体而言，我们研究了语法注意结构(SAS)，这是MLMs中自然形成的一个特性，其中特定的Transformer头倾向于关注特定的句法关系。我们发现在训练的一个短暂窗口内，模型突然获得了SAS，并发现这个窗口与损失的陡峭下降同时发生。此外，SAS促使了随后对语言能力的习得。然后，我们通过引入一个正则化项来操纵训练过程中的SAS，来研究SAS的因果作用，并进行了实验证明。

    Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
    
[^107]: 如何（不）在主观NLP任务中使用社会人口统计信息

    How (Not) to Use Sociodemographic Information for Subjective NLP Tasks. (arXiv:2309.07034v1 [cs.CL])

    [http://arxiv.org/abs/2309.07034](http://arxiv.org/abs/2309.07034)

    该论文研究了如何使用社会人口统计信息在主观NLP任务中，发现社会人口提示技术在某些任务上有效，但也存在一些限制和挑战。

    

    注释者的社会人口背景（即性别，年龄，教育背景等个体组成）对其在主观NLP任务中的决策有很大影响，比如仇恨言论检测。通常，异质的背景会导致高度分歧。为了建模这种差异，最近的研究探索了社会人口提示技术，这种技术将基于提示的模型的输出引导到具有特定社会人口特征的人类可能给出的答案。然而，现有的NLP文献对这种技术的效果存在分歧 - 它仍然不清楚它能在哪些任务和场景中有帮助，并且评估仅限于特定任务。我们通过展示迄今为止最大和最全面的社会人口提示研究来填补这一研究空白。具体来说，我们评估了七个数据集和六个经过指导调整的模型家族中的几个提示形式。我们发现，尽管社会人口提示对某些任务有效，但也存在一些限制和挑战。

    Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as hate speech detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique -- it remains unclear, for which tasks and scenarios it can help and evaluations are limited to specific tasks only. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. Concretely, we evaluate several prompt formulations across seven datasets and six instruction-tuned model families. We find that (1) while sociodemographic prompt
    
[^108]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^109]: S2vNTM: 半监督vMF神经主题建模

    S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])

    [http://arxiv.org/abs/2307.04804](http://arxiv.org/abs/2307.04804)

    S2vNTM是一种半监督的vMF神经主题建模方法，通过利用关键词的模式来识别潜在的主题，并优化主题关键词集的质量，提高了分类准确度，并且速度至少比基线模型快两倍。

    

    基于语言模型的方法对于文本分类来说是一种强大的技术。然而，这些模型存在一些缺点：（1）很难整合人类知识，比如关键词；（2）训练模型需要大量资源；（3）依赖大规模文本数据进行预训练。本文中，我们提出了半监督vMF神经主题建模（S2vNTM）来克服这些困难。S2vNTM将一些种子关键词作为主题的输入。S2vNTM利用关键词的模式来识别潜在的主题，并优化主题关键词集的质量。在各种数据集上，S2vNTM在提供有限关键词的情况下，在分类准确度方面优于现有的半监督主题建模方法。S2vNTM至少比基线模型快两倍。

    Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.
    
[^110]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    

