# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences.](http://arxiv.org/abs/2307.08036) | 这项研究介绍了一种神经符号方法来解决验证语法正确句子的问题。 |
| [^2] | [Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach.](http://arxiv.org/abs/2307.07994) | 本文介绍了一种利用积极情绪引发来促进多轮情感支持对话的强化学习方法，通过调整引发强度并保持会话目标一致性，实现了积极情绪引发的优越性。 |
| [^3] | [A Survey of Techniques for Optimizing Transformer Inference.](http://arxiv.org/abs/2307.07982) | 本文综述了优化Transformer推理的技术，包括知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计。 |
| [^4] | [MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning.](http://arxiv.org/abs/2307.07951) | MinT通过多视角微调方法，利用不同标注风格的数学问题数据集提升了数学推理中小型语言模型的泛化能力。 |
| [^5] | [Model Adaptation for ASR in low-resource Indian Languages.](http://arxiv.org/abs/2307.07948) | 本研究提出了一种适用于低资源印度语言的ASR模型适应方法，通过利用同源语言来克服数据的低资源性质，解决了多种方言和相似语言存在的复杂性，以及对声学和文本模态的重要性进行了讨论。 |
| [^6] | [Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling.](http://arxiv.org/abs/2307.07946) | 本文提出了一种统一令牌级别和跨度级别监督的CDAP网络用于少样本序列标注，通过在不同粒度上进行联合训练和一致损失，实现了两个网络的协同学习，在推理阶段使用了一致贪婪推理算法来选择非重叠跨度。 |
| [^7] | [Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions.](http://arxiv.org/abs/2307.07940) | 本文提出了一种在编程问题中去除重复程序并排名的方法，以鼓励学习者参考不同的解决方法，从而学习更好的解决方案。 |
| [^8] | [GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT.](http://arxiv.org/abs/2307.07930) | GeoGPT是一个基于自主GPT的系统，旨在通过将大规模语言模型的语义理解能力与地理空间任务相结合，降低非专业用户解决地理空间任务的门槛。 |
| [^9] | [Communicative Agents for Software Development.](http://arxiv.org/abs/2307.07924) | 本文介绍了一种创新的软件开发范式，利用大型语言模型(LLMs)在整个软件开发过程中实现自然语言交流，消除了每个阶段需要专门模型的需求。该范式使用ChatDev作为一个虚拟聊天驱动的软件开发公司，通过设计、编码、测试和文档化四个阶段的代理人团队促进协作。 |
| [^10] | [Zero-shot NLG evaluation through Pairware Comparisons with LLMs.](http://arxiv.org/abs/2307.07889) | 本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。 |
| [^11] | [Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding.](http://arxiv.org/abs/2307.07880) | 提示为基础的微调在多语种任务中的表现仍然有限，本研究通过ProFiT流程对此进行了深入研究，实验证明在不同的少样本和全数据设置下，提示为基础的微调具有不同的性能变化趋势。 |
| [^12] | [Large Language Models as Superpositions of Cultural Perspectives.](http://arxiv.org/abs/2307.07870) | 大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。 |
| [^13] | [CIDER: Context sensitive sentiment analysis for short-form text.](http://arxiv.org/abs/2307.07864) | CIDER是一种上下文感知的短文本情感分析方法，通过从整个语料库中推断出情感词的倾向来评分个别文本，相比通用方法在天气推文集合上表现更优。 |
| [^14] | [AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge.](http://arxiv.org/abs/2307.07851) | AspectCSE是一种使用对比学习和结构化知识进行基于方面的语义文本相似性的句子嵌入方法，它在信息检索任务中相比之前的最好结果平均提高了3.97%，通过同时考虑多个特定方面的嵌入模型优于单方面嵌入。 |
| [^15] | [Transformers are Universal Predictors.](http://arxiv.org/abs/2307.07843) | Transformers架构在语言建模中具有通用的预测性质，并且在非渐近数据环境中表现良好。 |
| [^16] | [Opinion mining using Double Channel CNN for Recommender System.](http://arxiv.org/abs/2307.07798) | 本文提出了一种使用深度学习模型进行情感分析的方法，通过双通道卷积神经网络进行意见挖掘，并用于推荐产品。通过应用SMOTE算法增加评论数量并对数据进行平衡，使用张量分解算法为聚类分配权重，提高了推荐系统的性能。 |
| [^17] | [Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model.](http://arxiv.org/abs/2307.07740) | 本论文使用CNN-LSTM模型对波斯推特的政治情感进行分析，使用ParsBERT进行词汇表示，并比较了机器学习和深度学习模型的效果。实验结果表明，深度学习模型表现更好，其中CNN-LSTM模型在两个数据集上分别达到了89%和71%的分类准确率。 |
| [^18] | [CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models.](http://arxiv.org/abs/2307.07705) | CPET提出了一种基于压缩LLM的有效参数优化框架，通过引入知识继承和恢复策略，解决了在参数有效调整中压缩LLM的推理计算瓶颈问题。 |
| [^19] | [Leveraging Large Language Models to Generate Answer Set Programs.](http://arxiv.org/abs/2307.07699) | 本文提出了一种神经符号方法，将大型语言模型和答案集编程的优势相结合，通过使用大型语言模型将自然语言描述转化为答案集程序，以实现处理复杂推理问题的能力。 |
| [^20] | [Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph.](http://arxiv.org/abs/2307.07697) | Think-on-Graph是一个利用知识图谱增强大型语言模型深度和负责任推理能力的新框架，在复杂的多跳推理问答任务上表现出色，解决了现有方法中存在的限制。 |
| [^21] | [Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text.](http://arxiv.org/abs/2307.07696) | 这项研究将大型语言模型与逻辑编程相结合，通过将自然语言句子转换为逻辑形式，使模型能够进行强大且通用的推理。该方法只需要少量的示例和可重用的知识模块，即可在多个问答任务中取得最先进的性能。 |
| [^22] | [Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features.](http://arxiv.org/abs/2307.07683) | 本论文介绍了三种区分真实声音和试图冒充特定人物声音的克隆声音的技术，分别采用不同的特征提取方法，并展示了在单个和多个声音上训练时的有效性。学习特征具有较高的准确性并且对对抗性清洗具有相当的鲁棒性。 |
| [^23] | [Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models.](http://arxiv.org/abs/2307.07645) | 通过对2.1M英语Yelp评论的餐厅进行语言分析，研究发现移民美食更容易被构架为客观和他者化，而非西方移民美食受欢迎程度更高。 |
| [^24] | [Towards Generalizable Detection of Urgency of Discussion Forum Posts.](http://arxiv.org/abs/2307.07614) | 本研究旨在解决学生在线课程讨论论坛问题的规模扩展难题。通过构建预测模型，自动确定论坛帖子的紧急程度，并提供给教师注意。与之前的工作不同，本研究通过预测7分制紧急程度级别，实现了更细粒度的预测。通过在大规模数据集上的训练和测试，证明了模型的通用性。 |
| [^25] | [QontSum: On Contrasting Salient Content for Query-focused Summarization.](http://arxiv.org/abs/2307.07586) | 本文针对查询重点摘要 (QFS) 提出了一种名为 QontSum 的新方法。该方法利用对比学习帮助模型集中注意力于输入文档中最相关的区域，并在多个基准数据集上展示了优于现有方法或在减少计算成本的同时表现出可比较性能的结果。 |
| [^26] | [Exploring the Emotional and Mental Well-Being of Individuals with Long COVID Through Twitter Analysis.](http://arxiv.org/abs/2307.07558) | 通过分析 Twitter，深入了解长期 COVID 患者的情绪和心理健康状况，发现负面情绪主导，并与关键时期相关。这对制定应对长期 COVID 患者心理健康挑战的政策和措施具有重要意义。 |
| [^27] | [A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge.](http://arxiv.org/abs/2307.07544) | 这个论文介绍了一个用于评估日常生活活动的对话系统，通过模拟评估员和参与者之间的交互，提高了评估的一致性。 |
| [^28] | [PapagAI:Automated Feedback for Reflective Essays.](http://arxiv.org/abs/2307.07523) | PapagAI是第一个基于教学理论并实现为混合AI系统的开源自动反馈工具，旨在提高学生学习成果并补充讲师的教学活动。 |
| [^29] | [Translating Latin with Artificial Intelligence.](http://arxiv.org/abs/2307.07520) | 用人工智能翻译工具可以解决研究早期科学文献的拉丁文翻译难题，通过基准测试发现ChatGPT算法在表现上更出色，并将其应用于约翰·伯努利给欧拉的一封信的翻译。 |
| [^30] | [CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model.](http://arxiv.org/abs/2307.07518) | CephGPT-4是一个具有视觉大语言模型的交互式多模态颅颌测量与诊断系统，它能够通过自动分析颅颌标志点和生成诊断报告，实现出色的性能，为正畸测量和诊断应用带来革命性的潜力。 |
| [^31] | [Voting-based Multimodal Automatic Deception Detection.](http://arxiv.org/abs/2307.07516) | 本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。 |
| [^32] | [An empirical study of using radiology reports and images to improve ICU mortality prediction.](http://arxiv.org/abs/2307.07513) | 本研究利用放射学报告和图像构建了一个基于深度学习的多模态数据生存预测模型，用于预测重症监护病房（ICU）的死亡率，并在MIMIC-IV数据集上取得了0.7829的平均C-index。 |
| [^33] | [RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition.](http://arxiv.org/abs/2307.07417) | RoPDA是一种用于低资源NER的数据增强方法，通过基于预训练语言模型和连续提示进行实体和上下文增强，并提出了自一致性过滤和混合技术以优化增强样本的利用。 |
| [^34] | [Parmesan: mathematical concept extraction for education.](http://arxiv.org/abs/2307.06699) | Parmesan是一个原型系统，用于在上下文中搜索和定义数学概念，特别关注范畴论领域。该系统利用自然语言处理组件进行概念提取、关系提取、定义提取和实体链接。通过该系统的开发，可以解决现有技术不能直接应用于范畴论领域的问题，并提供了两个数学语料库以支持系统的使用。 |
| [^35] | [Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations.](http://arxiv.org/abs/2307.06576) | 本文介绍了一种名为GLORY的模型，通过全局图与本地表示相结合，增强了个性化推荐系统。该模型通过构建全局感知历史新闻编码器来融合历史新闻表示，并考虑了用户隐藏的动机和行为。 |
| [^36] | [Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures.](http://arxiv.org/abs/2307.05360) | 本文全面评估了ChatGPT在编码算法和数据结构方面的能力，基于最大的编码挑战目录，重点关注Python编程语言和数据结构算法两个基础主题。总结测试中ChatGPT的代码解决问题的准确性、代码质量和运行时错误的性质。 |
| [^37] | [ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey.](http://arxiv.org/abs/2307.04251) | ChatGPT是由OpenAI创建的一种大型语言模型（LLM），它在自然语言处理（NLP）领域引起了革命性的变革。它是第一个实现公众大规模与生成式人工智能（GAI）互动的关键技术，也引发了类似技术的研究兴趣和应用探索。 |
| [^38] | [On decoder-only architecture for speech-to-text and large language model integration.](http://arxiv.org/abs/2307.03917) | 该论文介绍了一种新颖的方法Speech-LLaMA，将声学信息有效地融入基于文本的大型语言模型中。通过使用连接主义时序分类和简单的音频编码器，将压缩的声学特征映射到大型语言模型的连续语义空间中，实现了语音到文本任务中的实质性提升。 |
| [^39] | [Emoji Prediction using Transformer Models.](http://arxiv.org/abs/2307.02054) | 使用基于Transformer的方法，在大型语料库上微调BERT模型以预测给定文本的表情符号。实验结果显示，该方法在预测准确率上优于其他最先进的模型，具有潜在的自然语言处理和社交媒体营销应用价值。 |
| [^40] | [InstructEval: Systematic Evaluation of Instruction Selection Methods.](http://arxiv.org/abs/2307.00259) | InstructEval开发了一个评估套件，用于对指令选择方法进行全面评估。通过使用策划的手动编写的指令，可以显著提高性能。 |
| [^41] | [Modeling Human-like Concept Learning with Bayesian Inference over Natural Language.](http://arxiv.org/abs/2306.02797) | 该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。 |
| [^42] | [A Study of Situational Reasoning for Traffic Understanding.](http://arxiv.org/abs/2306.02520) | 本研究提出了三个新的基于文本的交通领域情境推理任务，旨在评估语言模型在情境决策、事件因果关系推理和解决人类驾驶考试方面的能力。研究采用了四种知识增强方法，具有潜力在不同语言推理任务中实现模型的泛化能力。 |
| [^43] | [Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition.](http://arxiv.org/abs/2306.00804) | 该论文提出了一种自适应上下文偏置方法，基于Context-Aware Transformer Transducer (CATT) 来进行流式语音识别的预测。实验结果表明，与基线相比，该方法在 WER 和 CER 上可以分别减少6.7%和20.7%，减少了96.7%和84.9% 的相对 WER 和 CER 增加。 |
| [^44] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^45] | [Training Socially Aligned Language Models in Simulated Human Society.](http://arxiv.org/abs/2305.16960) | 本研究提出了一种在模拟人类社会中训练语言模型的新方法，相比于现有方法，该方法具有更大的可扩展性和高效性，并在对齐基准和人类评估中展示出更优异的性能。 |
| [^46] | [Understanding and Mitigating Spurious Correlations in Text Classification.](http://arxiv.org/abs/2305.13654) | 本文研究了深度学习模型容易利用训练集中存在但通常不成立的伪相关性的问题，并提出了一种邻域分析框架以解释语言模型如何利用伪相关性。通过一系列正则化方法NFL（不要忘记你的语言）避免了这种情况，并在实验中证明了其鲁棒性方面的显著改进。 |
| [^47] | [Cross-Modal Retrieval for Motion and Text via MildTriple Loss.](http://arxiv.org/abs/2305.04195) | 本论文提出了一个创新模型，使用MildTriple Loss捕捉长期依赖并模拟跨模态人类动作序列与文本检索任务，具有重要的应用价值。 |
| [^48] | [SCOTT: Self-Consistent Chain-of-Thought Distillation.](http://arxiv.org/abs/2305.01879) | 本研究提出了一种忠实的知识蒸馏方法，从比教师模型大数倍的模型中学习一个小的、自我一致的思路串模型。实验结果表明，该方法有助于证明决策并提高性能，特别是在较小的语言模型中。 |
| [^49] | [Learning to Compress Prompts with Gist Tokens.](http://arxiv.org/abs/2304.08467) | 该论文提出了一种名为"gisting"的方法，通过训练语言模型将提示压缩为更小的"要点"标记集，以提高计算效率。通过这种方法，可以实现高达26倍的提示压缩，减少40％的FLOPs、4.2％的墙时速度提升，并节省存储空间，同时最小化输出质量损失。 |
| [^50] | [Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation.](http://arxiv.org/abs/2303.06662) | 本文提出了一种在非自回归机器翻译中通过模糊对齐解决多模态问题的方法，通过最大化图与参考之间的模糊对齐得分来训练模型，实验证明了方法显著提高了翻译性能，并增加了预测置信度，创造了非自回归机器翻译的新的state of the art。 |
| [^51] | [Gradient-Free Structured Pruning with Unlabeled Data.](http://arxiv.org/abs/2303.04185) | 本文提出了一种使用无标签数据的无梯度结构化剪枝方法，在GLUE和SQuAD基准测试上的实验证明了其有效性，仅需几分钟就能将原始FLOP计数的最高40%减少而准确度仅下降不超过4%。 |
| [^52] | [A Neural Span-Based Continual Named Entity Recognition Model.](http://arxiv.org/abs/2302.12200) | 本文提出了一种基于神经网络的跨时期命名实体识别模型SpanKL，通过知识蒸馏和多标签预测来实现记忆保留和冲突防止，该模型在跨时期NER任务中表现出色，显示出高实际价值。 |
| [^53] | [Lived Experience Matters: Automatic Detection of Stigma on Social Media Toward People Who Use Substances.](http://arxiv.org/abs/2302.02064) | 本文通过分析大量Reddit帖子数据集，探讨了对使用物质的人（PWUS）的污名化。结果表明，具有物质使用经验的工人更有可能评为具有污名化，为此建立了一个以亲身物质使用经验的工人为中心的机器学习框架。 |
| [^54] | [Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning.](http://arxiv.org/abs/2301.08913) | 本文提出了一个统一的学习框架，将明确的结构推理和语言预训练相结合，以赋予预训练语言模型结构推理能力。 |
| [^55] | [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models.](http://arxiv.org/abs/2211.16198) | 本文提出了SuS-X，一种无需训练的基于名称的视觉语言模型迁移方法，具有较高的零样本分类能力。 |
| [^56] | [Undesirable biases in NLP: Averting a crisis of measurement.](http://arxiv.org/abs/2211.13709) | 这项研究提供了一个跨学科的方法来探讨NLP模型偏见的问题，通过采用心理测量学的视角，特别关注构念效度和测量工具的信度，在衡量模型偏见的情境中如何应用。 |
| [^57] | [DialoGen: Generalized Long-Range Context Representation for Dialogue Systems.](http://arxiv.org/abs/2210.06282) | DialoGen是一种对话系统，采用了广义上下文表示方法，在对话理解和生成任务中表现出色。 |
| [^58] | [Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution.](http://arxiv.org/abs/2210.00131) | 本研究通过提供一个因果模型，在语言建模任务中探讨了不充分规范化的作用，提出了两种轻量级黑盒评估方法来帮助检测任务的不充分规范化，并在性别代词消解任务中应用这些方法，同时发现了性别与时间、性别与位置之间的虚假相关性。 |
| [^59] | [Elaboration-Generating Commonsense Question Answering at Scale.](http://arxiv.org/abs/2209.01232) | 本论文提出了一种规模较小的语言模型框架，通过微调生成有用的中间上下文来提高常识问答性能，并在人工评估中获得高质量的生成详细解释。 |
| [^60] | [Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets.](http://arxiv.org/abs/2205.11472) | 本研究发现，在论证挖掘任务中，使用精心组织的训练样本和预训练模型可以在减小训练样本大小至少85％的情况下，达到最大性能的95％。同时提供了一个新的数据集供未来基准测试。 |
| [^61] | [The Hierarchical Organization of Syntax.](http://arxiv.org/abs/2112.05783) | 对历史句法网络的分层组织分析揭示了句法演化的方式，同时证明了说话者的交流需求是句法组织的重要驱动力。 |
| [^62] | [Automated scholarly paper review: Concepts, technologies, and challenges.](http://arxiv.org/abs/2111.07533) | 提出自动学术论文审稿（ASPR）的概念和流程，综述了实现全面计算机化审稿流程的相关文献和技术，同时指出实现中存在的挑战，如文档解析和表达不完美、数据不足、人机交互缺陷和发现低质量文章的难度。 |
| [^63] | [Pre-trained Language Models in Biomedical Domain: A Systematic Survey.](http://arxiv.org/abs/2110.05006) | 本文系统调查了生物医学领域中的预训练语言模型，总结了它们的最新进展和应用，并提出了分类法。 |

# 详细

[^1]: 一种神经符号方法实现对语法正确句子的识别

    A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences. (arXiv:2307.08036v1 [cs.CL])

    [http://arxiv.org/abs/2307.08036](http://arxiv.org/abs/2307.08036)

    这项研究介绍了一种神经符号方法来解决验证语法正确句子的问题。

    

    我们身边的文本内容每天都在增长。在网上报纸、博客或社交媒体上，我们正在撰写大量的文章。类似地，人工智能领域的最新进展，如语言模型或传统的经典人工智能方法，正在利用以上所有内容，提高他们学习的表示能力，以实现类似人类的准确性来应对自然语言处理挑战。众所周知，获取来自有效来源的写作文本对于应对文本摘要、问答、机器翻译甚至代词消解等挑战至关重要。例如，要进行良好的摘要，需要选择最重要的句子，然后将它们连接起来形成摘要。然而，如果我们没有获取到写得好的英文句子甚至非有效的句子会怎么样呢？尽管获得写得好的句子的重要性被广泛认可，但找出验证它们的方法仍然是一个开放的研究领域。为了解决这个问题，我们提出了一种神经符号方法。

    Textual content around us is growing on a daily basis. Numerous articles are being written as we speak on online newspapers, blogs, or social media. Similarly, recent advances in the AI field, like language models or traditional classic AI approaches, are utilizing all the above to improve their learned representation to tackle NLP challenges with human-like accuracy. It is commonly accepted that it is crucial to have access to well-written text from valid sources to tackle challenges like text summarization, question-answering, machine translation, or even pronoun resolution. For instance, to summarize well, one needs to select the most important sentences in order to concatenate them to form the summary. However, what happens if we do not have access to well-formed English sentences or even non-valid sentences? Despite the importance of having access to well-written sentences, figuring out ways to validate them is still an open area of research. To address this problem, we present a 
    
[^2]: 利用积极情绪引发来促进多轮情感支持对话：一种强化学习方法

    Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach. (arXiv:2307.07994v1 [cs.CL])

    [http://arxiv.org/abs/2307.07994](http://arxiv.org/abs/2307.07994)

    本文介绍了一种利用积极情绪引发来促进多轮情感支持对话的强化学习方法，通过调整引发强度并保持会话目标一致性，实现了积极情绪引发的优越性。

    

    情感支持对话旨在提供情感支持以改善人们的心理状态。现有研究仅局限于配合回应和回策略（如问题），忽略了对情感支持的影响，并缺乏明确的目标来引导积极情感转变。为此，我们引入了一种新的范式，将多轮情感支持对话形式化为积极情感引发的过程。解决这个任务需要在对话过程中细调情感引发的强度，同时保持对话的一致性等会话目标。在本文中，我们提出了一种基于混合专家的强化学习模型Supporter，并设计了情感支持和对话一致性奖励来指导回应策略的学习。实验证明，Supporter在实现积极情感引发的同时，保持了包括一致性在内的会话目标的优越性。

    Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy's learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.
    
[^3]: 优化Transformer推理技术的综述

    A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])

    [http://arxiv.org/abs/2307.07982](http://arxiv.org/abs/2307.07982)

    本文综述了优化Transformer推理的技术，包括知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计。

    

    近年来，Transformer神经网络的性能和应用范围取得了显著的增长。包括BERT、GPT和ViT在内的Transformer网络家族，在自然语言处理（NLP）和计算机视觉（CV）领域展现了效果。ChatGPT等基于Transformer的网络也影响了普通人的生活。然而，追求高预测性能导致了Transformer的内存和计算消耗的指数增长。研究人员提出了在抽象级别的Transformer推理优化技术。本文对Transformer网络推理阶段的优化技术进行了全面的调查。我们在算法层面上调查了知识蒸馏、修剪、量化、神经架构搜索和轻量级网络设计等技术。

    Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review
    
[^4]: MinT: 通过多视角微调提升数学推理的泛化性能

    MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])

    [http://arxiv.org/abs/2307.07951](http://arxiv.org/abs/2307.07951)

    MinT通过多视角微调方法，利用不同标注风格的数学问题数据集提升了数学推理中小型语言模型的泛化能力。

    

    对于相对较小的语言模型（LM），在数学推理中进行推理仍然是一个重大挑战。许多当前方法专注于在数学推理中专门化LM，并且过度依赖于强大但低效的大型LM（LLM）所提供的知识蒸馏。在这项工作中，我们探索了一种避免过度依赖LLM教师的新思路，引入了一种利用具有不同标注风格的现有数学问题数据集的多视角微调方法。我们的方法将不同的标注格式视为不同的“视图”，并在模型训练中利用它们。通过将不同的指令附加到输入问题上，模型可以学习以灵活的方式生成不同格式的解决方案。实验证明，我们的策略使LLaMA-7B模型能够超越利用知识蒸馏的先前方法以及精心建立的基准。此外，所提出的方法使模型取得了活跃

    Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different "views" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promi
    
[^5]: 适用于低资源印度语言的ASR模型适应

    Model Adaptation for ASR in low-resource Indian Languages. (arXiv:2307.07948v1 [eess.AS])

    [http://arxiv.org/abs/2307.07948](http://arxiv.org/abs/2307.07948)

    本研究提出了一种适用于低资源印度语言的ASR模型适应方法，通过利用同源语言来克服数据的低资源性质，解决了多种方言和相似语言存在的复杂性，以及对声学和文本模态的重要性进行了讨论。

    

    最近几年，自监督学习（SSL）为基础的声学模型（如wav2vec2）和大规模多语言训练（如Whisper）使自动语音识别（ASR）的性能得到了显著提高。然而，在低资源语言中仍存在巨大挑战，这些语言的语音和文本可用性都很有限。印度语言中存在多种方言，进一步增加了复杂性。然而，许多印度语言可以分为相同的语系，共享相同的书写和语法结构。在这种情况下，可以应用许多适应和微调技术来克服数据的低资源性质，通过利用资源丰富的相似语言。在这种情况下，重要的是了解各种模态（如声学和文本）在构建可靠的ASR中的重要性。可能的情况是，在某种语言中，丰富的声学数据降低了对大型纯文本语料库的需求。

    Automatic speech recognition (ASR) performance has improved drastically in recent years, mainly enabled by self-supervised learning (SSL) based acoustic models such as wav2vec2 and large-scale multi-lingual training like Whisper. A huge challenge still exists for low-resource languages where the availability of both audio and text is limited. This is further complicated by the presence of multiple dialects like in Indian languages. However, many Indian languages can be grouped into the same families and share the same script and grammatical structure. This is where a lot of adaptation and fine-tuning techniques can be applied to overcome the low-resource nature of the data by utilising well-resourced similar languages.  In such scenarios, it is important to understand the extent to which each modality, like acoustics and text, is important in building a reliable ASR. It could be the case that an abundance of acoustic data in a language reduces the need for large text-only corpora. Or, 
    
[^6]: 联合令牌级别和跨度级别的监督用于少样本序列标注

    Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling. (arXiv:2307.07946v1 [cs.CL])

    [http://arxiv.org/abs/2307.07946](http://arxiv.org/abs/2307.07946)

    本文提出了一种统一令牌级别和跨度级别监督的CDAP网络用于少样本序列标注，通过在不同粒度上进行联合训练和一致损失，实现了两个网络的协同学习，在推理阶段使用了一致贪婪推理算法来选择非重叠跨度。

    

    少样本序列标注旨在仅依据少量标注样本来识别新的类别。现有的方法主要通过设计基于度量学习的令牌级别或跨度级别标注模型来解决数据稀缺问题。然而，这些方法只在单一粒度上训练（即令牌级别或跨度级别），并且具有相应粒度的一些弱点。在本文中，我们首次统一了令牌级别和跨度级别的监督，并提出了一种一致双自适应原型网络（CDAP）用于少样本序列标注。CDAP包含令牌级别和跨度级别的网络，在不同粒度上进行联合训练。为了使两个网络的输出保持一致，我们进一步提出了一种一致损失，使它们可以互相学习。在推理阶段，我们提出了一种一致贪婪推理算法，首先调整预测概率，然后贪婪地选择具有最大概率的非重叠跨度。大量实验证明

    Few-shot sequence labeling aims to identify novel classes based on only a few labeled samples. Existing methods solve the data scarcity problem mainly by designing token-level or span-level labeling models based on metric learning. However, these methods are only trained at a single granularity (i.e., either token level or span level) and have some weaknesses of the corresponding granularity. In this paper, we first unify token and span level supervisions and propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot sequence labeling. CDAP contains the token-level and span-level networks, jointly trained at different granularities. To align the outputs of two networks, we further propose a consistent loss to enable them to learn from each other. During the inference phase, we propose a consistent greedy inference algorithm that first adjusts the predicted probability and then greedily selects non-overlapping spans with maximum probability. Extensive experiments show t
    
[^7]: 为推荐参考解决方案而去重和排名解决方案程序

    Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions. (arXiv:2307.07940v1 [cs.SE])

    [http://arxiv.org/abs/2307.07940](http://arxiv.org/abs/2307.07940)

    本文提出了一种在编程问题中去除重复程序并排名的方法，以鼓励学习者参考不同的解决方法，从而学习更好的解决方案。

    

    在编程教育中，参考其他用户编写的解决方案程序对学习者很有帮助。然而，当前的在线评测系统只是列出用户提交的所有解决方案程序供参考，并根据提交日期、执行时间或用户评分进行排序，忽视了程序能够成为参考的程度。此外，由于存在太多重复和近似重复的程序，用户很难参考多种解决方法。为了激励学习者参考不同的解决方法以学习更好的解决方案，本文提出了一种在每个编程问题中去重和排名常见解决方案程序的方法。基于更多重复的程序采用更常见的方法并可作为参考的假设，我们删除了近似重复的解决方案程序，并根据重复计数对唯一的程序进行排序。实验结果表明这个方法能够有效地去重和排名解决方案程序。

    Referring to the solution programs written by the other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the program can be a reference. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate the learners to refer to various solutions to learn the better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Based on the hypothesis that the more duplicated programs adopt a more common approach and can be a reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a rea
    
[^8]: GeoGPT:通过自主GPT理解和处理地理空间任务

    GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT. (arXiv:2307.07930v1 [cs.CL])

    [http://arxiv.org/abs/2307.07930](http://arxiv.org/abs/2307.07930)

    GeoGPT是一个基于自主GPT的系统，旨在通过将大规模语言模型的语义理解能力与地理空间任务相结合，降低非专业用户解决地理空间任务的门槛。

    

    GIS决策者需要结合一系列的空间算法和操作来解决地理空间任务。最近，经过预训练的Transformer模型在语义理解和推理方面表现出很强的性能。受到这些研究的启发，我们尝试通过将预训练模型的语义理解能力与地理空间任务相结合，降低非专业用户解决地理空间任务的门槛。

    Decision-makers in GIS need to combine a series of spatial algorithms and operations to solve geospatial tasks. For example, in the task of facility siting, the Buffer tool is usually first used to locate areas close or away from some specific entities; then, the Intersect or Erase tool is used to select candidate areas satisfied multiple requirements. Though professionals can easily understand and solve these geospatial tasks by sequentially utilizing relevant tools, it is difficult for non-professionals to handle these problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents strong performance in semantic understanding and reasoning. Especially, AutoGPT can further extend the capabilities of large language models (LLMs) by automatically reasoning and calling externally defined tools. Inspired by these studies, we attempt to lower the threshold of non-professional users to solve geospatial tasks by integrating the semantic understanding ability inherent in LLMs 
    
[^9]: 软件开发中的交流型代理

    Communicative Agents for Software Development. (arXiv:2307.07924v1 [cs.SE])

    [http://arxiv.org/abs/2307.07924](http://arxiv.org/abs/2307.07924)

    本文介绍了一种创新的软件开发范式，利用大型语言模型(LLMs)在整个软件开发过程中实现自然语言交流，消除了每个阶段需要专门模型的需求。该范式使用ChatDev作为一个虚拟聊天驱动的软件开发公司，通过设计、编码、测试和文档化四个阶段的代理人团队促进协作。

    

    软件工程是一个以微妙的直觉和咨询为特征的领域，决策过程复杂。深度学习的最新进展已经开始通过在软件开发的各个阶段实施精心设计来革新软件工程实践。在本文中，我们提出了一种创新的范式，通过自然语言交流，在整个软件开发过程中利用大型语言模型(LLMs)，简化和统一关键流程，从而消除了在每个阶段需要专门的模型的需要。这个范式的核心是ChatDev，一个虚拟的聊天驱动软件开发公司，它模仿了已经建立的瀑布模型，将开发过程细分为四个不同的时间阶段：设计、编码、测试和文档化。每个阶段都涉及一个团队的代理人，如程序员、代码审查人员和测试工程师，促进协作。

    Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborativ
    
[^10]: 通过基于大型语言模型的比较判定进行零样本NLG评估

    Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])

    [http://arxiv.org/abs/2307.07889](http://arxiv.org/abs/2307.07889)

    本研究提出了一种使用开源大型语言模型进行零样本自然语言生成（NLG）评估的方法，通过配对比较判定来确定候选回应的优劣。结果表明，相较于绝对评分，比较评估是一种更有效的方法，并使得较小的开源LLMs达到了与更大的公共访问API相当的性能。

    

    评估自然语言生成（NLG）输出是至关重要但费时费力的。虽然已经提出了各种自动NLG评估方法，但它们通常是特定任务特定领域的，需要针对特定领域和属性进行工程设计。在这项工作中，我们提出了一种使用开源大型语言模型（LLMs）进行零样本NLG评估的稳健方法，采用了配对比较判定的方式。这种方法的动机是，即使作为人类，确定两个选项中哪个更好要比独立客观评分每个选项更容易。我们利用这一观察结果并利用LLMs新兴的能力，通过探测FlanT5，确定两个候选回应中哪一个更好，而不是指定绝对分数。我们的结果表明，比较评估是比绝对评分更有效的方法，使得较小的开源LLMs能够达到与更大的公共访问API相当的性能。我们评估了系统。

    Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste
    
[^11]: 提示为基础的微调总是比原始微调更好吗？来自跨语言理解的见解。

    Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding. (arXiv:2307.07880v1 [cs.CL])

    [http://arxiv.org/abs/2307.07880](http://arxiv.org/abs/2307.07880)

    提示为基础的微调在多语种任务中的表现仍然有限，本研究通过ProFiT流程对此进行了深入研究，实验证明在不同的少样本和全数据设置下，提示为基础的微调具有不同的性能变化趋势。

    

    多语种预训练语言模型（MPLM）通过在源语言（例如英语）上针对特定任务的标注数据上对MPLM进行微调，并在各种目标语言上进行评估，已经在零转化跨语言传递的各种自然语言理解任务中展现出了显著的性能提升。最近的研究表明，在少样本场景下，基于提示的微调超过了常规微调。然而，在多语种任务中，提示为基础的学习的探索仍然有限。在本研究中，我们提出了ProFiT流程，以研究基于提示的微调的跨语言能力。我们在多样跨语言语言理解任务（情感分类、释义识别和自然语言推断）上进行了全面的实验，并在不同的少样本和全数据设置下经验性地分析了基于提示的微调性能的变化趋势。我们的结果揭示了提示为基础的微调在跨语言传递中的性能变化趋势。

    Multilingual pretrained language models (MPLMs) have demonstrated substantial performance improvements in zero-shot cross-lingual transfer across various natural language understanding tasks by finetuning MPLMs on task-specific labelled data of a source language (e.g. English) and evaluating on a wide range of target languages. Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios. However, the exploration of prompt-based learning in multilingual tasks remains limited. In this study, we propose the ProFiT pipeline to investigate the cross-lingual capabilities of Prompt-based Finetuning. We conduct comprehensive experiments on diverse cross-lingual language understanding tasks (sentiment classification, paraphrase identification, and natural language inference) and empirically analyze the variation trends of prompt-based finetuning performance in cross-lingual transfer across different few-shot and full-data settings. Our results reveal the 
    
[^12]: 大型语言模型作为文化角度的叠加

    Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])

    [http://arxiv.org/abs/2307.07870](http://arxiv.org/abs/2307.07870)

    大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。

    

    大型语言模型（LLMs）常常被错误地认为具有个性或一套价值观。我们认为LLMs可以看作是具有不同价值观和个性特征的角度叠加。LLMs表现出依赖于上下文的价值观和个性特征，这些特征基于产生的角度而改变（与人类相反，人类在不同情境下通常具有更一致的价值观和个性特征）。我们引入了“角度可控性”的概念，指的是模型采用不同具有不同价值观和个性特征的角度的能力。在我们的实验中，我们使用心理学问卷（PVQ、VSM、IPIP）来研究展示的价值观和个性特征如何基于不同角度而改变。通过定性实验，我们展示了当提示中（隐式或显式）暗示了某些价值观时，LLMs表达出不同的价值观，即使在没有明显暗示的情况下，LLMs也会表达出不同的价值观。

    Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
    
[^13]: CIDER: 上下文感知的短文本情感分析

    CIDER: Context sensitive sentiment analysis for short-form text. (arXiv:2307.07864v1 [cs.CL])

    [http://arxiv.org/abs/2307.07864](http://arxiv.org/abs/2307.07864)

    CIDER是一种上下文感知的短文本情感分析方法，通过从整个语料库中推断出情感词的倾向来评分个别文本，相比通用方法在天气推文集合上表现更优。

    

    研究人员通常对大量关于特定主题、主题或事件的短文本进行情感分析，如推文、Reddit帖子或报纸头条。通常使用通用情感分析方法，这些方法在平均意义上表现良好，但会忽略不同上下文中发生的意义变化，例如，“active”一词在“active lifestyle”和“active volcano”中具有非常不同的意图和倾向。本研究提出了一种新的方法，即CIDER（上下文感知词典和情感推理器），它进行上下文感知的情感分析，其中从整个语料库中推断出情感词的倾向，然后再用于评分个别文本。在本文中，我们详细介绍了CIDER算法，并证明它在大量关于天气的推文集合上优于最先进的通用情感分析方法。我们已将CIDER的实现以python代码的形式提供。

    Researchers commonly perform sentiment analysis on large collections of short texts like tweets, Reddit posts or newspaper headlines that are all focused on a specific topic, theme or event. Usually, general purpose sentiment analysis methods are used which perform well on average but miss the variation in meaning that happens across different contexts, for example, the word "active" has a very different intention and valence in the phrase "active lifestyle" versus "active volcano". This work presents a new approach, CIDER (Context Informed Dictionary and sEntiment Reasoner), which performs context sensitive sentiment analysis, where the valence of sentiment laden terms is inferred from the whole corpus before being used to score the individual texts. In this paper we detail the CIDER algorithm and demonstrate that it outperforms state-of-the-art generalist sentiment analysis on a large collection of tweets about the weather. We have made our implementation of CIDER available as a pyth
    
[^14]: AspectCSE: 使用对比学习和结构化知识进行基于方面的语义文本相似性的句子嵌入

    AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])

    [http://arxiv.org/abs/2307.07851](http://arxiv.org/abs/2307.07851)

    AspectCSE是一种使用对比学习和结构化知识进行基于方面的语义文本相似性的句子嵌入方法，它在信息检索任务中相比之前的最好结果平均提高了3.97%，通过同时考虑多个特定方面的嵌入模型优于单方面嵌入。

    

    通用的句子嵌入提供了对语义文本相似性的粗略近似，但忽略了使文本相似的特定方面。相反，基于方面的句子嵌入提供了基于预定义方面的文本相似性。因此，文本的相似性预测更加针对特定要求，并且更容易解释。在本文中，我们提出了AspectCSE，一种用于基于方面的对比学习句子嵌入的方法。结果表明，与之前最好的结果相比，AspectCSE在多个方面的信息检索任务中实现了平均改善3.97%。我们还提出使用Wikidata知识图属性来训练多方面句子嵌入模型，其中在相似性预测过程中同时考虑多个特定方面。我们证明了多方面嵌入在特定方面信息检索任务上优于单方面嵌入。最后，我们展示了嵌入模型的可解释性，并提出通过对比学习来改进嵌入质量。

    Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
    
[^15]: Transformers是通用的预测器

    Transformers are Universal Predictors. (arXiv:2307.07843v1 [cs.LG])

    [http://arxiv.org/abs/2307.07843](http://arxiv.org/abs/2307.07843)

    Transformers架构在语言建模中具有通用的预测性质，并且在非渐近数据环境中表现良好。

    

    我们找到了Transformer架构在语言建模中的局限性，并证明了它在信息理论意义上具有通用的预测性质。我们进一步分析了在非渐近数据环境中的性能，以了解Transformer架构的各个组件在数据高效训练的背景下的作用。我们通过对合成数据集和真实数据集的实验验证了我们的理论分析。

    We find limits to the Transformer architecture for language modeling and show it has a universal prediction property in an information-theoretic sense. We further analyze performance in non-asymptotic data regimes to understand the role of various components of the Transformer architecture, especially in the context of data-efficient training. We validate our theoretical analysis with experiments on both synthetic and real datasets.
    
[^16]: 使用双通道卷积神经网络进行推荐系统的意见挖掘

    Opinion mining using Double Channel CNN for Recommender System. (arXiv:2307.07798v1 [cs.IR])

    [http://arxiv.org/abs/2307.07798](http://arxiv.org/abs/2307.07798)

    本文提出了一种使用深度学习模型进行情感分析的方法，通过双通道卷积神经网络进行意见挖掘，并用于推荐产品。通过应用SMOTE算法增加评论数量并对数据进行平衡，使用张量分解算法为聚类分配权重，提高了推荐系统的性能。

    

    随着互联网和社交媒体的发展，产生了大量的非结构化数据。这些数据中包括用户对在线商店和社交媒体上产品的意见。通过对这些意见进行探索和分类，可以获取有用的信息，包括用户满意度、用户对特定事件的反馈、预测特定产品的销售情况等。在本文中，我们提出了一种使用深度学习模型进行情感分析并用于推荐产品的方法。我们使用了一个具有五层的双通道卷积神经网络模型进行意见挖掘，该模型从数据中提取了重要的特征。我们通过应用SMOTE算法来增加初始数据集的评论数量，并对数据进行平衡。然后我们对这些评论进行了聚类。我们还使用张量分解算法为每个聚类分配了一个权重，从而提高了推荐系统的性能。我们提出的方法已经在实验中取得了很好的结果。

    Much unstructured data has been produced with the growth of the Internet and social media. A significant volume of textual data includes users' opinions about products in online stores and social media. By exploring and categorizing them, helpful information can be acquired, including customer satisfaction, user feedback about a particular event, predicting the sale of a specific product, and other similar cases. In this paper, we present an approach for sentiment analysis with a deep learning model and use it to recommend products. A two-channel convolutional neural network model has been used for opinion mining, which has five layers and extracts essential features from the data. We increased the number of comments by applying the SMOTE algorithm to the initial dataset and balanced the data. Then we proceed to cluster the aspects. We also assign a weight to each cluster using tensor decomposition algorithms that improve the recommender system's performance. Our proposed method has re
    
[^17]: 使用CNN-LSTM模型对波斯推特的政治情感进行分析

    Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model. (arXiv:2307.07740v1 [cs.CL])

    [http://arxiv.org/abs/2307.07740](http://arxiv.org/abs/2307.07740)

    本论文使用CNN-LSTM模型对波斯推特的政治情感进行分析，使用ParsBERT进行词汇表示，并比较了机器学习和深度学习模型的效果。实验结果表明，深度学习模型表现更好，其中CNN-LSTM模型在两个数据集上分别达到了89%和71%的分类准确率。

    

    情感分析是识别和分类人们对各种话题的情感或观点的过程。近年来，对Twitter情感的分析成为一个越来越受欢迎的话题。在本文中，我们提出了几种机器学习和深度学习模型，用于分析波斯政治推特的情感。我们使用词袋模型和ParsBERT进行词汇表示的分析。我们应用了高斯朴素贝叶斯、梯度提升、逻辑回归、决策树、随机森林以及CNN和LSTM的组合来分类推特的极性。本研究的结果表明，使用ParsBERT嵌入的深度学习模型比机器学习表现更好。CNN-LSTM模型在第一个有三种类别的数据集上的分类准确率为89％，在第二个有七种类别的数据集上的分类准确率为71％。由于波斯语的复杂性，达到这一效率水平是一项困难的任务。

    Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. The analysis of Twitter sentiment has become an increasingly popular topic in recent years. In this paper, we present several machine learning and a deep learning model to analysis sentiment of Persian political tweets. Our analysis was conducted using Bag of Words and ParsBERT for word representation. We applied Gaussian Naive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random Forests, as well as a combination of CNN and LSTM to classify the polarities of tweets. The results of this study indicate that deep learning with ParsBERT embedding performs better than machine learning. The CNN-LSTM model had the highest classification accuracy with 89 percent on the first dataset with three classes and 71 percent on the second dataset with seven classes. Due to the complexity of Persian, it was a difficult task to achieve this level of efficiency.
    
[^18]: CPET: 高效压缩大型语言模型的参数优化

    CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])

    [http://arxiv.org/abs/2307.07705](http://arxiv.org/abs/2307.07705)

    CPET提出了一种基于压缩LLM的有效参数优化框架，通过引入知识继承和恢复策略，解决了在参数有效调整中压缩LLM的推理计算瓶颈问题。

    

    近年来，参数有效调整（PET）因为在调整相对较少的参数（PET模块）的同时仍能激活大型语言模型（LLM）的足够知识以用于下游任务而得到广泛研究。此外，当PET用于为多个任务提供服务时，可以在冻结的LLM上构建不同的任务特定PET模块，避免冗余LLM部署。虽然PET显著降低了调优和部署LLM的成本，但其推理仍然受到LLM计算瓶颈的影响。为了解决上述问题，我们提出了一种基于压缩LLM的有效PET框架，称为“CPET”。在CPET中，我们评估了主流LLM压缩技术对PET性能的影响，然后引入了知识继承和恢复策略来恢复由这些压缩技术引起的知识丢失。我们的实验结果表明，由于CPET的恢复策略，合作

    Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named "CPET". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating
    
[^19]: 利用大型语言模型生成答案集程序

    Leveraging Large Language Models to Generate Answer Set Programs. (arXiv:2307.07699v1 [cs.AI])

    [http://arxiv.org/abs/2307.07699](http://arxiv.org/abs/2307.07699)

    本文提出了一种神经符号方法，将大型语言模型和答案集编程的优势相结合，通过使用大型语言模型将自然语言描述转化为答案集程序，以实现处理复杂推理问题的能力。

    

    大型语言模型（LLMs），如GPT-3和GPT-4，在各种自然语言处理任务中展现了出色的性能，并显示出解决某些推理问题的能力。然而，尽管采用了各种提示技术，它们的推理能力有限且相对浅显。相反，形式逻辑擅长处理复杂推理，但将自然语言描述转化为形式逻辑是一个非专家难以应对的挑战。本文提出了一种神经符号方法，它结合了大型语言模型和答案集编程的优势。具体而言，我们使用一个LLM将逻辑谜题的自然语言描述转化为答案集程序。我们精心设计了LLM的提示，以逐步将自然语言描述转化为答案集程序。令人惊讶的是，仅仅通过几个上下文学习示例，LLMs就能生成相当复杂的答案集。

    Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer se
    
[^20]: Think-on-Graph: 利用知识图谱进行大型语言模型的深度和负责任的推理

    Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])

    [http://arxiv.org/abs/2307.07697](http://arxiv.org/abs/2307.07697)

    Think-on-Graph是一个利用知识图谱增强大型语言模型深度和负责任推理能力的新框架，在复杂的多跳推理问答任务上表现出色，解决了现有方法中存在的限制。

    

    大型语言模型（LLMs）在各种任务中取得了重大进展，但在需要知识追溯性、及时性和准确性至关重要的场景中，它们经常在复杂推理和表现方面遇到困难。为了解决这些限制，我们提出了Think-on-Graph（ToG），这是一个利用知识图谱增强LLMs深度和负责任推理能力的新框架。通过使用ToG，我们可以确定与给定问题相关的实体，并对外部知识数据库进行探索和推理，以检索相关三元组。这个迭代过程生成包含顺序连接的三元组的多个推理路径，直到收集到足够的信息来回答问题或达到最大深度为止。通过在复杂的多跳推理问答任务上的实验证明，ToG优于现有方法，有效地解决了LLMs的前述限制。

    Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring 
    
[^21]: 将大型语言模型与逻辑编程相结合，实现文本的强大和通用推理

    Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])

    [http://arxiv.org/abs/2307.07696](http://arxiv.org/abs/2307.07696)

    这项研究将大型语言模型与逻辑编程相结合，通过将自然语言句子转换为逻辑形式，使模型能够进行强大且通用的推理。该方法只需要少量的示例和可重用的知识模块，即可在多个问答任务中取得最先进的性能。

    

    尽管像GPT-3这样的大型语言模型在鲁棒性和通用性方面表现出色，但它们的推理能力还不能与针对特定自然语言推理问题训练的最佳模型相竞争。本研究观察到，大型语言模型可以作为一种高效的少示例语义解析器。它可以将自然语言句子转换为逻辑形式，作为答案集程序的输入，该程序是一种基于逻辑的声明性知识表示形式。这种组合结果形成了一个强大而通用的系统，可以处理多个问答任务，无需为每个新任务重新训练。它只需要少量示例来指导LLM适应特定任务，以及可应用于多个任务的可重用ASP知识模块。我们证明这种方法在几个NLP基准测试中取得了最先进的性能，包括bAbI、StepGame、CLUTRR和gSCAN。此外，它还成功解决了机器人规划问题。

    While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot pla
    
[^22]: 单声道和多声道克隆声音检测：从感知到学习特征

    Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features. (arXiv:2307.07683v1 [cs.SD])

    [http://arxiv.org/abs/2307.07683](http://arxiv.org/abs/2307.07683)

    本论文介绍了三种区分真实声音和试图冒充特定人物声音的克隆声音的技术，分别采用不同的特征提取方法，并展示了在单个和多个声音上训练时的有效性。学习特征具有较高的准确性并且对对抗性清洗具有相当的鲁棒性。

    

    近年来，合成语音克隆技术取得了显著进展，引发了一系列潜在的危害。从小规模和大规模的金融欺诈到虚假信息传播活动，需要可靠的方法来区分真实和合成声音是至关重要的。我们描述了三种区分真实声音和试图冒充特定人物声音的克隆声音的技术。这三种方法在特征提取阶段上有所不同，低维感知特征具有较高的可解释性但准确性较低，而通用的频谱特征和端到端学习特征具有较高的准确性但可解释性较低。我们展示了这些方法在单个说话人的声音上训练和在多个声音上训练时的有效性。学习特征始终保持着$0\%$至$4\%$之间的等错误率，并且对对抗性清洗具有相当的鲁棒性。

    Synthetic-voice cloning technologies have seen significant advances in recent years, giving rise to a range of potential harms. From small- and large-scale financial fraud to disinformation campaigns, the need for reliable methods to differentiate real and synthesized voices is imperative. We describe three techniques for differentiating a real from a cloned voice designed to impersonate a specific person. These three approaches differ in their feature extraction stage with low-dimensional perceptual features offering high interpretability but lower accuracy, to generic spectral features, and end-to-end learned features offering less interpretability but higher accuracy. We show the efficacy of these approaches when trained on a single speaker's voice and when trained on multiple voices. The learned features consistently yield an equal error rate between $0\%$ and $4\%$, and are reasonably robust to adversarial laundering.
    
[^23]: 美国餐厅评论和大型语言模型中的移民美食他者化和低声望构架

    Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])

    [http://arxiv.org/abs/2307.07645](http://arxiv.org/abs/2307.07645)

    通过对2.1M英语Yelp评论的餐厅进行语言分析，研究发现移民美食更容易被构架为客观和他者化，而非西方移民美食受欢迎程度更高。

    

    识别和理解对食物的隐含态度有助于减轻因食物作为文化和种族身份的标志而导致的社会偏见。对食物的刻板印象是一种微侵略，它对有害的公共话语做出了贡献，这可能反过来加深对民族群体的偏见，并对餐馆的经济结果产生负面影响。通过仔细的语言分析，我们在一项大规模研究中评估了对移民美食态度的社会理论。该研究使用了2.1M英语Yelp评论的餐厅在14个美国州的框架差异。在控制了餐厅价格和邻里种族多样性等因素后，我们发现移民美食更有可能以客观和他者化的形式进行构架，如真实性（例如，真实，传统），异国情调（例如，异国，不同）和典型性（例如，典型，通常）。但非西方移民美食（例如，印度，墨西哥）更受欢迎。

    Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive mor
    
[^24]: 实现对讨论论坛帖子紧急性的通用检测

    Towards Generalizable Detection of Urgency of Discussion Forum Posts. (arXiv:2307.07614v1 [cs.LG])

    [http://arxiv.org/abs/2307.07614](http://arxiv.org/abs/2307.07614)

    本研究旨在解决学生在线课程讨论论坛问题的规模扩展难题。通过构建预测模型，自动确定论坛帖子的紧急程度，并提供给教师注意。与之前的工作不同，本研究通过预测7分制紧急程度级别，实现了更细粒度的预测。通过在大规模数据集上的训练和测试，证明了模型的通用性。

    

    参与在线课程（如MOOC）的学生会使用课程的讨论论坛，在遇到问题时向教师提问或寻求帮助。然而，由于需要考虑每条消息的时间，阅读和回复学生的问题难以扩展。结果，一些关键问题可能得不到解决，学生可能失去继续学习的动力。为了解决这个问题，我们建立了能够自动确定每个论坛帖子紧急性的预测模型，以便可以将这些帖子带给教师的注意。本文通过预测不仅二进制决策切割点，而且还预测了帖子在7分制紧急程度上的级别，从而超越了先前的工作。首先，我们在宾夕法尼亚大学的MOOCs的3503条原始数据集上训练和交叉验证了多个模型。其次，为了确定我们模型的通用性，我们在一个29,604条帖子的先前发表的数据集上测试了它们的性能。

    Students who take an online course, such as a MOOC, use the course's discussion forum to ask questions or reach out to instructors when encountering an issue. However, reading and responding to students' questions is difficult to scale because of the time needed to consider each message. As a result, critical issues may be left unresolved, and students may lose the motivation to continue in the course. To help address this problem, we build predictive models that automatically determine the urgency of each forum post, so that these posts can be brought to instructors' attention. This paper goes beyond previous work by predicting not just a binary decision cut-off but a post's level of urgency on a 7-point scale. First, we train and cross-validate several models on an original data set of 3,503 posts from MOOCs at University of Pennsylvania. Second, to determine the generalizability of our models, we test their performance on a separate, previously published data set of 29,604 posts fro
    
[^25]: QontSum: 对比突出查询重点的摘要生成

    QontSum: On Contrasting Salient Content for Query-focused Summarization. (arXiv:2307.07586v1 [cs.CL])

    [http://arxiv.org/abs/2307.07586](http://arxiv.org/abs/2307.07586)

    本文针对查询重点摘要 (QFS) 提出了一种名为 QontSum 的新方法。该方法利用对比学习帮助模型集中注意力于输入文档中最相关的区域，并在多个基准数据集上展示了优于现有方法或在减少计算成本的同时表现出可比较性能的结果。

    

    查询重点摘要 (QFS) 是自然语言处理中一项具有挑战性的任务，它生成以解决特定查询为目的的摘要。更广泛的生成式信息检索 (Gen-IR) 领域旨在通过生成式方法改变从庞大的文档语料库中提取信息的方式，包括生成式文档检索 (GDR) 和基础答案检索 (GAR)。本文强调了 QFS 在基础答案生成 (GAR) 中的作用，这是 Gen-IR 的一个关键子领域，它产生与查询直接对应、以相关文档为基础的可读性答案。在本研究中，我们提出了 QontSum，一种新颖的 QFS 方法，它利用对比学习帮助模型集中注意力于输入文档中最相关的区域。我们在一些 QFS 的基准数据集上评估了我们的方法，并证明它要么优于现有的最先进方法，要么在显著减少计算成本的同时表现出可比较的性能。

    Query-focused summarization (QFS) is a challenging task in natural language processing that generates summaries to address specific queries. The broader field of Generative Information Retrieval (Gen-IR) aims to revolutionize information extraction from vast document corpora through generative approaches, encompassing Generative Document Retrieval (GDR) and Grounded Answer Retrieval (GAR). This paper highlights the role of QFS in Grounded Answer Generation (GAR), a key subdomain of Gen-IR that produces human-readable answers in direct correspondence with queries, grounded in relevant documents. In this study, we propose QontSum, a novel approach for QFS that leverages contrastive learning to help the model attend to the most relevant regions of the input document. We evaluate our approach on a couple of benchmark datasets for QFS and demonstrate that it either outperforms existing state-of-the-art or exhibits a comparable performance with considerably reduced computational cost through
    
[^26]: 通过 Twitter 分析探索长期 COVID 患者的情绪和心理健康状况

    Exploring the Emotional and Mental Well-Being of Individuals with Long COVID Through Twitter Analysis. (arXiv:2307.07558v1 [cs.SI])

    [http://arxiv.org/abs/2307.07558](http://arxiv.org/abs/2307.07558)

    通过分析 Twitter，深入了解长期 COVID 患者的情绪和心理健康状况，发现负面情绪主导，并与关键时期相关。这对制定应对长期 COVID 患者心理健康挑战的政策和措施具有重要意义。

    

    COVID-19 疫情导致了长期 COVID，这是在感染后持续存在的一系列症状。长期 COVID 患者可能还会面临心理健康挑战，因此了解个体的情绪和心理健康状况至关重要。本研究旨在深入了解长期 COVID 患者的情绪和心理健康状况，确定他们最关注的主题，并探索他们的情绪与社交媒体活动之间的潜在关联。具体来说，我们根据内容将推文分类为四个类别，检测其中的六种基本情绪，并提取常见主题。我们的分析结果表明，在整个研究期间，负面情绪占主导地位，并在关键时期（如新冠变种爆发）达到两个高峰。本研究的发现对制定政策和措施以应对长期 COVID 患者的心理健康挑战具有重要意义，并为未来的工作奠定了基础。

    The COVID-19 pandemic has led to the emergence of Long COVID, a cluster of symptoms that persist after infection. Long COVID patients may also experience mental health challenges, making it essential to understand individuals' emotional and mental well-being. This study aims to gain a deeper understanding of Long COVID individuals' emotional and mental well-being, identify the topics that most concern them, and explore potential correlations between their emotions and social media activity. Specifically, we classify tweets into four categories based on the content, detect the presence of six basic emotions, and extract prevalent topics. Our analyses reveal that negative emotions dominated throughout the study period, with two peaks during critical periods, such as the outbreak of new COVID variants. The findings of this study have implications for policy and measures for addressing the mental health challenges of individuals with Long COVID and provide a foundation for future work.
    
[^27]: 一个用于评估日常生活活动的对话系统：通过扎根知识提高一致性

    A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge. (arXiv:2307.07544v1 [cs.CL])

    [http://arxiv.org/abs/2307.07544](http://arxiv.org/abs/2307.07544)

    这个论文介绍了一个用于评估日常生活活动的对话系统，通过模拟评估员和参与者之间的交互，提高了评估的一致性。

    

    在医疗保健中，自我照顾能力体现在“日常生活活动（ADL）”中，ADL作为功能能力（运作能力）的衡量。功能能力不足可能导致需要个人护理和协助的恶劣生活条件。为了准确识别需要支持的人，协助计划会持续评估参与者在各个领域的功能能力。然而，当涉及多个具有不同水平的专家评估员时，评估过程可能遇到一致性问题。尤其是初学者评估员可能缺乏与参与者进行实际互动所需的必要准备。为了解决这个问题，我们开发了一个对话系统，以自然且可重现的方式模拟评估员和具有不同功能能力的个体之间的交互。对话系统由两个主要模块组成，一个用于自然语言理解（NLU），一个用于自然语言生成（NLG）。

    In healthcare, the ability to care for oneself is reflected in the "Activities of Daily Living (ADL)," which serve as a measure of functional ability (functioning). A lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need of support, assistance programs continuously evaluate participants' functioning across various domains. However, the assessment process may encounter consistency issues when multiple assessors with varying levels of expertise are involved. Novice assessors, in particular, may lack the necessary preparation for real-world interactions with participants. To address this issue, we developed a dialogue system that simulates interactions between assessors and individuals of varying functioning in a natural and reproducible way. The dialogue system consists of two major modules, one for natural language understanding (NLU) and one for natural language generation (NLG), respectively. In order to gen
    
[^28]: PapagAI：反思性文章的自动反馈

    PapagAI:Automated Feedback for Reflective Essays. (arXiv:2307.07523v1 [cs.AI])

    [http://arxiv.org/abs/2307.07523](http://arxiv.org/abs/2307.07523)

    PapagAI是第一个基于教学理论并实现为混合AI系统的开源自动反馈工具，旨在提高学生学习成果并补充讲师的教学活动。

    

    在高等教育期间，撰写反思性实践是预备教师进行的常规练习。通常情况下，他们的讲师需要提供个别反馈，这可能是一项常规任务。本文提出了第一个基于教学理论并实现为混合AI系统的开源自动反馈工具。我们描述了组件，并讨论了我们的系统与现有最先进的生成式大型语言模型相比的优势和劣势。我们的工作的主要目标是实现学生的更好学习结果，并补充讲师的教学活动。

    Written reflective practice is a regular exercise pre-service teachers perform during their higher education. Usually, their lecturers are expected to provide individual feedback, which can be a challenging task to perform on a regular basis. In this paper, we present the first open-source automated feedback tool based on didactic theory and implemented as a hybrid AI system. We describe the components and discuss the advantages and disadvantages of our system compared to the state-of-art generative large language models. The main objective of our work is to enable better learning outcomes for students and to complement the teaching activities of lecturers.
    
[^29]: 用人工智能翻译拉丁文

    Translating Latin with Artificial Intelligence. (arXiv:2307.07520v1 [math.HO])

    [http://arxiv.org/abs/2307.07520](http://arxiv.org/abs/2307.07520)

    用人工智能翻译工具可以解决研究早期科学文献的拉丁文翻译难题，通过基准测试发现ChatGPT算法在表现上更出色，并将其应用于约翰·伯努利给欧拉的一封信的翻译。

    

    研究早期科学文献的主要障碍是拉丁文作品的现代语言翻译的可获得性。这尤其适用于欧拉的作品，他撰写了约850份手稿，写了一千封信并收到了近两千封回信。这些手稿、书籍和信件的翻译已经在过去两个世纪中发布在各种来源中，但还有许多尚未出版。幸运的是，现如今可以利用人工智能翻译技术来解决翻译如此大量文本的挑战。为了验证这一工具，进行了基准测试以比较两种流行的人工智能翻译算法，即谷歌翻译和ChatGPT的性能。由于发现ChatGPT在这些测试中表现更好，因此该翻译支持工具随后被用于将约翰·伯努利在1739年写给欧拉的一封信的摘录进行翻译。

    The major hindrance in the study of earlier scientific literature is the availability of Latin translations into modern languages. This is particular true for the works of Euler who authored about 850 manuscripts and wrote a thousand letters and received back almost two thousand more. The translation of many of these manuscripts, books and letters have been published in various sources over the last two centuries, but many more have not yet appeared. Fortunately, nowadays, the artificial intelligence AI translation can be used to circumvent the challenges of translating such substantial number of texts. To validate this tool, benchmark tests have been performed to compare the performance of two popular AI translating algorithms, namely Google Translate and ChatGPT. Since it was found that ChatGPT performed better on these tests, this translating support was then used on an excerpt of a 1739 letter from Johann Bernoulli to Euler, where he notifies that he was sending to Euler the first 
    
[^30]: CephGPT-4:一种具有视觉大语言模型的交互式多模态颅颌测量与诊断系统

    CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model. (arXiv:2307.07518v1 [cs.AI])

    [http://arxiv.org/abs/2307.07518](http://arxiv.org/abs/2307.07518)

    CephGPT-4是一个具有视觉大语言模型的交互式多模态颅颌测量与诊断系统，它能够通过自动分析颅颌标志点和生成诊断报告，实现出色的性能，为正畸测量和诊断应用带来革命性的潜力。

    

    大规模多模态语言模型在一般领域取得了显著的成功。然而，在基于多模态颅颌医学数据的诊断语言模型的探索仍然有限。本文提出了一种新颖的多模态颅颌分析和诊断对话模型。首先，构建了多模态正畸医学数据集，包括颅颌影像和医患对话数据，并使用U-net自动分析颅颌标志点和生成诊断报告。然后，将颅颌数据集和生成的诊断报告分别在Minigpt-4和VisualGLM上进行微调。结果表明，CephGPT-4模型表现出优异的性能，并具有颠覆正畸测量和诊断应用的潜力。这些创新在正畸学领域具有革命性的应用潜力。

    Large-scale multimodal language models (LMMs) have achieved remarkable success in general domains. However, the exploration of diagnostic language models based on multimodal cephalometric medical data remains limited. In this paper, we propose a novel multimodal cephalometric analysis and diagnostic dialogue model. Firstly, a multimodal orthodontic medical dataset is constructed, comprising cephalometric images and doctor-patient dialogue data, with automatic analysis of cephalometric landmarks using U-net and generation of diagnostic reports. Then, the cephalometric dataset and generated diagnostic reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results demonstrate that the CephGPT-4 model exhibits excellent performance and has the potential to revolutionize orthodontic measurement and diagnostic applications. These innovations hold revolutionary application potential in the field of orthodontics.
    
[^31]: 基于投票的多模态自动欺骗检测

    Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])

    [http://arxiv.org/abs/2307.07516](http://arxiv.org/abs/2307.07516)

    本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。

    

    自动欺骗检测一直是一个热门的研究课题，利用机器学习和深度学习自动检测欺骗给这一旧领域带来了新的光明。在本文中，我们提出了一种基于投票的方法，用于从视频中使用音频、视觉和文本特征进行自动欺骗检测。我们在两个数据集上进行了实验，分别是密歇根大学的真实试验数据集和迈阿密大学的欺骗检测数据集。视频样本被分成图像、音频和手稿的帧。我们提出的多模态投票解决方案包括三个模型。第一个模型是用于从图像中检测欺骗的卷积神经网络（CNN），第二个模型是用于从音频中检测欺骗的Mel频谱图上的支持向量机（SVM），第三个模型是用于从手稿中检测欺骗的支持向量机（SVM）上的Word2Vec。我们提出的解决方案优于现有技术水平。在图像、音频和文本上取得的最佳结果分别为97％、96％、9

    Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
    
[^32]: 使用放射学报告和图像改善ICU死亡率预测的实证研究

    An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])

    [http://arxiv.org/abs/2307.07513](http://arxiv.org/abs/2307.07513)

    本研究利用放射学报告和图像构建了一个基于深度学习的多模态数据生存预测模型，用于预测重症监护病房（ICU）的死亡率，并在MIMIC-IV数据集上取得了0.7829的平均C-index。

    

    背景：预测重症监护病房（ICU）的评分系统在ICU管理中起着重要作用，因为它能预测重要的结果，尤其是死亡率。许多评分系统已经在ICU中开发和使用。这些评分系统主要基于电子健康记录（EHR）中的结构化临床数据，但可能会丧失叙述和图像中的重要临床信息。方法：在这项工作中，我们利用多模态数据建立了一个基于深度学习的生存预测模型来预测ICU死亡率。我们调查了四组特征：（1）简化急性生理学评分（SAPS）II的生理测量、（2）放射专家预定义的常见胸部疾病、（3）基于BERT的文本表示和（4）胸部X射线图像特征。我们使用Medical Information Mart for Intensive Care IV（MIMIC-IV）数据集评估了提出的模型。结果：我们的模型达到了0.7829的平均C-index（95%的置信区间）

    Background: The predictive Intensive Care Unit (ICU) scoring system plays an important role in ICU management because it predicts important outcomes, especially mortality. Many scoring systems have been developed and used in the ICU. These scoring systems are primarily based on the structured clinical data in the electronic health record (EHR), which may suffer the loss of important clinical information in the narratives and images. Methods: In this work, we build a deep learning based survival prediction model with multi-modality data to predict ICU mortality. Four sets of features are investigated: (1) physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2) common thorax diseases pre-defined by radiologists, (3) BERT-based text representations, and (4) chest X-ray image features. We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the proposed model. Results: Our model achieves the average C-index of 0.7829 (95% confidence i
    
[^33]: RoPDA：用于低资源命名实体识别的鲁棒基于提示的数据增强

    RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])

    [http://arxiv.org/abs/2307.07417](http://arxiv.org/abs/2307.07417)

    RoPDA是一种用于低资源NER的数据增强方法，通过基于预训练语言模型和连续提示进行实体和上下文增强，并提出了自一致性过滤和混合技术以优化增强样本的利用。

    

    数据增强在低资源NER任务中被广泛使用以解决数据稀缺的问题。然而，先前的数据增强方法存在破坏句法结构、标记-标签不匹配和对外部知识或手动工作的需求的缺点。为了解决这些问题，我们提出了RoPDA: 一种用于低资源NER的鲁棒基于提示的数据增强方法。基于预训练语言模型（PLMs）和连续提示，RoPDA通过五个基本的增强操作进行实体增强和上下文增强，生成标签翻转和保留标签的样本。为了优化增强样本的利用，我们提出了两种技术：自一致性过滤和混合。前者有效地消除低质量样本，后者防止直接利用标签翻转样本导致性能下降。在三个基准测试中进行了大量实验...

    Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata \textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe
    
[^34]: Parmesan：教育中的数学概念提取

    Parmesan: mathematical concept extraction for education. (arXiv:2307.06699v1 [cs.CL])

    [http://arxiv.org/abs/2307.06699](http://arxiv.org/abs/2307.06699)

    Parmesan是一个原型系统，用于在上下文中搜索和定义数学概念，特别关注范畴论领域。该系统利用自然语言处理组件进行概念提取、关系提取、定义提取和实体链接。通过该系统的开发，可以解决现有技术不能直接应用于范畴论领域的问题，并提供了两个数学语料库以支持系统的使用。

    

    数学是一个高度专业化的领域，具有自己独特的挑战，但在自然语言处理领域的研究却有限。然而，数学在许多不同领域的跨学科研究中经常依赖于对数学概念的理解。为了帮助来自其他领域的研究人员，我们开发了一个原型系统，用于在上下文中搜索和定义数学概念，重点关注范畴论领域。这个系统名为Parmesan，依赖于自然语言处理组件，包括概念提取、关系提取、定义提取和实体链接。在开发这个系统的过程中，我们展示了现有技术不能直接应用于范畴论领域，并提出了一种混合技术，这种技术表现良好，但我们预计系统将随着时间的推移而不断演变。我们还提供了两个清理过的数学语料库，用于支持原型系统，这些语料库基于期刊文章。

    Mathematics is a highly specialized domain with its own unique set of challenges that has seen limited study in natural language processing. However, mathematics is used in a wide variety of fields and multidisciplinary research in many different domains often relies on an understanding of mathematical concepts. To aid researchers coming from other fields, we develop a prototype system for searching for and defining mathematical concepts in context, focusing on the field of category theory. This system, Parmesan, depends on natural language processing components including concept extraction, relation extraction, definition extraction, and entity linking. In developing this system, we show that existing techniques cannot be applied directly to the category theory domain, and suggest hybrid techniques that do perform well, though we expect the system to evolve over time. We also provide two cleaned mathematical corpora that power the prototype system, which are based on journal articles 
    
[^35]: 超越本地范围：全球图增强个性化新闻推荐

    Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])

    [http://arxiv.org/abs/2307.06576](http://arxiv.org/abs/2307.06576)

    本文介绍了一种名为GLORY的模型，通过全局图与本地表示相结合，增强了个性化推荐系统。该模型通过构建全局感知历史新闻编码器来融合历史新闻表示，并考虑了用户隐藏的动机和行为。

    

    精确地向用户推荐候选新闻文章一直是个性化新闻推荐系统的核心挑战。大多数近期的研究主要集中在使用先进的自然语言处理技术从丰富的文本数据中提取语义信息，使用从本地历史新闻派生的基于内容的方法。然而，这种方法缺乏全局视角，未能考虑用户隐藏的动机和行为，超越语义信息。为了解决这个问题，我们提出了一种新颖的模型 GLORY（Global-LOcal news Recommendation sYstem），它结合了从其他用户学到的全局表示和本地表示，来增强个性化推荐系统。我们通过构建一个全局感知历史新闻编码器来实现这一目标，其中包括一个全局新闻图，并使用门控图神经网络来丰富新闻表示，从而通过历史新闻聚合器融合历史新闻表示。

    Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.
    
[^36]: 揭开巨人的真面目：对ChatGPT在编码算法和数据结构方面的熟练程度进行全面评估

    Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])

    [http://arxiv.org/abs/2307.05360](http://arxiv.org/abs/2307.05360)

    本文全面评估了ChatGPT在编码算法和数据结构方面的能力，基于最大的编码挑战目录，重点关注Python编程语言和数据结构算法两个基础主题。总结测试中ChatGPT的代码解决问题的准确性、代码质量和运行时错误的性质。

    

    大型语言模型(LLMs)的转变性影响深刻地重塑了人工智能(AI)技术领域。值得注意的是，ChatGPT在这些模型中有着独特之处，展示出卓越的多轮对话性能，并在多种语言中展示出对编码的熟练程度。在本文中，我们根据迄今为止最大的编码挑战目录对ChatGPT的编码能力进行了全面评估。我们的重点是Python编程语言，以及集中在数据结构和算法上的问题，这两个主题是计算机科学的基础。我们评估ChatGPT解决所提交问题的能力，评估其代码质量以及代码引发的运行时错误的性质。当ChatGPT的代码成功执行但未能解决手头问题时，我们会研究通过的测试案例中的模式，以了解ChatGPT代码中的错误之处。

    The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
    
[^37]: ChatGPT在生成式AI和大语言模型时代的应用：一份简洁的调查报告

    ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])

    [http://arxiv.org/abs/2307.04251](http://arxiv.org/abs/2307.04251)

    ChatGPT是由OpenAI创建的一种大型语言模型（LLM），它在自然语言处理（NLP）领域引起了革命性的变革。它是第一个实现公众大规模与生成式人工智能（GAI）互动的关键技术，也引发了类似技术的研究兴趣和应用探索。

    

    ChatGPT是由OpenAI创建的一种大型语言模型（LLM），经过精心训练并使用了大量数据。它在自然语言处理（NLP）领域引起了革命性的变革，并推动了LLM能力的边界。ChatGPT在大规模范围内实现了普遍公众与生成式人工智能（GAI）的互动，起到了关键作用。它还引发了开发类似技术和研究其应用和影响的兴趣。本文的主要目标是对ChatGPT及其演化的当前研究方向进行简明调查。我们同时考虑了ChatGPT的玻璃盒和黑盒视角，包括技术的组成部分和基本要素，以及其应用、影响和影响。玻璃盒方法着重于理解技术的内部运作，而黑盒方法将其视为一个复杂系统，因此研究其输入，

    ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
    
[^38]: 关于仅解码器架构在语音到文本和大型语言模型集成中的应用

    On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])

    [http://arxiv.org/abs/2307.03917](http://arxiv.org/abs/2307.03917)

    该论文介绍了一种新颖的方法Speech-LLaMA，将声学信息有效地融入基于文本的大型语言模型中。通过使用连接主义时序分类和简单的音频编码器，将压缩的声学特征映射到大型语言模型的连续语义空间中，实现了语音到文本任务中的实质性提升。

    

    大型语言模型在自然语言处理领域取得了显著的成功，能够使用自然语言实现更好的人机交互。然而，如何将语音信号无缝地集成到大型语言模型中尚未得到很好的探索。同时，关于语音处理任务的“仅解码器”架构也没有得到很好的研究。在这项研究中，我们介绍了Speech-LLaMA，一种新颖的方法，有效地将声学信息融入基于文本的大型语言模型中。我们的方法利用了连接主义时序分类和简单的音频编码器，将压缩的声学特征映射到大型语言模型的连续语义空间中。此外，我们进一步探索了仅解码器架构在语音到文本任务中的应用，通过仅使用语音-文本配对数据训练一个较小规模、随机初始化的Speech-LLaMA模型。我们在多语言语音到文本翻译任务上进行了实验，证明与强基准相比有明显的改进。

    Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The "decoder-only" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baseli
    
[^39]: 使用Transformer模型预测表情符号

    Emoji Prediction using Transformer Models. (arXiv:2307.02054v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02054](http://arxiv.org/abs/2307.02054)

    使用基于Transformer的方法，在大型语料库上微调BERT模型以预测给定文本的表情符号。实验结果显示，该方法在预测准确率上优于其他最先进的模型，具有潜在的自然语言处理和社交媒体营销应用价值。

    

    近年来，社交媒体中使用表情符号的频率大幅增加，使得它们成为了理解在线沟通的重要元素。然而，由于其含糊的特性，预测给定文本中表情符号的含义是一项具有挑战性的任务。在本研究中，我们提出了一种基于Transformer的方法来使用BERT进行表情符号预测，BERT是一种广泛使用的预训练语言模型。我们在一个包含文本和表情符号的大型语料库上对BERT进行微调，以预测给定文本的最合适的表情符号。我们的实验结果表明，我们的方法在预测表情符号方面的准确率超过了75％，优于几种最先进的模型。该研究在自然语言处理、情感分析和社交媒体营销方面具有潜在的应用前景。

    In recent years, the use of emojis in social media has increased dramatically, making them an important element in understanding online communication. However, predicting the meaning of emojis in a given text is a challenging task due to their ambiguous nature. In this study, we propose a transformer-based approach for emoji prediction using BERT, a widely-used pre-trained language model. We fine-tuned BERT on a large corpus of text containing both text and emojis to predict the most appropriate emoji for a given text. Our experimental results demonstrate that our approach outperforms several state-of-the-art models in predicting emojis with an accuracy of over 75 percent. This work has potential applications in natural language processing, sentiment analysis, and social media marketing.
    
[^40]: InstructEval: 系统评估指令选择方法

    InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])

    [http://arxiv.org/abs/2307.00259](http://arxiv.org/abs/2307.00259)

    InstructEval开发了一个评估套件，用于对指令选择方法进行全面评估。通过使用策划的手动编写的指令，可以显著提高性能。

    

    上下文学习 (ICL) 通过使用指令和一小组注释示例来提示一个大型语言模型 (LLM) 来执行任务。最近的工作表明，提示中使用的输入的细节对 ICL 有着重要影响，这激励了指令选择算法的发展。然而，指令选择的影响尚未得到深入探索，现有的分析仅限于模型和任务的浅层子集，这限制了洞察力的普适性。我们开发了一个 ICL 评估套件，以对这些技术进行全面评估。该套件包括来自4个不同模型家族的13个开源LLM，涵盖9个不同的任务，代表了3个分类中各种类型的任务。在本研究中，我们使用我们的基准测试评估了7种受欢迎的指令选择方法相对于ICL相关的五项期望性能。我们发现使用策划的手动编写的指令可以显著地提高性能。

    In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that the precise details of the inputs used in the prompt significantly impacts ICL, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses being restricted to shallow subsets of models and tasks, which limits the generalizability of their insights. We develop an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from 4 distinct model families and covers 9 different tasks, representing a range of task types across 3 categories. In this work, we evaluate the relative performance of 7 popular instruction selection methods using our benchmark over five desiderata relevant to ICL. We discover that using curated manually-written instru
    
[^41]: 用贝叶斯推理模拟人类类人概念学习

    Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02797](http://arxiv.org/abs/2306.02797)

    该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。

    

    我们通过在自然语言中进行贝叶斯推理来模拟对抽象符号概念的学习。为了高效推理，我们使用一个大型语言模型作为提议分布。我们根据人类数据拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行评估。

    We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
    
[^42]: 交通理解的情境推理研究

    A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02520](http://arxiv.org/abs/2306.02520)

    本研究提出了三个新的基于文本的交通领域情境推理任务，旨在评估语言模型在情境决策、事件因果关系推理和解决人类驾驶考试方面的能力。研究采用了四种知识增强方法，具有潜力在不同语言推理任务中实现模型的泛化能力。

    

    智能交通监控(ITMo)技术有潜力改善道路安全/安全性，实现智能城市基础设施。了解交通情况需要将感知信息与领域特定和因果常识知识复杂融合。尽管之前的工作已经为交通监控提供了基准和方法，但模型能否有效地对齐这些信息来源并在新场景中推理仍不清楚。为了解决这种评估差距，我们设计了三个用于交通领域情境推理的新型文本任务：i) BDD-QA，评估语言模型(LMs)执行情境决策的能力，ii) TV-QA，评估LMs推理复杂事件因果关系的能力，iii) HDT-QA，评估模型解决人类驾驶考试的能力。我们采用了之前工作中已经显示出语言推理任务通用性的四种知识增强方法。

    Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work
    
[^43]: 自适应上下文偏置的基于Transducer的流式语音识别

    Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition. (arXiv:2306.00804v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.00804](http://arxiv.org/abs/2306.00804)

    该论文提出了一种自适应上下文偏置方法，基于Context-Aware Transformer Transducer (CATT) 来进行流式语音识别的预测。实验结果表明，与基线相比，该方法在 WER 和 CER 上可以分别减少6.7%和20.7%，减少了96.7%和84.9% 的相对 WER 和 CER 增加。

    

    通过加入额外的上下文信息，深度偏置方法已经成为解决个性化话语识别的有希望的解决方案。然而，在实际的语音助手中，总是将高预测分数的个性化词语进行偏置处理会显著降低对常见词语的识别性能。为了解决这个问题，我们提出了一种基于上下文感知Transformer Transducer（CATT）的自适应上下文偏置方法，该方法利用偏置编码器和预测器嵌入来执行上下文短语出现的流式预测。这种预测然后被用来动态地切换偏置列表的开关，使模型适应个性化和常见情景。在Librispeech和内部语音助手数据集上的实验证明，与基线相比，我们的方法在WER和CER上分别可以达到6.7%和20.7%的相对降低，将相对WER和CER的增加减少到96.7%和84.9%。

    By incorporating additional contextual information, deep biasing methods have emerged as a promising solution for speech recognition of personalized words. However, for real-world voice assistants, always biasing on such personalized words with high prediction scores can significantly degrade the performance of recognizing common words. To address this issue, we propose an adaptive contextual biasing method based on Context-Aware Transformer Transducer (CATT) that utilizes the biased encoder and predictor embeddings to perform streaming prediction of contextual phrase occurrences. Such prediction is then used to dynamically switch the bias list on and off, enabling the model to adapt to both personalized and common scenarios. Experiments on Librispeech and internal voice assistant datasets show that our approach can achieve up to 6.7% and 20.7% relative reduction in WER and CER compared to the baseline respectively, mitigating up to 96.7% and 84.9% of the relative WER and CER increase 
    
[^44]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^45]: 在模拟人类社会中训练社会对齐的语言模型

    Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16960](http://arxiv.org/abs/2305.16960)

    本研究提出了一种在模拟人类社会中训练语言模型的新方法，相比于现有方法，该方法具有更大的可扩展性和高效性，并在对齐基准和人类评估中展示出更优异的性能。

    

    AI系统中的社会对齐旨在确保这些模型按照既定的社会价值行事。然而，与人类不同，人们通过社交互动得出对价值判断的共识，当前的语言模型（LMs）则在孤立地复制其训练语料库时被训练出来，导致在陌生场景中表现不佳，并易受到对抗攻击。本研究提出了一种新的训练范式，允许LMs从模拟的社交互动中学习。与现有方法相比，我们的方法具有更大的可扩展性和高效性，在对齐基准和人类评估中展示出更优异的性能。这种LMs训练中的范式转变使我们离开发能够强有力且准确反映社会规范和价值的AI系统更近了一步。

    Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
    
[^46]: 理解和减少文本分类中的伪相关性

    Understanding and Mitigating Spurious Correlations in Text Classification. (arXiv:2305.13654v1 [cs.CL])

    [http://arxiv.org/abs/2305.13654](http://arxiv.org/abs/2305.13654)

    本文研究了深度学习模型容易利用训练集中存在但通常不成立的伪相关性的问题，并提出了一种邻域分析框架以解释语言模型如何利用伪相关性。通过一系列正则化方法NFL（不要忘记你的语言）避免了这种情况，并在实验中证明了其鲁棒性方面的显著改进。

    

    最近的研究表明，深度学习模型容易利用训练集中存在但通常不成立的伪相关性。例如情感分类器可能会错误地学习到令人愉悦的电影评论总是与“Spielberg”这个词相关联。依赖于伪相关性可能会导致泛化性能显著降低，因此应该避免。本文提出了一种邻域分析框架来解释语言模型如何利用伪相关性。在此基础上，我们提出了一系列正则化方法NFL（不要忘记你的语言），以避免这种情况。在两个文本分类任务上的实验表明，NFL相对于标准的微调算法在鲁棒性方面带来了显著的改进，而没有牺牲在数据内部的准确性。

    Recent work has shown that deep learning models are prone to exploit spurious correlations that are present in the training set, yet may not hold true in general. A sentiment classifier may erroneously learn that the token spielberg is always tied to positive movie reviews. Relying on spurious correlations may lead to significant degradation in generalizability and should be avoided. In this paper, we propose a neighborhood analysis framework to explain how exactly language models exploit spurious correlations. Driven by the analysis, we propose a family of regularization methods, NFL (do Not Forget your Language) to prevent the situation. Experiments on two text classification tasks show that NFL brings a significant improvement over standard fine-tuning in terms of robustness without sacrificing in-distribution accuracy.
    
[^47]: MildTriple Loss模型下的运动和文本跨模态检索

    Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])

    [http://arxiv.org/abs/2305.04195](http://arxiv.org/abs/2305.04195)

    本论文提出了一个创新模型，使用MildTriple Loss捕捉长期依赖并模拟跨模态人类动作序列与文本检索任务，具有重要的应用价值。

    

    跨模态检索已成为计算机视觉和自然语言处理中的重要研究课题，随着图像文本和视频文本检索技术的进步。尽管在虚拟现实等广泛应用中具有重要价值，但人类动作序列与文本之间的跨模态检索尚未引起足够的关注。这个任务存在一些挑战，包括对两种语言的共同建模，要求从文本中理解以人为中心的信息，并从三维人体运动序列中学习行为特征。以往的运动数据建模主要依赖于自回归特征提取器，这可能会遗忘以前的信息，而我们提出了一种创新模型，其中包括简单而强大的基于变换器的运动和文本编码器，可以从两种不同的模态中学习表示并捕捉长期依赖

    Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
    
[^48]: SCOTT: 自我一致性思路串提炼

    SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])

    [http://arxiv.org/abs/2305.01879](http://arxiv.org/abs/2305.01879)

    本研究提出了一种忠实的知识蒸馏方法，从比教师模型大数倍的模型中学习一个小的、自我一致的思路串模型。实验结果表明，该方法有助于证明决策并提高性能，特别是在较小的语言模型中。

    

    超出一定规模的大型语言模型表现出通过一系列连续的思考过程获得自由文本理由的突出能力。虽然思路串可以显著提高性能，但仅在足够大的语言模型中才能观察到这种收益。更令人担忧的是，生成的理由很少保证与语言模型的预测保持一致或者忠实地证明决策。在这项工作中，我们提出了一种忠实的知识蒸馏方法，从比教师模型大数倍的模型中学习一个小的、自我一致的思路串模型。为了形成更好的监督，我们通过对比解码引导教师模型产生支持正确答案的理由，这鼓励教师模型生成的token只在考虑到答案时才更加可信。为了保证忠实的蒸馏，我们使用教师生成的理由来学习一个学生模型，该模型具有反事实推理目标，即根据具有自我一致性且忠实于教师预测的思路串理由预测决策。在自然语言推理和抽象摘要基准测试上，我们证明了学生模型中的自我一致性有助于证明决策并提高性能，特别是在较小的语言模型中。

    Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which pre
    
[^49]: 学习使用要点标记压缩提示语

    Learning to Compress Prompts with Gist Tokens. (arXiv:2304.08467v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08467](http://arxiv.org/abs/2304.08467)

    该论文提出了一种名为"gisting"的方法，通过训练语言模型将提示压缩为更小的"要点"标记集，以提高计算效率。通过这种方法，可以实现高达26倍的提示压缩，减少40％的FLOPs、4.2％的墙时速度提升，并节省存储空间，同时最小化输出质量损失。

    

    提示是利用语言模型的多任务能力的主要方式，但是提示占据了输入上下文窗口中宝贵的空间，重复编码相同的提示在计算上是低效的。微调和蒸馏方法可以实现语言模型的专门化，但需要为每个任务重新训练模型。为了完全避免这种权衡，我们提出了gist，它训练一个语言模型将提示压缩成更小的“要点”标记集，可以用于计算效率的缓存和重用。通过简单地修改Transformer的注意力掩码，可以在没有额外成本的情况下对gist模型进行训练，从而实现对提示的高达26倍的压缩，从而减少高达40％的FLOPs、4.2％的墙时速度提升，并节省存储空间，同时最小化输出质量损失。

    Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of "gist" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.
    
[^50]: 非自回归机器翻译中有向无环图的模糊对齐

    Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation. (arXiv:2303.06662v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.06662](http://arxiv.org/abs/2303.06662)

    本文提出了一种在非自回归机器翻译中通过模糊对齐解决多模态问题的方法，通过最大化图与参考之间的模糊对齐得分来训练模型，实验证明了方法显著提高了翻译性能，并增加了预测置信度，创造了非自回归机器翻译的新的state of the art。

    

    非自回归翻译（NAT）通过引入顶点间的依赖关系，解决了多模态问题，但由于与参考标记和顶点之间需要严格对齐，严重削弱了处理多种翻译模式的能力。本文认为图中的所有路径都与参考句子模糊对齐，不需要精确对齐，而是训练模型以最大化图与参考之间的模糊对齐得分，考虑到所有模态中捕捉到的翻译。在主要的WMT基准测试上进行了广泛实验，结果显示我们的方法显著提高了翻译性能，并增加了预测置信度，创造了NA的新的state of the art。

    Non-autoregressive translation (NAT) reduces the decoding latency but suffers from performance degradation due to the multi-modality problem. Recently, the structure of directed acyclic graph has achieved great success in NAT, which tackles the multi-modality problem by introducing dependency between vertices. However, training it with negative log-likelihood loss implicitly requires a strict alignment between reference tokens and vertices, weakening its ability to handle multiple translation modalities. In this paper, we hold the view that all paths in the graph are fuzzily aligned with the reference sentence. We do not require the exact alignment but train the model to maximize a fuzzy alignment score between the graph and reference, which takes captured translations in all modalities into account. Extensive experiments on major WMT benchmarks show that our method substantially improves translation performance and increases prediction confidence, setting a new state of the art for NA
    
[^51]: 无梯度结构化剪枝方法与无标签数据

    Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04185](http://arxiv.org/abs/2303.04185)

    本文提出了一种使用无标签数据的无梯度结构化剪枝方法，在GLUE和SQuAD基准测试上的实验证明了其有效性，仅需几分钟就能将原始FLOP计数的最高40%减少而准确度仅下降不超过4%。

    

    大型语言模型在许多领域中解决困难任务方面取得了巨大成功，但这种成功伴随着高计算成本和推理延迟。随着开发人员和第三方对这些模型进行个性化定制，提供高效的推理需求也越来越大。许多努力尝试通过剪枝和蒸馏等模型压缩技术来减少推理成本。然而，这些技术要么需要有标签的数据，要么因为需要重新训练压缩模型以恢复准确性而耗时。本文提出了一种仅使用无标签数据的无梯度结构化剪枝框架。使用BERT$_{BASE}$和DistilBERT在GLUE和SQuAD基准测试上的评估结果表明了该方法的有效性。仅使用预训练模型的权重和无标签数据，在单个GPU上仅需几分钟，即可将原始FLOP计数的最高40%减少，准确度下降不超过4%。

    Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accur
    
[^52]: 一种基于神经网络的跨时期命名实体识别模型

    A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12200](http://arxiv.org/abs/2302.12200)

    本文提出了一种基于神经网络的跨时期命名实体识别模型SpanKL，通过知识蒸馏和多标签预测来实现记忆保留和冲突防止，该模型在跨时期NER任务中表现出色，显示出高实际价值。

    

    在实体类型不断增加的领域（例如个人助手）中，具备持续学习能力的命名实体识别模型（NER）具有现实价值。与此同时，NER的学习范式逐渐发展出了新的模式，如基于跨度的方法。然而，跨时期学习在这方面的潜力尚未被充分探索。本文提出了SpanKL，一种简单而有效的基于跨度的模型，利用知识蒸馏（KD）来保留记忆，并采用多标签预测来防止跨时期NER中的冲突。与之前的序列标记方法不同，在SpanKL中，跨度和实体级别的独立建模以及设计的一致优化促进了每个增量步骤的学习，并减轻了遗忘。在从OntoNotes和Few-NERD衍生的合成CL数据集上的实验表明，SpanKL在许多方面显著优于先前的最优结果，并且从CL到上限的差距最小，显示了其高实际价值。

    Named Entity Recognition (NER) models capable of Continual Learning (CL) are realistically valuable in areas where entity types continuously increase (e.g., personal assistants). Meanwhile the learning paradigm of NER advances to new patterns such as the span-based methods. However, its potential to CL has not been fully explored. In this paper, we propose SpanKL, a simple yet effective Span-based model with Knowledge distillation (KD) to preserve memories and multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence labeling approaches, the inherently independent modeling in span and entity level with the designed coherent optimization on SpanKL promotes its learning at each incremental step and mitigates the forgetting. Experiments on synthetic CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly outperforms previous SoTA in many aspects, and obtains the smallest gap from CL to the upper bound revealing its high practiced value. The code i
    
[^53]: 重在亲身经历：自动检测社交媒体对使用物质的人的污名化

    Lived Experience Matters: Automatic Detection of Stigma on Social Media Toward People Who Use Substances. (arXiv:2302.02064v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.02064](http://arxiv.org/abs/2302.02064)

    本文通过分析大量Reddit帖子数据集，探讨了对使用物质的人（PWUS）的污名化。结果表明，具有物质使用经验的工人更有可能评为具有污名化，为此建立了一个以亲身物质使用经验的工人为中心的机器学习框架。

    

    对使用物质的人（PWUS）的污名化是寻求治疗的主要障碍。此外，如果他们遭受更高水平的污名化，接受治疗的人更有可能中途退出。尽管与憎恨言论和有毒性有关的概念已经成为自动内容监管研究的重点，包括那些针对弱势群体的内容，但污名化，特别是对使用物质的人的污名化尚未得到关注。本文利用大约5000个公开的Reddit帖子数据集来探讨对PWUS的污名化。我们进行了一个众包注释任务，要求工人注释每个帖子是否存在对PWUS的污名化，并回答与他们对物质使用经验相关的一系列问题。结果显示，使用物质或认识有物质使用障碍的工人更有可能将帖子评为具有污名化。基于此，我们使用一个有监督的机器学习框架，以亲身物质使用经验的工人为中心进行建模。

    Stigma toward people who use substances (PWUS) is a leading barrier to seeking treatment.Further, those in treatment are more likely to drop out if they experience higher levels of stigmatization. While related concepts of hate speech and toxicity, including those targeted toward vulnerable populations, have been the focus of automatic content moderation research, stigma and, in particular, people who use substances have not. This paper explores stigma toward PWUS using a data set of roughly 5,000 public Reddit posts. We performed a crowd-sourced annotation task where workers are asked to annotate each post for the presence of stigma toward PWUS and answer a series of questions related to their experiences with substance use. Results show that workers who use substances or know someone with a substance use disorder are more likely to rate a post as stigmatizing. Building on this, we use a supervised machine learning framework that centers workers with lived substance use experience to 
    
[^54]: 统一结构推理和语言模型预训练以进行复杂推理

    Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning. (arXiv:2301.08913v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.08913](http://arxiv.org/abs/2301.08913)

    本文提出了一个统一的学习框架，将明确的结构推理和语言预训练相结合，以赋予预训练语言模型结构推理能力。

    

    最近，配备基础推理技能的预训练语言模型（PLMs）在下游复杂任务上表现出了显著的性能。然而，显著的结构推理技能很少被研究，这涉及对文本中的隐含结构信息进行建模，并对其进行明确的逻辑推理以推导出结论。本文提出了一个统一的学习框架，将明确的结构推理和语言预训练相结合，赋予PLMs结构推理能力。它首先识别上下文中的几个基本结构，构建结构化查询，并沿着查询逐步推理以确定答案实体。通过使用PLMs学习的上下文表示来初始化结构的表示空间，并在这个语义表示空间上进行逐步推理，实现了文本语义和结构推理的融合。在四个数据集上的实验证明了该方法的有效性。

    Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four d
    
[^55]: SuS-X：无需训练的基于名称的视觉语言模型迁移方法

    SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16198](http://arxiv.org/abs/2211.16198)

    本文提出了SuS-X，一种无需训练的基于名称的视觉语言模型迁移方法，具有较高的零样本分类能力。

    

    对比语言-图像预训练（CLIP）已成为训练大规模视觉语言模型的一种简单而有效的方法。尽管CLIP在多种下游任务的零样本分类和检索方面展示出卓越的性能，但要发挥其全部潜力，微调仍然是必要的。微调整个CLIP模型会消耗资源且不稳定。此外，最近的方法虽然旨在避免对下游任务进行微调，但仍需要访问目标分布中的图像。本文探索了另一种方法——无需训练的“仅基于名称迁移”的方法。我们提出了一种新颖的方法SuS-X，由两个关键构建块——SuS和TIP-X组成，既不需要密集的微调，也不需要昂贵的标记数据。SuS-X在19个基准数据集上实现了最先进的零样本分类结果。

    Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. 
    
[^56]: NLP中的不良偏见：避免衡量危机

    Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13709](http://arxiv.org/abs/2211.13709)

    这项研究提供了一个跨学科的方法来探讨NLP模型偏见的问题，通过采用心理测量学的视角，特别关注构念效度和测量工具的信度，在衡量模型偏见的情境中如何应用。

    

    随着大型语言模型和自然语言处理（NLP）技术的快速发展和普及，预测其使用可能对人们造成伤害变得至关重要。近年来，一个受到关注的问题是这一技术在行为中显示出有害偏见。尽管已经投入了大量的努力来评估和减轻这些偏见，但我们衡量NLP模型偏见的方法存在严重问题（例如，通常不清楚它们到底衡量了什么）。在本文中，我们采用心理测量学的视角，提供了一个跨学科的方法来讨论NLP模型偏见的问题，心理测量学专注于衡量不直接可观察到的概念，如偏见。具体而言，我们将探讨心理测量学的两个核心概念，即构念效度和测量工具的信度，并讨论它们在衡量模型偏见的情境中如何应用。我们的目标是提供一个全面的视角来解决这个问题。

    As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
    
[^57]: DialoGen: 对话系统的广义长程上下文表示

    DialoGen: Generalized Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06282](http://arxiv.org/abs/2210.06282)

    DialoGen是一种对话系统，采用了广义上下文表示方法，在对话理解和生成任务中表现出色。

    

    长程上下文建模对于对话理解和生成都至关重要。对话上下文表示最常用的方法是将最后-k个先前的话语连接在一起。然而，对于包含长程依赖关系的对话而言，这种方法可能不是最理想的，因为它无法超越最后-k个话语。在这项工作中，我们提出了DialoGen，一种新颖的基于编码器-解码器的对话响应生成框架，其具有可以超越最后-k个话语的广义上下文表示。因此，该方法适应具有长程依赖的对话。我们的方法的主要思想是识别和利用最相关的历史话语，而不是按时间顺序的最后-k个话语。我们在对话生成（开放域）和理解（DST）任务上研究了我们提出的方法的有效性。DialoGen在DailyDialog数据集上实现了与最先进模型相当的性能。

    Long-range context modeling is crucial to both dialogue understanding and generation. The most popular method for dialogue context representation is to concatenate the last-$k$ previous utterances. However, this method may not be ideal for conversations containing long-range dependencies as it cannot look beyond last-$k$ utterances. In this work, we propose DialoGen, a novel encoder-decoder based framework for conversational response generation with a generalized context representation that can look beyond the last-$k$ utterances. Hence the method is adaptive to conversations with long-range dependencies. The main idea of our approach is to identify and utilize the most relevant historical utterances instead of the last-$k$ utterances in chronological order. We study the effectiveness of our proposed method on both dialogue generation (open-domain) and understanding (DST) tasks. DialoGen achieves comparable performance with the state-of-the-art models on DailyDialog dataset. We also ob
    
[^58]: 语言建模任务中的不充分规范化：一个以因果关系为基础的性别代词消解研究

    Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.00131](http://arxiv.org/abs/2210.00131)

    本研究通过提供一个因果模型，在语言建模任务中探讨了不充分规范化的作用，提出了两种轻量级黑盒评估方法来帮助检测任务的不充分规范化，并在性别代词消解任务中应用这些方法，同时发现了性别与时间、性别与位置之间的虚假相关性。

    

    现代语言建模任务常常存在不充分规范化的问题：对于给定的标记预测，在推断时可能有多个单词符合用户产生自然语言的意图，然而在训练时只有一个单词能够最小化任务的损失函数。我们提供了一个简单而合理的因果机制，描述了不充分规范化在生成虚假相关性方面的作用。尽管其简洁性，我们的因果模型直接指导了两种轻量级黑盒评估方法的开发，我们将其应用于广泛的语言模型任务中的性别代词消解上，以帮助 1) 检测推断时任务的不充分规范化，利用了 2）之前未报道的性别与时间、性别与位置的虚假相关性，涵盖了 A）不同规模的语言模型，从BERT-base到GPT 3.5，B）不同的预训练目标，从遮蔽和自回归语言建模到这些目标的混合，以及C）不同的训练阶段，从仅预训练到增强训练。

    Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked & autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement l
    
[^59]: 以规模生成常识问答的详细解释

    Elaboration-Generating Commonsense Question Answering at Scale. (arXiv:2209.01232v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.01232](http://arxiv.org/abs/2209.01232)

    本论文提出了一种规模较小的语言模型框架，通过微调生成有用的中间上下文来提高常识问答性能，并在人工评估中获得高质量的生成详细解释。

    

    在需要常识的问答任务中，语言模型（例如GPT-3）被用来生成表达背景知识以提高性能的文本。然而，使用这样的模型的成本非常高；在这项工作中，我们通过微调较小的语言模型来生成有用的中间上下文，称为详细解释。我们的框架交替更新两个语言模型-详细解释生成器和答案预测器-允许它们相互影响。使用不到GPT-3参数的0.5％，我们的模型在与类似规模的替代方法相比表现更好，并在四个常识问答基准测试上接近GPT-3。人工评估显示生成的详细解释质量很高。

    In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models -- an elaboration generator and an answer predictor -- allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap on GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high.
    
[^60]: 多样性超过规模：关于论证挖掘数据集的样本和主题大小的影响

    Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets. (arXiv:2205.11472v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.11472](http://arxiv.org/abs/2205.11472)

    本研究发现，在论证挖掘任务中，使用精心组织的训练样本和预训练模型可以在减小训练样本大小至少85％的情况下，达到最大性能的95％。同时提供了一个新的数据集供未来基准测试。

    

    论证挖掘的任务是从大规模文档来源中提取特定主题的论证句子，这对于机器学习模型和人类来说都是一项困难的任务，因为大规模的论证挖掘数据集很少，而识别论证句子需要专业知识。如果还涉及到检测检索到的论证的立场，这个任务就更加困难了。鉴于创建合适规模的论证挖掘数据集的成本和复杂性，我们想知道是否有必要为了达到可接受的性能而增加数据集的大小。我们的研究结果表明，当使用精心组织的训练样本和在相关任务上预训练的模型时，我们可以将训练样本的大小减少至少85％，同时达到最大性能的95％。这种收益在三个不同数据集上的三个论证挖掘任务中是一致的。我们还发布了一个新的数据集供未来进行基准测试。

    The task of Argument Mining, that is extracting argumentative sentences for a specific topic from large document sources, is an inherently difficult task for machine learning models and humans alike, as large Argument Mining datasets are rare and recognition of argumentative sentences requires expert knowledge. The task becomes even more difficult if it also involves stance detection of retrieved arguments. Given the cost and complexity of creating suitably large Argument Mining datasets, we ask whether it is necessary for acceptable performance to have datasets growing in size. Our findings show that, when using carefully composed training samples and a model pretrained on related tasks, we can reach 95% of the maximum performance while reducing the training sample size by at least 85%. This gain is consistent across three Argument Mining tasks on three different datasets. We also publish a new dataset for future benchmarking.
    
[^61]: 句法的分层组织

    The Hierarchical Organization of Syntax. (arXiv:2112.05783v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.05783](http://arxiv.org/abs/2112.05783)

    对历史句法网络的分层组织分析揭示了句法演化的方式，同时证明了说话者的交流需求是句法组织的重要驱动力。

    

    分层结构是复杂系统的隐藏骨架，通过对其分析可以更深入地理解其结构和演变方式。我们认为语言也是一种复杂自适应系统，具有多个复杂网络，捕捉了其结构和功能。因此，我们决定分析历史句法网络的分层组织，以了解句法随时间演变的方式。我们使用从11世纪到17世纪的德语文本语料库创建了这些网络，重点关注这些网络的分层水平，并将其映射到说话者的特定交流需求上。我们开发了一个框架来从历时的角度实证追踪句法结构的出现，从而能够将说话者的交流需求与这些结构进行映射。我们将这些句法结构称为"句法交际层次结构"。我们展示了说话者的交流需求是句法的组织力量。因此，我们认为...

    Hierarchies are the hidden backbones of complex systems and their analysis allows for a deeper understanding of their structure and how they evolve. We consider languages also to be complex adaptive systems with several intricate networks that capture their structure and function. Hence, we decided to analyze the hierarchical organization of historical syntactic networks to understand how syntax evolves over time. We created these networks from a corpus of German texts from the 11th to 17th centuries, focusing on the hierarchical levels of these networks. diachronically and to map them to specific communicative needs of speakers. We developed a framework to empirically track the emergence of syntactic structures diachronically, enabling us to map the communicative needs of speakers with these structures. We named these syntactic structures "syntactic communicative hierarchies." We showed that the communicative needs of speakers are the organizational force of syntax. Thus, we argue tha
    
[^62]: 自动学术论文审稿：概念、技术与挑战。

    Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2111.07533](http://arxiv.org/abs/2111.07533)

    提出自动学术论文审稿（ASPR）的概念和流程，综述了实现全面计算机化审稿流程的相关文献和技术，同时指出实现中存在的挑战，如文档解析和表达不完美、数据不足、人机交互缺陷和发现低质量文章的难度。

    

    同行评审是研究评价的广泛接受机制，在学术出版中扮演着重要的角色。然而，由于其效率低下和可重复性差，这一机制长期以来备受批评。近年来，人工智能应用于辅助同行评审。尽管如此，在涉及人员的情况下，这些限制仍是不可避免的。本文提出了自动学术论文审稿（ASPR）的概念和流程，并综述了实现全面计算机化审稿流程的相关文献和技术。在审查和讨论的基础上，我们得出结论：ASPR 的每个阶段已经有相应的研究和初步实施。我们还进一步探讨了ASPR存在的挑战。主要困难在于文档解析和表达不完美、数据不足、人机交互缺陷和发现低质量文章的难度。

    Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled on this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in imperfect document parsing and representation, inadequate data, defective human-computer interaction, and fla
    
[^63]: 生物医学领域中的预训练语言模型：系统调查

    Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.05006](http://arxiv.org/abs/2110.05006)

    本文系统调查了生物医学领域中的预训练语言模型，总结了它们的最新进展和应用，并提出了分类法。

    This paper systematically surveys pre-trained language models in the biomedical domain, summarizes their recent progress and applications, and proposes a taxonomy.

    预训练语言模型（PLMs）已成为大多数自然语言处理（NLP）任务的事实标准。这也有益于生物医学领域：来自信息学、医学和计算机科学（CS）社区的研究人员提出了各种在生物医学数据集上训练的PLMs，例如生物医学文本、电子健康记录、蛋白质和DNA序列，用于各种生物医学任务。然而，生物医学PLMs的跨学科特性阻碍了它们在社区之间的传播；一些现有的工作相互孤立，缺乏全面的比较和讨论。期望一项调查，不仅系统地审查生物医学PLMs及其应用的最新进展，而且标准化术语和基准。在本文中，我们总结了生物医学领域中预训练语言模型的最新进展以及它们在生物医学下游任务中的应用。特别是，我们讨论了动机并提出了现有生物医学PLMs的分类法。

    Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing (NLP) tasks. This also benefits biomedical domain: researchers from informatics, medicine, and computer science (CS) communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It expects a survey that not only systematically reviews recent advances of biomedical PLMs and their applications but also standardizes terminology and benchmarks. In this paper, we summarize the recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks. Particularly, we discuss the motivations and propose a taxonomy of existing b
    

