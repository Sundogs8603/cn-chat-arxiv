# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Studying the Impact of Semi-Cooperative Drivers on Overall Highway Flow.](http://arxiv.org/abs/2304.11693) | 本论文研究了半合作驾驶汇入自动驾驶将会对公路整体流量产生什么影响，并通过实验表明了半合作的好处不成比例地影响利己和高速驾驶员。 |
| [^2] | [Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release.](http://arxiv.org/abs/2304.11679) | DomMa 是一个用于测试大型语言模型领域知识理解能力的综合基准，它分为中英文10万个问题，并基于112个一级学科分类不断更新数据集。 |
| [^3] | [Hold the Suspect! : An Analysis on Media Framing of Itaewon Halloween Crowd Crush.](http://arxiv.org/abs/2304.11666) | 本文基于韩国前40家新闻提供商的10.9K篇文章，采用词向量嵌入和聚类的方法，分析了梨泰院万圣节踩踏事件的媒体框架。结果发现，保守派媒体注重政治反应和嫌疑人身份，而自由派媒体则关注政府的责任和对低收入产业工人可能产生的不公平溢出效应。媒体在报道过程中明显展现了政治偏见。 |
| [^4] | [IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources.](http://arxiv.org/abs/2304.11664) | IslamicPCQA是第一个基于非结构化信息源回答复杂问题的波斯语数据集，包含从9部伊斯兰百科全书中提取的12,282个问题-答案对，旨在方便回答涉及伊斯兰文本资源的复杂波斯语问题。 |
| [^5] | [Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models.](http://arxiv.org/abs/2304.11657) | 本文提出 Iter-CoT 方法，在大型语言模型中进行迭代增强的思维链提示，通过选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，提高了模型的泛化能力，同时使模型能够更准确地生成推理链。 |
| [^6] | [Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness.](http://arxiv.org/abs/2304.11633) | 本研究使用7个细粒度信息提取任务评估ChatGPT的能力，发现其在标准IE下表现较差但在OpenIE下表现出色，提供了高品质和可信的解释，但存在过度自信导致的低标定度问题。 |
| [^7] | [Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding.](http://arxiv.org/abs/2304.11618) | 本文提出了一种适用于多模态知识图谱嵌入的模态感知负采样方法(MANS)，通过对知识图谱中实体的结构性和视觉嵌入进行对齐，MANS能够学习更有意义的嵌入以在多模态KGE中达到更好的效果，同时保持轻量级和高效率。 |
| [^8] | [Differentiate ChatGPT-generated and Human-written Medical Texts.](http://arxiv.org/abs/2304.11567) | 本研究旨在区分由ChatGPT生成和人类撰写的医学文本，并通过设计机器学习工作流来有效检测医学领域的人工智能生成内容，以避免可能导致的假消息和对公众造成的损害。 |
| [^9] | [Divide and Prompt: Chain of Thought Prompting for Text-to-SQL.](http://arxiv.org/abs/2304.11556) | 本文通过思维链逐个解决子任务的方式，提出了一种新的Text-to-SQL提示方法的范例，运用于LLM模型可以有效地提高其执行准确性。 |
| [^10] | [Graph Neural Networks for Text Classification: A Survey.](http://arxiv.org/abs/2304.11534) | 该综述介绍了基于图神经网络的文本分类技术，该技术可以直接处理复杂结构化文本数据并利用全局信息。许多真实的文本分类应用程序可以自然地表示为一个图。本综述覆盖到2023年的方法，包括语料库级别和文档级别的图神经网络，并详细讨论了每种方法的图构建机制和基于图的学习过程。涵盖了数据集、评估指标和实验设计，并总结了在公开可用的基准数据集上发布的性能。 |
| [^11] | [Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices.](http://arxiv.org/abs/2304.11520) | 本文探究了在资源受限的嵌入式设备上部署基于BERT的NLP模型的挑战，并得出结论：虽然DistilBERT和TinyBERT等轻量级模型相对占用更少内存，但它们在复杂的NLP任务上表现较差；ResNet-based BERT模型可以在精度和资源效率之间取得良好的平衡，适合在嵌入式设备上部署。 |
| [^12] | [Translationese Reduction using Abstract Meaning Representation.](http://arxiv.org/abs/2304.11501) | 研究使用抽象意义表示（AMR）作为跨语言方案可以减少翻译语言学数量。 |
| [^13] | [Boosting Theory-of-Mind Performance in Large Language Models via Prompting.](http://arxiv.org/abs/2304.11490) | 本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。 |
| [^14] | [Understanding Lexical Biases when Identifying Gang-related Social Media Communications.](http://arxiv.org/abs/2304.11485) | 本研究使用自然语言处理工具有效识别了帮派相关社交媒体上可能需要社区资源帮助的人群，拓展了社区成员照顾的范畴。 |
| [^15] | [(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis.](http://arxiv.org/abs/2304.11473) | 本文主张将产品搜索看作程序合成，相比向量空间模型有着重大优势。 |
| [^16] | [L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT.](http://arxiv.org/abs/2304.11434) | 该论文提出了一种简单但有效的方法，使用合成语料库将BERT模型转换成SBERT模型。该方法在10种主要的印欧语言中具有很好的效果，并展示了其在非印欧语言上的应用性。 |
| [^17] | [A bounded rationality account of dependency length minimization in Hindi.](http://arxiv.org/abs/2304.11410) | 本研究测试了在华语中追求依赖长度极小化的做法，发现仅将最短的前置成分放置在主谓动词旁边可以解释词序偏好，普及该方法可视为一种最小化努力策略，符合有限理性的观点。 |
| [^18] | [LaMP: When Large Language Models Meet Personalization.](http://arxiv.org/abs/2304.11406) | 本论文强调了当前自然语言处理领域中个性化的重要性，并提出了LaMP（一种用于训练和评估大型语言模型的新的个性化基准），并针对大型语言模型的生成任务，设计了七项个性化任务以及一种检索增强方法，结果表明在利用用户配置文件扩展大型语言模型的基础上，其生成结果明显优于传统方法。 |
| [^19] | [Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens.](http://arxiv.org/abs/2304.11389) | 本文研究表明，基于Transformer的语言模型在观察约20亿个训练令牌后能够提供最佳拟合，其surprisal估计能力能够最好地预测人类阅读时间。在语言模型收敛时，较小的模型变体会出现“临界点”，使语言模型困惑度下降，从而导致较差的人类拟合度。 |
| [^20] | [SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval.](http://arxiv.org/abs/2304.11370) | 本文提出了一种结构感知预训练语言模型SAILER，针对法律案例检索中的长文本序列和关键法律要素敏感问题，采用遮蔽语言建模任务和结构感知连贯性预测任务相结合的多任务预训练策略，在两个法律案例检索数据集上实现了显著优于强基线模型的性能表现。 |
| [^21] | [Romanian Multiword Expression Detection Using Multilingual Adversarial Training and Lateral Inhibition.](http://arxiv.org/abs/2304.11350) | 本文使用多语言对抗训练和横向抑制技术对罗马尼亚语多词表达进行自动识别，提高了已有模型在未见过的多词表达上的F1分数，达到了最先进水平。 |
| [^22] | [Semantic Specialization for Knowledge-based Word Sense Disambiguation.](http://arxiv.org/abs/2304.11340) | 本文提出了一种基于词汇知识的词义消歧语义专业化方法，通过调整上下文嵌入向量，将语义相关的“sense”和上下文彼此加近，将不相关的“sense”彼此远离。 |
| [^23] | [On the Identification of the Energy related Issues from the App Reviews.](http://arxiv.org/abs/2304.11292) | 本文研究了自动提取与能源相关的应用程序评论的不同技术。结果表明，神经网络优于其他机器学习模型。 |
| [^24] | [The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students.](http://arxiv.org/abs/2304.11276) | 本研究探讨了如何利用语言模型帮助香港中学生在创意写作方面，实证结果表明，语言模型在帮助学生作家更有创造力方面发挥着不同的作用，例如作为合作者、挑衅者等。 |
| [^25] | [UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis.](http://arxiv.org/abs/2304.11256) | 该研究旨在解决非洲14个不同语言的情感分析任务，使用转移学习的方法，并取得了不错的成果，可应用于其他语言的情感分析任务。 |
| [^26] | [A Group-Specific Approach to NLP for Hate Speech Detection.](http://arxiv.org/abs/2304.11223) | 本文提出了一种面向特定群体的自然语言处理方法，通过将特定群体的历史和语言知识纳入仇恨言论检测模型中，分析有关歧视历史的数据，以更好地预测对该群体的仇恨言论激增，并通过交叉性和伦理标准对模型进行批判性评估。 |
| [^27] | [Learn What NOT to Learn: Towards Generative Safety in Chatbots.](http://arxiv.org/abs/2304.11220) | 本文提出了一种名为“LOT”的框架，采用对比损失训练聊天机器人以从正面和负面训练信号中增强泛化能力，并使用离散度将生成向量从不安全子空间指向安全子空间，从而避免生成不安全的内容。 |
| [^28] | [Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs.](http://arxiv.org/abs/2304.11164) | 本文介绍了一种辩证评估方式，旨在描绘语言模型系统失败的边界并检查其一致性，以及对LLMs通识空间推理能力进行了初步定性研究。 |
| [^29] | [ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity.](http://arxiv.org/abs/2304.11163) | 必须投资于协作的AI安全和伦理研究，开发可持续和公正的标准以创建有益人工智能，否则我们将看到技术进步超越我们的伦理和社会影响能力的未来。 |
| [^30] | [Text2Time: Transformer-based article time period predictor.](http://arxiv.org/abs/2304.10859) | 本文提出了一个基于Transformer模型的文章时间段预测器，使用预训练的BERT模型对新闻文章进行分类的结果表现优于先前尝试的模型，具有很高的准确性。 |
| [^31] | [IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases.](http://arxiv.org/abs/2304.10637) | 本文提出了一种基于知识库的上下文增强的多语言命名实体识别方法，通过识别、链接和预测实体类别，能够准确地分类细粒度和新兴实体。 |
| [^32] | [Learning to Program with Natural Language.](http://arxiv.org/abs/2304.10464) | 该论文提出了一种用自然语言作为编程语言并通过学习编程方法让大语言模型直接生成自然语言程序并指导推理的方法。实验结果表明，这种方法在解决编程任务上比基线方法有更高的成功率。 |
| [^33] | [Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks.](http://arxiv.org/abs/2304.10145) | 本文研究了ChatGPT在社交计算任务中是否可以复制人类生成的标签注释，结果表明ChatGPT有潜力处理这些数据注释任务，尽管仍存在许多挑战。 |
| [^34] | [A Latent Space Theory for Emergent Abilities in Large Language Models.](http://arxiv.org/abs/2304.09960) | 本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。 |
| [^35] | [GeneGPT: Teaching Large Language Models to Use NCBI Web APIs.](http://arxiv.org/abs/2304.09667) | GeneGPT通过少量NCBI API调用URL请求作为演示，教授大型语言模型使用NCBI Web API回答基因组问题，并在GeneTuring测试中达到了优异的结果。 |
| [^36] | [SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts.](http://arxiv.org/abs/2304.09548) | SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。 |
| [^37] | [Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models.](http://arxiv.org/abs/2304.07619) | 本研究探究了使用ChatGPT及其他大型语言模型预测股市回报的潜力，发现ChatGPT的预测表现优于传统情感分析方法，而基础模型无法准确预测股票价格变化，表明复杂模型可预测能力的崛起。这表明在投资决策过程中引入先进的语言模型可以提高预测准确性并增强定量交易策略的表现。 |
| [^38] | [PDF-VQA: A New Dataset for Real-World VQA on PDF Documents.](http://arxiv.org/abs/2304.06447) | 该研究提出了一个新的文档VQA数据集PDF-VQA，以多个页面的完整文档作为研究对象，通过机器学习模型识别与处理文档元素、结构和内容等方面，为解决真实世界中的文档理解问题提供新的资源。 |
| [^39] | [Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding.](http://arxiv.org/abs/2304.04099) | 本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。 |
| [^40] | [SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers.](http://arxiv.org/abs/2304.03518) | 本文描述了使用细调BERT模型和多数投票集成模型来检测和解释在线性别歧视的方法。翻转显着降低了女性在社交媒体平台上经历不成比例的性别歧视的风险。 |
| [^41] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^42] | [$\mathcal{E}$ K\'U [MASK]: Integrating Yor\`ub\'a cultural greetings into machine translation.](http://arxiv.org/abs/2303.17972) | 本文研究了将尤鲁巴文化问候语（$\mathcal{E}$ k\'u [MASK]）整合到机器翻译中，通过IkiniYor\`ub\'a数据集，我们发现大规模多语言神经机器翻译（NMT）系统无法准确翻译，而微调现有的NMT模型在翻译中有更好的表现。 |
| [^43] | [The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces.](http://arxiv.org/abs/2303.14334) | 本文探讨了利用人工智能和人机交互技术为研究论文提供智能、交互式和无障碍的阅读界面的可行性，并介绍了跨机构合作的语义阅读器项目。 |
| [^44] | [Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer.](http://arxiv.org/abs/2303.13099) | 本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。 |
| [^45] | [TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection.](http://arxiv.org/abs/2303.09314) | 本文针对隐式危害检测的挑战，提出一种面向表情包情境的拓扑感知最优传输框架TOT，利用最优传输核方法从多个模态中捕捉互补信息。 |
| [^46] | [DeltaScore: Evaluating Story Generation with Differentiating Perturbations.](http://arxiv.org/abs/2303.08991) | DeltaScore利用差分扰动来评估故事生成的细粒度方面，并通过计算故事在特定方面扰动前后的可能性差异来衡量影响。该方法在多个故事领域中得到了评估，并与人类判断的相关性进行了研究。 |
| [^47] | [Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction.](http://arxiv.org/abs/2303.02468) | 本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。 |
| [^48] | [UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction.](http://arxiv.org/abs/2303.01194) | 本文介绍了UZH_CLyp在SemEval 2023任务9中的表现。我们的跨语言迁移学习方法包含了先使用Head-First Fine-Tuning（HeFiT）方法更新回归头参数，再降低学习率更新预训练transformer的参数。同时，我们研究了在没有人工标记数据的情况下，使用ChatGPT生成的自动小型样例来解决低资源问题。研究发现，HeFiT稳定训练并显著提高预训练模型的性能，使用合成数据时也能提高跨语言学习的性能。 |
| [^49] | [Learning Multimodal Data Augmentation in Feature Space.](http://arxiv.org/abs/2212.14453) | 本文介绍了一种名为LeMDA的易于使用的方法，它在特征空间中自动学习联合增强多模态数据，提高了多模态学习算法的性能。 |
| [^50] | [MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification.](http://arxiv.org/abs/2212.12061) | 本文介绍了一个包含10,917篇新闻文章的多标签数据集，可用于训练机器学习模型自动按主题对新闻文章进行分类，对新闻结构、分类和预测未来事件的研究人员非常有帮助。 |
| [^51] | [A Unified Encoder-Decoder Framework with Entity Memory.](http://arxiv.org/abs/2210.03273) | 本论文提出了一种名为EDMem的具有实体记忆的编码器-解码器框架，可以将实体知识融入到信息性文本生成中。实体知识以潜在表示形式存储在记忆中，并利用记忆中的实体链接限制实体生成。实验结果表明，EDMem优于基于记忆的自动编码器模型和非记忆编码器-解码器模型。 |
| [^52] | [Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning.](http://arxiv.org/abs/2210.01338) | 本文提出了一种图像字幕生成器，学习组合视觉和语言神经模块进行图像字幕生成。该方法使用了可区分的模块设计，并采用有效的模块组合和多目标学习方式，实现了在MSCOCO和Flickr30k上的性能优于现有方法的结果。 |
| [^53] | [Incorporating Task-specific Concept Knowledge into Script Learning.](http://arxiv.org/abs/2209.00068) | 本文提出了一个新任务Tetris，并提出了概念提示和面向脚本的对比学习来解决输入包括用户上下文时的问题，两种方法均能提高任务完成性能。 |
| [^54] | [GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition.](http://arxiv.org/abs/2207.12261) | 本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。 |
| [^55] | [Code Translation with Compiler Representations.](http://arxiv.org/abs/2207.03578) | 本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。 |
| [^56] | [Understanding EFL Student Idea Generation Strategies for Creative Writing with NLG Tools.](http://arxiv.org/abs/2207.01484) | 本研究探讨了EFL学生如何使用NLG工具进行创意写作。研究发现学生在使用NLG工具搜索和评估思路时可能已经有现有的想法。 |
| [^57] | [A Thorough Examination on Zero-shot Dense Retrieval.](http://arxiv.org/abs/2204.12755) | 本文第一次全面探讨了 DR 模型在零-shot检索能力上的表现，并分析了影响表现的关键因素，为开发更好的零-shot DR 模型提供了重要的证据。 |
| [^58] | [Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies.](http://arxiv.org/abs/2204.08952) | 本文提出了一种集成检索模型的数据增强框架，并应用于隐私政策问答中，成功地提高了性能。该方法捕获政策文档中的相关文本段并利用多个预训练语言模型，通过对增强数据进行级联和噪声减少滤波来提高数据多样性和质量，获得了该领域的新最优F1分数50\%。 |
| [^59] | [Learning to Compose Soft Prompts for Compositional Zero-Shot Learning.](http://arxiv.org/abs/2204.03574) | 通过学习组合式软提示实现了预测看不见的属性-对象组合，超过了基准数据集上的特定体系结构，并在曲线下面积上平均高10.9％。 |
| [^60] | [Question-Answer Sentence Graph for Joint Modeling Answer Selection.](http://arxiv.org/abs/2203.03549) | 本文研究了基于图的方法用于答案选择，通过构建相关训练图并集成最先进模型，成功解决了检索型问答系统中的AS2任务，并在实验中表现优异。 |
| [^61] | [A Unified Review of Deep Learning for Automated Medical Coding.](http://arxiv.org/abs/2201.02797) | 本文综述了深度学习在自动医疗编码领域的发展，提出了一个统一框架，总结了最新的高级模型，并讨论了未来发展的挑战和方向。 |
| [^62] | [RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking.](http://arxiv.org/abs/2110.07367) | 本文提出了一种新的联合训练方法来优化稠密Passage检索和Passage重新排名，并引入了动态listwise蒸馏和混合数据增强策略以提高性能。 |
| [^63] | [PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval.](http://arxiv.org/abs/2108.06027) | PAIR算法是一种新方法，在密集型段落检索中同时考虑查询中心和段落中心相似关系，通过正式公式、知识蒸馏和两阶段训练实现。实验证明，该方法优于先前方法。 |
| [^64] | [Aspect-based Sentiment Analysis in Document -- FOMC Meeting Minutes on Economic Projection.](http://arxiv.org/abs/2108.04080) | 本文提出了一种用于训练金融文档上基于方面的情感分析的模型，并研究了其对宏观经济指标的预测能力。 |
| [^65] | [Human Attention during Goal-directed Reading Comprehension Relies on Task Optimization.](http://arxiv.org/abs/2107.05799) | 本文研究了阅读理解中人类关注力分配的计算模型。研究表明，在执行相同的阅读任务时，深度神经网络可以预测每个单词的阅读时间，读者在第一遍阅读和重新阅读过程中分别关注基本文本特征和与问题相关的信息，并且文本特征和问题相关性会分别调节注意力权重。 |

# 详细

[^1]: 研究半合作驾驶对公路整体流量的影响。

    Studying the Impact of Semi-Cooperative Drivers on Overall Highway Flow. (arXiv:2304.11693v1 [cs.CL])

    [http://arxiv.org/abs/2304.11693](http://arxiv.org/abs/2304.11693)

    本论文研究了半合作驾驶汇入自动驾驶将会对公路整体流量产生什么影响，并通过实验表明了半合作的好处不成比例地影响利己和高速驾驶员。

    

    半合作行为是人类驾驶员固有的特性，应该考虑到自动驾驶。此外，新的自主规划器可以考虑人类驾驶员的社会价值取向（SVO）以生成符合社会准则的轨迹。然而，这种新型规划器对交通流量的整体影响仍需了解。在这项工作中，我们研究了隐式半合作驾驶，其中代理人部署了一个博弈论版本的迭代最佳响应，假定知道其他代理人的SVO。我们模拟名义交通流量，并研究了道路上的利他代理人比例是否影响个体或系统级驾驶表现。实验表明，利他代理人的比例对整体交通流量影响较小，而半合作的好处不成比例地影响利己和高速驾驶员。

    Semi-cooperative behaviors are intrinsic properties of human drivers and should be considered for autonomous driving. In addition, new autonomous planners can consider the social value orientation (SVO) of human drivers to generate socially-compliant trajectories. Yet the overall impact on traffic flow for this new class of planners remain to be understood. In this work, we present study of implicit semi-cooperative driving where agents deploy a game-theoretic version of iterative best response assuming knowledge of the SVOs of other agents. We simulate nominal traffic flow and investigate whether the proportion of prosocial agents on the road impact individual or system-wide driving performance. Experiments show that the proportion of prosocial agents has a minor impact on overall traffic flow and that benefits of semi-cooperation disproportionally affect egoistic and high-speed drivers.
    
[^2]: 领域掌握水平基准：用于评估大型语言模型全面领域知识的不断更新的基准——初步发布。（arXiv:2304.11679v1 [cs.CL]）

    Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release. (arXiv:2304.11679v1 [cs.CL])

    [http://arxiv.org/abs/2304.11679](http://arxiv.org/abs/2304.11679)

    DomMa 是一个用于测试大型语言模型领域知识理解能力的综合基准，它分为中英文10万个问题，并基于112个一级学科分类不断更新数据集。

    

    领域知识指对特定主题、行业、领域或特别兴趣领域的深入理解、专业知识和熟悉程度。现有的基准都缺乏对领域知识评估的整体设计。我们坚信，领域语言理解的真正能力只能通过全面深入的基准来公平地评估，因此我们提出了Domma领域掌握水平基准。DomMa旨在测试大型语言模型（LLM）对领域知识的理解能力，具有广泛的领域覆盖、大量数据和基于中国112个一级学科分类不断更新的数据集。DomMa包含10万个问题，其中包括中文和英文，来源于中国大学的研究生入学考试和本科考试。我们还提出了更适合LLMs的基准和评估流程的设计。

    Domain knowledge refers to the in-depth understanding, expertise, and familiarity with a specific subject, industry, field, or area of special interest. The existing benchmarks are all lack of an overall design for domain knowledge evaluation. Holding the belief that the real ability of domain language understanding can only be fairly evaluated by an comprehensive and in-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa targets at testing Large Language Models (LLMs) on their domain knowledge understanding, it features extensive domain coverage, large data volume, and a continually updated data set based on Chinese 112 first-level subject classifications. DomMa consist of 100,000 questions in both Chinese and English sourced from graduate entrance examinations and undergraduate exams in Chinese college. We have also propose designs to make benchmark and evaluation process more suitable to LLMs.
    
[^3]: “抓住嫌疑人！”：分析梨泰院万圣节踩踏事件的媒体框架

    Hold the Suspect! : An Analysis on Media Framing of Itaewon Halloween Crowd Crush. (arXiv:2304.11666v1 [cs.CL])

    [http://arxiv.org/abs/2304.11666](http://arxiv.org/abs/2304.11666)

    本文基于韩国前40家新闻提供商的10.9K篇文章，采用词向量嵌入和聚类的方法，分析了梨泰院万圣节踩踏事件的媒体框架。结果发现，保守派媒体注重政治反应和嫌疑人身份，而自由派媒体则关注政府的责任和对低收入产业工人可能产生的不公平溢出效应。媒体在报道过程中明显展现了政治偏见。

    

    本文基于韩国前40家新闻提供商的10.9K篇文章，分析了事件发生后前72小时内媒体对梨泰院万圣节踩踏事件的报道。通过采用词向量嵌入和聚类，发现保守派媒体注重政党的反应和嫌疑人的身份，而自由派媒体则关注政府的责任和对低收入产业工人可能产生的不公平溢出效应。尽管该社会悲剧与制度政治没有直接联系，但媒体在报道过程中明显展现了政治偏见。

    Based on the 10.9K articles from top 40 news providers of South Korea, this paper analyzed the media framing of Itaewon Halloween Crowd Crush during the first 72 hours after the incident. By adopting word-vector embedding and clustering, we figured out that conservative media focused on political parties' responses and the suspect's identity while the liberal media covered the responsibility of the government and possible unequal spillover effect on the low-income industry workers. Although the social tragedy was not directly connected to institutional politics, the media clearly exhibited political bias in the coverage process.
    
[^4]: IslamicPCQA：基于伊斯兰文本资源的波斯语多跳复杂问答数据集

    IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources. (arXiv:2304.11664v1 [cs.CL])

    [http://arxiv.org/abs/2304.11664](http://arxiv.org/abs/2304.11664)

    IslamicPCQA是第一个基于非结构化信息源回答复杂问题的波斯语数据集，包含从9部伊斯兰百科全书中提取的12,282个问题-答案对，旨在方便回答涉及伊斯兰文本资源的复杂波斯语问题。

    

    现在，问答系统面临的主要挑战之一是使用各种信息源回答复杂问题。多跳问题是一种需要多步推理才能回答的复杂问题。本文介绍了IslamicPCQA数据集，这是第一份基于非结构化信息源回答复杂问题的波斯语数据集，包含从9部伊斯兰百科全书中提取的12,282个问题-答案对。该数据集受HotpotQA英语数据集方法的启发，经过定制以适应波斯语的复杂性。回答该数据集的问题需要多个段落和推理过程。问题不限于任何先前的知识库或本体论，并且为了提供强大的推理能力，该数据集还包括支持事实和关键句子。准备好的数据集涵盖了广泛的伊斯兰主题，旨在方便回答涉及伊斯兰文本资源的复杂波斯语问题。

    Nowadays, one of the main challenges for Question Answering Systems is to answer complex questions using various sources of information. Multi-hop questions are a type of complex questions that require multi-step reasoning to answer. In this article, the IslamicPCQA dataset is introduced. This is the first Persian dataset for answering complex questions based on non-structured information sources and consists of 12,282 question-answer pairs extracted from 9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA English dataset approach, which was customized to suit the complexities of the Persian language. Answering questions in this dataset requires more than one paragraph and reasoning. The questions are not limited to any prior knowledge base or ontology, and to provide robust reasoning ability, the dataset also includes supporting facts and key sentences. The prepared dataset covers a wide range of Islamic topics and aims to facilitate answering complex Persi
    
[^5]: 在大型语言模型中加强迭代增强的思维链提示

    Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])

    [http://arxiv.org/abs/2304.11657](http://arxiv.org/abs/2304.11657)

    本文提出 Iter-CoT 方法，在大型语言模型中进行迭代增强的思维链提示，通过选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，提高了模型的泛化能力，同时使模型能够更准确地生成推理链。

    

    通过逐步引导思维链 (CoT) 作为示范，大型语言模型 (LLMs) 可以在各种推理任务上实现高度有效的性能。然而，LLMs 生成的演示推理链容易出现错误，这可能会导致推理过程中的错误。此外，不恰当的示例 (过于简单或复杂) 可以影响在不同难度级别下的整体性能。我们引入了Iter-CoT (迭代引导思维链提示) 的迭代引导方法，用于选择实例并生成推理链。通过利用迭代增强，我们的方法使LLMs 自主更正错误，从而产生更精确、全面的推理链。同时，我们的方法选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，从而增强LLMs 的泛化能力。

    Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
    
[^6]: 评估ChatGPT的信息提取能力：性能、可解释性、标定度和忠实度的评估

    Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. (arXiv:2304.11633v1 [cs.CL])

    [http://arxiv.org/abs/2304.11633](http://arxiv.org/abs/2304.11633)

    本研究使用7个细粒度信息提取任务评估ChatGPT的能力，发现其在标准IE下表现较差但在OpenIE下表现出色，提供了高品质和可信的解释，但存在过度自信导致的低标定度问题。

    

    大型语言模型（LLMs）如ChatGPT理解用户意图并提供合理的响应的能力，近来变得非常流行。本文聚焦于使用7个细粒度信息提取（IE）任务评估ChatGPT的总体能力。特别地，我们通过测量ChatGPT的性能、可解释性、标定度和忠实度来进行系统分析，结果得出了15个来自ChatGPT或领域专家的关键点。我们的研究发现，ChatGPT在标准IE设置下的表现较差，但在OpenIE设置下却有出色的表现，人类评估证明了这一点。此外，我们的研究表明，ChatGPT提供了高品质和可信的决策解释。然而，ChatGPT在预测时有过度自信的问题，导致了低标定度。此外，ChatGPT在主要文本方面表现出了高度的忠实度。

    The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the major
    
[^7]: 多模态知识图谱嵌入中的模态感知负采样

    Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding. (arXiv:2304.11618v1 [cs.CL])

    [http://arxiv.org/abs/2304.11618](http://arxiv.org/abs/2304.11618)

    本文提出了一种适用于多模态知识图谱嵌入的模态感知负采样方法(MANS)，通过对知识图谱中实体的结构性和视觉嵌入进行对齐，MANS能够学习更有意义的嵌入以在多模态KGE中达到更好的效果，同时保持轻量级和高效率。

    

    在知识图谱嵌入中，负采样被广泛应用以在训练过程中产生负三元组以进行正负区分。然而，现有的负采样方法在多模态信息考虑时不适用于KGE模型，而且由于它们的复杂设计而效率低下。本文提出了适用于多模态知识图谱嵌入（MMKGE）的模态感知负采样（MANS），以解决上述问题。MANS能够对知识图谱中的实体进行结构性和视觉嵌入的对齐，学习有意义的嵌入以在多模态KGE中表现更好，同时保持轻量级和高效率。在两个基准测试上的实证结果表明，MANS优于现有的NS方法。同时，我们对MANS进行了进一步探索以证实其有效性。

    Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness.
    
[^8]: 区分ChatGPT生成和人写的医学文本

    Differentiate ChatGPT-generated and Human-written Medical Texts. (arXiv:2304.11567v1 [cs.CL])

    [http://arxiv.org/abs/2304.11567](http://arxiv.org/abs/2304.11567)

    本研究旨在区分由ChatGPT生成和人类撰写的医学文本，并通过设计机器学习工作流来有效检测医学领域的人工智能生成内容，以避免可能导致的假消息和对公众造成的损害。

    

    背景：像ChatGPT这样的大型语言模型可以生成语法完美、类似人类文本内容的大量文本。然而，临床病历和诊断等医学文本需要严格的验证，由ChatGPT生成的错误医学内容可能导致假消息，从而对医疗保健和公众造成重大危害。目的：本研究是关于医疗领域负责和道德的人工智能生成内容的第一项研究。我们专注于分析由人类专家撰写的医学文本和由ChatGPT生成的文本之间的差异，并设计机器学习工作流来有效地检测和区分由ChatGPT生成的医学文本。方法：我们首先构建了一套包含人类专家撰写和由ChatGPT生成的医学文本的数据集合。接着，我们分析了这些文本的语言特征。

    Background: Large language models such as ChatGPT are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the Internet. However, medical texts such as clinical notes and diagnoses require rigorous validation, and erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.  Objective: This research is among the first studies on responsible and ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus on analyzing the differences between medical texts written by human experts and generated by ChatGPT, and designing machine learning workflows to effectively detect and differentiate medical texts generated by ChatGPT.  Methods: We first construct a suite of datasets containing medical texts written by human experts and generated by ChatGPT. In the next step, we analyze the linguistic features o
    
[^9]: 分而治之，思维链指导下的Text-to-SQL

    Divide and Prompt: Chain of Thought Prompting for Text-to-SQL. (arXiv:2304.11556v1 [cs.CL])

    [http://arxiv.org/abs/2304.11556](http://arxiv.org/abs/2304.11556)

    本文通过思维链逐个解决子任务的方式，提出了一种新的Text-to-SQL提示方法的范例，运用于LLM模型可以有效地提高其执行准确性。

    

    链式思维与大型语言模型结合已在复杂推理任务上取得了令人鼓舞的结果。Text-to-SQL是一个将自然语言问题转换为SQL语句的关键语义分析任务，涉及复杂的推理过程。然而，很少有研究使用思维链指导来激活LLM在Text-to-SQL任务中的推理能力。本文提出了一个新的Text-to-SQL提示方法的范式，称为分而治之，通过先将任务分解为子任务，然后通过思维链逐个解决子任务。我们提出了3种基于提示的方法来增强LLM的Text-to-SQL能力。实验证明，这些提示引导LLM生成具有更高执行准确性的Text-to-SQL。

    Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.
    
[^10]: 基于图神经网络的文本分类综述

    Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v1 [cs.CL])

    [http://arxiv.org/abs/2304.11534](http://arxiv.org/abs/2304.11534)

    该综述介绍了基于图神经网络的文本分类技术，该技术可以直接处理复杂结构化文本数据并利用全局信息。许多真实的文本分类应用程序可以自然地表示为一个图。本综述覆盖到2023年的方法，包括语料库级别和文档级别的图神经网络，并详细讨论了每种方法的图构建机制和基于图的学习过程。涵盖了数据集、评估指标和实验设计，并总结了在公开可用的基准数据集上发布的性能。

    

    文本分类是自然语言处理中最基本和最重要的问题。虽然许多最近的文本分类模型采用了序列深度学习技术，但是基于图神经网络的模型可以直接处理复杂结构化文本数据并利用全局信息。许多真实的文本分类应用程序可以自然地表示为一个图，其中捕获了单词、文档和语料库的全局特征。本综述将覆盖到2023年的方法，包括语料库级别和文档级别的图神经网络。我们详细讨论了每种方法，包括图构建机制和基于图的学习过程。除了技术综述，我们还关注了使用图神经网络进行文本分类的问题和未来方向。我们还涵盖了数据集、评估指标和实验设计，并呈现了在公开可用的基准数据集上发布的性能总结，以更好地了解基于图神经网络的文本分类领域的最新技术发展。

    Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchma
    
[^11]: 在资源受限的嵌入式设备上部署基于BERT的NLP模型的挑战探究

    Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])

    [http://arxiv.org/abs/2304.11520](http://arxiv.org/abs/2304.11520)

    本文探究了在资源受限的嵌入式设备上部署基于BERT的NLP模型的挑战，并得出结论：虽然DistilBERT和TinyBERT等轻量级模型相对占用更少内存，但它们在复杂的NLP任务上表现较差；ResNet-based BERT模型可以在精度和资源效率之间取得良好的平衡，适合在嵌入式设备上部署。

    

    基于BERT的神经架构已经成为许多下游NLP任务的流行先进技术基准。然而，这些架构对数据依赖性强，占用大量内存和能量，经常阻碍它们在许多实时、资源受限的应用程序中的部署。现有的BERT轻量级版本（例如DistilBERT和TinyBERT）通常在复杂的NLP任务上无法表现出良好的性能。更重要的是，从设计师的角度来看，要为特定的NLP任务使用何种“正确的”基于BERT的架构，以在资源可用性和最终用户需求的最小精度之间实现最佳权衡，尚不确定。系统工程师必须花费大量时间进行试错实验，以找到合适的答案。本文在不同的资源限制和精度预算下对BERT-based模型进行了探究性研究，以得出有关此资源/精度权衡的经验性观察结果。我们的研究发现，虽然DistilBERT和TinyBERT等更轻量级的模型相对BERT-base占用的内存要少得多，但它们在复杂的NLP任务中精度的下降是明显的。我们还观察到，特别是基于ResNet的BERT模型，可以在准确性和资源效率之间取得良好的平衡，使其成为在资源受限的嵌入式设备中部署的良好候选模型。

    BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer's perspective, it is unclear what is the "right" BERT-based architecture to use for a given NLP task that can strike the optimal trade-off between the resources available and the minimum accuracy desired by the end user. System engineers have to spend a lot of time conducting trial-and-error experiments to find a suitable answer to this question. This paper presents an exploratory study of BERT-based models under different resource constraints and accuracy budgets to derive empirical observations about this resource/accuracy trade-offs. Our findin
    
[^12]: 使用抽象意义表示减少翻译语言的特征

    Translationese Reduction using Abstract Meaning Representation. (arXiv:2304.11501v1 [cs.CL])

    [http://arxiv.org/abs/2304.11501](http://arxiv.org/abs/2304.11501)

    研究使用抽象意义表示（AMR）作为跨语言方案可以减少翻译语言学数量。

    

    翻译文本或话语具有与本地语言不同的几个明显特征。这种现象被称为翻译语言学，已经有了很好的文献记录，当出现在训练或测试集中时，会影响模型性能。但是，减少人工翻译文本中翻译语言学的研究工作还很少。我们假设抽象意义表示（AMR）作为一种从表面形式抽象出来的语义表示，可以用作减少翻译语言学数量的一种跨语言方案。通过将英文翻译解析成AMR图，然后从该AMR生成文本，我们可以获得更接近非翻译语言文本的文本，通过宏观级别的度量，我们展示了使用AMR作为跨语言方案可以实现减少翻译语言学，我们将我们的结果与另外两种方法进行了比较：基于往返机器翻译的方法和基于句法控制生成的方法。

    Translated texts or utterances bear several hallmarks distinct from texts originating in the language. This phenomenon, known as translationese, is well-documented, and when found in training or test sets can affect model performance. Still, work to mitigate the effect of translationese in human translated text is understudied. We hypothesize that Abstract Meaning Representation (AMR), a semantic representation which abstracts away from the surface form, can be used as an interlingua to reduce the amount of translationese in translated texts. By parsing English translations into an AMR graph and then generating text from that AMR, we obtain texts that more closely resemble non-translationese by macro-level measures. We show that across four metrics, and qualitatively, using AMR as an interlingua enables the reduction of translationese and we compare our results to two additional approaches: one based on round-trip machine translation and one based on syntactically controlled generation
    
[^13]: 通过提示提高大型语言模型的心智理论表现

    Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])

    [http://arxiv.org/abs/2304.11490](http://arxiv.org/abs/2304.11490)

    本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。

    

    2023年，大型语言模型（LLMs）在许多任务中表现出色，但在复杂推理方面仍面临挑战。心智理论（ToM）任务需要理解代理人的信念、目标和心理状态，对于涉及人类的常识推理至关重要，因此提高LLM在这方面的表现至关重要。本研究测量了GPT-4和三个GPT-3.5变体（Davinci-2、Davinci-3、GPT-3.5-Turbo）的ToM表现，并研究了上下文学习提高它们的ToM理解力的有效性。我们评估了包含两步思维推理和逐步思考说明的提示。我们发现，通过人类反馈的强化学习（RLHF）训练的LLMs（除Davinci-2外的所有模型）通过上下文学习提高了它们的ToM准确性。GPT-4在零轮情况下表现最佳，达到了近80%的ToM准确性，但仍不足测试集上87%的人类准确性。然而，当提供上下文学习的提示时，GPT-4和三个GPT-3.5变体的ToM准确性显著高于无提示时，其中表现最好的模型（GPT-3.5-Turbo）达到了92%的准确性。我们的研究展示了上下文学习提升LLM在复杂推理尤其是ToM任务中表现的潜力。

    Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
    
[^14]: 识别与帮助社区成员的帮派相关社交媒体交流中的词汇偏差研究

    Understanding Lexical Biases when Identifying Gang-related Social Media Communications. (arXiv:2304.11485v1 [cs.CL])

    [http://arxiv.org/abs/2304.11485](http://arxiv.org/abs/2304.11485)

    本研究使用自然语言处理工具有效识别了帮派相关社交媒体上可能需要社区资源帮助的人群，拓展了社区成员照顾的范畴。

    

    参与帮派活动的人使用包括Facebook和Twitter在内的主流社交媒体来表达嘲讽和威胁以及哀悼和纪念。然而，识别帮派相关活动的影响以通过社交媒体为社区成员提供帮助是具有独特挑战的，这包括道德上识别受帮派活动影响的个体的训练数据的困难和需要考虑这些个体在推文中常用的非标准语言风格。我们的研究提供了证据表明，自然语言处理工具可以有效地识别可能需要社区照顾资源，如顾问、冲突调解者或学术/专业培训计划的个体。我们证明了我们的二元逻辑分类器在识别受帮派相关暴力影响的个体时，在使用与2015年巴尔的摩暴动相关的帮派相关推文样本时，优于基线标准。

    Individuals involved in gang-related activity use mainstream social media including Facebook and Twitter to express taunts and threats as well as grief and memorializing. However, identifying the impact of gang-related activity in order to serve community member needs through social media sources has a unique set of challenges. This includes the difficulty of ethically identifying training data of individuals impacted by gang activity and the need to account for a non-standard language style commonly used in the tweets from these individuals. Our study provides evidence of methods where natural language processing tools can be helpful in efficiently identifying individuals who may be in need of community care resources such as counselors, conflict mediators, or academic/professional training programs. We demonstrate that our binary logistic classifier outperforms baseline standards in identifying individuals impacted by gang-related violence using a sample of gang-related tweets associ
    
[^15]: (向量)空间不是最后的疆域：将产品搜索看作程序合成

    (Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])

    [http://arxiv.org/abs/2304.11473](http://arxiv.org/abs/2304.11473)

    本文主张将产品搜索看作程序合成，相比向量空间模型有着重大优势。

    

    随着电子商务的不断增长，巨额投资用于信息检索的机器学习和自然语言处理也随之而来。虽然向量空间模型主宰了产品搜索中的检索模型，但随着深度学习的出现，向量化本身也发生了巨大变化。我们的立场论文以相反的方式主张，即程序合成对许多查询和市场中的大量参与者提供了重大优势。我们详细说明了所提出方法的行业重要性，概述了具体实现细节，并基于我们在Tooso构建类似系统的经验，回答了一些常见的反对意见。

    As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
    
[^16]: L3Cube-IndicSBERT: 使用多语言BERT学习跨语言句子表示的简单方法

    L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])

    [http://arxiv.org/abs/2304.11434](http://arxiv.org/abs/2304.11434)

    该论文提出了一种简单但有效的方法，使用合成语料库将BERT模型转换成SBERT模型。该方法在10种主要的印欧语言中具有很好的效果，并展示了其在非印欧语言上的应用性。

    

    多语言句子BERT (SBERT) 模型将不同语言映射到共同的表示空间，对于跨语言相似性和挖掘任务非常有用。我们提出了一种简单而有效的方法，使用合成语料库将普通的多语言BERT模型转换成多语言句子BERT模型。我们简单地聚合低资源目标语言的翻译 NLI 或 STS 数据集，并对普通的多语言BERT模型进行类似SBERT的微调。我们表明，多语言BERT模型具有内在的跨语言学习能力，这种简单的微调方法没有显式的跨语言训练，却产生了非常出色的跨语言表示效果。我们展示了我们的方法在10种主要的印欧语言中的有效性，并展示了我们的方法适用于非印欧语言德语和法语。利用这种方法，我们进一步提出了L3Cube-IndicSBERT，这是第一个专门针对印度语言印地语和马来语的多语言句子表示模型。

    The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, M
    
[^17]: 华语中依赖长度极小化的有限理性解析

    A bounded rationality account of dependency length minimization in Hindi. (arXiv:2304.11410v1 [cs.CL])

    [http://arxiv.org/abs/2304.11410](http://arxiv.org/abs/2304.11410)

    本研究测试了在华语中追求依赖长度极小化的做法，发现仅将最短的前置成分放置在主谓动词旁边可以解释词序偏好，普及该方法可视为一种最小化努力策略，符合有限理性的观点。

    

    依赖长度极小化原则旨在把句子中语法关联的单词放在接近的位置，被认为对于有效的沟通来说，这种原则在塑造人类语言结构时普遍起作用。然而，人类语言系统中应用依赖长度极小化的程度尚未得到完全理解。在句子中，长时限词元放在短时限词元之前，短时限词元放在长时限词元之前已知可以最小化句子的整体依赖长度。在本研究中，我们测试了一种假设：在华语（一种SOV语言）中，将只有最短的前置成分放置在主谓动词旁边可以解释词序偏好，而不是全局最小化依赖长度。我们将此方法描述为最小化努力策略，因为这是一种成本有效的方法，可以缩短动词和其前置依赖之间的所有依赖。因此，这种方法符合有限理性的观点。

    The principle of DEPENDENCY LENGTH MINIMIZATION, which seeks to keep syntactically related words close in a sentence, is thought to universally shape the structure of human languages for effective communication. However, the extent to which dependency length minimization is applied in human language systems is not yet fully understood. Preverbally, the placement of long-before-short constituents and postverbally, short-before-long constituents are known to minimize overall dependency length of a sentence. In this study, we test the hypothesis that placing only the shortest preverbal constituent next to the main-verb explains word order preferences in Hindi (a SOV language) as opposed to the global minimization of dependency length. We characterize this approach as a least-effort strategy because it is a cost-effective way to shorten all dependencies between the verb and its preverbal dependencies. As such, this approach is consistent with the bounded-rationality perspective according t
    
[^18]: LaMP：当大型语言模型遇见个性化

    LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])

    [http://arxiv.org/abs/2304.11406](http://arxiv.org/abs/2304.11406)

    本论文强调了当前自然语言处理领域中个性化的重要性，并提出了LaMP（一种用于训练和评估大型语言模型的新的个性化基准），并针对大型语言模型的生成任务，设计了七项个性化任务以及一种检索增强方法，结果表明在利用用户配置文件扩展大型语言模型的基础上，其生成结果明显优于传统方法。

    

    本文强调在当前自然语言理解和生成领域的个性化的重要性，并介绍了LaMP基准——用于训练和评估生成个性化输出的语言模型的新典范。LaMP提供了一个全面的评估框架，具有多样化的语言任务和每个用户的多个条目，包括三个分类任务和四个文本生成任务的七个个性化任务。我们还提出了一种检索增强方法，可从用户配置文件中检索个性化项目，构建大型语言模型的个性化提示。我们的基线零-shot和微调模型的结果表明，利用个人资料扩展的LM优于不考虑个人资料信息的对应模型。

    This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.
    
[^19]: 基于Transformer的语言模型Surprisal最佳的预测人类阅读时间的训练令牌数约为20亿

    Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])

    [http://arxiv.org/abs/2304.11389](http://arxiv.org/abs/2304.11389)

    本文研究表明，基于Transformer的语言模型在观察约20亿个训练令牌后能够提供最佳拟合，其surprisal估计能力能够最好地预测人类阅读时间。在语言模型收敛时，较小的模型变体会出现“临界点”，使语言模型困惑度下降，从而导致较差的人类拟合度。

    

    最近的心理语言学研究对语言模型的质量与其surprisal估计能力预测人类阅读时间的关系得出了相互矛盾的结论，这可能是由于研究中的训练数据量和模型容量的巨大差距所致。本文旨在通过评估基于Transformer的语言模型变体的Surprisal估计能力，这些变体在训练数据量和模型容量方面有系统变化，以整合这些发现。结果表明，大多数具有现代模型容量的变体的Surprisal估计在观察约20亿个训练令牌后提供最佳拟合，此后它们开始偏离与人类期望相符的界限。此外，新训练的较小模型变体在收敛时显示出“临界点”，在此之后，语言模型困惑度的下降开始导致较差的人类拟合度。

    Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human
    
[^20]: SAILER: 面向法律案例检索的结构感知预训练语言模型

    SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval. (arXiv:2304.11370v1 [cs.IR])

    [http://arxiv.org/abs/2304.11370](http://arxiv.org/abs/2304.11370)

    本文提出了一种结构感知预训练语言模型SAILER，针对法律案例检索中的长文本序列和关键法律要素敏感问题，采用遮蔽语言建模任务和结构感知连贯性预测任务相结合的多任务预训练策略，在两个法律案例检索数据集上实现了显著优于强基线模型的性能表现。

    

    针对智能法律系统中的核心工作——法律案例检索，本文提出了一种新的结构感知预训练语言模型SAILER。与通用文档相比，法律案例文件通常具有固有的逻辑结构，并包含关键的法律要素。SAILER采用多任务预训练策略，包括遮蔽语言建模任务和适用于法律文档的结构感知连贯性预测任务。实验结果表明，SAILER在两个法律案例检索数据集上显著优于几个强基线模型。

    Legal case retrieval, which aims to find relevant cases for a query case, plays a core role in the intelligent legal system. Despite the success that pre-training has achieved in ad-hoc retrieval tasks, effective pre-training strategies for legal case retrieval remain to be explored. Compared with general documents, legal case documents are typically long text sequences with intrinsic logical structures. However, most existing language models have difficulty understanding the long-distance dependencies between different structures. Moreover, in contrast to the general retrieval, the relevance in the legal domain is sensitive to key legal elements. Even subtle differences in key legal elements can significantly affect the judgement of relevance. However, existing pre-trained language models designed for general purposes have not been equipped to handle legal elements.  To address these issues, in this paper, we propose SAILER, a new Structure-Aware pre-traIned language model for LEgal c
    
[^21]: 利用多语言对抗训练和横向抑制技术的方法优化罗马尼亚语多词表达的识别

    Romanian Multiword Expression Detection Using Multilingual Adversarial Training and Lateral Inhibition. (arXiv:2304.11350v1 [cs.CL])

    [http://arxiv.org/abs/2304.11350](http://arxiv.org/abs/2304.11350)

    本文使用多语言对抗训练和横向抑制技术对罗马尼亚语多词表达进行自动识别，提高了已有模型在未见过的多词表达上的F1分数，达到了最先进水平。

    

    多词表达是开发大规模、语言学上可靠的自然语言处理技术的关键因素。本文描述了我们在PARSEM v1.2共享任务语料库上自动识别罗马尼亚语多词表达方面所做的改进。我们的方法基于最近引入的横向抑制层和对抗训练的多语言视角，以提高所采用的多语言语言模型的性能。在这两种方法的帮助下，我们提高了XLM-RoBERTa在未见过的多词表达方面的F1分数，也就是PARSEME 1.2版本的主要任务，约为2.7%。此外，我们的结果可以被认为是最先进技术，因为它们超过了本次比赛中参赛者在罗马尼亚语方面的先前结果。

    Multiword expressions are a key ingredient for developing large-scale and linguistically sound natural language processing technology. This paper describes our improvements in automatically identifying Romanian multiword expressions on the corpus released for the PARSEME v1.2 shared task. Our approach assumes a multilingual perspective based on the recently introduced lateral inhibition layer and adversarial training to boost the performance of the employed multilingual language models. With the help of these two methods, we improve the F1-score of XLM-RoBERTa by approximately 2.7% on unseen multiword expressions, the main task of the PARSEME 1.2 edition. In addition, our results can be considered SOTA performance, as they outperform the previous results on Romanian obtained by the participants in this competition.
    
[^22]: 基于语义专业化的知识驱动词义消歧研究

    Semantic Specialization for Knowledge-based Word Sense Disambiguation. (arXiv:2304.11340v1 [cs.CL])

    [http://arxiv.org/abs/2304.11340](http://arxiv.org/abs/2304.11340)

    本文提出了一种基于词汇知识的词义消歧语义专业化方法，通过调整上下文嵌入向量，将语义相关的“sense”和上下文彼此加近，将不相关的“sense”彼此远离。

    

    基于预训练语言模型计算“sense”和上下文嵌入向量相似度的方法是目前词义消歧任务的一种有前途的方法。本文提出了一种基于词汇知识的词义消歧语义专业化方法，通过调整上下文嵌入向量，将语义相关的“sense”和上下文彼此加近，将不相关的“sense”彼此远离。采用Attract-Repel优化方式优化sense对，采用自训练的方式优化context-sense对，并控制嵌入向量与原来的差距。该方法在英语、意大利语和中文三种语言的知识驱动词义消歧任务中表现优异，取得了最好的成绩。

    A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the \textit{sense} and \textit{context} embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-
    
[^23]: 关于应用评论中能源相关问题的识别

    On the Identification of the Energy related Issues from the App Reviews. (arXiv:2304.11292v1 [cs.AI])

    [http://arxiv.org/abs/2304.11292](http://arxiv.org/abs/2304.11292)

    本文研究了自动提取与能源相关的应用程序评论的不同技术。结果表明，神经网络优于其他机器学习模型。

    

    应用程序的能源效率问题可能会对应用程序用户造成重大问题，并在应用商店广泛讨论。之前的研究表明，研究与能源相关的应用程序评论以确定能源相关用户反馈的主要原因或类别的重要性。然而，还没有研究有效地自动提取与能源相关的应用程序评论。在本文中，我们经验性地研究了不同的技术，以自动提取与能源相关的用户反馈。我们比较了许多机器学习模型的准确性、F1分数和运行时间，并与相关特征组合和相对较新的基于神经网络的模型进行比较。总共比较了60个机器学习模型，以及使用六种神经网络架构和三种单词嵌入模型构建的30个模型。我们开发了一个可视化工具，通过该工具，开发人员可以遍历这个大规模的结果集。结果表明，神经网络优于其他机器学习模型。

    The energy inefficiency of the apps can be a major issue for the app users which is discussed on App Stores extensively. Previous research has shown the importance of investigating the energy related app reviews to identify the major causes or categories of energy related user feedback. However, there is no study that efficiently extracts the energy related app reviews automatically. In this paper, we empirically study different techniques for automatic extraction of the energy related user feedback. We compare the accuracy, F1-score and run time of numerous machine-learning models with relevant feature combinations and relatively modern Neural Network-based models. In total, 60 machine learning models are compared to 30 models that we build using six neural network architectures and three word embedding models. We develop a visualization tool for this study through which a developer can traverse through this large-scale result set. The results show that neural networks outperform the 
    
[^24]: 人工智能在香港中学生创意写作中的角色

    The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students. (arXiv:2304.11276v1 [cs.CL])

    [http://arxiv.org/abs/2304.11276](http://arxiv.org/abs/2304.11276)

    本研究探讨了如何利用语言模型帮助香港中学生在创意写作方面，实证结果表明，语言模型在帮助学生作家更有创造力方面发挥着不同的作用，例如作为合作者、挑衅者等。

    

    最近自然语言处理能力的进步导致了语言模型（例如ChatGPT）的开发，它能够生成类似于人的语言。在本研究中，我们探讨了如何利用语言模型来帮助创意写作的构思方面。我们的实证结果表明，语言模型在帮助学生作家更有创造力方面发挥着不同的作用，例如作为合作者、挑衅者等。

    The recent advancement in Natural Language Processing (NLP) capability has led to the development of language models (e.g., ChatGPT) that is capable of generating human-like language. In this study, we explore how language models can be utilized to help the ideation aspect of creative writing. Our empirical findings show that language models play different roles in helping student writers to be more creative, such as the role of a collaborator, a provocateur, etc
    
[^25]: UBC-DLNLP在SemEval-2023任务12中的贡献：转移学习对非洲情感分析的影响

    UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis. (arXiv:2304.11256v1 [cs.CL])

    [http://arxiv.org/abs/2304.11256](http://arxiv.org/abs/2304.11256)

    该研究旨在解决非洲14个不同语言的情感分析任务，使用转移学习的方法，并取得了不错的成果，可应用于其他语言的情感分析任务。

    

    我们描述了我们在SemEval 2023 AfriSenti-SemEval共享任务中的贡献，其中我们解决了14种不同非洲语言的情感分析任务。我们在完全监督的条件下开发了单语和多语模型（子任务A和B）。我们还为零-shot设置（子任务C）开发了模型。我们的方法涉及使用六种语言模型进行转移学习的实验，包括一些模型的进一步调整以及最后的微调阶段。我们效果最好的模型在开发数据上实现了70.36的F1分数，在测试数据上实现了66.13的F1分数。我们的结果表明了跨多种语言的情感分析中，转移学习和微调技术的有效性。我们的方法可以应用于不同语言和领域的其他情感分析任务。

    We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared task, where we tackle the task of sentiment analysis in 14 different African languages. We develop both monolingual and multilingual models under a full supervised setting (subtasks A and B). We also develop models for the zero-shot setting (subtask C). Our approach involves experimenting with transfer learning using six language models, including further pertaining of some of these models as well as a final finetuning stage. Our best performing models achieve an F1-score of 70.36 on development data and an F1-score of 66.13 on test data. Unsurprisingly, our results demonstrate the effectiveness of transfer learning and fine-tuning techniques for sentiment analysis across multiple languages. Our approach can be applied to other sentiment analysis tasks in different languages and domains.
    
[^26]: 面向特定群体的自然语言处理检测仇恨言论的方法

    A Group-Specific Approach to NLP for Hate Speech Detection. (arXiv:2304.11223v1 [cs.CL])

    [http://arxiv.org/abs/2304.11223](http://arxiv.org/abs/2304.11223)

    本文提出了一种面向特定群体的自然语言处理方法，通过将特定群体的历史和语言知识纳入仇恨言论检测模型中，分析有关歧视历史的数据，以更好地预测对该群体的仇恨言论激增，并通过交叉性和伦理标准对模型进行批判性评估。

    

    自动检测仇恨言论是一项重要但复杂的任务，需要了解常识、受保护群体的刻板印象和歧视历史，这些内容可能会不断演变。在本文中，我们提出了一种面向特定群体的自然语言处理方法，用于在线检测仇恨言论。该方法包括将有关特定保护群体的历史和语言知识纳入仇恨言论检测模型中，分析有关受保护群体歧视的历史数据，以更好地预测对该群体的仇恨言论激增，并通过交叉性和伦理标准对仇恨言论检测模型进行批判性评估。我们通过针对反犹主义仇恨言论检测的案例研究来演示这种方法。该案例研究综合了目前关于自然语言处理检测反犹主义言论的英语文献，并引入了一个涵盖 20 世纪至今反犹主义历史和语言的新型知识图谱。

    Automatic hate speech detection is an important yet complex task, requiring knowledge of common sense, stereotypes of protected groups, and histories of discrimination, each of which may constantly evolve. In this paper, we propose a group-specific approach to NLP for online hate speech detection. The approach consists of creating and infusing historical and linguistic knowledge about a particular protected group into hate speech detection models, analyzing historical data about discrimination against a protected group to better predict spikes in hate speech against that group, and critically evaluating hate speech detection models through lenses of intersectionality and ethics. We demonstrate this approach through a case study on NLP for detection of antisemitic hate speech. The case study synthesizes the current English-language literature on NLP for antisemitism detection, introduces a novel knowledge graph of antisemitic history and language from the 20th century to the present, in
    
[^27]: 学习“不学习”: 朝向聊天机器人中的生成安全

    Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v1 [cs.CL])

    [http://arxiv.org/abs/2304.11220](http://arxiv.org/abs/2304.11220)

    本文提出了一种名为“LOT”的框架，采用对比损失训练聊天机器人以从正面和负面训练信号中增强泛化能力，并使用离散度将生成向量从不安全子空间指向安全子空间，从而避免生成不安全的内容。

    

    生成式、开放领域的对话模型尤其容易生成不安全的内容，因为它们是在基于Web的社交数据上训练的。先前缓解这个问题的方法存在缺点，如打断对话流程、对未见过的有毒输入环境的泛化能力不强、为了安全而牺牲对话质量等。在本文中，我们提出了一种新颖的框架，称为“LOT”（Learn NOT to），它采用对比损失来增强泛化能力，通过同时从正面和负面训练信号中学习来做到这一点。相较于标准的对比学习框架，我们的方法从先前学习的安全和不安全语言分布中自动获得正、负信号。LOT框架利用离散度将生成向量从不安全子空间指向安全子空间，同时维持对话的流程。我们的方法内存和时间效率高，在SafeDialog数据集上实现了最先进的性能。

    Conversational models that are generative and open-domain are particularly susceptible to generating unsafe content since they are trained on web-based social data. Prior approaches to mitigating this issue have drawbacks, such as disrupting the flow of conversation, limited generalization to unseen toxic input contexts, and sacrificing the quality of the dialogue for the sake of safety. In this paper, we present a novel framework, named "LOT" (Learn NOT to), that employs a contrastive loss to enhance generalization by learning from both positive and negative training signals. Our approach differs from the standard contrastive learning framework in that it automatically obtains positive and negative signals from the safe and unsafe language distributions that have been learned beforehand. The LOT framework utilizes divergence to steer the generations away from the unsafe subspace and towards the safe subspace while sustaining the flow of conversation. Our approach is memory and time-ef
    
[^28]: 辩证语言模型评估：对LLMs通识空间推理能力的初步评估

    Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs. (arXiv:2304.11164v1 [cs.CL])

    [http://arxiv.org/abs/2304.11164](http://arxiv.org/abs/2304.11164)

    本文介绍了一种辩证评估方式，旨在描绘语言模型系统失败的边界并检查其一致性，以及对LLMs通识空间推理能力进行了初步定性研究。

    

    最近语言模型非常受欢迎，并且其能力得到了许多赞誉，包括通识推理。鉴于当前语言模型在以往通识推理静态基准上取得越来越好的结果，我们探索了一种另类的辩证评估方式。这种评估的目标不是获得一个总体性能值，而是找到失败的地方并描绘系统的边界。与系统对话可以检查其一致性，并获得越过单一证据的边界保证。在本文中，我们就空间推理（通识推理的基本方面）的特定情况进行了一些定性研究。结论包括对未来工作的一些建议，旨在提高语言模型的能力并将这种辩证评估系统化。

    Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
    
[^29]: ChatGPT、大语言技术以及造福人类的曲折之路

    ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity. (arXiv:2304.11163v1 [cs.CY])

    [http://arxiv.org/abs/2304.11163](http://arxiv.org/abs/2304.11163)

    必须投资于协作的AI安全和伦理研究，开发可持续和公正的标准以创建有益人工智能，否则我们将看到技术进步超越我们的伦理和社会影响能力的未来。

    

    新兴的人工智能技术无疑具有吸引力。然而，只有当我们缺乏对于日益扩大的全球不平等和紧迫的存在威胁下人性究竟应该是什么的细致理解时，人工智能技术将让所有人受益的承诺是空洞的。为了顺利前进，我们需要投资于严谨而协作的人工智能安全和伦理研究，需要开发可持续且公正的标准以区分仅仅是推测性的和经过充分研究的问题。只有后者才能使我们共同构建和推广创造有益人工智能所必需的价值观。不这样做可能会导致未来我们的人工智能技术进步超过我们导航它们的伦理和社会影响的能力。我们不想走上这条路。

    The allure of emerging AI technologies is undoubtedly thrilling. However, the promise that AI technologies will benefit all of humanity is empty so long as we lack a nuanced understanding of what humanity is supposed to be in the face of widening global inequality and pressing existential threats. Going forward, it is crucial to invest in rigorous and collaborative AI safety and ethics research. We also need to develop standards in a sustainable and equitable way that differentiate between merely speculative and well-researched questions. Only the latter enable us to co-construct and deploy the values that are necessary for creating beneficial AI. Failure to do so could result in a future in which our AI technological advancements outstrip our ability to navigate their ethical and social implications. This path we do not want to go down.
    
[^30]: Text2Time: 基于Transformer的文章时间段预测器

    Text2Time: Transformer-based article time period predictor. (arXiv:2304.10859v1 [cs.CL])

    [http://arxiv.org/abs/2304.10859](http://arxiv.org/abs/2304.10859)

    本文提出了一个基于Transformer模型的文章时间段预测器，使用预训练的BERT模型对新闻文章进行分类的结果表现优于先前尝试的模型，具有很高的准确性。

    

    本论文探讨利用文本内容预测文章发表时间段的问题。我们创建了一个包含超过35万篇《纽约时报》历时六十年的标记数据集。我们实现了一个简单的朴素贝叶斯基准模型，它在准确性方面表现出人意料之外的不错性能。最后，我们使用了一个预训练的BERT模型，对其进行了微调以实现文本分类的任务。这个模型的性能超过了我们的预期，并提供了一些非常令人印象深刻的结果，准确地将新闻文章分类至其出版的年代。结果超过了先前尝试的这种相对不受关注的文本预测任务模型的性能。

    We explore the problem of predicting the publication period of text document, such as a news article, using the text from that document. In order to do so, we created our own extensive labeled dataset of over 350,000 news articles published by The New York Times over six decades. We then provide an implementation of a simple Naive Bayes baseline model, which surprisingly achieves decent performance in terms of accuracy.Finally, for our approach, we use a pretrained BERT model fine-tuned for the task of text classification. This model exceeds our expectations and provides some very impressive results in terms of accurately classifying news articles into their respective publication decades. The results beat the performance of the few previously tried models for this relatively unexplored task of time prediction from text.
    
[^31]: IXA/Cogcomp在SemEval-2023任务2中的表现：基于知识库的上下文增强多语言命名实体识别。

    IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v1 [cs.CL])

    [http://arxiv.org/abs/2304.10637](http://arxiv.org/abs/2304.10637)

    本文提出了一种基于知识库的上下文增强的多语言命名实体识别方法，通过识别、链接和预测实体类别，能够准确地分类细粒度和新兴实体。

    

    命名实体识别(NER)是一项核心的自然语言处理任务，在这方面，预训练的语言模型表现出了卓越的性能。然而，像CoNLL 2003等标准基准并没有解决部署NER系统需要面对的许多挑战，例如需要以细粒度的方式对新兴或复杂实体进行分类。本文提出了一种新颖的NER级联方法，包括三个步骤：首先，识别输入句子中的实体候选项；其次，将每个候选项链接到现有的知识库；第三，预测每个实体候选项的细粒度类别。我们通过实验证明了外部知识库在准确分类细粒度和新兴实体方面的重要性。我们的系统在MultiCoNER2共享任务中表现出了鲁棒性能，即使在低资源语言环境中，我们也能利用高资源语言的知识库。

    Named Entity Recognition (NER) is a core natural language processing task in which pre-trained language models have shown remarkable performance. However, standard benchmarks like CoNLL 2003 \cite{conll03} do not address many of the challenges that deployed NER systems face, such as having to classify emerging or complex entities in a fine-grained way. In this paper we present a novel NER cascade approach comprising three steps: first, identifying candidate entities in the input sentence; second, linking the each candidate to an existing knowledge base; third, predicting the fine-grained category for each entity candidate. We empirically demonstrate the significance of external knowledge bases in accurately classifying fine-grained and emerging entities. Our system exhibits robust performance in the MultiCoNER2 \cite{multiconer2-data} shared task, even in the low-resource language setting where we leverage knowledge bases of high-resource languages.
    
[^32]: 用自然语言学习编程

    Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])

    [http://arxiv.org/abs/2304.10464](http://arxiv.org/abs/2304.10464)

    该论文提出了一种用自然语言作为编程语言并通过学习编程方法让大语言模型直接生成自然语言程序并指导推理的方法。实验结果表明，这种方法在解决编程任务上比基线方法有更高的成功率。

    

    大语言模型在各种基本自然语言任务中表现出卓越性能，这引起了实现人工通用智能的希望。为了更好地完成复杂任务，我们需要利用大语言模型进行编程，然后按照程序生成特定的解决方案。我们提出使用自然语言作为一种新的编程语言来描述任务过程，使它们易于人类和大语言模型理解。虽然大语言模型能够直接生成自然语言程序，但这些程序可能仍然存在错误或不完整的步骤。因此，我们进一步提出了学习编程（LP）的方法，要求大语言模型从复杂任务的训练数据集中学习自然语言程序，然后使用学习到的程序来指导推理。我们在AMPS（高中数学）和Math（竞赛数学问题）数据集上的实验证明了我们方法的有效性。在测试ChatGP解决编程任务时，LP能够实现80%的成功率，优于基线方法。

    Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks, which raises hopes for achieving Artificial General Intelligence. To better complete complex tasks, we need LLMs to program for the task and then follow the program to generate a specific solution for the test sample. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs. LLMs are capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program (LP) method to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference. Our experiments on the AMPS (high school math) and Math (competition mathematics problems) datasets demonstrate the effectiveness of our approach. When testing ChatGP
    
[^33]: ChatGPT能否复制人类生成的标签？对社交计算任务的研究

    Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])

    [http://arxiv.org/abs/2304.10145](http://arxiv.org/abs/2304.10145)

    本文研究了ChatGPT在社交计算任务中是否可以复制人类生成的标签注释，结果表明ChatGPT有潜力处理这些数据注释任务，尽管仍存在许多挑战。

    

    ChatGPT的发布揭示了语言模型可以取代人类智慧的各种可能性。本文旨在了解ChatGPT是否有潜力在社交计算任务中复制人类生成的标签注释。这样的成就可以显著降低社交计算研究的成本和复杂性。因此，我们使用ChatGPT重新标记了五个具有里程碑意义的数据集，涉及立场检测（2个）、情感分析、仇恨言论和机器人检测。我们的结果表明，ChatGPT有潜力处理这些数据注释任务，尽管仍存在许多挑战。ChatGPT获得了平均精度0.609。 ChatGPT对情感分析数据集的表现最佳，正确注释了64.9％的推文。然而，我们显示性能在不同标签之间有很大差异。我们认为这项工作可以开辟新的分析线路，并作为未来利用ChatGPT进行数据注释的基础。

    The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to re-label five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average precision 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploi
    
[^34]: 大规模语言模型中潜在空间理论对应新兴能力

    A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])

    [http://arxiv.org/abs/2304.09960](http://arxiv.org/abs/2304.09960)

    本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。

    

    语言并不是随机生成，而是为了传递信息。语言与其底层含义之间存在强烈的关联，在其相关性方面有着严重偏差的稀疏联合分布。此外，由于稀疏性，这些高峰值恰好与语言的边缘分布匹配。随着大数据和大模型上训练的LLMs的出现，我们现在可以精确评估语言的边缘分布，这提供了一种方便的探索联合分布稀疏结构实现有效推理的方式。在本文中，我们将语言分类为明确与{\epsilon}-模糊，并提出定量结果，以表明LLMs的新兴能力（例如语言理解、上下文学习、思路启发以及有效指令微调）都可以归因于对稀疏联合分布进行贝叶斯推断。

    Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
    
[^35]: GeneGPT: 教授大型语言模型使用NCBI Web API

    GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])

    [http://arxiv.org/abs/2304.09667](http://arxiv.org/abs/2304.09667)

    GeneGPT通过少量NCBI API调用URL请求作为演示，教授大型语言模型使用NCBI Web API回答基因组问题，并在GeneTuring测试中达到了优异的结果。

    

    本文介绍了GeneGPT，一种新颖的方法，用于教授大型语言模型（LLM）使用国家生物技术信息中心（NCBI）的Web应用程序编程接口（API），并回答基因组问题。具体而言，我们通过少量的NCBI API调用URL请求作为上下文学习的演示，启发Codex（code-davinci-002）解决GeneTuring测试。在推理过程中，一旦检测到调用请求，我们就停止解码并使用生成的URL进行API调用。我们然后将NCBI API返回的原始执行结果附加到生成的文本中，并继续生成直到找到答案或检测到另一个API调用。初步结果表明，GeneGPT在GeneTuring数据集的四个One-shot任务中取得了三个最先进的结果，在五个Zero-shot任务中取得了四个最先进的结果。总体而言，GeneGPT的宏平均分数为0.76，远高于检索增强LLM，如New Bin。

    In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
    
[^36]: SemEval 2023 任务6: LegalEval -- 理解法律文本

    SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])

    [http://arxiv.org/abs/2304.09548](http://arxiv.org/abs/2304.09548)

    SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。

    

    在人口众多的国家，待处理的法律案件呈指数增长。有必要开发基于自然语言处理的技术，对法律文件进行处理和自动理解。为了促进在法律自然语言处理领域的研究，我们在 SemEval 2023 上组织了共享任务 LegalEval - 理解法律文本。LegalEval 任务有三个子任务：Task-A（修辞角色标记）是自动将法律文件结构化为语义连贯的单元，Task-B（法律命名实体识别）处理在法律文件中识别相关实体，而 Task-C（法院判决预测与解释）探索了自动预测法律案件结果以及提供预测解释的可能性。共有26个团队（分布在全球的约100名参与者）提交了系统论文。在每个子任务中，所提出的系统都优于基准线；但是，仍然有很大的改进空间。本文介绍了 LegalEval 任务的组织和细节，并概述了参与系统及其性能。

    In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
    
[^37]: ChatGPT是否能够预测股票价格波动？回报可预测性与大语言模型。

    Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.07619](http://arxiv.org/abs/2304.07619)

    本研究探究了使用ChatGPT及其他大型语言模型预测股市回报的潜力，发现ChatGPT的预测表现优于传统情感分析方法，而基础模型无法准确预测股票价格变化，表明复杂模型可预测能力的崛起。这表明在投资决策过程中引入先进的语言模型可以提高预测准确性并增强定量交易策略的表现。

    

    本文研究了使用情感分析预测股市回报的潜力，探讨了使用ChatGPT以及其他大语言模型在预测股市回报方面的表现。我们使用ChatGPT判断新闻标题对公司股票价格是好消息、坏消息或无关消息。通过计算数字分数，我们发现这些"ChatGPT分数"和随后的日常股票市场回报之间存在正相关性。而且，ChatGPT的表现优于传统的情感分析方法。同时，我们发现GPT-1、GPT-2和BERT等基础模型无法准确预测回报，这表明回报可预测性是复杂模型的一种新兴能力。我们的研究结果表明，将先进的语言模型纳入投资决策过程可以产生更准确的预测，并提高定量交易策略的表现。

    We examine the potential of ChatGPT, and other large language models, in predicting stock market returns using sentiment analysis of news headlines. We use ChatGPT to indicate whether a given headline is good, bad, or irrelevant news for firms' stock prices. We then compute a numerical score and document a positive correlation between these ``ChatGPT scores'' and subsequent daily stock market returns. Further, ChatGPT outperforms traditional sentiment analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT cannot accurately forecast returns, indicating return predictability is an emerging capacity of complex models. Our results suggest that incorporating advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies.
    
[^38]: PDF-VQA: 一个新的用于PDF文件真实世界VQA的数据集

    PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])

    [http://arxiv.org/abs/2304.06447](http://arxiv.org/abs/2304.06447)

    该研究提出了一个新的文档VQA数据集PDF-VQA，以多个页面的完整文档作为研究对象，通过机器学习模型识别与处理文档元素、结构和内容等方面，为解决真实世界中的文档理解问题提供新的资源。

    

    基于文档的视觉问答（VQA）研究文档图像的文档理解问题。我们提出了一个新的基于文档的VQA数据集PDF-VQA，从文档元素识别、文档布局结构理解以及上下文理解和关键信息提取等各个方面全面探讨文档理解问题。我们的PDF-VQA数据集将文档理解的规模从单个文档页面扩展到询问多个页面的完整文档。我们还提出了一个新的基于图形的VQA模型，明确地集成了不同文档元素之间的空间和层次结构关系，以提高文档结构的理解能力。该性能与多个基线模型相比较，可以适用于不同的问题类型和任务。

    Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
    
[^39]: 通过可扩展的主题嵌入从连续新闻流中无监督地发现故事

    Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])

    [http://arxiv.org/abs/2304.04099](http://arxiv.org/abs/2304.04099)

    本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。

    

    无监督地发现实时相关新闻文章故事，有助于人们在不需要昂贵人工注释的情况下消化大量的新闻流。现有的无监督在线故事发现研究的普遍方法是用符号或基于图的嵌入来表示新闻文章，并将它们逐步聚类成故事。最近的大型语言模型有望进一步改善嵌入，但是通过无差别地编码文章中的所有信息来直接采用这些模型无法有效处理富含文本且不断发展的新闻流。在这项工作中，我们提出了一种新颖的主题嵌入方法，使用现成的预训练句子编码器来动态表示文章和故事，并考虑它们共享的时间主题。为了实现无监督在线故事发现的想法，引入了一个可扩展框架USTORY，包括两个主要技术，即主题和时间感知的动态嵌入和新颖性感知的自适应聚类。

    Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
    
[^40]: SSS在SemEval-2023任务10中的论文：使用投票细调变压器可解释的检测在线性别歧视。 (arXiv：2304.03518v1 [cs.CL])

    SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])

    [http://arxiv.org/abs/2304.03518](http://arxiv.org/abs/2304.03518)

    本文描述了使用细调BERT模型和多数投票集成模型来检测和解释在线性别歧视的方法。翻转显着降低了女性在社交媒体平台上经历不成比例的性别歧视的风险。

    

    本文描述了我们在SemEval 2023任务10中提交的作品-可解释的在线性别歧视检测（EDOS），分为三个子任务。社交媒体平台的不断增长导致女性在社交媒体平台上面临不成比例的性别歧视。这使得检测和解释在线性别歧视内容变得比以往更加重要，以使社交媒体对女性更加安全和可访问。我们的方法包括实验和微调基于BERT的模型，并使用多数投票集合模型，该模型优于单个基线模型得分。我们的系统在任务A中实现了宏F1分数0.8392，在任务B中为0.6092，在任务C中为0.4319。

    This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
    
[^41]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^42]: $\mathcal{E}$ K\'U [MASK]: 将尤鲁巴文化问候语整合到机器翻译中

    $\mathcal{E}$ K\'U [MASK]: Integrating Yor\`ub\'a cultural greetings into machine translation. (arXiv:2303.17972v1 [cs.CL])

    [http://arxiv.org/abs/2303.17972](http://arxiv.org/abs/2303.17972)

    本文研究了将尤鲁巴文化问候语（$\mathcal{E}$ k\'u [MASK]）整合到机器翻译中，通过IkiniYor\`ub\'a数据集，我们发现大规模多语言神经机器翻译（NMT）系统无法准确翻译，而微调现有的NMT模型在翻译中有更好的表现。

    

    本文研究了大规模多语言神经机器翻译（NMT）系统在将尤鲁巴语问候语（$\mathcal{E}$ k\'u [MASK]）翻译成英文时的表现。为了评估这些模型，我们介绍了一个尤鲁巴语-英语翻译数据集IkiniYor\`ub\'a，其中包含尤鲁巴语问候语和样例用例。我们分析了包括Google和NLLB在内的不同多语言NMT系统的表现，并显示这些模型在准确翻译尤鲁巴语问候语到英语时存在困难。此外，我们通过在IkiniYor\`ub\'a的训练集上微调现有的NMT模型来训练一个尤鲁巴语-英语模型，与预训练的多语言NMT模型相比，其表现更好，尽管它们经过大量数据的训练。

    This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yor\`ub\'a greetings ($\mathcal{E}$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings, and sample use cases. We analysed the performance of different multilingual NMT systems including Google and NLLB and show that these models struggle to accurately translate Yor\`ub\'a greetings into English. In addition, we trained a Yor\`ub\'a-English model by finetuning an existing NMT model on the training split of IkiniYor\`ub\'a and this achieved better performance when compared to the pre-trained multilingual NMT models, although they were trained on a large volume of data.
    
[^43]: 语义阅读器项目：利用人工智能驱动的交互式阅读界面增强学术文档

    The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])

    [http://arxiv.org/abs/2303.14334](http://arxiv.org/abs/2303.14334)

    本文探讨了利用人工智能和人机交互技术为研究论文提供智能、交互式和无障碍的阅读界面的可行性，并介绍了跨机构合作的语义阅读器项目。

    

    学术出版物是学者向他人传递知识的关键。然而，研究论文信息密集，随着科学文献量的增长，需要新技术支持阅读过程。与通过互联网技术转变的查找论文过程不同，阅读研究论文的体验几十年来几乎没有改变。虽然PDF格式因其便携性而广泛使用，但它有重大缺点，包括：静态内容，低视觉读者的可访问性差，以及在移动设备上阅读困难。本文探讨“最近的AI和HCI进展能否为遗留的PDF提供智能，交互式和无障碍的阅读界面？”，我们描述了语义阅读器项目，这是多个机构的协作努力，旨在探索为研究论文自动创建动态阅读界面的方法。

    Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this pro
    
[^44]: 多视角的零样本开放意图归纳：多领域批处理和代理梯度转移

    Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])

    [http://arxiv.org/abs/2303.13099](http://arxiv.org/abs/2303.13099)

    本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。

    

    在任务导向的对话系统中，检测和诱导新的意图是将该系统应用于实际应用的两个主要挑战。本文提出了语义多视角模型来解决这两个难题：（1）用于一般嵌入的SBERT（2）多领域批处理（MDB）用于对话领域知识，以及（3）用于集群专业语义的代理梯度转移（PGT）。 MDB一次向模型提供多种对话数据集，通过学习多领域知识来解决多领域问题。我们引入了一种新的方法PGT，它采用Siamese网络直接使用聚类方法微调模型。我们的模型可以学习如何使用PGT聚类对话语句。实验结果表明，与基线系统相比，我们的多视角模型与MDB和PGT显着提高了Open Intent Induction的性能。

    In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
    
[^45]: TOT：面向多模态仇恨检测的拓扑感知最优传输

    TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])

    [http://arxiv.org/abs/2303.09314](http://arxiv.org/abs/2303.09314)

    本文针对隐式危害检测的挑战，提出一种面向表情包情境的拓扑感知最优传输框架TOT，利用最优传输核方法从多个模态中捕捉互补信息。

    

    多模态仇恨检测旨在识别在线有害内容（如表情包等），是构建健康的互联网环境至关重要。以往的研究重点关注显式仇恨言论的检测，而忽略了隐式危害的分析，这在存在着扭曲或缺乏明显文本标记和人口统计视觉线索的情况下面临着特别大的挑战。本文提出了TOT：一种面向表情包情境的拓扑感知最优传输框架，将跨模态对齐问题转化为最优传输方案的求解。具体来说，我们利用最优传输核方法从多个模态中捕捉互补信息。核嵌入提供了一种非线性转换能力，以重现输入的分布。

    Multimodal hate detection, which aims to identify harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these semantic gaps issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce 
    
[^46]: DeltaScore: 利用差分扰动评价故事生成

    DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])

    [http://arxiv.org/abs/2303.08991](http://arxiv.org/abs/2303.08991)

    DeltaScore利用差分扰动来评估故事生成的细粒度方面，并通过计算故事在特定方面扰动前后的可能性差异来衡量影响。该方法在多个故事领域中得到了评估，并与人类判断的相关性进行了研究。

    

    自然语言生成的各种评价指标存在，但对于故事生成的实用性有限，因为它们通常与人类判断的相关性不强，也不能测量细粒度的故事方面，例如流畅度与相关性，因为它们旨在评估整体生成质量。本文提出DeltaScore，一种利用扰动来评估细粒度故事方面的方法。我们的核心思想是基于这样的假设：故事在特定方面表现得越好（例如流畅度），它就会受到特定扰动（例如引入错别字）的影响越大。为了衡量影响，我们使用语言模型计算扰动前后故事的可能性差异。我们在多个故事领域中使用DeltaScore评估了基于状态的最新模型和传统基于相似性的指标，并研究了它与人类在五个细粒度故事方面的判断之间的相关性。

    Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
    
[^47]: SemEval-2023任务11的Lon-ea：软硬标签预测中激活函数的比较。

    Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02468](http://arxiv.org/abs/2303.02468)

    本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。

    

    我们研究在学习不同意任务的软硬标签预测中，深度神经网络模型输出层中不同激活函数的影响。在该任务中，目标是通过预测软标签来量化不同意量。为了预测软标签，我们使用基于BERT的预处理器和编码器，并改变输出层中使用的激活函数，同时保持其他参数不变。然后将软标签用于硬标签预测。考虑的激活函数包括sigmoid函数以及添加到模型中的阶跃函数和本文中首次介绍的正弦激活函数。

    We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
    
[^48]: UZH_CLyp在SemEval-2023任务9中的表现：基于Head-First Fine-Tuning和ChatGPT数据生成的跨语言学习方法用于推文亲密度预测

    UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01194](http://arxiv.org/abs/2303.01194)

    本文介绍了UZH_CLyp在SemEval 2023任务9中的表现。我们的跨语言迁移学习方法包含了先使用Head-First Fine-Tuning（HeFiT）方法更新回归头参数，再降低学习率更新预训练transformer的参数。同时，我们研究了在没有人工标记数据的情况下，使用ChatGPT生成的自动小型样例来解决低资源问题。研究发现，HeFiT稳定训练并显著提高预训练模型的性能，使用合成数据时也能提高跨语言学习的性能。

    

    本文介绍了UZH_CLyp在SemEval 2023任务9“多语言推文亲密度分析”中的表现。我们在所有10种语言中均取得了第二好的结果，根据官方的Pearson相关系数回归评估指标。我们的跨语言迁移学习方法探索了使用Head-First Fine-Tuning方法（HeFiT）的益处，该方法首先仅更新回归头参数，然后再以降低的学习率更新预训练的transformer编码器参数。此外，我们研究了在低资源设置中使用一小组自动生成的示例（在我们的情况下，来自ChatGPT）对没有人工标记数据的情况的影响。我们的研究表明，HeFiT稳定了培训并且对于缺乏推文领域适应的预训练模型一致地提高了结果。我们的研究还表明，当使用合成数据时，跨语言学习的性能显着提高，证实了当前文本生成模型用于解决低资源情况的实用性。

    This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9 "Multilingual Tweet Intimacy Analysis". We achieved second-best results in all 10 languages according to the official Pearson's correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text gen
    
[^49]: 在特征空间中学习多模态数据增强

    Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14453](http://arxiv.org/abs/2212.14453)

    本文介绍了一种名为LeMDA的易于使用的方法，它在特征空间中自动学习联合增强多模态数据，提高了多模态学习算法的性能。

    

    能够联合学习多个模态（如文本、音频和视觉数据）是智能系统的一个核心特征。尽管设计神经网络来利用多模态数据取得了令人鼓舞的进展，但数据增强的巨大成功仍然局限于单模态任务，如图像分类。确实，在保留数据的整体语义结构的同时增强每个模态是特别困难的；例如，在应用了标准增强方法（例如翻译）之后，标题可能不再是图像的良好描述。此外，仍然很难指定不针对特定模态的合理变换。本文介绍了一种易于使用的方法，名为LeMDA（Learning Multimodal Data Augmentation），它在特征空间中自动学习联合增强多模态数据，而不限制模态的身份或模态之间的关系。我们在标准基准测试上展示了我们方法的有效性，证明LeMDA在多个领域（包括图像字幕和语音识别）中提高了多模态学习算法的性能。

    The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalit
    
[^50]: MN-DS：新闻文章层次分类的多标签数据集

    MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.12061](http://arxiv.org/abs/2212.12061)

    本文介绍了一个包含10,917篇新闻文章的多标签数据集，可用于训练机器学习模型自动按主题对新闻文章进行分类，对新闻结构、分类和预测未来事件的研究人员非常有帮助。

    

    本文介绍了一个数据集，其中包含10,917篇新闻文章，涵盖了从2019年1月1日到2019年12月31日的层次新闻分类。我们根据17个一级类别和109个二级类别的层次分类手动标记了这些文章。该数据集可用于训练机器学习模型，以自动按主题分类新闻文章。该数据集对于从事新闻结构、分类和根据发布的新闻预测未来事件的研究人员非常有帮助。

    This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
    
[^51]: 一种具有实体记忆的统一编码器-解码器框架

    A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.03273](http://arxiv.org/abs/2210.03273)

    本论文提出了一种名为EDMem的具有实体记忆的编码器-解码器框架，可以将实体知识融入到信息性文本生成中。实体知识以潜在表示形式存储在记忆中，并利用记忆中的实体链接限制实体生成。实验结果表明，EDMem优于基于记忆的自动编码器模型和非记忆编码器-解码器模型。

    

    实体作为现实世界知识的重要载体，在许多自然语言处理任务中起着关键作用。本文关注如何将实体知识融入到编码器-解码器框架中，以进行信息性文本生成。现有方法尝试通过索引、检索和阅读外部文档作为证据，但是它们的计算开销很大。为此，我们提出了一个名为EDMem的编码器-解码器框架，其中实体知识以潜在表示形式存储在记忆中，并在维基百科上预先训练记忆和编码器-解码器参数。为了精确生成实体名称，我们设计了三种解码方法，以通过记忆中的实体链接限制实体生成。EDMem是一个统一的框架，可用于各种实体密集型问答和生成任务。广泛的实验结果表明，EDMem优于基于记忆的自动编码器模型和非记忆编码器-解码器模型。

    Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.
    
[^52]: 学习组合视觉语言神经模块进行图像字幕生成

    Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning. (arXiv:2210.01338v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.01338](http://arxiv.org/abs/2210.01338)

    本文提出了一种图像字幕生成器，学习组合视觉和语言神经模块进行图像字幕生成。该方法使用了可区分的模块设计，并采用有效的模块组合和多目标学习方式，实现了在MSCOCO和Flickr30k上的性能优于现有方法的结果。

    

    人们倾向于将句子分解为不同的部分，如“sth在someplace做sth”，然后为每个部分填入某些内容。受此启发，我们遵循“模块化设计原则”提出了一种新的图像字幕生成器：学习组合视觉语言神经模块（CVLNM）。与VQA中广泛使用的神经模块网络不同，其中语言（即问题）是完全可见的，组合视觉语言模块的任务更具挑战性，因为语言仅部分可见，需要在图像字幕生成过程中动态组合模块。因此，我们提出了以下技术贡献：1）可区分的模块设计——编码器中包括一个函数词语言模块和三个不同内容词的视觉模块（即名词、形容词和动词），解码器中另外一个语言模块用于生成语言；2）有效的模块组合——基于强化学习的方法，为每个句子段动态组合适当的视觉和语言模块；3）多目标学习——联合训练目标，平衡字幕生成质量和模块差异性。在MSCOCO和Flickr30k上的实验结果证明了我们的方法的有效性，优于两个数据集上的现有方法。

    Humans tend to decompose a sentence into different parts like \textsc{sth do sth at someplace} and then fill each part with certain content. Inspired by this, we follow the \textit{principle of modular design} to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the \re{widely used} neural module networks in VQA, where the language (\ie, question) is fully observable, \re{the task of collocating visual-linguistic modules is more challenging.} This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: 1) \textit{distinguishable module design} -- \re{four modules in the encoder} including one linguistic module for function words and three visual modules for different content words (\ie, noun, adjective, and verb) and another linguistic one in the decoder 
    
[^53]: 将任务特定的概念知识纳入脚本学习中

    Incorporating Task-specific Concept Knowledge into Script Learning. (arXiv:2209.00068v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.00068](http://arxiv.org/abs/2209.00068)

    本文提出了一个新任务Tetris，并提出了概念提示和面向脚本的对比学习来解决输入包括用户上下文时的问题，两种方法均能提高任务完成性能。

    

    本文提出了一个名为Tetris的新任务，名为目标导向脚本完成。与之前的工作不同的是，它考虑了一个更为现实和普遍的场景，输入不仅包括目标，还包括用户的额外上下文，包括喜好和历史。为了解决这个问题，我们提出了一个新颖的方法，它使用两种技术来提高性能：概念提示和面向脚本的对比学习，以解决步骤重复和幻觉问题。在基于WikiHow的数据集上，我们发现两种方法都可以提高性能。该数据集、仓库和模型将公开提供，以促进对这个新任务的进一步研究。

    In this paper, we present Tetris, a new task of Goal-Oriented Script Completion. Unlike previous work, it considers a more realistic and general setting, where the input includes not only the goal but also additional user context, including preferences and history. To address this problem, we propose a novel approach, which uses two techniques to improve performance: (1) concept prompting, and (2) script-oriented contrastive learning that addresses step repetition and hallucination problems. On our WikiHow-based dataset, we find that both methods improve performance. The dataset, repository, and models will be publicly available to facilitate further research on this new task.
    
[^54]: 基于有向图的跨模态特征补充方法用于多模态对话情感识别

    GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.12261](http://arxiv.org/abs/2207.12261)

    本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。

    

    对话情感识别在人机交互系统中起着重要作用，因为它可以提供有共情心理的服务。多模态对话情感识别可以缓解单模态方法的缺点。最近，由于关系建模方面的卓越性能，图神经网络已被广泛用于各种领域。在多模态对话情感识别中，图神经网络能够提取远距离的上下文信息和跨模态的交互信息。不幸的是，由于现有方法（如MMGCN）直接融合多个模态，可能会产生冗余信息，且可能丢失多样化的信息。在本文中，我们提出了一种基于有向图的跨模态特征补充（GraphCFC）模块，可以有效地模拟上下文和互动信息。GraphCFC通过利用多个子空间提取器和成对跨模态补充（PairCC）策略，缓解了多模态融合中的异构性差距问题。

    Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
    
[^55]: 利用编译器中间表示进行代码翻译

    Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2207.03578](http://arxiv.org/abs/2207.03578)

    本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。

    

    本文提出了一种利用低级别的编译器中间表示（IR）来改进代码翻译的方法。我们的方法结合了神经机器翻译（NMT）和IR，能够更好地捕捉代码的语义，避免以往方法存在的常见错误，从而提高了翻译的质量和实用性。

    In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
    
[^56]: 使用NLG工具了解EFL学生创意写作的思路

    Understanding EFL Student Idea Generation Strategies for Creative Writing with NLG Tools. (arXiv:2207.01484v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2207.01484](http://arxiv.org/abs/2207.01484)

    本研究探讨了EFL学生如何使用NLG工具进行创意写作。研究发现学生在使用NLG工具搜索和评估思路时可能已经有现有的想法。

    

    自然语言生成（NLG）是人工智能中的一个过程，其中计算机系统从信息中生成人类可理解的语言文本。 EFL学生使用NLG工具可能有助于他们的创意生成，这对于创意写作是基础。 然而，我们对EFL学生如何与NLG工具交互生成思路知之甚少。 本研究探讨了EFL学生使用NLG工具搜索思路、评估NLG工具生成的思路以及选择NLG工具进行思路生成时采用的策略。 四名香港中学生参加了研讨会，他们学习编写包括自己的话语和NLG工具生成的单词的故事。 研讨会结束后，他们回答问题以反思使用NLG工具的写作经验。 在书面反思的主题分析中，我们发现学生在使用NLG工具搜索和评估思路时可能已经有现有的想法。

    Natural language generation (NLG) is a process within artificial intelligence where computer systems produce human-comprehensible language texts from information. English as a foreign language (EFL) students' use of NLG tools might facilitate their idea generation, which is fundamental to creative writing. However, little is known about how EFL students interact with NLG tools to generate ideas. This study explores strategies adopted by EFL students when searching for ideas using NLG tools, evaluating ideas generated by NLG tools and selecting NLG tools for ideas generation. Four Hong Kong secondary school students attended workshops where they learned to write stories comprising their own words and words generated by NLG tools. After the workshops, they answered questions to reflect on their writing experience with NLG tools. In a thematic analysis of the written reflections, we found students may have existing ideas when searching for ideas and evaluating ideas with NLG tools. Studen
    
[^57]: 零-shot 密集检索的全面探讨

    A Thorough Examination on Zero-shot Dense Retrieval. (arXiv:2204.12755v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.12755](http://arxiv.org/abs/2204.12755)

    本文第一次全面探讨了 DR 模型在零-shot检索能力上的表现，并分析了影响表现的关键因素，为开发更好的零-shot DR 模型提供了重要的证据。

    

    近年来，基于强大的预训练语言模型（PLM）的密集检索（DR）取得了显著进展。DR 模型在几个基准数据集上取得了出色的性能，但在零-shot检索设置下，它们显示出的竞争力不如传统的稀疏检索模型（例如BM25）。然而，在相关文献中，仍缺乏对零-shot检索的详细和全面的研究。在本文中，我们首次对 DR 模型的零-shot能力进行了全面的研究。我们旨在确定关键因素并分析它们如何影响零-shot检索性能。特别地，我们讨论了与源训练集相关的几个关键因素的影响，分析了目标数据集的潜在偏差，并回顾和比较现有的零-shot DR 模型。我们的发现为更好地理解和开发零-shot DR 模型提供了重要的证据。

    Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting. However, in the related literature, there still lacks a detailed and comprehensive study on zero-shot retrieval. In this paper, we present the first thorough examination of the zero-shot capability of DR models. We aim to identify the key factors and analyze how they affect zero-shot retrieval performance. In particular, we discuss the effect of several key factors related to source training set, analyze the potential bias from the target dataset, and review and compare existing zero-shot DR models. Our findings provide important evidence to better understand and develop zero-shot DR models.
    
[^58]: 面向隐私政策的问答检索增强数据增强

    Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.08952](http://arxiv.org/abs/2204.08952)

    本文提出了一种集成检索模型的数据增强框架，并应用于隐私政策问答中，成功地提高了性能。该方法捕获政策文档中的相关文本段并利用多个预训练语言模型，通过对增强数据进行级联和噪声减少滤波来提高数据多样性和质量，获得了该领域的新最优F1分数50\%。

    

    在隐私政策领域，先前的研究将问答任务框架化为根据用户查询从政策文件中识别最相关的文本段落或句子列表。现有的标记数据集极度不平衡（只有少数相关段落），限制了该领域中QA的性能。本文提出一种基于集成检索模型的数据增强框架，从未标记的政策文件中捕获相关的文本段并扩展训练集中的正例。此外，为了提高增强数据的多样性和质量，我们利用多个预训练语言模型（LMs）并将它们级联噪声减少滤波模型。在PrivacyQA基准测试中使用我们的增强数据，我们将现有基线提高了很大程度（10\% F1），并取得了50\%的新的最优F1得分。我们的消融研究进一步揭示了我们方法的有效性。

    Prior studies in privacy policies frame the question answering (QA) task as identifying the most relevant text segment or a list of sentences from a policy document given a user query. Existing labeled datasets are heavily imbalanced (only a few relevant segments), limiting the QA performance in this domain. In this paper, we develop a data augmentation framework based on ensembling retriever models that captures the relevant text segments from unlabeled policy documents and expand the positive examples in the training set. In addition, to improve the diversity and quality of the augmented data, we leverage multiple pre-trained language models (LMs) and cascade them with noise reduction filter models. Using our augmented data on the PrivacyQA benchmark, we elevate the existing baseline by a large margin (10\% F1) and achieve a new state-of-the-art F1 score of 50\%. Our ablation studies provide further insights into the effectiveness of our approach.
    
[^59]: 学习组合软提示以用于组合式零样本学习

    Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03574](http://arxiv.org/abs/2204.03574)

    通过学习组合式软提示实现了预测看不见的属性-对象组合，超过了基准数据集上的特定体系结构，并在曲线下面积上平均高10.9％。

    

    我们提出了组合软提示（CSP），这是一种参数高效的学习技术，用于改善大规模预训练的视觉语言模型（VLMs）如CLIP的零样本组合性。我们将CSP开发用于组合式零样本学习，也就是预测看不见的属性-对象组合（例如老猫和小老虎）。VLM具有灵活的文本编码器，可以用自然语言提示表示任意类别，但它们通常在组合零样本基准数据集上表现不如特定任务的体系结构。CSP将定义类别的属性和对象视为可学习的词汇标记。在训练期间，词汇表被调整为识别以多种方式组成令牌的类别（例如老猫和白猫）。在测试时，我们将所学的属性-对象词汇表以新的组合方式重新组合，以识别新的类别。我们展示了CSP在基准数据集上比CLIP平均高10.9个百分点的AUC（曲线下面积指标）。

    We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP
    
[^60]: 问题-回答句子图用于联合建模答案选择

    Question-Answer Sentence Graph for Joint Modeling Answer Selection. (arXiv:2203.03549v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.03549](http://arxiv.org/abs/2203.03549)

    本文研究了基于图的方法用于答案选择，通过构建相关训练图并集成最先进模型，成功解决了检索型问答系统中的AS2任务，并在实验中表现优异。

    

    本研究研究了基于图的方法用于答案选择（AS2），这是检索型问答（QA）系统的重要组成部分。在离线学习中，我们的模型以无监督的方式为每个问题构建一个小规模的相关训练图，并与图神经网络集成。图节点是问题句子到答案句子的对。我们训练并集成了用于计算问题-问题、问题-答案和答案-答案对之间得分的最先进模型，并使用相关得分阈值来创建图边缘。然后进行在线推理以解决看不见的查询的AS2任务。在两个知名的学术基准和一个现实世界的数据集上的实验表明，我们的方法始终优于SOTA QA基线模型。

    This research studies graph-based approaches for Answer Sentence Selection (AS2), an essential component for retrieval-based Question Answering (QA) systems. During offline learning, our model constructs a small-scale relevant training graph per question in an unsupervised manner, and integrates with Graph Neural Networks. Graph nodes are question sentence to answer sentence pairs. We train and integrate state-of-the-art (SOTA) models for computing scores between question-question, question-answer, and answer-answer pairs, and use thresholding on relevance scores for creating graph edges. Online inference is then performed to solve the AS2 task on unseen queries. Experiments on two well-known academic benchmarks and a real-world dataset show that our approach consistently outperforms SOTA QA baseline models.
    
[^61]: 深度学习在自动医疗编码中的应用综述

    A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.02797](http://arxiv.org/abs/2201.02797)

    本文综述了深度学习在自动医疗编码领域的发展，提出了一个统一框架，总结了最新的高级模型，并讨论了未来发展的挑战和方向。

    

    自动医疗编码是医疗运营和服务的基本任务，通过从临床文档中预测医疗编码来管理非结构化数据。近年来，深度学习和自然语言处理的进步已广泛应用于该任务。但基于深度学习的自动医疗编码缺乏对神经网络架构设计的统一视图。本综述提出了一个统一框架，以提供对医疗编码模型组件的一般理解，并总结了在此框架下最近的高级模型。我们的统一框架将医疗编码分解为四个主要组件，即用于文本特征提取的编码器模块、构建深度编码器架构的机制、用于将隐藏表示转换成医疗代码的解码器模块以及辅助信息的使用。最后，我们介绍了基准和真实世界中的使用情况，讨论了关键的研究挑战和未来方向。

    Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning and natural language processing have been widely applied to this task. However, deep learning-based medical coding lacks a unified view of the design of neural network architectures. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we introduce the benchmarks and real-world usage and discuss key research challenges and future directions.
    
[^62]: RocketQAv2：稠密Passage检索和Passage重新排名的联合训练法。

    RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.07367](http://arxiv.org/abs/2110.07367)

    本文提出了一种新的联合训练方法来优化稠密Passage检索和Passage重新排名，并引入了动态listwise蒸馏和混合数据增强策略以提高性能。

    

    在自然语言处理任务中，Passage检索和Passage重新排名是寻找和排序相关信息的两个关键步骤。由于两个步骤都对最终性能有贡献，因此联合优化它们以实现相互提高至关重要。本文提出了一种新颖的稠密Passage检索和Passage重新排名的联合训练方法。其中一个主要贡献是引入了动态listwise蒸馏，在其中为检索器和重新排名器设计了统一的listwise训练方法。在动态蒸馏过程中，检索器和重新排名器可以根据彼此的相关性信息自适应地改进。我们还提出了一种混合数据增强策略，用于构造丰富的listwise训练实例。广泛的实验表明，我们的方法在MSMARCO和自然问题数据集上都非常有效。我们的代码可在https://github.com/microsoft/DensePhrases上获得。

    In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage re-ranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other's relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.c
    
[^63]: PAIR：利用段落中心的相似关系改进密集型段落检索

    PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2108.06027](http://arxiv.org/abs/2108.06027)

    PAIR算法是一种新方法，在密集型段落检索中同时考虑查询中心和段落中心相似关系，通过正式公式、知识蒸馏和两阶段训练实现。实验证明，该方法优于先前方法。

    

    最近，密集型段落检索已成为在各种自然语言处理任务中找到相关信息的一种主流方法。许多研究致力于改进广泛采用的双编码器架构。但是，大多数以前的研究在学习双编码器检索器时仅考虑了查询中心的相似关系。为了捕捉更全面的相似关系，我们提出了一种新方法，利用查询中心和段落中心的相似关系（称为PAIR）进行密集型段落检索。为了实现我们的方法，我们提出了两种相似关系的正式公式，通过知识蒸馏生成高质量的伪标记数据，并设计了一种有效的两阶段训练过程，其中包括段落中心相似关系约束。广泛的实验表明，我们的方法显著优于先前的方法。

    Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous s
    
[^64]: 基于方面的情感分析在文件中的应用--以经济预测FOMC会议记录为例

    Aspect-based Sentiment Analysis in Document -- FOMC Meeting Minutes on Economic Projection. (arXiv:2108.04080v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2108.04080](http://arxiv.org/abs/2108.04080)

    本文提出了一种用于训练金融文档上基于方面的情感分析的模型，并研究了其对宏观经济指标的预测能力。

    

    美联储内的联邦公开市场委员会负责管理通胀、最大化就业和稳定利率。会议记录在市场波动中发挥着重要作用，因为它们提供了这种经济复杂性不断重新权衡的鸟瞰图。因此，越来越多的人对从大型金融文本中分析和提取各种方面的情感以进行经济预测感兴趣。然而，由于缺乏大型标记数据集，基于方面的情感分析在金融数据中并不常见。在本文中，作者提出了一种方法，利用弱监督训练金融文件上的ABSA，并分析其对各种宏观经济指标的预测能力。

    The Federal Open Market Committee within the Federal Reserve System is responsible for managing inflation, maximizing employment, and stabilizing interest rates. Meeting minutes play an important role for market movements because they provide the birds eye view of how this economic complexity is constantly re-weighed. Therefore, There has been growing interest in analyzing and extracting sentiments on various aspects from large financial texts for economic projection. However, Aspect-based Sentiment Analysis is not widely used on financial data due to the lack of large labeled dataset. In this paper, I propose a model to train ABSA on financial documents under weak supervision and analyze its predictive power on various macroeconomic indicators.
    
[^65]: 人的注意力在目标定向阅读理解时依赖于任务优化

    Human Attention during Goal-directed Reading Comprehension Relies on Task Optimization. (arXiv:2107.05799v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2107.05799](http://arxiv.org/abs/2107.05799)

    本文研究了阅读理解中人类关注力分配的计算模型。研究表明，在执行相同的阅读任务时，深度神经网络可以预测每个单词的阅读时间，读者在第一遍阅读和重新阅读过程中分别关注基本文本特征和与问题相关的信息，并且文本特征和问题相关性会分别调节注意力权重。

    

    复杂任务中关注力分配的计算原则仍然不明确。目标定向阅读，即阅读一篇文章以回答脑海中的问题，是一种强烈引发注意力的常见真实世界任务。在这里，我们研究了什么计算模型可以解释这种复杂任务中的关注力分配。我们展示了在基于Transformer的深度神经网络（DNN）中，优化执行相同阅读任务的关注权重可以预测每个单词上的阅读时间。眼动跟踪进一步揭示了读者在第一遍阅读和重新阅读过程中分别关注基本文本特征和与问题相关的信息。类似地，文本特征和问题相关性在浅层和深层DNN层中分别调节注意力权重。此外，当读者在脑海中没有问题的情况下扫描一篇文章时，他们的阅读时间可以由为单词预测任务优化的DNN预测。因此，在真实世界的阅读中关注力分配依赖于任务优化。

    The computational principles underlying attention allocation in complex goal-directed tasks remain elusive. Goal-directed reading, i.e., reading a passage to answer a question in mind, is a common real-world task that strongly engages attention. Here, we investigate what computational models can explain attention distribution in this complex task. We show that the reading time on each word is predicted by the attention weights in transformer-based deep neural networks (DNNs) optimized to perform the same reading task. Eye-tracking further reveals that readers separately attend to basic text features and question-relevant information during first-pass reading and rereading, respectively. Similarly, text features and question relevance separately modulate attention weights in shallow and deep DNN layers. Furthermore, when readers scan a passage without a question in mind, their reading time is predicted by DNNs optimized for a word prediction task. Therefore, attention during real-world 
    

