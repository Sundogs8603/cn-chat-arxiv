# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion.](http://arxiv.org/abs/2308.06696) | 提出了一种名为MACO的模态对抗和对比框架，用于解决多模态知识图完成中的模态缺失问题，通过对抗训练生成器和判别器生成缺失模态特征，并使用跨模态对比损失改善生成器性能。 |
| [^2] | [Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation.](http://arxiv.org/abs/2308.06610) | 本研究通过探索大型语言模型在医学系统性评述中的应用，尤其是通过指令调优来提高摘要筛选的性能，提出了一种名称为Bio-SIEVE的模型，该模型在医学领域中表现出优异的泛化能力和性能，但在安全性优先场景下仍存在挑战。同时，我们也尝试了多任务训练，但发现其不能与单任务的Bio-SIEVE性能相匹配。这一研究是为了将语言模型专门用于生物医学系统性评述过程迈出的重要一步。我们提供了模型、代码和DOI列表以供复现。 |
| [^3] | [VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use.](http://arxiv.org/abs/2308.06595) | VisIT-Bench是一个用于评价真实世界中视觉语言模型指示遵循的基准，包含了各种任务并提供了详细描述，可以自动评估多模态生成的质量差距。 |
| [^4] | [MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction.](http://arxiv.org/abs/2308.06552) | MT4CrossOIE是一种多阶段调优框架，用于增强跨语种开放信息提取。它通过向共享模型注入语言特定知识，并利用多个低秩语言特定模块进行模型转移。 |
| [^5] | [Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition.](http://arxiv.org/abs/2308.06547) | 本论文提出了一种称为替代伪标签的半监督学习框架，以解决伪标签噪声的问题。通过引入广义的CTC损失函数处理噪声伪标签，并使用自适应伪标签更新策略和消融正则化来改进模型的性能和泛化能力。 |
| [^6] | [MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction.](http://arxiv.org/abs/2308.06546) | 本文提出了一个新的多方面交叉整合框架，用于从药物相关文档中提取药物事件/实体。该框架能够捕捉并对齐不同的上下文/语言/知识属性，并实现药物事件信息的全面检测和理解。 |
| [^7] | [With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector.](http://arxiv.org/abs/2308.06527) | 本研究努力复现了Vamvas和Sennrich（2022年）论文中的人工评估实验，结果确认了原研究的结论并提出了评估的可变性。 |
| [^8] | [HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion.](http://arxiv.org/abs/2308.06512) | HyperFormer是一个考虑局部级顺序信息的模型，用于解决超关系知识图谱补全中多跳信息引入噪音的问题。 |
| [^9] | [AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models.](http://arxiv.org/abs/2308.06507) | AutoConv利用大型语言模型自动生成高质量的信息寻求对话，相对于强基线具有显著改进，并减轻了对人工注释的依赖。 |
| [^10] | [Three Ways of Using Large Language Models to Evaluate Chat.](http://arxiv.org/abs/2308.06502) | 本文提出了三种使用大型语言模型（LLMs）评估聊天的方法，并报告了相对于基准线的改进。还分析了另外两种方法的性能，并提出了未来工作的改进方向。 |
| [^11] | [NewsDialogues: Towards Proactive News Grounded Conversation.](http://arxiv.org/abs/2308.06501) | 本文提出了一个新任务-主动引导的新闻基础对话，其中对话系统可以根据新闻关键话题主动引导对话。研究者还收集了一个人机中文对话数据集，提出了一个名为"Predict-Generate-Rank"的方法用于回复生成和排序。 |
| [^12] | [Generating Faithful Text From a Knowledge Graph with Noisy Reference Text.](http://arxiv.org/abs/2308.06488) | 本文提出了一种KG到文本生成模型，能够在存在噪声参考文本的情况下，从给定的图生成忠实的自然语言文本 |
| [^13] | [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher.](http://arxiv.org/abs/2308.06463) | 这项研究发现，通过使用密码进行聊天可以绕过大型语言模型（LLMs）的安全对齐技术。研究人员提出了一种名为CipherChat的框架，用于系统地检查安全对齐在非自然语言（密码）中的普适性，并通过实验评估了ChatGPT和GPT-4等最先进的LLMs对不同代表性人类密码在11个安全领域中的影响。 |
| [^14] | [Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation.](http://arxiv.org/abs/2308.06457) | 本文提出了一个两阶段框架，用于零样本身份不可知言语生成，首先利用预训练模型进行文本到语音转换，然后使用音频驱动的说话头生成方法产生引人入胜的视频。这项研究提供了对不同方法的比较分析，识别出最有效的方法。 |
| [^15] | [Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension.](http://arxiv.org/abs/2308.06454) | 本研究提出了一种基于演示的学习方法，通过将生物医学命名实体识别重新定义为机器阅读理解问题，来解决少样本学习场景下的生物医学实体识别问题。实验证明，在少样本学习中，该方法比其他先进方法平均提高了1.1%的F1分数。 |
| [^16] | [Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy.](http://arxiv.org/abs/2308.06450) | 本研究提出了一种基于课程学习策略的新型Emotion Recognition Network (ERNetCL)模型，通过简化网络结构并充分建模上下文来高效地捕捉对话中的情感线索，实现了文本对话情感识别的性能优化。 |
| [^17] | [Performance Prediction for Multi-hop Questions.](http://arxiv.org/abs/2308.06431) | 我们提出了一种新的预测开放领域多跳问题性能的方法，并在广泛的评估中表现出优于传统方法的性能。 |
| [^18] | [Dynamic Planning with a LLM.](http://arxiv.org/abs/2308.06391) | 这项工作提出了一个称为LLM-DP的神经符号框架，它利用大型语言模型（LLM）与传统规划器合作，以更快、更高效地解决具体任务。相比于仅使用朴素LLM方法，LLM-DP在解决Alfworld任务上表现更好。 |
| [^19] | [ZYN: Zero-Shot Reward Models with Yes-No Questions.](http://arxiv.org/abs/2308.06385) | 本文提出了ZYN框架，通过使用是非问题作为奖励模型的提示，以及增强学习来微调语言模型，使其生成的文本与人类操作者的偏好对齐。实验证据表明该方法在不同领域的文本生成任务中具有很好的效果，包括解毒、情感优化和个性化提示生成器等。 |
| [^20] | [Large Language Models and Knowledge Graphs: Opportunities and Challenges.](http://arxiv.org/abs/2308.06374) | 大型语言模型和知识图谱的结合为知识表示带来了新的机遇和挑战。 |
| [^21] | [Large Language Models to Identify Social Determinants of Health in Electronic Health Records.](http://arxiv.org/abs/2308.06354) | 本研究利用大型语言模型从电子健康记录中提取社会健康决定因素（SDoH），并通过合成临床文本改进了这些极有价值但很少被记录的临床数据的提取。最佳模型为经过微调的Flan-T5 XL和Flan-T5 XXL，其中小型模型改进了性能。 |
| [^22] | [Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss.](http://arxiv.org/abs/2308.06327) | 本研究提出了一种双语的自动语音识别解决方案，采用字素单位和辅助单语损失，并通过大规模训练和测试任务验证了其在双语应用中的强大性能。 |
| [^23] | [Self-Alignment with Instruction Backtranslation.](http://arxiv.org/abs/2308.06259) | 本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。 |
| [^24] | [Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models.](http://arxiv.org/abs/2308.06111) | 这项研究提出了ZeroShotALI，它使用了一种新颖的推荐系统来改进金融审计中的零样本文本匹配。通过采用大型语言模型（LLM）和经过领域优化的基于transformer的文本匹配解决方案，该系统实现了从报告中推荐与法律要求相符的相关文本段落，并在现有方法上取得了显著的性能提升。 |
| [^25] | [Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling.](http://arxiv.org/abs/2308.05081) | 本论文提出了一种全面的时空场景图表示来解决视频语义角色标注中细粒度空间语义和时间建模的问题，并设计了一个面向特定目标的VidSRL框架，通过场景-事件映射机制来改进性能。 |
| [^26] | [NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?.](http://arxiv.org/abs/2308.04889) | 该报告关注当前最具影响力的AI论文，并以标准化引用计数为依据编制了40篇最受欢迎的论文列表。观察到在2023年上半年，大型语言模型（LLMs）和具体而言的ChatGPT相关的论文占主导地位，ChatGPT表现出下降的趋势。 |
| [^27] | [Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval.](http://arxiv.org/abs/2308.04711) | 本论文提出了两种方法来改进在具有充分解释性背景下，使用较小语言模型回答训练中未见的挑战性短问题回答任务。第一种方法是使用理据生成和密集检索结合的方式，并通过理据排名模型进行评分和组合。第二种方法是使用增强检索训练数据集训练较小的推理模型，以利用长文本序列中的相关信息。 |
| [^28] | [Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias.](http://arxiv.org/abs/2308.04566) | 本论文针对机器阅读理解中的答案位置偏倚问题，提出了一种名为单句阅读器的新方法，该方法使用六种不同模型实现。实验证明，单句阅读器与传统训练集上训练的模型几乎具有相当的性能，有效解决了答案位置偏倚问题。 |
| [^29] | [Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition.](http://arxiv.org/abs/2308.04502) | 本研究重新审视对话多模态情感识别中的模态和上下文，提出了一种双层解缠机制来同时建模特征的多模态性和对话的语境化，以进一步提高任务性能。 |
| [^30] | [DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs.](http://arxiv.org/abs/2308.04498) | 本研究将核指代解决方案引入到对话关系抽取领域，并通过新的数据集DialogRE^C+进行评估。研究表明，通过高质量的核指代知识可以增强参数关系的推理能力，从而在提升DRE任务中起到积极作用。 |
| [^31] | [A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition.](http://arxiv.org/abs/2308.04424) | 我们提出了一种双向多跳推理模型，用于联合对话情感分类和行为识别。该模型能够全面理解对话上下文，并显式建模情感和行为标签之间的关联，从而提取丰富的情感和行为线索，实现有效且准确的推理。 |
| [^32] | [Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue.](http://arxiv.org/abs/2308.03549) | 本研究介绍了中精，这是一种基于LLaMA的中文医学LLM，通过整个训练流程，并结合人类反馈，以及引入一个中文多轮医学对话数据集CMtMedQA，大大提升了模型在复杂对话和主动询问发起方面的能力。 |
| [^33] | [Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever.](http://arxiv.org/abs/2308.03365) | 本文提出了一种粗到细的基于词典的检索器，用于改进少样本和零样本实体链接。实验结果表明，该方法能够在不需要大量微调的情况下获得优越的性能，并在NLPCC 2023中的中文少样本和零样本实体链接共享任务中排名第一。 |
| [^34] | [ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation.](http://arxiv.org/abs/2308.01861) | ClassEval是一种手工构建的类级代码生成基准，该研究首次尝试在这一更具挑战性的场景中评估LLMs，并发现现有LLMs在类级代码生成上的性能相对较差。GPT-4和GPT-3.5在类级代码生成方面表现出相对其他LLMs更卓越的优势。 |
| [^35] | [Does Correction Remain An Problem For Large Language Models?.](http://arxiv.org/abs/2308.01776) | 本文通过两个实验探讨了在大型语言模型背景下纠错问题的作用，第一个实验是将纠错作为独立任务进行研究，第二个实验则是探讨纠错作为其他NLP任务的准备任务的概念。研究结果将揭示纠错在大型语言模型时代的重要性及其对各种NLP应用的影响。 |
| [^36] | [Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models.](http://arxiv.org/abs/2308.00304) | 本文介绍了一种名为技能指导 (SKiC) 的提示策略，通过在上下文中演示基本技能和组合性示例，使大型语言模型具备解决更复杂问题的能力，并在泛化能力上取得几乎完美的表现。 |
| [^37] | [Contrastive Learning for API Aspect Analysis.](http://arxiv.org/abs/2307.16878) | 这项研究提出了一种对API方面分析进行对比学习的方法，通过训练变换器模型并使用监督对比损失函数，可以显著改进性能，对Performance、Security、Usability和Documentation等方面的检测效果进行了评估，并进行了实证和开发者研究验证。 |
| [^38] | [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey.](http://arxiv.org/abs/2307.16680) | 本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。 |
| [^39] | [Efficient Guided Generation for LLMs.](http://arxiv.org/abs/2307.09702) | 本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。 |
| [^40] | [MMBench: Is Your Multi-modal Model an All-around Player?.](http://arxiv.org/abs/2307.06281) | MMBench是一个新型的多模态基准测试，旨在解决大型视觉语言模型评估的挑战，通过开发全面的评估流程和精心策划的数据集进行细粒度能力评估。 |
| [^41] | [A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.](http://arxiv.org/abs/2307.03987) | 本文提出了一种通过验证低置信度生成来检测和减轻LLMs的幻觉的方法。通过对模型生成的候选项进行验证，并缓解幻觉，改进了模型的可靠性。 |
| [^42] | [GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition.](http://arxiv.org/abs/2306.07848) | 本文提出了GEmo-CLAP模型用于语音情感识别，结合了性别属性信息，相比于其他先进方法，该模型在IEMOCAP上实现了更优越的识别性能。 |
| [^43] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^44] | [Learning Human-Human Interactions in Images from Weak Textual Supervision.](http://arxiv.org/abs/2304.14104) | 本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。 |
| [^45] | [Learning Semantic Text Similarity to rank Hypernyms of Financial Terms.](http://arxiv.org/abs/2303.13475) | 本文介绍了一种能够提取和排序金融术语上义词的系统，使用神经网络学习不同术语间的语义相似度。该系统帮助用户更好地了解复杂金融术语，帮助投资者做出明智的决策。 |
| [^46] | [Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer.](http://arxiv.org/abs/2303.13099) | 本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。 |
| [^47] | [Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images.](http://arxiv.org/abs/2303.07274) | WHOOPS!是一个新的视觉常识数据集和基准测试，包括了图像字幕、跨模态匹配和视觉问答等若干个任务，引入了解释生成任务，挑战了AI模型识别和解释不合常规的图像的能力。 |
| [^48] | [SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading.](http://arxiv.org/abs/2303.05221) | SEAM是一种集成了眼动控制和句子处理的模型，为实现阅读中自然语言理解的完整数学模型迈出了重要一步。 |
| [^49] | [Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization.](http://arxiv.org/abs/2302.12324) | 本文提出了利用自动化文本摘要生成科学文献的插图标题的方法，并使用预训练的抽象化摘要模型 PEGASUS 对引用图表的段落进行摘要。实验结果表明该方法在自动评估和人工评估中均优于之前的视觉方法。研究还发现了两个关键挑战：低质量作者撰写的标题的普遍存在以及对好标题缺乏明确的标准。 |
| [^50] | [LabelPrompt: Effective Prompt-based Learning for Relation Classification.](http://arxiv.org/abs/2302.08068) | LabelPrompt是一种面向关系分类任务的提示式学习方法，通过定义额外的令牌来表示关系标签，并使用提示模板方法明确构建它们，从而解决了将填充掩码标记的自然语言词汇与语义关系标签相关联的挑战。同时，该方法还实现了一个实体感知模块来减轻预测关系和给定实体之间的不一致性。 |
| [^51] | [Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition.](http://arxiv.org/abs/2211.08233) | 该论文提出了一种新的时间情感建模方法，通过学习不同时间尺度的多尺度情感上下文表示，并融合多个时间尺度的特征，实现了更好的语音情感识别效果。 |
| [^52] | [NECE: Narrative Event Chain Extraction Toolkit.](http://arxiv.org/abs/2208.08063) | NECE是一个开源的故事事件链提取工具包，能够自动提取和对齐故事事件，并可用于分析故事偏见。 |
| [^53] | [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes.](http://arxiv.org/abs/2208.01066) | 本研究通过考虑在上下文中学习线性函数的问题，证明了标准的Transformers模型可以从头训练，在推断时实现线性函数的上下文学习能力。 |
| [^54] | [Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages.](http://arxiv.org/abs/2208.00463) | 本文提出了一种针对低资源语言的无监督翻译质量评估方法，通过使用XLMRScore和一些改进措施来解决未翻译标记和语言不匹配的问题。 |
| [^55] | [PInKS: Preconditioned Commonsense Inference with Minimal Supervision.](http://arxiv.org/abs/2206.07920) | PInKS是一种通过最小监督实现前提推理的改进模型，通过提高常识知识前提推理基准测试的结果，证明了其有效性。 |
| [^56] | [MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education.](http://arxiv.org/abs/2106.07340) | MathBERT是一个基于BASE BERT模型在大规模数学语料库上进行预训练的模型，为数学教育中的通用NLP任务提供了新的解决方案。 |
| [^57] | [PaCo: Preconditions Attributed to Commonsense Knowledge.](http://arxiv.org/abs/2104.08712) | 本研究提出了一个名为PaCo的数据集，用于推理常识知识的环境前提条件。通过三个评估任务，我们发现目前的语言模型在理解情境前提条件方面与人类表现存在10-30%的差距，这表明推理前提条件是一个未解决的挑战。 |

# 详细

[^1]: MACO: 一种用于模态缺失多模态知识图完成的模态对抗和对比框架

    MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion. (arXiv:2308.06696v1 [cs.CL])

    [http://arxiv.org/abs/2308.06696](http://arxiv.org/abs/2308.06696)

    提出了一种名为MACO的模态对抗和对比框架，用于解决多模态知识图完成中的模态缺失问题，通过对抗训练生成器和判别器生成缺失模态特征，并使用跨模态对比损失改善生成器性能。

    

    近年来，多模态知识图完成（MMKGC）取得了显著进展。MMKGC通过整合多模态实体信息来增强知识图完成（KGC），从而促进了在大规模知识图中发现未观察到的三元组。然而，现有方法强调设计优雅的KGC模型以促进模态交互，忽略了知识图中缺失模态的真实问题。缺失的模态信息阻碍了模态交互，从而削弱了模型的性能。在本文中，我们提出了一种模态对抗和对比框架（MACO）来解决MMKGC中的模态缺失问题。MACO通过对抗性地训练生成器和判别器来生成可以整合到MMKGC模型中的缺失模态特征。同时，我们设计了一个跨模态对比损失来提高生成器的性能。在公共基准上进行了实验和进一步的探索。

    Recent years have seen significant advancements in multi-modal knowledge graph completion (MMKGC). MMKGC enhances knowledge graph completion (KGC) by integrating multi-modal entity information, thereby facilitating the discovery of unobserved triples in the large-scale knowledge graphs (KGs). Nevertheless, existing methods emphasize the design of elegant KGC models to facilitate modality interaction, neglecting the real-life problem of missing modalities in KGs. The missing modality information impedes modal interaction, consequently undermining the model's performance. In this paper, we propose a modality adversarial and contrastive framework (MACO) to solve the modality-missing problem in MMKGC. MACO trains a generator and discriminator adversarially to generate missing modality features that can be incorporated into the MMKGC model. Meanwhile, we design a cross-modal contrastive loss to improve the performance of the generator. Experiments on public benchmarks with further explorati
    
[^2]: Bio-SIEVE：探索针对系统性评述自动化的大型语言模型的指令调优

    Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation. (arXiv:2308.06610v1 [cs.CL])

    [http://arxiv.org/abs/2308.06610](http://arxiv.org/abs/2308.06610)

    本研究通过探索大型语言模型在医学系统性评述中的应用，尤其是通过指令调优来提高摘要筛选的性能，提出了一种名称为Bio-SIEVE的模型，该模型在医学领域中表现出优异的泛化能力和性能，但在安全性优先场景下仍存在挑战。同时，我们也尝试了多任务训练，但发现其不能与单任务的Bio-SIEVE性能相匹配。这一研究是为了将语言模型专门用于生物医学系统性评述过程迈出的重要一步。我们提供了模型、代码和DOI列表以供复现。

    

    医学系统性评述成本高、资源密集。我们探索了如何使用大型语言模型（LLMs）在提供详细的选择标准的情况下支持和训练其执行文献筛选。具体而言，我们对LLaMA和Guanaco模型进行了指令调优，以执行医学系统性评述的摘要筛选。我们的最佳模型Bio-SIEVE在性能上超过了ChatGPT和经过训练的传统方法，并在医学领域中具有更好的泛化能力。然而，将模型调整为以安全为先的场景仍然具有挑战。我们还探索了与Bio-SIEVE-Multi的多任务训练的影响，包括PICO提取和排除推理等任务，但发现它无法达到单任务Bio-SIEVE的性能。我们认为Bio-SIEVE是为生物医学系统性评述过程专门化的语言模型的重要一步，同时还探索了其未来的发展机会。我们发布了我们的模型、代码和一份DOI列表以供重现。

    Medical systematic reviews can be very costly and resource intensive. We explore how Large Language Models (LLMs) can support and be trained to perform literature screening when provided with a detailed set of selection criteria. Specifically, we instruction tune LLaMA and Guanaco models to perform abstract screening for medical systematic reviews. Our best model, Bio-SIEVE, outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains. However, there remains the challenge of adapting the model to safety-first scenarios. We also explore the impact of multi-task training with Bio-SIEVE-Multi, including tasks such as PICO extraction and exclusion reasoning, but find that it is unable to match single-task Bio-SIEVE's performance. We see Bio-SIEVE as an important step towards specialising LLMs for the biomedical systematic review process and explore its future developmental opportunities. We release our models, code and a list of DOIs to reconst
    
[^3]: VisIT-Bench: 一个受真实世界使用启发的视觉语言指示评估基准

    VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])

    [http://arxiv.org/abs/2308.06595](http://arxiv.org/abs/2308.06595)

    VisIT-Bench是一个用于评价真实世界中视觉语言模型指示遵循的基准，包含了各种任务并提供了详细描述，可以自动评估多模态生成的质量差距。

    

    我们引入了VisIT-Bench（Visual InsTruction Benchmark），这是一个评价用于真实世界使用的视觉语言模型的指示遵循基准。我们的起点是策划了70个“指示家族”，我们认为指示调优的视觉语言模型应该能够解决这些家族。任务不仅限于VQAv2和COCO等评估，涵盖了从基本识别到游戏玩法和创造性生成的各种任务。在策划之后，我们的数据集包括592个测试查询，每个查询都带有一个人工编写的指示条件化的字幕。这些描述展现了特定指示因素，例如对于询问店面对于轮椅用户的易访问性的指示，条件化的字幕描述了斜坡/潜在障碍物。这些描述使得我们可以：1）收集每个实例的人工验证的参考输出；2）使用仅文本的语言模型对候选多模态生成进行自动评估，与人类判断相一致。我们量化了质量差距。

    We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
    
[^4]: MT4CrossOIE: 多阶段调优用于跨语种开放信息提取

    MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction. (arXiv:2308.06552v1 [cs.CL])

    [http://arxiv.org/abs/2308.06552](http://arxiv.org/abs/2308.06552)

    MT4CrossOIE是一种多阶段调优框架，用于增强跨语种开放信息提取。它通过向共享模型注入语言特定知识，并利用多个低秩语言特定模块进行模型转移。

    

    跨语种开放信息提取旨在从多语言的原始文本中提取结构化信息。先前的工作使用共享的跨语种预训练模型来处理不同的语言，但未充分利用语言特定表示的潜力。本文提出了一种名为MT4CrossIE的有效多阶段调优框架，旨在通过向共享模型注入语言特定知识来增强跨语种开放信息提取。具体而言，首先在固定编码器中调整跨语种预训练模型的共享语义空间（例如嵌入矩阵），然后在第二阶段优化其他组件。经过足够的训练后，我们冻结预训练模型，并使用混合LoRAs优化多个额外的低秩语言特定模块，以进行基于模型的跨语种转移。此外，我们利用两阶段提示来促使大型语言模型（LLM）注释多语种数据。

    Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called MT4CrossIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture-of-LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual 
    
[^5]: 半监督自动语音识别的替代伪标签方法

    Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition. (arXiv:2308.06547v1 [eess.AS])

    [http://arxiv.org/abs/2308.06547](http://arxiv.org/abs/2308.06547)

    本论文提出了一种称为替代伪标签的半监督学习框架，以解决伪标签噪声的问题。通过引入广义的CTC损失函数处理噪声伪标签，并使用自适应伪标签更新策略和消融正则化来改进模型的性能和泛化能力。

    

    当标记数据不足时，使用伪标签技术的半监督学习可以显著提高自动语音识别的性能。然而，伪标签通常包含许多错误的标记，将噪声标签视为损失函数中的真实标记会导致次优性能。先前的研究尝试通过过滤最嘈杂的伪标签或提高伪标签的整体质量来缓解这个问题。虽然这些方法在一定程度上是有效的，但完全消除伪标签中的错误标记是不现实的。在本工作中，我们提出了一种名为替代伪标签的新框架，以从训练目标的角度解决嘈杂伪标签的问题。该框架包括几个组成部分。首先，引入了广义的CTC损失函数来处理噪声伪标签，接受在错误标记的位置上的替代标记。然后，在训练过程中使用自适应伪标签更新策略，不仅能减轻伪标签的噪声，还可以帮助模型收敛到更好的结果。最后，通过自适应伪标签消融正则化来加强模型的泛化能力，并进一步提高性能。

    When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss 
    
[^6]: MC-DRE: 多方面交叉整合用于药物事件/实体提取

    MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction. (arXiv:2308.06546v1 [cs.CL])

    [http://arxiv.org/abs/2308.06546](http://arxiv.org/abs/2308.06546)

    本文提出了一个新的多方面交叉整合框架，用于从药物相关文档中提取药物事件/实体。该框架能够捕捉并对齐不同的上下文/语言/知识属性，并实现药物事件信息的全面检测和理解。

    

    提取有意义的药物相关信息块，如不良药物事件（ADE），对于预防疾病和拯救许多生命至关重要。大多数ADE是通过医疗背景下的非结构化对话报告的。因此，应用通用实体识别方法是不足够的。关键在于如何整合和对齐多个关键方面来检测药物事件信息，包括药物事件语义、句法结构和医学领域术语。在本文中，我们提出了一个新的多方面交叉整合框架，通过从药物相关文档中捕捉和对齐不同的上下文/语言/知识属性，用于药物实体/事件检测。我们首先构建多方面编码器来描述语义、句法和医学文档上下文信息，方法包括槽标注任务、主要药物实体/事件检测、词性标注和通用医学命名实体识别。然后，每个编码器进行交叉整合。

    Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADE are reported via an unstructured conversation with the medical context. Hence, applying a general entity recognition approach is not sufficient enough. The key is how to integrate and align multiple crucial aspects to detect drug event information, including drug event semantics, syntactic structures, and medical domain terminology. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross int
    
[^7]: 从作者那里获得一点帮助：复现机器翻译错误检测的人工评估

    With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector. (arXiv:2308.06527v1 [cs.CL])

    [http://arxiv.org/abs/2308.06527](http://arxiv.org/abs/2308.06527)

    本研究努力复现了Vamvas和Sennrich（2022年）论文中的人工评估实验，结果确认了原研究的结论并提出了评估的可变性。

    

    本研究介绍了我们努力复现Vamvas和Sennrich（2022年）论文中呈现的人工评估实验的结果。该实验评估了自动系统在机器翻译输出中检测到的超翻译和低翻译（翻译比原文包含更多或更少信息）的能力。尽管作者提供了高质量的文档和代码，我们在复现实验设计方面发现了一些问题，并提出了提高可重复性的建议。我们复现的结果总体上验证了原始研究的结论，但在某些情况下，观察到了统计上显著的差异，表明人工标注存在较高的变异性。

    This work presents our efforts to reproduce the results of the human evaluation experiment presented in the paper of Vamvas and Sennrich (2022), which evaluated an automatic system detecting over- and undertranslations (translations containing more or less information than the original) in machine translation (MT) outputs. Despite the high quality of the documentation and code provided by the authors, we discuss some problems we found in reproducing the exact experimental setup and offer recommendations for improving reproducibility. Our replicated results generally confirm the conclusions of the original study, but in some cases, statistically significant differences were observed, suggesting a high variability of human annotation.
    
[^8]: HyperFormer: 增强超关系知识图谱补全中的实体和关系交互

    HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion. (arXiv:2308.06512v1 [cs.AI])

    [http://arxiv.org/abs/2308.06512](http://arxiv.org/abs/2308.06512)

    HyperFormer是一个考虑局部级顺序信息的模型，用于解决超关系知识图谱补全中多跳信息引入噪音的问题。

    

    超关系知识图谱（HKGs）通过将属性-值修饰符与三元组相关联来扩展标准知识图谱，这有效地表示了与其关联三元组的额外细粒度信息。超关系知识图谱补全（HKGC）旨在在考虑其修饰符的情况下推断未知三元组。大多数现有的HKGC方法利用全局级图结构将超关系知识编码为图卷积消息传递过程。然而，多跳信息的添加可能会在三元组预测过程中引入噪音。为了解决这个问题，我们提出了HyperFormer，这是一个考虑局部级顺序信息的模型，它编码了三元组的实体、关系和修饰符的内容。更具体地说，HyperFormer由三个不同的模块组成：一个实体邻居聚合器模块，用于整合实体邻居的信息，以捕捉不同的视角。

    Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by associating attribute-value qualifiers to triples, which effectively represent additional fine-grained information about its associated triple. Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown triples while considering its qualifiers. Most existing approaches to HKGC exploit a global-level graph structure to encode hyper-relational knowledge into the graph convolution message passing process. However, the addition of multi-hop information might bring noise into the triple prediction process. To address this problem, we propose HyperFormer, a model that considers local-level sequential information, which encodes the content of the entities, relations and qualifiers of a triple. More precisely, HyperFormer is composed of three different modules: an entity neighbor aggregator module allowing to integrate the information of the neighbors of an entity to capture different perspectives of
    
[^9]: 使用大型语言模型自动生成信息寻求对话

    AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models. (arXiv:2308.06507v1 [cs.CL])

    [http://arxiv.org/abs/2308.06507](http://arxiv.org/abs/2308.06507)

    AutoConv利用大型语言模型自动生成高质量的信息寻求对话，相对于强基线具有显著改进，并减轻了对人工注释的依赖。

    

    近年来，通过对话帮助用户收集信息的信息寻求对话取得了巨大进展。然而，研究仍受到训练数据稀缺的困扰。为了解决这个问题，我们提出了AutoConv用于合成对话生成，利用大型语言模型（LLM）的少样本学习能力和生成能力。具体而言，我们将对话生成问题制定为语言建模任务，然后使用少量人类对话微调LLM，以捕捉信息寻求过程的特征，并用之生成具有高质量的合成对话。在两个常用数据集上的实验结果验证了AutoConv相对于强基线的显著改进，减轻了对人工注释的依赖。此外，我们还提供了几个分析研究以促进未来研究。

    Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.
    
[^10]: 使用大型语言模型评估聊天的三种方法

    Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])

    [http://arxiv.org/abs/2308.06502](http://arxiv.org/abs/2308.06502)

    本文提出了三种使用大型语言模型（LLMs）评估聊天的方法，并报告了相对于基准线的改进。还分析了另外两种方法的性能，并提出了未来工作的改进方向。

    

    本文描述了团队6提交的ChatEval系统，这是DSTC 11 Track 4竞赛的一部分。我们提出了三种不同的方法来预测基于大型语言模型（LLMs）的聊天机器人回复的转向级别质量。我们通过从向量存储中使用动态的少样本示例作为ChatGPT的提示，报告了相对于基准线的改进。我们还分析了另外两种方法的性能，并报告了未来工作的改进方向。我们仅用两周时间开发了这三个系统，展示了LLMs在此任务中的潜力。在挑战截止日期后进行的消融研究表明，新的Llama 2模型正在缩小ChatGPT和开源LLMs之间的性能差距。然而，我们发现Llama 2模型无法像ChatGPT那样从少样本示例中受益。

    This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.
    
[^11]: NewsDialogues：面向主动引导的新闻基础对话

    NewsDialogues: Towards Proactive News Grounded Conversation. (arXiv:2308.06501v1 [cs.CL])

    [http://arxiv.org/abs/2308.06501](http://arxiv.org/abs/2308.06501)

    本文提出了一个新任务-主动引导的新闻基础对话，其中对话系统可以根据新闻关键话题主动引导对话。研究者还收集了一个人机中文对话数据集，提出了一个名为"Predict-Generate-Rank"的方法用于回复生成和排序。

    

    新闻基础对话长期以来受到任务定义不清晰和数据稀缺的制约。本文提出了一个新颖的任务，即主动引导的新闻基础对话，其中对话系统可以根据新闻的关键话题主动引导对话。此外，该任务包括信息检索和闲聊场景，用户可以就新闻细节提问或表达观点，并渴望进行聊天。为了进一步开发这一新颖任务，我们收集了一个人机中文对话数据集"NewsDialogues"，包括1000个对话，共有14.6K个话语，并详细注释了目标话题和知识范围。此外，我们提出了一种名为"Predict-Generate-Rank"的方法，包括用于基于知识预测和生成回复的生成器以及用于多个候选回复排序的排序器。

    Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset \ts{NewsDialogues}, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a generator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple 
    
[^12]: 无噪声参考文本的知识图生成忠实文本

    Generating Faithful Text From a Knowledge Graph with Noisy Reference Text. (arXiv:2308.06488v1 [cs.CL])

    [http://arxiv.org/abs/2308.06488](http://arxiv.org/abs/2308.06488)

    本文提出了一种KG到文本生成模型，能够在存在噪声参考文本的情况下，从给定的图生成忠实的自然语言文本

    

    知识图（KG）到文本的生成旨在生成流畅的自然语言文本，准确地表示给定知识图的信息。虽然通过利用预训练语言模型（PLMs）与适当的图结构感知模块在这个任务上取得了显著的进展，但现有模型在生成忠实文本方面仍存在不足，特别是当地面真实语言文本中包含图中不存在的额外信息时。本文提出了一种KG到文本生成模型，能够在存在噪声参考文本的情况下，从给定的图生成忠实的自然语言文本。我们的框架融合了两个核心思想：首先，我们利用对比学习增强模型区分文本中的忠实和虚构信息的能力，从而鼓励解码器生成与输入图对齐的文本。其次，我们赋予解码器控制文本生成过程的能力，

    Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level 
    
[^13]: GPT-4太聪明以至于不安全：通过密码与LLMs进行隐蔽聊天

    GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. (arXiv:2308.06463v1 [cs.CL])

    [http://arxiv.org/abs/2308.06463](http://arxiv.org/abs/2308.06463)

    这项研究发现，通过使用密码进行聊天可以绕过大型语言模型（LLMs）的安全对齐技术。研究人员提出了一种名为CipherChat的框架，用于系统地检查安全对齐在非自然语言（密码）中的普适性，并通过实验评估了ChatGPT和GPT-4等最先进的LLMs对不同代表性人类密码在11个安全领域中的影响。

    

    安全性是大型语言模型（LLMs）开发的核心。关于将LLMs与人类伦理和偏好进行对齐的工作已经很多，包括在预训练中进行数据筛选、通过监督微调、通过人类反馈进行强化学习以及红队测试等等。在这项研究中，我们发现使用密码进行聊天可以绕过LLMs的安全对齐技术，这些技术主要是在自然语言中进行的。我们提出了一个新颖的框架CipherChat，用于系统地检查安全对齐在非自然语言（密码）中的普适性。CipherChat使人们能够通过加密提示和少量加密演示与LLMs进行聊天。我们使用CipherChat在英语和中文中评估最先进的LLMs，包括ChatGPT和GPT-4在11个安全领域中的不同代表性人类密码。实验结果表明，某些密码成功地绕过了安全对齐技术，几乎100%的时间都能够成功。

    Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment
    
[^14]: 文本到视频：零样本身份不可知言语生成的两阶段框架

    Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation. (arXiv:2308.06457v1 [cs.CV])

    [http://arxiv.org/abs/2308.06457](http://arxiv.org/abs/2308.06457)

    本文提出了一个两阶段框架，用于零样本身份不可知言语生成，首先利用预训练模型进行文本到语音转换，然后使用音频驱动的说话头生成方法产生引人入胜的视频。这项研究提供了对不同方法的比较分析，识别出最有效的方法。

    

    ChatGPT的出现引入了创新的信息收集和分析方法。然而，ChatGPT提供的信息仅限于文本，其可视化仍受限。先前的研究探索了零样本文本到视频（TTV）的方法，将文本转换为视频。然而，这些方法缺乏对生成音频身份的控制，即不是身份不可知的，从而影响了其有效性。为了解决这个限制，我们提出了一个新颖的两阶段框架，用于面向人物不可知的视频克隆，特别关注TTV生成。在第一阶段，我们利用预训练的零样本模型实现了文本到语音（TTS）转换。在第二阶段，采用基于音频驱动的说话头生成方法来产生引人入胜的视频，提供了第一阶段生成的音频。本文对不同的TTS和基于音频驱动的说话头生成方法进行了比较分析，识别了最有效的方法。

    The advent of ChatGPT has introduced innovative methods for information gathering and analysis. However, the information provided by ChatGPT is limited to text, and the visualization of this information remains constrained. Previous research has explored zero-shot text-to-video (TTV) approaches to transform text into videos. However, these methods lacked control over the identity of the generated audio, i.e., not identity-agnostic, hindering their effectiveness. To address this limitation, we propose a novel two-stage framework for person-agnostic video cloning, specifically focusing on TTV generation. In the first stage, we leverage pretrained zero-shot models to achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven talking head generation method is employed to produce compelling videos privided the audio generated in the first stage. This paper presents a comparative analysis of different TTS and audio-driven talking head generation methods, identifying the mo
    
[^15]: 基于演示的学习方法用于少样本生物医学命名实体识别中的机器阅读理解

    Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension. (arXiv:2308.06454v1 [cs.CL])

    [http://arxiv.org/abs/2308.06454](http://arxiv.org/abs/2308.06454)

    本研究提出了一种基于演示的学习方法，通过将生物医学命名实体识别重新定义为机器阅读理解问题，来解决少样本学习场景下的生物医学实体识别问题。实验证明，在少样本学习中，该方法比其他先进方法平均提高了1.1%的F1分数。

    

    虽然深度学习技术在许多领域已经取得了显著的成就，但它们通常依赖大量手工标注的数据，并且在少样本场景下表现不佳。本研究的目标是设计一种能够改进模型在少样本学习场景下识别生物医学实体的能力的策略。通过将生物医学命名实体识别（BioNER）重新定义为机器阅读理解（MRC）问题，我们提出了一种基于演示的学习方法来解决少样本BioNER问题，该方法涉及构建适当的任务演示。在评估我们提出的方法时，我们使用了包括BC4CHEMD、BC5CDR-Chemical、BC5CDR-Disease、NCBI-Disease、BC2GM和JNLPBA在内的六个基准数据集，将所提出的方法与现有的先进方法进行了比较。我们通过报告25样本和50样本学习实验的F1分数来检查模型的效果。在25样本学习中，我们观察到平均F1分数提高了1.1%。

    Although deep learning techniques have shown significant achievements, they frequently depend on extensive amounts of hand-labeled data and tend to perform inadequately in few-shot scenarios. The objective of this study is to devise a strategy that can improve the model's capability to recognize biomedical entities in scenarios of few-shot learning. By redefining biomedical named entity recognition (BioNER) as a machine reading comprehension (MRC) problem, we propose a demonstration-based learning method to address few-shot BioNER, which involves constructing appropriate task demonstrations. In assessing our proposed method, we compared the proposed method with existing advanced methods using six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical, BC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models' efficacy by reporting F1 scores from both the 25-shot and 50-shot learning experiments. In 25-shot learning, we observed 1.1% improvements in the average F1 scores 
    
[^16]: 简单模型也有效：基于课程学习策略的文本对话情感识别网络的新模型

    Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy. (arXiv:2308.06450v1 [cs.CL])

    [http://arxiv.org/abs/2308.06450](http://arxiv.org/abs/2308.06450)

    本研究提出了一种基于课程学习策略的新型Emotion Recognition Network (ERNetCL)模型，通过简化网络结构并充分建模上下文来高效地捕捉对话中的情感线索，实现了文本对话情感识别的性能优化。

    

    对话情感识别（ERC）已成为对话机器人和问答系统等领域的研究热点。如何高效地获取上下文中的情感线索一直是ERC任务中的关键挑战之一。现有的方法未能充分建模上下文，并采用复杂的网络结构，导致计算资源消耗过大而没有实质性的性能提升。在本文中，我们提出了一种基于课程学习策略的新型Emotion Recognition Network（ERNetCL）。该方法主要由Temporal Encoder（TE）、Spatial Encoder（SE）和Curriculum Learning（CL） loss组成。我们利用TE和SE以简洁的方式结合了以前方法的优点，以有效地捕捉对话中的时间和空间上下文信息。为了模拟人类从易到难的课程学习方式，我们将CL的思想应用到ERC任务中，逐步优化网络构架。

    Emotion Recognition in Conversation (ERC) has emerged as a research hotspot in domains such as conversational robots and question-answer systems. How to efficiently and adequately retrieve contextual emotional cues has been one of the key challenges in the ERC task. Existing efforts do not fully model the context and employ complex network structures, resulting in excessive computational resource overhead without substantial performance improvement. In this paper, we propose a novel Emotion Recognition Network based on Curriculum Learning strategy (ERNetCL). The proposed ERNetCL primarily consists of Temporal Encoder (TE), Spatial Encoder (SE), and Curriculum Learning (CL) loss. We utilize TE and SE to combine the strengths of previous methods in a simplistic manner to efficiently capture temporal and spatial contextual information in the conversation. To simulate the way humans learn curriculum from easy to hard, we apply the idea of CL to the ERC task to progressively optimize the ne
    
[^17]: 多跳问题的性能预测

    Performance Prediction for Multi-hop Questions. (arXiv:2308.06431v1 [cs.CL])

    [http://arxiv.org/abs/2308.06431](http://arxiv.org/abs/2308.06431)

    我们提出了一种新的预测开放领域多跳问题性能的方法，并在广泛的评估中表现出优于传统方法的性能。

    

    我们研究了开放领域多跳问答（QA）中查询性能预测（QPP）的问题，任务是估计对语料库进行多跳问题评估的难度。尽管在预测即席和QA检索模型性能方面已经进行了广泛的研究，但对多跳问题难度的评估研究还缺乏。由于检索过程的多步骤性质、步骤之间的潜在依赖关系和推理的复杂性，这个问题具有挑战性。为了解决这个挑战，我们提出了一种新的预检索方法multHP，用于预测开放领域多跳问题的性能。我们对最大的多跳QA数据集进行了广泛评估，使用了几种现代QA系统，结果显示所提出的模型是性能的强有力预测者，优于传统的单跳QPP模型。此外，我们证明了我们的方法可以有效地用于优化QA系统的参数。

    We study the problem of Query Performance Prediction (QPP) for open-domain multi-hop Question Answering (QA), where the task is to estimate the difficulty of evaluating a multi-hop question over a corpus. Despite the extensive research on predicting the performance of ad-hoc and QA retrieval models, there has been a lack of study on the estimation of the difficulty of multi-hop questions. The problem is challenging due to the multi-step nature of the retrieval process, potential dependency of the steps and the reasoning involved. To tackle this challenge, we propose multHP, a novel pre-retrieval method for predicting the performance of open-domain multi-hop questions. Our extensive evaluation on the largest multi-hop QA dataset using several modern QA systems shows that the proposed model is a strong predictor of the performance, outperforming traditional single-hop QPP models. Additionally, we demonstrate that our approach can be effectively used to optimize the parameters of QA syste
    
[^18]: 使用LLM进行动态规划

    Dynamic Planning with a LLM. (arXiv:2308.06391v1 [cs.CL])

    [http://arxiv.org/abs/2308.06391](http://arxiv.org/abs/2308.06391)

    这项工作提出了一个称为LLM-DP的神经符号框架，它利用大型语言模型（LLM）与传统规划器合作，以更快、更高效地解决具体任务。相比于仅使用朴素LLM方法，LLM-DP在解决Alfworld任务上表现更好。

    

    虽然大型语言模型（LLM）可以在零-shot情景下解决许多自然语言处理任务，但涉及具体代理的应用仍然存在问题。特别是，需要多步推理的复杂计划随着上下文窗口的增长变得困难和昂贵。规划需要理解行动的可能影响，并确定当前环境是否满足目标状态。虽然符号规划器可以快速找到最优解，但它们需要完整准确地表示规划问题，严重限制了它们在实际场景中的使用。相比之下，现代LLM在推理任务时能够处理嘈杂的观察结果和高水平的不确定性。我们的工作提出了LLM-Dynamic Planner（LLM-DP）：一个神经符号框架，在解决具体任务时LLM与传统规划器携手合作。给定行动描述，LLM-DP比朴素LLM ReAct基准更快、更高效地解决了Alfworld任务。

    While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.
    
[^19]: ZYN：零式奖励模型与是非问题

    ZYN: Zero-Shot Reward Models with Yes-No Questions. (arXiv:2308.06385v1 [cs.CL])

    [http://arxiv.org/abs/2308.06385](http://arxiv.org/abs/2308.06385)

    本文提出了ZYN框架，通过使用是非问题作为奖励模型的提示，以及增强学习来微调语言模型，使其生成的文本与人类操作者的偏好对齐。实验证据表明该方法在不同领域的文本生成任务中具有很好的效果，包括解毒、情感优化和个性化提示生成器等。

    

    在这项工作中，我们解决了将语言模型的文本生成定向于期望行为的问题，将生成的文本与人类操作者的偏好对齐。我们建议使用另一个语言模型作为批评者，通过一个表示用户偏好的是非问题的提示，以零式方式作为奖励模型，而不需要进一步标记数据。这种零式奖励模型为进一步微调基本语言模型提供了学习信号，使用增强学习，就像在RLAIF中一样；然而我们的方法在其他上下文中也是兼容的，例如质量多样性搜索。通过在与文本生成相关的不同领域进行实验，包括解毒、优化电影评论的情感或任何其他属性、引导模型可能具有的关于特定主题的观点，以及个性化的文本到图像任务的提示生成器，提供了对所提出的ZYN框架能力的大量证据。代码将在\url处发布。

    In this work, we address the problem of directing the text generations of a LLM towards a desired behavior, aligning the generated text with the preferences of the human operator. We propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a Yes-No question that represents the user preferences, without requiring further labeled data. This zero-shot reward model provides the learning signal to further fine-tune the base LLM using reinforcement learning, as in RLAIF; yet our approach is also compatible in other contexts such as quality-diversity search. Extensive evidence of the capabilities of the proposed ZYN framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. Code to be released at \url
    
[^20]: 大型语言模型和知识图谱：机遇与挑战

    Large Language Models and Knowledge Graphs: Opportunities and Challenges. (arXiv:2308.06374v1 [cs.AI])

    [http://arxiv.org/abs/2308.06374](http://arxiv.org/abs/2308.06374)

    大型语言模型和知识图谱的结合为知识表示带来了新的机遇和挑战。

    

    大型语言模型（LLMs）已经给知识表示领域和整个世界带来了风暴。这一转折点标志着知识表示从显式知识表示转向显式知识和参数化知识混合表示的焦点的重塑。在这篇立场论文中，我们将讨论LLMs（参数化知识）和知识图谱（显式知识）领域内的一些常见争论，展望焦点重塑所带来的机遇和愿景，以及相关的研究主题和挑战。

    Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.
    
[^21]: 大型语言模型在电子健康记录中识别社会健康决定因素

    Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])

    [http://arxiv.org/abs/2308.06354](http://arxiv.org/abs/2308.06354)

    本研究利用大型语言模型从电子健康记录中提取社会健康决定因素（SDoH），并通过合成临床文本改进了这些极有价值但很少被记录的临床数据的提取。最佳模型为经过微调的Flan-T5 XL和Flan-T5 XXL，其中小型模型改进了性能。

    

    社会健康决定因素（SDoH）对患者的结果有重要影响，但在电子健康记录（EHR）中的收集不完整。本研究研究了大型语言模型从EHR中提取SDoH的能力，并探讨了合成临床文本在改进这些少见但极有价值的临床数据提取中的作用。对800份患者记录进行了SDoH类别的注释，并评估了几个基于transformer的模型。本研究还尝试了合成数据生成，并评估了算法偏差。我们表现最佳的模型是经过微调的Flan-T5 XL（macro-F1 0.71）用于任何SDoH，以及Flan-T5 XXL（macro-F1 0.70）。通过合成数据辅助微调的效益因模型架构和大小而异，在较小的Flan-T5模型（基础和大型）中表现出最大的性能提升（delta F1 +0.12到+0.23）。模型性能。

    Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance 
    
[^22]: 使用字素单位和辅助单语损失的双语流式自动语音识别研究

    Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss. (arXiv:2308.06327v1 [eess.AS])

    [http://arxiv.org/abs/2308.06327](http://arxiv.org/abs/2308.06327)

    本研究提出了一种双语的自动语音识别解决方案，采用字素单位和辅助单语损失，并通过大规模训练和测试任务验证了其在双语应用中的强大性能。

    

    我们介绍了一种双语解决方案，以支持在混合自动语音识别（ASR）环境中，英语作为大多数主要语言环境的第二语言。我们的关键开发包括：（a）使用字素单位而不是音素单位的发音词典，（b）完全双语对齐模型，随后是双语流式转换器模型，（c）带有语言识别（LID）损失的平行编码器结构，（d）带有辅助损失的平行编码器，用于单语投影。我们得出结论，与LID损失相比，我们提出的辅助损失在将平行编码器专门用于各自的单语环境方面更优秀，这有助于更强大的双语学习。我们在双语西班牙语（ES）和双语意大利语（IT）应用的大规模训练和测试任务上评估了我们的工作。我们的双语模型展示了较强的英语混合能力。特别是，双语IT模型改善了英语混合IT ta的错误率（WER）。

    We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT ta
    
[^23]: 使用指令反向翻译的自动对齐方法

    Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])

    [http://arxiv.org/abs/2308.06259](http://arxiv.org/abs/2308.06259)

    本论文提出了一种自动对齐方法，通过为人工编写的文本添加指令标签来构建高质量的指令跟踪语言模型。该方法通过自我增强和自我筛选生成训练示例，并且在Alpaca排行榜上表现出非常高效的自动对齐能力。

    

    我们提出了一种可扩展的方法，通过自动为人工编写的文本添加相应的指令标签来构建高质量的指令跟踪语言模型。我们的方法名为指令反向翻译，它从在少量种子数据和给定的网络语料库上微调的语言模型开始。种子模型用于通过为网络文档生成指令提示（自我增强）来构建训练示例，然后从这些候选示例中选择高质量的示例（自我筛选）。然后使用这些数据来微调更强的模型。通过使用我们方法的两次迭代来微调LLaMa，我们得到的模型在Alpaca排行榜上击败了所有其他基于LLaMa的模型，而无需依赖蒸馏数据，展示了非常有效的自动对齐能力。

    We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
    
[^24]: 使用大型语言模型提升金融审计的零样本文本匹配

    Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])

    [http://arxiv.org/abs/2308.06111](http://arxiv.org/abs/2308.06111)

    这项研究提出了ZeroShotALI，它使用了一种新颖的推荐系统来改进金融审计中的零样本文本匹配。通过采用大型语言模型（LLM）和经过领域优化的基于transformer的文本匹配解决方案，该系统实现了从报告中推荐与法律要求相符的相关文本段落，并在现有方法上取得了显著的性能提升。

    

    审计金融文件是一个非常繁琐和耗时的过程。目前，通过使用基于人工智能的解决方案可以简化这一过程，以推荐与严格会计标准的法律要求相符的报告中的相关文本段落。然而，这些方法需要定期进行微调，并且通常在工业环境中缺乏大量的注释数据。因此，我们提出了ZeroShotALI，这是一个新颖的推荐系统，利用了最先进的大型语言模型（LLM）与领域特定的优化的基于transformer的文本匹配解决方案。我们发现，首先使用自定义BERT模型检索与法律要求相符的若干最佳匹配的文档部分，然后使用LLM对这些选择进行过滤，可以显著改善现有方法的性能。

    Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
    
[^25]: 为视频语义角色标注构建全面的时空场景图

    Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v1 [cs.CV])

    [http://arxiv.org/abs/2308.05081](http://arxiv.org/abs/2308.05081)

    本论文提出了一种全面的时空场景图表示来解决视频语义角色标注中细粒度空间语义和时间建模的问题，并设计了一个面向特定目标的VidSRL框架，通过场景-事件映射机制来改进性能。

    

    视频语义角色标注(VidSRL)旨在通过识别预测-参数事件结构和事件之间的相互关系，从给定的视频中检测出显著的事件。尽管最近的努力已经提出了VidSRL的方法，但它们主要存在两个关键缺点，包括缺乏细粒度的空间场景感知和不足的视频时间建模。为了解决这个问题，本文基于现有的动态场景图结构，探索了一种新颖的全面的时空场景图(Holistic Spatio-Temporal Scene Graph)表示，很好地模拟了视频的细粒度空间语义和时间动态特性以进行VidSRL。在Holistic Spatio-Temporal Scene Graph的基础上，我们提出了一种面向特定目标的VidSRL框架。首先设计了一种场景-事件映射机制，以弥合底层场景结构与高级事件语义结构之间的差距，形成一个整体层次的场景-事件(ICE)图结构。我们进一步进行迭代操作以逐步改进VidSRL性能。

    Video Semantic Role Labeling (VidSRL) aims to detect the salient events from given videos, by recognizing the predict-argument event structures and the interrelationships between events. While recent endeavors have put forth methods for VidSRL, they can be mostly subject to two key drawbacks, including the lack of fine-grained spatial scene perception and the insufficiently modeling of video temporality. Towards this end, this work explores a novel holistic spatio-temporal scene graph (namely HostSG) representation based on the existing dynamic scene graph structures, which well model both the fine-grained spatial semantics and temporal dynamics of videos for VidSRL. Built upon the HostSG, we present a nichetargeting VidSRL framework. A scene-event mapping mechanism is first designed to bridge the gap between the underlying scene structure and the high-level event semantic structure, resulting in an overall hierarchical scene-event (termed ICE) graph structure. We further perform itera
    
[^26]: NLLG季度arXiv报告 06/23：当前最具影响力的AI论文是什么？（arXiv:2308.04889v1 [cs.CY]）

    NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])

    [http://arxiv.org/abs/2308.04889](http://arxiv.org/abs/2308.04889)

    该报告关注当前最具影响力的AI论文，并以标准化引用计数为依据编制了40篇最受欢迎的论文列表。观察到在2023年上半年，大型语言模型（LLMs）和具体而言的ChatGPT相关的论文占主导地位，ChatGPT表现出下降的趋势。

    

    人工智能（AI）领域中的生成式人工智能（Generative Artificial Intelligence，特别是自然语言处理（Natural Language Processing，NLP）和机器学习（Machine Learning，ML））信息的快速增长给研究人员和从业者带来了巨大的挑战，使得他们难以跟上最新的发展。为了解决信息过载的问题，Bielefeld大学的自然语言学习组在本报告中专注于识别arXiv上最受欢迎的论文，特别关注NLP和ML。其目标是为最相关且被广泛讨论的研究提供快速指南，以帮助新来者和已有研究人员跟上当前趋势。具体而言，我们根据2023年上半年的标准化引用计数编制了一个由40篇最受欢迎的论文组成的列表。我们观察到在2023年上半年，与大型语言模型（Large Language Models，LLMs）和具体而言的ChatGPT相关的论文占主导地位，而ChatGPT显示出下降的趋势。

    The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
    
[^27]: 使用理由生成和密集检索回答未知问题的较小语言模型

    Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])

    [http://arxiv.org/abs/2308.04711](http://arxiv.org/abs/2308.04711)

    本论文提出了两种方法来改进在具有充分解释性背景下，使用较小语言模型回答训练中未见的挑战性短问题回答任务。第一种方法是使用理据生成和密集检索结合的方式，并通过理据排名模型进行评分和组合。第二种方法是使用增强检索训练数据集训练较小的推理模型，以利用长文本序列中的相关信息。

    

    在提供足够的解释性背景的情况下，已经证明较小的语言模型在挑战性的无法在训练中见过的短问题回答任务上展现出强大的推理能力。我们评估了两种进一步改进该场景的方法。这两种方法都注重将大型语言模型生成的理由与通过多轮密集检索系统创建的更长上下文结合起来。第一个方法（$RR$）涉及训练一个理据排名模型，以评分的方式衡量生成的理由和检索到的上下文的相关性和真实性。然后，我们使用这些评分使用多种组合策略从两个知识源中获得组合上下文。对于第二种方法（$RATD$），我们使用增强检索训练数据集训练较小的推理模型，使其能够熟练地利用来自更长文本序列的相关信息，这些信息可能部分具有证据性且频繁出现。

    When provided with sufficient explanatory context, smaller Language Models have been shown to exhibit strong reasoning ability on challenging short-answer question-answering tasks where the questions are unseen in training. We evaluate two methods for further improvement in this setting. Both methods focus on combining rationales generated by a larger Language Model with longer contexts created from a multi-hop dense retrieval system. The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness. We then use the scores to derive combined contexts from both knowledge sources using a number of combinatory strategies. For the second method ($\textit{RATD}$) we train a smaller Reasoning model using retrieval-augmented training datasets such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequen
    
[^28]: 单句阅读器：解决答案位置偏倚的新方法

    Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])

    [http://arxiv.org/abs/2308.04566](http://arxiv.org/abs/2308.04566)

    本论文针对机器阅读理解中的答案位置偏倚问题，提出了一种名为单句阅读器的新方法，该方法使用六种不同模型实现。实验证明，单句阅读器与传统训练集上训练的模型几乎具有相当的性能，有效解决了答案位置偏倚问题。

    

    机器阅读理解（MRC）模型往往利用伪相关性（也称为数据集偏差或研究界的标注工件）。因此，这些模型可能在不完全理解给定的上下文和问题的情况下执行MRC任务，这是不可取的，因为它可能导致对分布转移的低稳健性。本文深入探讨了答案位置偏倚的概念，其中训练问题中有相当比例的答案仅位于上下文的第一句。我们提出了一种名为单句阅读器的新方法来解决MRC中的答案位置偏倚问题。我们使用六种不同模型来实现这种方法，并对其性能进行了彻底分析。值得注意的是，我们提出的单句阅读器的结果几乎与传统训练集上训练的模型相当，证明了其有效性。我们的研究还讨论了我们的单句阅读器遇到的几个挑战和提出的应对策略。

    Machine Reading Comprehension (MRC) models tend to take advantage of spurious correlations (also known as dataset bias or annotation artifacts in the research community). Consequently, these models may perform the MRC task without fully comprehending the given context and question, which is undesirable since it may result in low robustness against distribution shift. This paper delves into the concept of answer-position bias, where a significant percentage of training questions have answers located solely in the first sentence of the context. We propose a Single-Sentence Reader as a new approach for addressing answer position bias in MRC. We implement this approach using six different models and thoroughly analyze their performance. Remarkably, our proposed Single-Sentence Readers achieve results that nearly match those of models trained on conventional training sets, proving their effectiveness. Our study also discusses several challenges our Single-Sentence Readers encounter and prop
    
[^29]: 在对话多模情感识别中重新审视模态和上下文的解缠和融合

    Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition. (arXiv:2308.04502v1 [cs.CL])

    [http://arxiv.org/abs/2308.04502](http://arxiv.org/abs/2308.04502)

    本研究重新审视对话多模态情感识别中的模态和上下文，提出了一种双层解缠机制来同时建模特征的多模态性和对话的语境化，以进一步提高任务性能。

    

    在对话场景下，使机器能够理解人类情感在多模态语境下的研究一直是一个热门研究课题，这个任务被称为对话式多模态情感分析（MM-ERC）。近年来，MM-ERC一直受到关注，许多方法已被提出以提高任务性能。大多数现有的研究将MM-ERC视为标准的多模态分类问题，并通过解缠和融合多模态特征来最大化特征的效用。然而在重新审视MM-ERC的特点后，我们认为在特征解缠和融合的步骤中，既应该适当地建模特征的多模态性，也应该建模对话的语境化。在这项工作中，我们将充分考虑上述观点来进一步提高任务性能。一方面，在特征解缠阶段，我们根据对比学习技术，设计了一个双层解缠机制。

    It has been a hot research topic to enable machines to understand human emotions in multimodal contexts under dialogue scenarios, which is tasked with multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received consistent attention in recent years, where a diverse range of methods has been proposed for securing better task performance. Most existing works treat MM-ERC as a standard multimodal classification problem and perform multimodal feature disentanglement and fusion for maximizing feature utility. Yet after revisiting the characteristic of MM-ERC, we argue that both the feature multimodality and conversational contextualization should be properly modeled simultaneously during the feature disentanglement and fusion steps. In this work, we target further pushing the task performance by taking full consideration of the above insights. On the one hand, during feature disentanglement, based on the contrastive learning technique, we devise a Dual-level Disentanglement Mec
    
[^30]: DialogRE^C+：DialogRE在对话中关系抽取中核指代帮助的扩展研究

    DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs. (arXiv:2308.04498v1 [cs.CL])

    [http://arxiv.org/abs/2308.04498](http://arxiv.org/abs/2308.04498)

    本研究将核指代解决方案引入到对话关系抽取领域，并通过新的数据集DialogRE^C+进行评估。研究表明，通过高质量的核指代知识可以增强参数关系的推理能力，从而在提升DRE任务中起到积极作用。

    

    对话关系抽取(DRE)是识别对话文本中参数对之间关系的任务，但常见的问题是人称代词、实体和发言者的核指代。本文引入了一个新的基准数据集DialogRE^C+，将核指代解决方案引入到DRE场景中。通过高质量的核指代知识，期望增强参数关系的推理能力。在DialogRE^C+数据集中，我们根据现有的DialogRE数据手动注释了总共5,068个核指代链，涵盖了36,369个参数提及。其中，明确标记了四种不同的核指代链类型，分别是发言者链、个人链、地点链和组织链。我们还开发了4个基于图的DRE模型，以学习有效的核指代表示，从而改进DRE任务。我们还基于我们的注释训练了一个核指代解决模型，并评估了自动提取的核指代对任务的影响。

    Dialogue relation extraction (DRE) that identifies the relations between argument pairs in dialogue text, suffers much from the frequent occurrence of personal pronouns, or entity and speaker coreference. This work introduces a new benchmark dataset DialogRE^C+, introducing coreference resolution into the DRE scenario. With the aid of high-quality coreference knowledge, the reasoning of argument relations is expected to be enhanced. In DialogRE^C+ dataset, we manually annotate total 5,068 coreference chains over 36,369 argument mentions based on the existing DialogRE data, where four different coreference chain types namely speaker chain, person chain, location chain and organization chain are explicitly marked. We further develop 4 coreference-enhanced graph-based DRE models, which learn effective coreference representations for improving the DRE task. We also train a coreference resolution model based on our annotations and evaluate the effect of automatically extracted coreference c
    
[^31]: 双向多跳推理模型用于联合对话情感分类和行为识别

    A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v1 [cs.CL])

    [http://arxiv.org/abs/2308.04424](http://arxiv.org/abs/2308.04424)

    我们提出了一种双向多跳推理模型，用于联合对话情感分类和行为识别。该模型能够全面理解对话上下文，并显式建模情感和行为标签之间的关联，从而提取丰富的情感和行为线索，实现有效且准确的推理。

    

    对话情感分类（DSC）和行为识别（DAR）的联合任务旨在同时预测对话中每个话语的情感标签和行为标签。然而，当前的方法只能单向编码对话上下文，限制了其对上下文的全面理解能力。此外，这些方法忽视了情感和行为标签之间的显式关联，导致对丰富的情感和行为线索的获取能力不足，从而阻碍了有效和准确的推理。为了解决这些问题，我们提出了一种双向多跳推理模型（BMIM），它利用特征选择网络和双向多跳推理网络来迭代地提取和整合丰富的情感和行为线索。我们还采用对比学习和双学习来明确建模情感和行为标签之间的关联。我们在两个广泛使用的数据集上的实验证明，BMIM的性能优于当前方法。

    The joint task of Dialog Sentiment Classification (DSC) and Act Recognition (DAR) aims to predict the sentiment label and act label for each utterance in a dialog simultaneously. However, current methods encode the dialog context in only one direction, which limits their ability to thoroughly comprehend the context. Moreover, these methods overlook the explicit correlations between sentiment and act labels, which leads to an insufficient ability to capture rich sentiment and act clues and hinders effective and accurate reasoning. To address these issues, we propose a Bi-directional Multi-hop Inference Model (BMIM) that leverages a feature selection network and a bi-directional multi-hop inference network to iteratively extract and integrate rich sentiment and act clues in a bi-directional manner. We also employ contrastive learning and dual learning to explicitly model the correlations of sentiment and act labels. Our experiments on two widely-used datasets show that BMIM outperforms s
    
[^32]: Zhongjing: 通过专家反馈和真实的多轮对话增强大型语言模型的中医能力

    Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03549](http://arxiv.org/abs/2308.03549)

    本研究介绍了中精，这是一种基于LLaMA的中文医学LLM，通过整个训练流程，并结合人类反馈，以及引入一个中文多轮医学对话数据集CMtMedQA，大大提升了模型在复杂对话和主动询问发起方面的能力。

    

    最近大型语言模型（LLM）的进展在理解和回应用户意图方面取得了显著突破。然而，在某些专业领域（如中医学）中，它们在常规使用中的表现仍然落后。目前将中医纳入LLM的方法主要依赖于使用单轮和精简对话数据进行监督微调（SFT）。这些模型缺乏医生一样的主动询问和多轮理解的能力，并且不能始终与专家的安全和专业性对齐回复。本研究中，我们介绍了中医基于LLaMA的中文医学LLM——中精，它实现了从预训练到强化学习的整个训练流程，并利用人类反馈（RLHF）。此外，我们还介绍了一个包含70,000个真实医患对话的中文多轮医学对话数据集CMtMedQA，它极大地增强了模型在复杂对话和主动询问发起方面的能力。

    Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a r
    
[^33]: 用粗到细的基于词典的检索器改进少样本和零样本实体链接

    Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever. (arXiv:2308.03365v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03365](http://arxiv.org/abs/2308.03365)

    本文提出了一种粗到细的基于词典的检索器，用于改进少样本和零样本实体链接。实验结果表明，该方法能够在不需要大量微调的情况下获得优越的性能，并在NLPCC 2023中的中文少样本和零样本实体链接共享任务中排名第一。

    

    少样本和零样本实体链接关注尾部和新兴实体，这更具挑战性但更接近真实场景。主流方法是“检索和重排序”两阶段框架。本文提出了一种粗到细的基于词典的检索器，以有效地检索实体候选项，它在两层进行操作。第一层利用实体名称检索粗粒度候选项，而第二层在粗粒度候选项中缩小搜索范围到细粒度候选项。此外，第二层使用实体描述来有效消除与现有热门实体同名的尾部或新实体的歧义。实验结果表明，我们的方法能够在不需要大量微调的情况下获得优越的性能。值得注意的是，我们的方法在NLPCC 2023共享任务6上中文少样本和零样本实体链接中排名第一。

    Few-shot and zero-shot entity linking focus on the tail and emerging entities, which are more challenging but closer to real-world scenarios. The mainstream method is the ''retrieve and rerank'' two-stage framework. In this paper, we propose a coarse-to-fine lexicon-based retriever to retrieve entity candidates in an effective manner, which operates in two layers. The first layer retrieves coarse-grained candidates by leveraging entity names, while the second layer narrows down the search to fine-grained candidates within the coarse-grained ones. In addition, this second layer utilizes entity descriptions to effectively disambiguate tail or new entities that share names with existing popular entities. Experimental results indicate that our approach can obtain superior performance without requiring extensive finetuning in the retrieval stage. Notably, our approach ranks the 1st in NLPCC 2023 Shared Task 6 on Chinese Few-shot and Zero-shot Entity Linking.
    
[^34]: ClassEval: 一种手工构建的用于评估LLMs在类级代码生成上的基准

    ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])

    [http://arxiv.org/abs/2308.01861](http://arxiv.org/abs/2308.01861)

    ClassEval是一种手工构建的类级代码生成基准，该研究首次尝试在这一更具挑战性的场景中评估LLMs，并发现现有LLMs在类级代码生成上的性能相对较差。GPT-4和GPT-3.5在类级代码生成方面表现出相对其他LLMs更卓越的优势。

    

    在这项工作中，我们首次尝试在更具挑战性的代码生成场景中评估LLMs，即类级代码生成。我们首先手动构建了第一个类级代码生成基准ClassEval，其中包含100个类级Python代码生成任务，总共耗时约500人小时。在此基础上，我们对11个最先进的LLMs在类级代码生成上进行了第一次研究。根据我们的结果，我们得出以下主要发现。首先，我们发现所有现有的LLMs在类级代码生成上的性能要远低于独立的方法级代码生成基准（如HumanEval）；而方法级的编码能力不能等同地反映LLMs在类级编码能力上的表现。其次，我们发现GPT-4和GPT-3.5在类级代码生成上仍然表现出相对其他LLMs更卓越的优势，而二线模型包括Instruct-Starcoder，Instruct-Codegen和Wizardcoder在性能上非常相似。

    In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. 
    
[^35]: 大型语言模型是否仍然存在纠错问题？

    Does Correction Remain An Problem For Large Language Models?. (arXiv:2308.01776v1 [cs.CL])

    [http://arxiv.org/abs/2308.01776](http://arxiv.org/abs/2308.01776)

    本文通过两个实验探讨了在大型语言模型背景下纠错问题的作用，第一个实验是将纠错作为独立任务进行研究，第二个实验则是探讨纠错作为其他NLP任务的准备任务的概念。研究结果将揭示纠错在大型语言模型时代的重要性及其对各种NLP应用的影响。

    

    随着GPT等大型语言模型不断提升自然语言处理（NLP）的能力，一个问题出现了：纠错问题是否仍然存在？本文通过进行两个实验，探讨了在大型语言模型的背景下纠错的作用。第一个实验将纠错作为独立的任务，使用GPT类模型和few-shot learning技术进行错误纠正。第二个实验则探讨了纠错作为其他NLP任务的准备任务的概念，检验大型语言模型在包含一定程度噪声或错误的文本上是否能够容忍并表现得足够好。通过解决这些实验，我们旨在揭示在大型语言模型时代纠错的重要性，以及对各种NLP应用的影响。

    As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.
    
[^36]: 在大型语言模型中解锁组合性的上下文提示: 技能指导策略

    Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])

    [http://arxiv.org/abs/2308.00304](http://arxiv.org/abs/2308.00304)

    本文介绍了一种名为技能指导 (SKiC) 的提示策略，通过在上下文中演示基本技能和组合性示例，使大型语言模型具备解决更复杂问题的能力，并在泛化能力上取得几乎完美的表现。

    

    本文考虑了如何通过一种新颖的提示策略，在大型语言模型（LLMs）中激发组合性泛化能力的问题。组合性泛化使得LLMs能够解决比它们所见过的问题更困难的问题（即易于难的泛化），这是人类智能的关键推理能力。然而，即使是当前最先进的LLMs在这种形式的推理上仍然存在困难。为了弥合这一差距，我们提出了在上下文中的技能指导（SKiC）提示，它指导LLMs如何组合基本技能来解决更复杂的问题。我们发现，在相同的提示上展示技能和组合性示例是至关重要的。仅仅通过两个示例，我们的SKiC提示在技能和它们的组合能力之间形成了强大的协同效应。值得注意的是，它赋予了LLMs解决需要创新技能组合的未见问题的能力，实现了几乎完美的泛化。

    We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b
    
[^37]: 对API方面分析的对比学习

    Contrastive Learning for API Aspect Analysis. (arXiv:2307.16878v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2307.16878](http://arxiv.org/abs/2307.16878)

    这项研究提出了一种对API方面分析进行对比学习的方法，通过训练变换器模型并使用监督对比损失函数，可以显著改进性能，对Performance、Security、Usability和Documentation等方面的检测效果进行了评估，并进行了实证和开发者研究验证。

    

    我们提出了一种新颖的方法- CLAA-用于API评论中的API方面检测，该方法利用了训练有监督对比损失目标函数的变换器模型。我们通过性能和影响分析评估了CLAA。对于性能分析，我们使用了从Stack Overflow收集的开发者讨论的基准数据集，并将结果与使用最先进的变换器模型获得的结果进行了比较。实验证明，对比学习可以显著提高变换器模型在检测性能、安全性、可用性和文档方面等方面的性能。对于影响分析，我们进行了实证和开发者研究。在随机选择的并人工标记的200个在线评论上，CLAA达到了92%的准确率，而最先进的基线方法只有81.5%。根据我们对10名参与者进行的开发者研究，使用“Stack Overflow + CLAA”在API选择过程中可以提高准确性和信心。复制程序包：

    We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of 'Stack Overflow + CLAA' resulted in increased accuracy and confidence during API selection. Replication package: 
    
[^38]: 关于最先进生成模型的可信度景观：一项综合调查

    On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16680](http://arxiv.org/abs/2307.16680)

    本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。

    

    扩散模型和大规模语言模型已经成为领先的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模型的实际应用也暴露出固有的风险，突显了它们的双重性质，并引发了对它们可信度的担忧。尽管有大量关于这个主题的文献，但针对大规模生成模型及其可信度的综合调查仍然很少见。为了弥补这一空白，本文调查了涉及这些模型的长期和新兴威胁，涵盖了隐私、安全、公平和责任这四个基本维度。通过这种方式，我们构建了一张详尽的地图，概述了这些模型的可信度，并提供了实际建议和未来的发展方向。这些努力对于促进这些模型的可信度部署至关重要。

    Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
    
[^39]: 高效的LLM引导生成

    Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])

    [http://arxiv.org/abs/2307.09702](http://arxiv.org/abs/2307.09702)

    本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。

    

    在本文中，我们描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。我们的方法在标记序列生成过程中几乎不增加任何开销，并使得引导生成在实际中可行。在开源Python库Outlines中提供了一个实现。

    In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
    
[^40]: MMBench: 您的多模态模型是全能球员吗？

    MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])

    [http://arxiv.org/abs/2307.06281](http://arxiv.org/abs/2307.06281)

    MMBench是一个新型的多模态基准测试，旨在解决大型视觉语言模型评估的挑战，通过开发全面的评估流程和精心策划的数据集进行细粒度能力评估。

    

    最近，大型视觉语言模型在视觉信息的感知和推理能力方面取得了显著进展。然而，如何有效评估这些大型视觉语言模型仍然是一个主要障碍，阻碍了未来模型的发展。传统的基准测试，如VQAv2或COCO Caption提供了定量的性能测量，但在细粒度能力评估和非鲁棒评估指标方面存在不足。最近的主观基准测试，如OwlEval，通过整合人力资源，对模型的能力进行了全面评估，但不可扩展并且存在显著的偏见。针对这些挑战，我们提出了MMBench，一种新型的多模态基准测试。MMBench系统地开发了一个全面的评估流程，主要由两个元素组成。第一个元素是精心策划的数据集，在评估数量和多样性方面超越了现有的类似基准测试。

    Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
    
[^41]: 孤掌难鸣：通过验证低置信度生成检测和减轻LLMs的幻觉

    A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. (arXiv:2307.03987v1 [cs.CL])

    [http://arxiv.org/abs/2307.03987](http://arxiv.org/abs/2307.03987)

    本文提出了一种通过验证低置信度生成来检测和减轻LLMs的幻觉的方法。通过对模型生成的候选项进行验证，并缓解幻觉，改进了模型的可靠性。

    

    最近开发的大型语言模型在生成流畅连贯的文本方面取得了显著的成功。然而，这些模型常常会发生“幻觉”，严重影响其可靠性。在这项工作中，我们解决了这个关键问题，并提出了一种主动检测和减轻生成过程中幻觉的方法。具体而言，我们首先利用模型的逻辑输出值识别可能的幻觉候选项，通过验证过程检查其正确性，减轻检测到的幻觉，然后继续生成过程。通过与“文章生成任务”的广泛实验，我们首先展示了我们的检测和减轻技术的个体有效性。具体而言，检测技术的召回率达到了88％，缓解技术成功缓解了57.6％被正确检测到的幻觉。重要的是，我们的减轻技术不会引入新的幻觉。

    Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of 88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new ha
    
[^42]: GEmo-CLAP: 面向语音情感识别的性别属性增强对比语音-语言预训练模型

    GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])

    [http://arxiv.org/abs/2306.07848](http://arxiv.org/abs/2306.07848)

    本文提出了GEmo-CLAP模型用于语音情感识别，结合了性别属性信息，相比于其他先进方法，该模型在IEMOCAP上实现了更优越的识别性能。

    

    对比语音-语言预训练（CLAP）最近在不同领域取得了惊人的成功。本文提出了一种名为GEmo-CLAP的高效性别属性增强CLAP模型，用于语音情感识别（SER）。具体而言，我们首先利用各种自监督学习的预训练模型构建了一种有效的情感CLAP模型（称为Emo-CLAP），用于SER。然后，考虑到在语音情感建模中性别属性的重要性，我们进一步提出了两种GEmo-CLAP方法，来整合语音信号的情感和性别信息，形成更合理的目标。在IEMOCAP语料库上进行的大量实验表明，我们提出的两种GEmo-CLAP方法始终优于基线Emo-CLAP模型（使用不同的预训练模型），同时与其他最先进的方法相比实现了更优越的识别性能。

    Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
    
[^43]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^44]: 从弱文本监督中学习图像中的人际互动

    Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])

    [http://arxiv.org/abs/2304.14104](http://arxiv.org/abs/2304.14104)

    本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。

    

    人际互动是多样且依赖于上下文的，但先前的工作将它们视为分类，忽略了可能的互动的重尾。本文提出了一种新的学习人际互动的范式，将其作为自由文本从单一的静态图像中学习，从而允许对情况和人际关系的无限空间进行灵活建模。为了克服缺乏特定于此任务的标记数据的问题，我们使用知识蒸馏应用于由大型语言模型产生的合成字幕数据，以此生成伪标签。我们展示了通过这个过程产生的伪标签可以用于训练一种字幕模型，能有效理解图像中的人际互动，通过衡量我们预测的文本和语义质量与事实的基础性的各种指标来衡量。我们进一步展示了我们的方法在这个任务上的性能优于SOTA的图像字幕和情境识别模型。我们将公开我们的代码。

    Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
    
[^45]: 学习语义文本相似度来排序金融术语的上义词

    Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v1 [cs.CL])

    [http://arxiv.org/abs/2303.13475](http://arxiv.org/abs/2303.13475)

    本文介绍了一种能够提取和排序金融术语上义词的系统，使用神经网络学习不同术语间的语义相似度。该系统帮助用户更好地了解复杂金融术语，帮助投资者做出明智的决策。

    

    近年来，用户使用金融服务的方式发生了变革。随着数字化技术的发展，越来越多的用户更喜欢在线方式来执行金融活动。这导致了大量的金融内容的生成。大多数投资者在做出决策前都会阅读这些内容。每个行业都有特定于其运营领域的术语。银行和金融服务也不例外。为了完全理解这些内容，需要对金融术语有深入的了解。当用它所属的广义类别来说明时，一个术语的基本概念变得容易。这个广义类别被称为上义词。例如，“债券”是金融术语“替代债券”的上义词。在本文中，我们提出了一个系统，能够提取和排序给定金融术语的上义词。该系统经过金融文本训练，并使用神经网络学习不同金融术语之间的语义相似度。所提出的系统可用于提高对复杂金融术语的理解，帮助投资者做出明智的决策。

    Over the years, there has been a paradigm shift in how users access financial services. With the advancement of digitalization more users have been preferring the online mode of performing financial activities. This has led to the generation of a huge volume of financial content. Most investors prefer to go through these contents before making decisions. Every industry has terms that are specific to the domain it operates in. Banking and Financial Services are not an exception to this. In order to fully comprehend these contents, one needs to have a thorough understanding of the financial terms. Getting a basic idea about a term becomes easy when it is explained with the help of the broad category to which it belongs. This broad category is referred to as hypernym. For example, "bond" is a hypernym of the financial term "alternative debenture". In this paper, we propose a system capable of extracting and ranking hypernyms for a given financial term. The system has been trained with fin
    
[^46]: 多视角的零样本开放意图归纳：多领域批处理和代理梯度转移

    Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])

    [http://arxiv.org/abs/2303.13099](http://arxiv.org/abs/2303.13099)

    本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。

    

    在任务导向的对话系统中，检测和诱导新的意图是将该系统应用于实际应用的两个主要挑战。本文提出了语义多视角模型来解决这两个难题：（1）用于一般嵌入的SBERT（2）多领域批处理（MDB）用于对话领域知识，以及（3）用于集群专业语义的代理梯度转移（PGT）。 MDB一次向模型提供多种对话数据集，通过学习多领域知识来解决多领域问题。我们引入了一种新的方法PGT，它采用Siamese网络直接使用聚类方法微调模型。我们的模型可以学习如何使用PGT聚类对话语句。实验结果表明，与基线系统相比，我们的多视角模型与MDB和PGT显着提高了Open Intent Induction的性能。

    In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
    
[^47]: 打破常识：WHOOPS！一个基于合成和组合图像的视觉与语言基准测试

    Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.07274](http://arxiv.org/abs/2303.07274)

    WHOOPS!是一个新的视觉常识数据集和基准测试，包括了图像字幕、跨模态匹配和视觉问答等若干个任务，引入了解释生成任务，挑战了AI模型识别和解释不合常规的图像的能力。

    

    奇怪、异常和神秘的图像会引起观察者的好奇心，因为它们挑战了常识。我们提出WHOOPS！一个新的视觉常识数据集和基准测试。该数据集由设计师使用Midjourney等公共可用图像生成工具制作，并包含若干个任务。除了图像字幕、跨模态匹配和视觉问答之外，我们还引入了一个困难的解释生成任务，其中模型必须识别并解释给定图像的异常之处。

    Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
    
[^48]: SEAM:一种集成了句子处理与阅读中眼动的激活耦合模型

    SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading. (arXiv:2303.05221v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2303.05221](http://arxiv.org/abs/2303.05221)

    SEAM是一种集成了眼动控制和句子处理的模型，为实现阅读中自然语言理解的完整数学模型迈出了重要一步。

    

    阅读中的眼动控制模型通常集中在视觉、注意、词汇和运动过程，但忽略了词汇后处理的语言处理。相比之下，句子理解过程的模型通常只关注词汇后处理的语言过程。我们提出了一种将这两种研究线索结合起来的模型，即整合眼动控制和句子处理。开发这样一个整合模型具有极大的挑战性和计算复杂性，但这样的整合是朝着完整的自然语言理解数学模型迈出的重要一步。我们将眼动控制模型SWIFT（Seelig等人，2020）与Lewis和Vasishth句子处理模型的关键组成部分（Lewis＆Vasishth，2005）结合在一起。这种整合首次变得可能，部分原因是因为。。

    Models of eye-movement control during reading, developed largely within psychology, usually focus on visual, attentional, lexical, and motor processes but neglect post-lexical language processing; by contrast, models of sentence comprehension processes, developed largely within psycholinguistics, generally focus only on post-lexical language processes. We present a model that combines these two research threads, by integrating eye-movement control and sentence processing. Developing such an integrated model is extremely challenging and computationally demanding, but such an integration is an important step toward complete mathematical models of natural language comprehension in reading. We combine the SWIFT model of eye-movement control (Seelig et al., 2020, doi:10.1016/j.jmp.2019.102313) with key components of the Lewis and Vasishth sentence processing model (Lewis & Vasishth, 2005, doi:10.1207/s15516709cog0000_25). This integration becomes possible, for the first time, due in part to
    
[^49]: 摘要即标题：利用自动化文本摘要生成科学文献的插图标题

    Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12324](http://arxiv.org/abs/2302.12324)

    本文提出了利用自动化文本摘要生成科学文献的插图标题的方法，并使用预训练的抽象化摘要模型 PEGASUS 对引用图表的段落进行摘要。实验结果表明该方法在自动评估和人工评估中均优于之前的视觉方法。研究还发现了两个关键挑战：低质量作者撰写的标题的普遍存在以及对好标题缺乏明确的标准。

    

    良好的插图标题可以帮助论文读者理解复杂的科学图表。然而，即使是已发表的论文，其标题常常写得很差。自动生成标题可以帮助论文作者提供良好的起始标题，以便进一步改进质量。之前的工作常将插图标题生成视为一项视觉到语言的任务。本文中，我们表明将其作为科学文献中的文本摘要任务更为有效。我们对预训练的抽象化摘要模型 PEGASUS 进行了微调，专门用于将引用图表的段落（例如，“图3显示...”）摘要为图表标题。在大规模 arXiv 图表上进行的实验表明，我们的方法在自动评估和人工评估中均优于之前的视觉方法。我们还进行了深入的研究，重点关注两个关键挑战：(i) 低质量作者撰写的标题的普遍存在以及 (ii) 对好标题缺乏明确的标准。我们提供了代码

    Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., "Figure 3 shows...") into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and
    
[^50]: LabelPrompt: 关于关系分类的有效的提示式学习

    LabelPrompt: Effective Prompt-based Learning for Relation Classification. (arXiv:2302.08068v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08068](http://arxiv.org/abs/2302.08068)

    LabelPrompt是一种面向关系分类任务的提示式学习方法，通过定义额外的令牌来表示关系标签，并使用提示模板方法明确构建它们，从而解决了将填充掩码标记的自然语言词汇与语义关系标签相关联的挑战。同时，该方法还实现了一个实体感知模块来减轻预测关系和给定实体之间的不一致性。

    

    最近，通过将自然语言处理（NLP）任务转换为填空式格式，以更好地与预训练语言模型（PLMs）对齐的方式，提示式学习在许多NLP任务中变得流行起来。然而，将这种方法应用于关系分类任务面临着独特的挑战。具体而言，将填充掩码标记的自然语言词汇与语义关系标签（如"org:founded_by"）相关联是困难的。为了解决这个挑战，本文提出了一种新颖的提示式学习方法，称为LabelPrompt，用于关系分类任务。受到“给予模型选择”的直觉的启发，我们首先定义额外的令牌来表示关系标签，将这些令牌视为具有语义初始化的口述者，并使用提示模板方法明确构建它们。然后，为了减轻预测关系和给定实体之间的不一致性，我们实现了一个实体感知模块，并采用对抗性的方式进行引导。

    Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\textit{e.g.} \textit{``org:founded\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contra
    
[^51]: 时间建模重要性：一种新的语音情感识别的时间情感建模方法

    Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.08233](http://arxiv.org/abs/2211.08233)

    该论文提出了一种新的时间情感建模方法，通过学习不同时间尺度的多尺度情感上下文表示，并融合多个时间尺度的特征，实现了更好的语音情感识别效果。

    

    语音情感识别在通过语音信号推断人类情感和情感状态，提高人机交互方面起着重要作用。最近的工作主要关注从手工特征中挖掘时空信息，我们探索如何从动态时间尺度建模语音情感的时间模式。为实现这一目标，我们提出了一种新的语音情感识别的时间感知双向多尺度网络（TIM-Net），该网络从不同的时间尺度学习多尺度情感上下文表示。具体而言，TIM-Net首先使用时间感知块学习时间情感表示，然后整合过去和未来的补充信息以丰富上下文表示，并最终融合多个时间尺度的特征以更好地适应情感变化。在六个基准语音情感识别数据集上的广泛实验结果表明

    Speech emotion recognition (SER) plays a vital role in improving the interactions between humans and machines by inferring human emotion and affective states from speech signals. Whereas recent works primarily focus on mining spatiotemporal information from hand-crafted features, we explore how to model the temporal patterns of speech emotions from dynamic temporal scales. Towards that goal, we introduce a novel temporal emotional modeling approach for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net), which learns multi-scale contextual affective representations from various time scales. Specifically, TIM-Net first employs temporal-aware blocks to learn temporal affective representation, then integrates complementary information from the past and the future to enrich contextual representations, and finally, fuses multiple time scale features for better adaptation to the emotional variation. Extensive experimental results on six benchmark SER datasets demonstrate th
    
[^52]: NECE: 故事事件链提取工具包

    NECE: Narrative Event Chain Extraction Toolkit. (arXiv:2208.08063v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.08063](http://arxiv.org/abs/2208.08063)

    NECE是一个开源的故事事件链提取工具包，能够自动提取和对齐故事事件，并可用于分析故事偏见。

    

    为了理解一个故事，理解事件的时间流动尤为重要，尤其是与主要角色相关的事件；然而，对于冗长和非结构化的故事文本，这可能是具有挑战性的。为了解决这个问题，我们介绍了NECE，一个开放获取的、基于文档级别的工具包，它可以自动提取和对齐故事事件，按照它们发生的时间顺序排列。通过广泛的评估，我们展示了NECE工具包的高质量，并展示了它在分析与性别相关的故事偏见方面的应用。我们还公开讨论了当前方法的缺点，以及未来工作中利用生成模型的潜力。最后，NECE工具包包括一个Python库和一个用户友好的Web界面，为专业人员和普通大众提供平等的访问权，可以可视化事件链，获取故事流程，或者研究故事偏见。

    To understand a narrative, it is essential to comprehend the temporal event flows, especially those associated with main characters; however, this can be challenging with lengthy and unstructured narrative texts. To address this, we introduce NECE, an open-access, document-level toolkit that automatically extracts and aligns narrative events in the temporal order of their occurrence. Through extensive evaluations, we show the high quality of the NECE toolkit and demonstrates its downstream application in analyzing narrative bias regarding gender. We also openly discuss the shortcomings of the current approach, and potential of leveraging generative models in future works. Lastly the NECE toolkit includes both a Python library and a user-friendly web interface, which offer equal access to professionals and layman audience alike, to visualize event chain, obtain narrative flows, or study narrative bias.
    
[^53]: Transformers可以在上下文中学习什么？一个简单函数类的案例研究。

    What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. (arXiv:2208.01066v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.01066](http://arxiv.org/abs/2208.01066)

    本研究通过考虑在上下文中学习线性函数的问题，证明了标准的Transformers模型可以从头训练，在推断时实现线性函数的上下文学习能力。

    

    上下文学习是指模型能够依赖于包含上下文示例（与某个任务对应的输入-输出对）和新的查询输入的提示序列，并生成相应的输出。关键是，在推断时，上下文学习仅发生在模型参数未更新的情况下。虽然像GPT-3这样的大型语言模型表现出了一定的上下文学习能力，但目前尚不清楚成功的任务与训练数据中的什么内容之间的关系。为了进一步理解上下文学习，我们考虑了一个明确定义的问题，即训练一个模型以在上下文中学习函数类（例如线性函数）：也就是说，给定从该类中导出的数据，我们能否训练一个模型来在上下文中学习“大多数”函数？我们通过实验证明，标准的Transformers可以从头开始训练，以在上下文中学习线性函数。

    In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions 
    
[^54]: 针对低资源语言的不匹配感知无监督翻译质量评估

    Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages. (arXiv:2208.00463v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.00463](http://arxiv.org/abs/2208.00463)

    本文提出了一种针对低资源语言的无监督翻译质量评估方法，通过使用XLMRScore和一些改进措施来解决未翻译标记和语言不匹配的问题。

    

    翻译质量评估（QE）是在没有参考的情况下预测机器翻译（MT）输出质量的任务。这个任务在机器翻译的实际应用中越来越受到关注。在本文中，我们首先提出了XLMRScore，它是通过XLM-RoBERTa（XLMR）模型计算的BERTScore的跨语言对应物。这个度量可以用作简单的无监督QE方法，但使用它会导致两个问题：一是导致意外高翻译分数的未翻译标记，二是在XLMRScore中应用贪婪匹配时源语言和假设语言之间不匹配错误的问题。为了减轻这些问题，我们建议使用未翻译的词替换为未知标记，并跨语言对齐预训练模型以更接近对齐的词。我们在WMT21 QE共享任务的四个低资源语言对上评估了所提出的方法。

    Translation Quality Estimation (QE) is the task of predicting the quality of machine translation (MT) output without any reference. This task has gained increasing attention as an important component in the practical applications of MT. In this paper, we first propose XLMRScore, which is a cross-lingual counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric can be used as a simple unsupervised QE method, while employing it results in two issues: firstly, the untranslated tokens leading to unexpectedly high translation scores, and secondly, the issue of mismatching errors between source and hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate these issues, we suggest replacing untranslated words with the unknown token and the cross-lingual alignment of the pre-trained model to represent aligned words closer to each other, respectively. We evaluate the proposed method on four low-resource language pairs of WMT21 QE shared task, as well as
    
[^55]: PInKS: 带有最小监督的预处理常识推理

    PInKS: Preconditioned Commonsense Inference with Minimal Supervision. (arXiv:2206.07920v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07920](http://arxiv.org/abs/2206.07920)

    PInKS是一种通过最小监督实现前提推理的改进模型，通过提高常识知识前提推理基准测试的结果，证明了其有效性。

    

    对于语言模型来说，以“玻璃可以用来装水，除非玻璃破碎”等前提进行推理仍然是一个未解决的问题。主要挑战在于前提数据的稀缺性以及模型对此类推理的支持不足。我们提出了PInKS（带有最小监督的预处理常识推理），这是一个通过最小监督来进行前提推理的改进模型。我们通过实验证明，PInKS在专注于常识知识前提推理的基准测试上改善了结果（最多提高了40%的Macro-F1分数）。我们进一步通过PAC-Bayesian信息分析、精确度指标和消融研究来研究PInKS。

    Reasoning with preconditions such as "glass can be used for drinking water unless the glass is shattered" remains an open problem for language models. The main challenge lies in the scarcity of preconditions data and the model's lack of support for such reasoning. We present PInKS, Preconditioned Commonsense Inference with WeaK Supervision, an improved model for reasoning with preconditions through minimum supervision. We show, both empirically and theoretically, that PInKS improves the results on benchmarks focused on reasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1 scores). We further investigate PInKS through PAC-Bayesian informativeness analysis, precision measures, and ablation study.
    
[^56]: MathBERT: 一种用于数学教育中的通用自然语言处理任务的预训练语言模型

    MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.07340](http://arxiv.org/abs/2106.07340)

    MathBERT是一个基于BASE BERT模型在大规模数学语料库上进行预训练的模型，为数学教育中的通用NLP任务提供了新的解决方案。

    

    自从原始的BERT（即BASE BERT）的引入以来，研究人员通过利用迁移学习的优势，开发了各种定制的BERT模型来改进特定领域和任务的性能。由于数学文本的性质，经常使用领域特定的词汇以及方程和数学符号，我们认为开发一个针对数学的新BERT模型将对许多数学下游任务有用。在这篇资源论文中，我们介绍了我们的多机构努力（即两个学习平台和三个美国学术机构）以满足这个需求：MathBERT，一个通过在大规模数学语料库上对BASE BERT模型进行预训练而创建的模型，该语料库涵盖了从学前教育（pre-k）到高中以及研究生水平的数学内容。此外，我们选择了三个常用于数学教育的通用NLP任务：知识组件预测，自动评分开放性问题和知识追溯。

    Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&A, and knowledge tr
    
[^57]: PaCo: 前提条件归因于常识知识

    PaCo: Preconditions Attributed to Commonsense Knowledge. (arXiv:2104.08712v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.08712](http://arxiv.org/abs/2104.08712)

    本研究提出了一个名为PaCo的数据集，用于推理常识知识的环境前提条件。通过三个评估任务，我们发现目前的语言模型在理解情境前提条件方面与人类表现存在10-30%的差距，这表明推理前提条件是一个未解决的挑战。

    

    人类可以无缝地推理常识知识的环境前提条件。我们知道一个玻璃是用来喝水的，除非玻璃破碎或水有毒。尽管最先进的语言模型在推断常识知识方面有着令人印象深刻的表现，但它们是否理解环境前提条件尚不清楚。为了解决这一差距，我们提出了一种新颖的具有环境前提的推理挑战。我们收集了一个名为PaCo的数据集，其中包含以自然语言表达的12.4千个常识陈述的前提条件。基于该数据集，我们创建了三个经典评估任务，并使用它们来检验现有语言模型理解情境前提条件的能力。我们的结果显示在这些任务上机器和人类的表现存在10-30%的差距，这表明推理前提条件是一个未解决的挑战。

    Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models' (LMs) impressive performance on inferring commonsense knowledge, it is unclear whether they understand the circumstantial preconditions. To address this gap, we propose a novel challenge of reasoning with circumstantial preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand preconditions of commonsense statements expressed in natural language. Based on this dataset, we create three canonical evaluation tasks and use them to examine the capability of existing LMs to understand situational preconditions. Our results reveal a 10-30% gap between machine and human performance on our tasks, which shows that reasoning with preconditions is an open challenge.
    

