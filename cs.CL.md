# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [VideoAgent: Long-form Video Understanding with Large Language Model as Agent](https://arxiv.org/abs/2403.10517) | 提出了一种新颖的基于代理的系统VideoAgent，利用大型语言模型作为中央代理，采用互动推理和计划来处理长视频理解问题，在挑战性基准测试中表现出卓越的效果和效率。 |
| [^2] | [Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study](https://arxiv.org/abs/2403.10499) | 本研究通过对多模态基础模型CLIP进行大规模鲁棒性基准测试，揭示了其在涵盖自然分布偏移、合成分布偏移和对抗攻击等多个方面的优异表现。 |
| [^3] | [Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases](https://arxiv.org/abs/2403.10446) | 利用RAG系统增强LLMs，提高其对私人知识库中特定领域和时效查询的事实准确性，尤其在使用外部数据集方面表现出潜力。 |
| [^4] | [Optimal Block-Level Draft Verification for Accelerating Speculative Decoding](https://arxiv.org/abs/2403.10444) | 提出了一种更好的草稿验证算法，通过将验证步骤制定为块级最优传输问题，实现了额外的墙钟速度提升，而不增加额外的计算成本和草稿标记 |
| [^5] | [Monotonic Representation of Numeric Properties in Language Models](https://arxiv.org/abs/2403.10381) | 介绍了一种在语言模型中找到和编辑数值属性表示的简单方法，发现编码数值属性的低维子空间以单调、可解释和可编辑的方式存在，通过编辑这些子空间中的方向，使语言模型的输出相应地发生变化。 |
| [^6] | [EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models](https://arxiv.org/abs/2403.10378) | EXAMS-V是一个独特的、跨学科多语言多模态考试基准，采用多种语言和多种教育系统的考试题目，需要进行复杂的推理并依赖于区域特定知识。 |
| [^7] | [TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale](https://arxiv.org/abs/2403.10351) | TriSum是一个框架，通过将大型语言模型的文本总结能力提炼到一个紧凑的本地模型中，从而在各种基准测试中提高了性能，并提高了可解释性。 |
| [^8] | [Investigating grammatical abstraction in language models using few-shot learning of novel noun gender](https://arxiv.org/abs/2403.10338) | 语言模型可以通过很少的例子学习新名词的性别，并在不同的一致上下文中应用所学性别，虽然有对阳性性别类别的偏见 |
| [^9] | [CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model](https://arxiv.org/abs/2403.10326) | 本文研究了通过应用预训练语言模型作为候选干扰项生成的替代方法来自动生成填空干扰项，并展示了这种PLM增强模型显著提高了性能。 |
| [^10] | [Uni-SMART: Universal Science Multimodal Analysis and Research Transformer](https://arxiv.org/abs/2403.10301) | 大型语言模型在总结文本方面的能力为改进科学文献分析提供了新途径，Uni-SMART旨在解决现有LLMs对多模态科学文献内容理解和分析能力的限制。 |
| [^11] | [MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank](https://arxiv.org/abs/2403.10293) | 这项研究提出了第一个多方言巴伐利亚通用依存树库（MaiBaam），手动标注了词类和句法依存信息，覆盖多种文本类型，突出了巴伐利亚话与德语之间的形态句法差异，并展示了说话者正字法的丰富变异性。 |
| [^12] | [Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning](https://arxiv.org/abs/2403.10281) | Team Trifecta在Factify 5WQA上以Fine-Tuning取得了首要地位，成功超越基准准确率103％，并保持了对第二名竞争者的70%领先优势。 |
| [^13] | [A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption](https://arxiv.org/abs/2403.10275) | 本文提出了一个方法来挑战为大型语言模型提供简单而丰富解释的可能性，研究发现使用基于特征的模型在信号传递方面效果更好。 |
| [^14] | [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://arxiv.org/abs/2403.10258) | 提出了通过本地语言提示来解决文化相关任务的方法，并呼吁发展强大的多语言LLMs。 |
| [^15] | [A Big Data Approach to Understand Sub-national Determinants of FDI in Africa](https://arxiv.org/abs/2403.10239) | 本文提出了一种基于文本挖掘和社交网络分析的新方法，通过分析超过167,000篇在线新闻文章，量化了影响非洲公司外资直接投资所有权的区域级属性，结果表明次国家级的结构和制度特征在外资直接投资中扮演着重要角色。 |
| [^16] | [A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream](https://arxiv.org/abs/2403.10237) | 本研究旨在深入研究波斯文主题检测中的最佳算法，确定对这些算法进行适配以适用于波斯语的必要改动，并在波斯社交网络文本上评估它们的性能。 |
| [^17] | [HawkEye: Training Video-Text LLMs for Grounding Text in Videos](https://arxiv.org/abs/2403.10228) | 本文提出了HawkEye，一个可以以完全文本方式执行时间视频定位的视频文本LLMs，并通过构建大规模视频文本语料库InternVid-G以及引入两个新的面向时间的训练目标，以及一种新的粗粒度表示视频段的方法来实现这一目标。 |
| [^18] | [Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis](https://arxiv.org/abs/2403.10214) | 本文提出了一种具有分层解缠结构的增强相干感知网络（ECAN）用于方面类别情感分析任务，通过探索相干性建模和分级解缠，解决了隐含方面和情感识别的问题，并有效区分了所有情感特征。 |
| [^19] | [Read between the lines -- Functionality Extraction From READMEs](https://arxiv.org/abs/2403.10205) | 本文介绍了一种新颖的文本处理任务——从 Git README 文件中提取功能，研究动机源自对大型语言模型在代码相关任务中应用的兴趣，通过开发小型微调模型，取得了70%和20%的性能提升。 |
| [^20] | [Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based Deception](https://arxiv.org/abs/2403.10185) | 论文提出了DeFaBel语料库，这是一个基于信仰的欺骗的众包资源，用于研究欺骗与事实性之间的关系，并强调了论证中事实性、个人信念和欺骗意图之间的重要性。 |
| [^21] | [NLP Verification: Towards a General Methodology for Certifying Robustness](https://arxiv.org/abs/2403.10144) | 本文尝试总结和评估由该领域迄今进展而形成的NLP验证流程的一般组成部分，贡献在于提出了将句子嵌入连续空间得到的可验证子空间的一般描述。 |
| [^22] | [The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation](https://arxiv.org/abs/2403.10135) | 探索在顺序推荐中使用上下文学习的方法，提出了一种聚合演示的新颖方法LLMSRec-Syn，在多个数据集上实验证明其优于现有基于LLM的方法。 |
| [^23] | [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) | 提出了一种名为RAFT的训练方法，通过引用相关文档中能够帮助回答问题的正确序列来改善模型在特定领域中回答问题的能力。 |
| [^24] | [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088) | 该研究引入了CoARL框架，通过模拟社会偏见中的语用启示来增强对抗性言论生成，利用顺序多指导调节和强化学习生成意图调节的对抗性言论。 |
| [^25] | [DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models](https://arxiv.org/abs/2403.10081) | 提出了一种新框架DRAGIN，旨在解决大型语言模型在文本生成过程中动态检索和生成中存在的问题。 |
| [^26] | [Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis](https://arxiv.org/abs/2403.10065) | 本文引入了Triple GNNs网络来增强对话基于方面的四重情感分析，通过使用GCN来建模话语内的句法依赖关系和DualGATs来构建话语之间的交互。 |
| [^27] | [Repoformer: Selective Retrieval for Repository-Level Code Completion](https://arxiv.org/abs/2403.10059) | 本文提出了一种选择性的检索增强生成框架，通过自监督学习方法使代码LM能够避免不必要的检索，并在各种基准测试上始终优于现有方法。 |
| [^28] | [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056) | 提出了一种基于关键部分信息增益的新型连续指导调整方法，通过动态重放数据和优化训练目标，使LLMs能够捕捉任务感知信息和减轻过度拟合。 |
| [^29] | [Lost in Overlap: Exploring Watermark Collision in LLMs](https://arxiv.org/abs/2403.10020) | 本研究探讨了在大型语言模型中关于水印冲突的问题，发现双水印冲突存在时会对水印算法的检测性能造成威胁。 |
| [^30] | [Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques](https://arxiv.org/abs/2403.09997) | 该论文调查了利用自然语言处理技术从数字健康记录中识别家族性疾病风险的文献，强调了基于规则的方法以及最近更注重构建基于神经网络的模型。 |
| [^31] | [GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery](https://arxiv.org/abs/2403.09974) | 本文提出了一种文本嵌入合成器（TES），用于为无标签数据生成伪文本嵌入，以解锁CLIP用于广义类别发现任务中的多模态潜力。 |
| [^32] | [Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers](https://arxiv.org/abs/2403.09972) | 提出了一种新的评估大型语言模型置信度的方法，通过反思和提供多个候选答案的理由来解决对不正确答案的过度自信问题 |
| [^33] | [Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction](https://arxiv.org/abs/2403.09963) | 本文调查了预训练语言模型在事实知识提取中存在的“提示偏见”，找到了不同类型提示的偏见程度，以及这种偏见对不同基准测试的影响，并提出了一种基于表示的方法来减轻这种提示偏见。 |
| [^34] | [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](https://arxiv.org/abs/2403.09919) | 本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。 |
| [^35] | [Geographically-Informed Language Identification](https://arxiv.org/abs/2403.09892) | 该论文提出了一种地理信息语言识别方法，在地理原点的基础上设置了16个区域性模型，显著提高了语言识别的准确性并对实际数据文集产生了显著影响。 |
| [^36] | [Fisher Mask Nodes for Language Model Merging](https://arxiv.org/abs/2403.09891) | 介绍了一种用于Transformers的新型模型合并方法，利用Fisher信息进行加权平均，提高了多任务模型的性能。 |
| [^37] | [Sabi\'a-2: A New Generation of Portuguese Large Language Models](https://arxiv.org/abs/2403.09887) | Sabi'a-2是一代新的葡萄牙大型语言模型，其中的Sabi'a-2 Medium模型在多个考试中的表现超越了GPT-4，且在大多数考试中超过了GPT-3.5，同时专业化对模型的性能有显著影响，可在无需增大模型尺寸的情况下以比GPT-4便宜10倍的价格提供。 |
| [^38] | [FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections](https://arxiv.org/abs/2403.09858) | FakeWatch框架是为了检测假新闻而设计的，整合了传统机器学习技术和前沿语言模型，在北美选举相关新闻数据集上表现出较高的分类准确性。 |
| [^39] | [Self-Consistency Boosts Calibration for Math Reasoning](https://arxiv.org/abs/2403.09849) | 基于自洽性的校准方法在数学推理任务中能够更好地建立模型信心和准确性之间的关联。 |
| [^40] | [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832) | 该研究探讨了在机器翻译任务上对多个家族的大型语言模型进行的提示注入攻击的影响，发现在某些条件下，更大的模型可能更容易受到成功攻击。 |
| [^41] | [Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention](https://arxiv.org/abs/2403.09795) | 研究人员探索大型语言模型在在线诱拐预防中的有效性，发现没有模型明显适用于此目的，存在潜在的有害答案生成。 |
| [^42] | [Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2403.09792) | 论文研究了多模态大语言模型对齐问题，揭示了图像输入对模型的漏洞，并提出了一种利用图像隐藏恶意意图、成功破解现有模型的方法。 |
| [^43] | [Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts](https://arxiv.org/abs/2403.09762) | 本文系统考察了人工智能在医疗文本中情感分析方面的应用，展示了算法精度、神经退行性疾病预测以及临床决策支持方面的显著进展。 |
| [^44] | [What Was Your Prompt? A Remote Keylogging Attack on AI Assistants](https://arxiv.org/abs/2403.09751) | 本文揭示了一种可以用于读取AI助手加密响应的新型旁路攻击——令牌长度旁路，并展示了如何通过利用大型语言模型，提供句子间上下文并进行已知明文攻击来克服这一挑战。 |
| [^45] | [Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models](https://arxiv.org/abs/2403.09750) | 通过广泛实验探索了LLMs中的陈述性知识和程序性知识对各种任务的影响，发现陈述性知识在大多数任务中的益处大于程序性知识，在简单逻辑推理任务中反之；随着预训练和规模的增加，模型利用两种知识的能力均显著提高。 |
| [^46] | [Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors](https://arxiv.org/abs/2403.09747) | 大型语言模型在假新闻检测中引入新的前沿，但仍需克服解决过时知识和低质量证据检索等问题 |
| [^47] | [Evaluating the Application of Large Language Models to Generate Feedback in Programming Education](https://arxiv.org/abs/2403.09744) | 评估了大型语言模型在编程教育中生成反馈的应用，结果显示大多数反馈有效地解决了代码错误，但存在不正确建议和虚构问题，需要进一步改进。 |
| [^48] | [The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions](https://arxiv.org/abs/2403.09743) | 人工智能领域中，这项研究探索了人类因素在检测大型语言模型的错误输出中的作用，有助于减轻其在专业环境中使用时所带来的风险。 |
| [^49] | [Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation](https://arxiv.org/abs/2403.09738) | 大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。 |
| [^50] | [Do Large Language Models Solve ARC Visual Analogies Like People Do?](https://arxiv.org/abs/2403.09734) | 该研究比较了人类和大型语言模型在ARC视觉类比问题上的表现，发现在特定任务上，人类和成年人的表现均优于大多数大型语言模型。对LLMs和年幼儿童错误分析揭示了类似的解决策略，同时指出了两种不同的错误类型，为我们理解LLMs如何解决视觉类比问题提供了新的启示。 |
| [^51] | [OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models](https://arxiv.org/abs/2403.09733) | OverleafCopilot是第一个无缝集成大型语言模型和Overleaf的工具，使研究人员能够在撰写论文时利用大型语言模型的能力。 |
| [^52] | [PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency](https://arxiv.org/abs/2403.09732) | 提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。 |
| [^53] | [Simulating Weighted Automata over Sequences and Trees with Transformers](https://arxiv.org/abs/2403.09728) | 变压器可以模拟加权有限自动机和加权树自动机，拓展了它们的应用范围。 |
| [^54] | [Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems](https://arxiv.org/abs/2403.09727) | RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems. |
| [^55] | [RAD-PHI2: Instruction Tuning PHI-2 for Radiology](https://arxiv.org/abs/2403.09725) | 本研究调查了将小语言模型用于放射学领域的应用，通过微调具有27亿参数的Phi-2，提出了具有良好性能的RadPhi-2-Base语言模型。 |
| [^56] | [ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs](https://arxiv.org/abs/2403.09724) | ClaimVer是一个人为中心的框架，通过知识图谱实现可解释的声明级验证和证据归因，致力于提高用户对文本验证方法的信任并强调细粒度证据的重要性。 |
| [^57] | [Prediction of readmission of patients by extracting biomedical concepts from clinical texts](https://arxiv.org/abs/2403.09722) | 从临床文本中提取生物医学概念预测患者的再入院情况，可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。 |
| [^58] | [A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction](https://arxiv.org/abs/2403.09721) | 提出了一种语义提及图增强模型（GAM），通过构建语义提及图和引入集成图变换器模块，有效解决了文档级事件论元抽取中的独立建模实体提及和文档提示隔离问题。 |
| [^59] | [Fine-tuning vs Prompting, Can Language Models Understand Human Values?](https://arxiv.org/abs/2403.09720) | 通过对Human Value Detection 2023任务中的微调和提示调整的探索，验证语言模型是否能理解人类价值观。 |
| [^60] | [Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language](https://arxiv.org/abs/2403.09719) | 介绍了用于希伯来语言的结论提取模型和数据集，包括摘要和结论提取数据集，并提供了相应模型和代码 |
| [^61] | [Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation](https://arxiv.org/abs/2403.09718) | TextCNN的全面实现提升了自然语言处理与系统推荐之间的协作，为NLP领域的文本分类任务带来了新的标准技术。 |
| [^62] | [Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking](https://arxiv.org/abs/2403.09717) | 本文提出将心理状态跟踪集成到大型语言模型中，以明确引导抑郁症诊断导向的聊天，从而能更好地捕捉患者在对话过程中的信息、情绪或症状变化。 |
| [^63] | [Textual analysis of End User License Agreement for red-flagging potentially malicious software](https://arxiv.org/abs/2403.09715) | 该论文提出了一种通过文本分析用户许可协议，识别潜在恶意软件的方法，并使用监督分类器对其进行分类，为解决EULA过长且难以理解的问题提供了一种解决方案。 |
| [^64] | [Linguistic Structure Induction from Language Models](https://arxiv.org/abs/2403.09714) | 该论文研究从语言模型中以非监督方式生成句法结构的方法，其中介绍了一种利用数值表示短语树的新方法。 |
| [^65] | [A Hybrid Intelligence Method for Argument Mining](https://arxiv.org/abs/2403.09713) | 提出了一种混合(人类+AI)方法HyEnA，用于从意见文本中提取论点，结合了自动化处理速度和人类理解推理能力，在公民反馈语料库上取得了更高的覆盖率和准确率。 |
| [^66] | [A Knowledge-Injected Curriculum Pretraining Framework for Question Answering](https://arxiv.org/abs/2403.09712) | 提出了一种通用的知识注入课程预训练框架（KICP），用于实现全面的知识图谱学习和利用以解决KBQA任务。 |
| [^67] | [Exploratory Data Analysis on Code-mixed Misogynistic Comments](https://arxiv.org/abs/2403.09709) | 本研究提出了一个新颖的混合代码Hinglish评论数据集，通过预处理和探索性数据分析技术得出在欠资源语言中检测女性憎恨评论的见解。 |
| [^68] | [Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs Using a Novel Natural Language Processing Algorithmic Pipeline](https://arxiv.org/abs/2403.09708) | 通过新型自然语言处理算法管道，研究对108,280份临床记录进行分析，发现ICIs治疗患者中IrAEs的发生情况，并进行了治疗中断率和生存曲线的构建。 |
| [^69] | [Schema-Aware Multi-Task Learning for Complex Text-to-SQL](https://arxiv.org/abs/2403.09706) | 提出了一个适用于复杂SQL查询的schema-aware多任务学习框架，通过设计schema链接鉴别器和定义6种关系类型来解决文本到SQL解析中的挑战。 |
| [^70] | [A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health](https://arxiv.org/abs/2403.09705) | 提出了一种用于评估大型语言模型在心理健康领域微妙对话能力的新框架，并展示GPT4 Turbo可以与已验证的治疗师表现出更相似的结果。 |
| [^71] | [Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations](https://arxiv.org/abs/2403.09704) | 本文提出了 Alignment Studio 架构，使应用开发者能够调整大型语言模型至他们特定的价值观、社会规范、法律和其他法规，并协调潜在冲突的需求。 |
| [^72] | [Concept-aware Data Construction Improves In-context Learning of Language Models](https://arxiv.org/abs/2403.09703) | 该研究提出了概念感知训练（CoAT）框架，用于构建训练场景，让语言模型从演示中学习利用类比推理概念，并发现通过使用CoAT，预训练的transformers可以更好地利用演示中的新潜在概念，使得上下文学习对函数变换更加 robust。 |
| [^73] | [Generator-Guided Crowd Reaction Assessment](https://arxiv.org/abs/2403.09702) | 本文提出了一种生成器引导的群体反应评估任务，利用生成式大型语言模型引导分类模型做出更好的预测，结果显示经微调的 FLANG-RoBERTa 模型表现最佳。 |
| [^74] | [Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models](https://arxiv.org/abs/2403.09676) | 该研究探讨了大型语言模型的欺骗行为并分类讨论其引发的社会影响和风险。 |
| [^75] | [Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation](https://arxiv.org/abs/2403.09674) | 本研究展示了AI（ChatGPT-4）如何轻松制造令人信服但完全虚构的科学数据，以制造出一个完全虚构的医学案例来警示医学误信息的危害。 |
| [^76] | [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539) | 大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能 |
| [^77] | [DevBench: A Comprehensive Benchmark for Software Development](https://arxiv.org/abs/2403.08604) | DevBench是一个综合基准测试，评估大型语言模型在软件开发生命周期各个阶段的表现，并发现现有的模型在其中存在挑战。 |
| [^78] | [Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models](https://arxiv.org/abs/2403.08281) | 通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。 |
| [^79] | [Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions](https://arxiv.org/abs/2403.07910) | MAGPIE是第一个为媒体偏见检测定制的大规模多任务预训练方法，在媒体偏见检测方面表现优异，并且相对于单一任务方法需要更少的微调步骤。 |
| [^80] | [Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations](https://arxiv.org/abs/2403.07769) | 文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。 |
| [^81] | [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://arxiv.org/abs/2403.07378) | SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。 |
| [^82] | [Multilingual Turn-taking Prediction Using Voice Activity Projection](https://arxiv.org/abs/2403.06487) | 本文研究了在口头对话中使用语音活动投影进行多语言交替预测，在多语言数据上训练的多语言模型展示出与单一语言模型相当的预测性能，并且学会了辨别输入信号的语言。 |
| [^83] | [CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean](https://arxiv.org/abs/2403.06412) | CLIcK介绍了一个包含1,995个问答对的韩国文化和语言智慧基准数据集，为填补韩语基准数据缺失的问题而来。 |
| [^84] | [A Comprehensive Overhaul of Multimodal Assistant with Small Language Models](https://arxiv.org/abs/2403.06199) | 通过设计多模态小语言模型(MSLMs)及提出高效多模态助手Mipha，实现了在多个方面的协同作用，击败了大语言模型，为开发强大MSLMs提供了见解和指南 |
| [^85] | [SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis](https://arxiv.org/abs/2403.01976) | SciAssess介绍了一个专为深度分析科学文献而设计的基准测试，旨在全面评估LLMs在科学领域记忆、理解和分析能力的有效性。 |
| [^86] | [VIXEN: Visual Text Comparison Network for Image Difference Captioning](https://arxiv.org/abs/2402.19119) | 提出了一种名为VIXEN的技术，能够用文本简洁地总结一对图像之间的视觉差异，为突出内容操作提供潜在的缓解方法 |
| [^87] | [LLM Inference Unveiled: Survey and Roofline Model Insights](https://arxiv.org/abs/2402.16363) | 本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。 |
| [^88] | [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302) | 本研究探讨了大型语言模型（LLMs）对指令中心响应的容忍度，并提出了一个包含复杂查询的数据集，旨在揭示触发不道德响应的方法。 |
| [^89] | [CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models](https://arxiv.org/abs/2402.13607) | 介绍了CODIS基准，用于评估模型利用自由形式文本提供的上下文来增强视觉理解的能力，发现多模态大型语言模型在此基准上表现未达到人类水平，需要提升模型理解视觉能力。 |
| [^90] | [Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales](https://arxiv.org/abs/2402.10948) | 该方法结合心理量表通过LLMs进行零-shot心理健康分析，实验结果表明其优于其他方法 |
| [^91] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^92] | [Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends](https://arxiv.org/abs/2401.07518) | 这篇论文调查了教育领域自然语言处理的最新进展，提出了分类体系，并总结了挑战和未来研究方向。 |
| [^93] | [MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following](https://arxiv.org/abs/2312.02436) | MUFFIN是一个新的指示遵循数据集策划方案，通过自动按比例扩大任务，通过多种输入方面使任务丰富多样。 |
| [^94] | [zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2311.10112) | 本文提出了一种在时间知识图上进行零样本关系学习的方法，该方法利用大型语言模型(LLM)生成关系表示，并将其引入基于嵌入的TKGF方法中，能够捕捉关系描述中的语义信息，从而使得关系在建模时能具有相似的语义含义。 |
| [^95] | [Exploring the Potential of Large Language Models in Computational Argumentation](https://arxiv.org/abs/2311.09022) | 该研究旨在评估大型语言模型在计算辩论领域的性能，包括零样本和少样本设置，标准化了14个开源数据集，并介绍了一个新的反言生成基准数据集。 |
| [^96] | [Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification](https://arxiv.org/abs/2311.07593) | 提出了一种零样本方法Follow-up Differential Descriptions（FuDD），通过为每个图像确定模糊类，并使用大型语言模型生成新的类描述，以更好地区分目标类。 |
| [^97] | [Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue](https://arxiv.org/abs/2311.07445) | 通过内在思考，本研究通过语言学和认知科学的灵感，赋予大型语言模型沟通技能，从而提高其拟人化和主动性，吸引用户进行更长时间的对话 |
| [^98] | [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791) | LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。 |
| [^99] | [XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners](https://arxiv.org/abs/2310.05502) | XAL提出了一种可解释的主动学习框架，鼓励分类器提供推断的理由并深入未标记数据，从而提升低资源文本分类的性能 |
| [^100] | [Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://arxiv.org/abs/2308.07317) | Platypus是一组经过精细调节和合并的大型语言模型（LLMs），在全球开放LLM排行榜上表现最出色，在微调数据和计算量上仅需其他方法的一小部分。 |
| [^101] | [Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models](https://arxiv.org/abs/2308.03449) | K-prune 是一种针对预训练编码器型语言模型的精确无需重训练的结构化剪枝算法，通过迭代剪枝过程保留有用知识以最小化剪枝错误，显著提升精确度。 |
| [^102] | [Continuous QA Learning with Structured Prompts](https://arxiv.org/abs/2208.14602) | 提出了一种名为Diana的动态架构终身QA模型，通过增强语言模型学习一系列QA任务，并使用四种层次组织的提示来捕获不同粒度的QA知识，以提高模型的泛化性能。 |
| [^103] | [Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing](https://arxiv.org/abs/2204.09601) | 通过自然语言处理从临床记录中提取睡眠信息，为研究睡眠与阿尔茨海默病发病关联提供了新的方法和工具 |
| [^104] | [Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech](https://arxiv.org/abs/2109.10952) | 该论文提出了一种跨语言一致的儿童语言语义和句法注释方法，利用通用依存关系方案和自动方法转换句子逻辑形式，为构建CDS语料库提供了新思路。 |
| [^105] | [Energy-based Automated Model Evaluation.](http://arxiv.org/abs/2401.12689) | 提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。 |
| [^106] | [BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining.](http://arxiv.org/abs/2401.06443) | 本研究提出了BOK-VQA数据集，包含多语言的视觉问答数据以及与问题-回答内容相关的知识信息。通过以图嵌入的形式预训练数据的知识信息，可以有效地将外部知识注入VQA系统中，实现更好的问答效果。 |
| [^107] | [Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias.](http://arxiv.org/abs/2401.01989) | 这项研究通过测量位置偏见，重访了大语言模型中的零-shot 抽象摘要。研究结果揭示了模型不公平地优先考虑某些部分的信息，从而导致不可取的行为。对多个LLM模型和预训练抽象摘要模型进行的实验提供了关于零-shot 总结任务的模型性能和位置偏见的新见解和讨论。 |
| [^108] | [Debiasing Algorithm through Model Adaptation.](http://arxiv.org/abs/2310.18913) | 本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。 |
| [^109] | [Compositional preference models for aligning LMs.](http://arxiv.org/abs/2310.13011) | 用于对齐语言模型的组合偏好模型（CPMs）是一种新颖的偏好模型框架，可以分解全局偏好评估并根据可解释的特征进行标量评分，得到更好的泛化能力和鲁棒性。 |
| [^110] | [Kosmos-G: Generating Images in Context with Multimodal Large Language Models.](http://arxiv.org/abs/2310.02992) | 本文介绍了Kosmos-G，一种利用多模态大型语言模型（MLLM）在上下文中生成图像的模型。该模型通过使用文本模态作为锚点，将MLLM的输出空间与CLIP对齐，并进行组合指令调整。Kosmos-G展示了零样本多实体主题驱动生成的独特能力。 |
| [^111] | [Enable Language Models to Implicitly Learn Self-Improvement From Data.](http://arxiv.org/abs/2310.00898) | 该论文探索了如何让语言模型隐式学习自我改进，并减少对人类标注的依赖。 |
| [^112] | [JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention.](http://arxiv.org/abs/2310.00535) | 本文提出了联合MLP/注意力（JoMA）动态，用于解析多层Transformer架构的训练过程。通过预测非线性激活情况下注意力的行为，我们解释了多层Transformer中标记的层次组合方法。实验证实了我们的理论发现。 |
| [^113] | [XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates.](http://arxiv.org/abs/2309.11063) | XATU是第一个细粒度基于指令的可解释性文本编辑基准测试，涵盖广泛的编辑类型，并通过引入细粒度指令和黄金标准编辑说明来提高可解释性。 |
| [^114] | [Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations.](http://arxiv.org/abs/2309.04849) | 该论文提出了EmoDistill，这是一个利用知识蒸馏来学习从语音中获取情感的强大的语言和语音表示的语音情感识别框架。通过在训练过程中利用经过SER微调的预训练语音和语言教师进行信息蒸馏，该方法在IEMOCAP基准测试中实现了最新的最高准确率，表明其在单模态和多模态技术中的优越性能。 |
| [^115] | [Cognitive Architectures for Language Agents.](http://arxiv.org/abs/2309.02427) | 本文提出了一种称为CoALA的认知架构，用于组织语言代理的现有研究并规划未来的发展方向。CoALA描述了一个具有模块化记忆组件、结构化行动空间和通用决策过程的语言代理。通过这一框架，有望发展出更强大的语言代理。 |
| [^116] | [Voting-based Multimodal Automatic Deception Detection.](http://arxiv.org/abs/2307.07516) | 本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。 |
| [^117] | [Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts.](http://arxiv.org/abs/2305.07019) | Musketeer是一种通用视觉语言模型，采用任务解释提示（TEP）机制，能够有效整合异构任务的知识，并在多个任务中表现均匀 |
| [^118] | [Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models.](http://arxiv.org/abs/2304.11657) | 本文提出 Iter-CoT 方法，在大型语言模型中进行迭代增强的思维链提示，通过选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，提高了模型的泛化能力，同时使模型能够更准确地生成推理链。 |
| [^119] | [WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset.](http://arxiv.org/abs/2303.17876) | WebQAmGaze是一个多语言低成本的阅读时眼动追踪数据集，包括332位参与者的数据，对相关段落的注视似乎能够反映回答理解问题的准确性。这份数据可以推动基于网络摄像头的阅读研究并开辟更便宜、更易获得的数据收集方式。 |
| [^120] | [Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques.](http://arxiv.org/abs/2303.13466) | 本文提出了一种基于规则的自然语言处理算法，用于从临床笔记中提取卒中患者治疗过程的锻炼信息，并与几个小型机器学习模型进行比较。在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。这些算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。 |
| [^121] | [Learning to Generate Questions by Enhancing Text Generation with Sentence Selection.](http://arxiv.org/abs/2212.12192) | 本研究提出了一种通过加强句子选择来增强文本生成的学习生成问题方法，该方法通过设计选择器和生成器两个模块，使模型更关注与答案相关的句子，并隐式结合局部信息和全局信息来生成问题。实验结果表明该方法在问题生成任务上优于强大的预训练模型。 |

# 详细

[^1]: 基于大型语言模型的视频代理：长视频理解

    VideoAgent: Long-form Video Understanding with Large Language Model as Agent

    [https://arxiv.org/abs/2403.10517](https://arxiv.org/abs/2403.10517)

    提出了一种新颖的基于代理的系统VideoAgent，利用大型语言模型作为中央代理，采用互动推理和计划来处理长视频理解问题，在挑战性基准测试中表现出卓越的效果和效率。

    

    长视频理解在计算机视觉中代表着一个重大挑战，需要一个能够推理长时间多模态序列的模型。受人类认知长视频过程的启发，我们强调互动推理和计划，而不是处理长篇视觉输入的能力。我们引入了一个新颖的基于代理的系统VideoAgent，它采用大型语言模型作为中央代理，迭代地识别和整理关键信息以回答问题，视觉语言基础模型作为工具来翻译和检索视觉信息。在具有挑战性的EgoSchema和NExT-QA基准测试中，VideoAgent在平均仅使用8.4和8.2帧的情况下分别实现了54.1%和71.3%的零-shot准确率。这些结果展示了我们方法相对于当前最先进方法的卓越效果和效率，突出了代理模型的潜力。

    arXiv:2403.10517v1 Announce Type: cross  Abstract: Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-base
    
[^2]: 基于多模态基础模型的零样本鲁棒性基准测试：一项试点研究

    Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study

    [https://arxiv.org/abs/2403.10499](https://arxiv.org/abs/2403.10499)

    本研究通过对多模态基础模型CLIP进行大规模鲁棒性基准测试，揭示了其在涵盖自然分布偏移、合成分布偏移和对抗攻击等多个方面的优异表现。

    

    通过从关于图像的原始文本中预训练图像表示，使得零样本视觉传输至下游任务成为可能。通过在互联网上采集的数百万样本上进行预训练，如CLIP之类的多模态基础模型产生了最先进的零样本结果，通常在无需任务特定训练的情况下达到与完全监督方法竞争力相当的水平。除了在分类准确性上表现鼓舞人心之外，报道称这些模型通过在自然分布偏移下与在ImageNet上训练的监督模型的表现相匹配来缩小鲁棒性差距。由于鲁棒性对于现实世界的应用至关重要，特别是对于安全关键的应用，本文提出了基于涵盖7种自然、3种合成分布偏移和11种对抗攻击的大规模鲁棒性基准测试的全面评估。我们以CLIP作为试点研究。我们展示了CLIP导致了显著

    arXiv:2403.10499v1 Announce Type: cross  Abstract: Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a signif
    
[^3]: 利用RAG增强LLM事实准确性以消除幻觉：私人知识库中特定领域查询的案例研究

    Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases

    [https://arxiv.org/abs/2403.10446](https://arxiv.org/abs/2403.10446)

    利用RAG系统增强LLMs，提高其对私人知识库中特定领域和时效查询的事实准确性，尤其在使用外部数据集方面表现出潜力。

    

    我们提出了一个端到端系统设计，利用检索增强生成（RAG）来提高大型语言模型（LLMs）对私人知识库中与特定领域和时效查询相关的事实准确性。我们的系统将RAG管道与上游数据集处理和下游性能评估整合在一起。解决LLM幻觉的挑战，我们用源自CMU广泛资源并用教师模型注释的筛选数据集对模型进行微调。我们的实验表明该系统在生成更准确的特定领域和时效查询答案方面的有效性。结果还揭示了使用小规模和倾斜数据集微调LLMs的局限性。这项研究突显了RAG系统在增强LLMs与外部数据集以改进知识密集型任务性能方面的潜力。我们的代码和模型可供使用。

    arXiv:2403.10446v1 Announce Type: new  Abstract: We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available 
    
[^4]: 用于加速推测解码的最佳块级草稿验证

    Optimal Block-Level Draft Verification for Accelerating Speculative Decoding

    [https://arxiv.org/abs/2403.10444](https://arxiv.org/abs/2403.10444)

    提出了一种更好的草稿验证算法，通过将验证步骤制定为块级最优传输问题，实现了额外的墙钟速度提升，而不增加额外的计算成本和草稿标记

    

    推测解码已被证明是在推理过程中加速大型语言模型（LLMs）无损加速的有效方法。 在每次迭代中，算法首先使用一个较小的模型起草一块标记。这些标记然后由大型模型并行验证，只有一部分标记将被保留，以确保最终输出遵循大型模型的分布。 在以往的所有推测解码工作中，起草验证是独立地逐个标记执行的。 在本工作中，我们提出了一个更好的起草验证算法，可提供额外的墙钟加速，而不需要额外的计算成本和起草标记。 我们首先将起草验证步骤制定为一个块级最优传输问题。 块级制定允许我们考虑更广泛的起草验证算法，并在一个起草中预期获得更多接受的标记数量

    arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
    
[^5]: 语言模型中数值属性的单调表示

    Monotonic Representation of Numeric Properties in Language Models

    [https://arxiv.org/abs/2403.10381](https://arxiv.org/abs/2403.10381)

    介绍了一种在语言模型中找到和编辑数值属性表示的简单方法，发现编码数值属性的低维子空间以单调、可解释和可编辑的方式存在，通过编辑这些子空间中的方向，使语言模型的输出相应地发生变化。

    

    语言模型（LMs）可以表达涉及数字属性的事实知识，例如Karl Popper出生在1902年。然而，这些信息如何被编码在模型的内部表示中尚不太为人所了解。在这里，我们介绍了一种简单方法，用于查找和编辑数值属性的表示，例如某个实体的出生年份。在经验上，我们发现编码数值属性的低维子空间单调地、可解释地和可编辑地。当沿着这些子空间中的方向编辑表示时，LM的输出相应地发生变化。例如，通过沿着“birthyear”方向修补激活，我们可以使LM表达越来越晚的出生年份：卡尔·波普尔生于1929年，卡尔·波普尔生于1957年，卡尔·波普尔生于1968年。在考虑的所有模型中，存在跨多个数字属性的属性编码方向，这表明了单调性表示的可能性。

    arXiv:2403.10381v1 Announce Type: new  Abstract: Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year. Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion. When editing representations along directions in these subspaces, LM output changes accordingly. For example, by patching activations along a "birthyear" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic repr
    
[^6]: EXAMS-V: 用于评估视觉语言模型的跨学科多语言多模态考试基准

    EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models

    [https://arxiv.org/abs/2403.10378](https://arxiv.org/abs/2403.10378)

    EXAMS-V是一个独特的、跨学科多语言多模态考试基准，采用多种语言和多种教育系统的考试题目，需要进行复杂的推理并依赖于区域特定知识。

    

    我们介绍了EXAMS-V，这是一个新的具有挑战性的跨学科多模态多语言考试基准，用于评估视觉语言模型。它包括20932道涵盖自然科学、社会科学以及其他各种研究领域（如宗教、美术、商业等）的多项选择题。EXAMS-V包含各种多模态特征，如文本、图像、表格、图表、图示、地图、科学符号和方程式。问题涵盖了来自7个语系的11种语言。与现有基准不同，EXAMS-V通过收集来自各个国家、具有多种教育系统的学校考试题目来独特策划。这种独特的方法需要在不同语言之间进行复杂推理，并依赖于特定地区的知识。解决数据集中的问题需要对图像的文本和视觉内容进行高级感知和联合推理。

    arXiv:2403.10378v1 Announce Type: new  Abstract: We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluat
    
[^7]: TriSum: 从大型语言模型中学习总结能力与结构化理由

    TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale

    [https://arxiv.org/abs/2403.10351](https://arxiv.org/abs/2403.10351)

    TriSum是一个框架，通过将大型语言模型的文本总结能力提炼到一个紧凑的本地模型中，从而在各种基准测试中提高了性能，并提高了可解释性。

    

    大型语言模型（LLMs）的出现显著推动了文本总结等自然语言处理任务。然而，它们的庞大大小和计算需求，加上数据传输中的隐私问题，限制了它们在资源受限和隐私为中心的环境中的使用。为了克服这一问题，我们引入了TriSum，一个将LLMs的文本总结能力提炼到一个紧凑的本地模型中的框架。最初，LLMs提取一组方面三元理由和总结，然后使用双评分方法对其进行优化。接下来，使用这些任务训练一个规模较小的本地模型，采用从简单到复杂任务的课程学习策略。我们的方法提升了本地模型在各种基准测试上的性能（CNN/DailyMail，XSum和ClinicalTrial），分别比基线提高了4.5％，8.5％和7.4％。它还通过提供对总结的见解来改善可解释性。

    arXiv:2403.10351v1 Announce Type: new  Abstract: The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summariz
    
[^8]: 使用少样本学习探究语言模型中的语法抽象性：新名词性别的学习

    Investigating grammatical abstraction in language models using few-shot learning of novel noun gender

    [https://arxiv.org/abs/2403.10338](https://arxiv.org/abs/2403.10338)

    语言模型可以通过很少的例子学习新名词的性别，并在不同的一致上下文中应用所学性别，虽然有对阳性性别类别的偏见

    

    人类可以从很少的例子中学习一个新单词，并推断其语法属性。他们对像语法性别和一致规则这样的语言属性具有抽象概念，可以应用到新的句法上下文和词语中。我们从心理语言学中汲取灵感，进行了一个名词学习实验，评估LSTM和仅解码器变压器是否可以实现对法语中的语法性别的类似人类的抽象。语言模型的任务是从几个例子中学习一个新名词嵌入的性别，并在另一个未见上下文中预测一致。我们发现，两种语言模型在一个语法一致上下文中能有效地将新名词的性别概括到一个到两个学习示例，并把所学的性别应用到一致上下文，尽管有对阳性性别类别的偏见。重要的是，少样本更新仅应用于嵌入层，证明了...

    arXiv:2403.10338v1 Announce Type: new  Abstract: Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating 
    
[^9]: 基于预训练语言模型的自动填空干扰项生成

    CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model

    [https://arxiv.org/abs/2403.10326](https://arxiv.org/abs/2403.10326)

    本文研究了通过应用预训练语言模型作为候选干扰项生成的替代方法来自动生成填空干扰项，并展示了这种PLM增强模型显著提高了性能。

    

    手动设计填空测试耗费大量时间和精力。主要挑战在于错误选项（干扰项）的选择。精心设计的干扰项提高了学习者能力评估的有效性。因此，自动生成填空干扰项的想法应运而生。本文通过探索预训练语言模型（PLMs）的应用作为候选干扰项生成的替代方法来研究填空干扰项生成。实验表明，PLM增强模型带来了显著的性能提升。我们的最佳模型将最先进的结果从14.94提升至34.17（NDCG@10分数）。我们的代码和数据集可在https://github.com/AndyChiangSH/CDGP 获取。

    arXiv:2403.10326v1 Announce Type: cross  Abstract: Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.
    
[^10]: Uni-SMART：通用科学多模态分析和研究变换器

    Uni-SMART: Universal Science Multimodal Analysis and Research Transformer

    [https://arxiv.org/abs/2403.10301](https://arxiv.org/abs/2403.10301)

    大型语言模型在总结文本方面的能力为改进科学文献分析提供了新途径，Uni-SMART旨在解决现有LLMs对多模态科学文献内容理解和分析能力的限制。

    

    在科学研究及其应用中，科学文献分析至关重要，因为它使研究人员能够借鉴他人的工作。然而，科学知识的快速增长导致学术文章数量大幅增加，使深入文献分析变得越来越具挑战性和耗时。大型语言模型（LLMs）的出现为解决这一挑战提供了新途径。以其在总结文本方面的强大能力而闻名，LLMs被视为改进科学文献分析的潜在工具。然而，现有的LLMs存在其局限性。科学文献通常包括各种多模态元素，如分子结构、表格和图表，这些对以文本为中心的LLMs而言难以理解和分析。这个问题指向了迫切需要新的解决方案，能够充分理解和分析科学文献中的多模态内容。为了应对这一需求...

    arXiv:2403.10301v1 Announce Type: new  Abstract: In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this deman
    
[^11]: MaiBaam：多方言巴伐利亚通用依存树库

    MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank

    [https://arxiv.org/abs/2403.10293](https://arxiv.org/abs/2403.10293)

    这项研究提出了第一个多方言巴伐利亚通用依存树库（MaiBaam），手动标注了词类和句法依存信息，覆盖多种文本类型，突出了巴伐利亚话与德语之间的形态句法差异，并展示了说话者正字法的丰富变异性。

    

    尽管Universal Dependencies (UD) 项目取得了成功，其语言广度令人印象深刻，但在“语言内广度”方面仍然存在缺乏：大多数树库专注于标准语言。即使对于德语这样在UD中标注量最大的语言，迄今为止还没有针对其语言变体之一的树库存在，这种语言变体是巴伐利亚话，使用人口超过1000万。为了填补这一缺口，我们提出了第一个多方言巴伐利亚树库（MaiBaam），在UD中手动标注了词类和句法依存信息，覆盖多种文本类型（维基、小说、语法示例、社交、非小说）。我们突出了巴伐利亚话与德语之间的形态句法差异，并展示了说话者正字法的丰富变异性。我们的语料库包括15k个标记，覆盖了来自涵盖三个国家的所有说巴伐利亚话的地区的方言。我们提供了基准解析和词类标记结果。

    arXiv:2403.10293v1 Announce Type: new  Abstract: Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in `within-language breadth': most treebanks focus on standard languages. Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian. To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction). We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers' orthographies. Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries. We provide baseline parsing and POS tagging results, 
    
[^12]: Team Trifecta在Factify 5WQA上设定了细化调整中事实验证的标准

    Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning

    [https://arxiv.org/abs/2403.10281](https://arxiv.org/abs/2403.10281)

    Team Trifecta在Factify 5WQA上以Fine-Tuning取得了首要地位，成功超越基准准确率103％，并保持了对第二名竞争者的70%领先优势。

    

    在本文中，我们介绍了Pre-CoFactv3，这是一个由问答和文本分类组件组成的全面框架，用于事实验证。通过利用上下文学习、微调大型语言模型（LLMs）和FakeNet模型，我们解决了事实验证面临的挑战。我们的实验探讨了不同的方法，比较了不同的预训练LLMs，引入了FakeNet，并实施了各种集成方法。值得注意的是，我们的团队Trifecta在AAAI-24 Factify 3.0研讨会上获得了第一名，比基准准确率高出103%，并保持了对第二名竞争对手的70%领先优势。这一成功突显了我们方法的有效性及其对推进事实验证研究的潜在贡献。

    arXiv:2403.10281v1 Announce Type: cross  Abstract: In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.
    
[^13]: 关于大型语言模型的可解释性问题和基于词级单变量一阶概率假设的研究

    A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption

    [https://arxiv.org/abs/2403.10275](https://arxiv.org/abs/2403.10275)

    本文提出了一个方法来挑战为大型语言模型提供简单而丰富解释的可能性，研究发现使用基于特征的模型在信号传递方面效果更好。

    

    最近研究表明，大型语言模型的解释对其训练中使用的随机性很敏感，因此需要对这种敏感性进行表征。本文提出了一个挑战为这些模型提供简单和信息丰富解释的表征方法。为此，我们为解释的信号、噪声和信噪比给出了统计定义。我们强调，在一个典型案例研究中，使用一阶统计工具分析基于单一特征的模型解释时，简单特征模型的解释传递更多信号并且噪声更少。然后，我们讨论了通过替代信号和噪声的定义来改进这些结果的可能性，这种方法可以捕捉更复杂的解释和分析方法，同时也质疑了与读者可信度之间的权衡。

    arXiv:2403.10275v1 Announce Type: cross  Abstract: The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.
    
[^14]: 翻译到底是你所需要的全部吗？使用大型语言模型解决多语言任务的研究

    Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models

    [https://arxiv.org/abs/2403.10258](https://arxiv.org/abs/2403.10258)

    提出了通过本地语言提示来解决文化相关任务的方法，并呼吁发展强大的多语言LLMs。

    

    大型语言模型（LLMs）展示了强大的多语言能力；然而，由于训练语料库不平衡，它们大多是以英语为中心的。现有研究利用这一现象来提高它们在自然语言处理任务上的多语言性能。在本研究中，我们将评估从自然语言处理任务扩展到真实用户查询。我们发现，尽管将文本翻译成英语可以帮助提高以英语为中心的LLMs在多语言自然语言处理任务中的性能，但并不一定适用于所有场景。对于需要深入理解语言的文化相关任务，以本地语言提示更为有前景，因为它可以捕捉与文化和语言相关的微妙之处。因此，我们主张着力发展强大的多语言LLMs，而不仅仅是以英语为中心的LLMs。

    arXiv:2403.10258v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.
    
[^15]: 一种理解非洲外资直接投资次国家决定因素的大数据方法

    A Big Data Approach to Understand Sub-national Determinants of FDI in Africa

    [https://arxiv.org/abs/2403.10239](https://arxiv.org/abs/2403.10239)

    本文提出了一种基于文本挖掘和社交网络分析的新方法，通过分析超过167,000篇在线新闻文章，量化了影响非洲公司外资直接投资所有权的区域级属性，结果表明次国家级的结构和制度特征在外资直接投资中扮演着重要角色。

    

    各种宏观经济和制度因素阻碍了外资直接投资流入，包括腐败、贸易开放性、融资途径和政治不稳定性。现有研究主要集中在国家层面数据，对企业层面数据的探索有限，尤其在发展中国家。鉴于这一空白，最近对研究的呼吁强调了对定性数据分析的需求，特别是在区域层面深入探讨外资直接投资决定因素。本文提出了一种新颖的方法，基于文本挖掘和社交网络分析，从超过167,000篇在线新闻文章获取信息，以量化影响非洲公司外资直接投资所有权的区域级（次国家级）属性。我们的分析扩展了世界银行企业调查所绘制的工业发展障碍信息。研究结果表明，区域级（次国家级）的结构和制度特征在决定外资直接投资方面可以发挥重要作用。

    arXiv:2403.10239v1 Announce Type: new  Abstract: Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based on text mining and social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in det
    
[^16]: 在波斯文本流中对频繁模式挖掘和聚类类别进行主题检测的综合研究

    A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream

    [https://arxiv.org/abs/2403.10237](https://arxiv.org/abs/2403.10237)

    本研究旨在深入研究波斯文主题检测中的最佳算法，确定对这些算法进行适配以适用于波斯语的必要改动，并在波斯社交网络文本上评估它们的性能。

    

    主题检测是一个复杂的过程，取决于语言，因为它需要对文本进行分析。迄今为止，在波斯文主题检测方面的研究很少，现有的算法并不引人注目。因此，我们旨在研究波斯文主题检测。本研究的目标是：1）对主题检测的最佳算法进行广泛研究，2）确定必要的调整，使这些算法适用于波斯语，3）评估它们在波斯社交网络文本上的性能。为实现这些目标，我们提出了两个研究问题：第一，鉴于波斯语研究不足，应对现有框架进行何种修改，特别是那些用英语开发的框架，以使它们与波斯语兼容？第二，这些算法的表现如何，哪种算法更优？有各种主题检测方法可以归为不同类别

    arXiv:2403.10237v1 Announce Type: new  Abstract: Topic detection is a complex process and depends on language because it somehow needs to analyze text. There have been few studies on topic detection in Persian, and the existing algorithms are not remarkable. Therefore, we aimed to study topic detection in Persian. The objectives of this study are: 1) to conduct an extensive study on the best algorithms for topic detection, 2) to identify necessary adaptations to make these algorithms suitable for the Persian language, and 3) to evaluate their performance on Persian social network texts. To achieve these objectives, we have formulated two research questions: First, considering the lack of research in Persian, what modifications should be made to existing frameworks, especially those developed in English, to make them compatible with Persian? Second, how do these algorithms perform, and which one is superior? There are various topic detection methods that can be categorized into differen
    
[^17]: HawkEye: 用于将文本与视频相关联的训练视频文本LLMs

    HawkEye: Training Video-Text LLMs for Grounding Text in Videos

    [https://arxiv.org/abs/2403.10228](https://arxiv.org/abs/2403.10228)

    本文提出了HawkEye，一个可以以完全文本方式执行时间视频定位的视频文本LLMs，并通过构建大规模视频文本语料库InternVid-G以及引入两个新的面向时间的训练目标，以及一种新的粗粒度表示视频段的方法来实现这一目标。

    

    视频文本大型语言模型（video-text LLMs）在回答问题和进行简单视频对话方面表现出色。然而，在长而复杂的视频中，它们在文本查询上的表现几乎与随机相同，几乎没有能力理解和推理关于时间信息的内容，这是视频和图像之间最基本的区别。本文提出了HawkEye，这是第一个可以完全以文本方式执行时间视频定位的视频文本LLMs之一。为了收集适用于时间视频定位的训练数据，我们构建了InternVid-G，一个具有分段级标题和负间距的大规模视频文本语料库，通过该语料库引入了两个新的面向时间的训练目标以供视频文本LLMs使用。我们还提出了一种表示视频中段的粗粒度方法，这种方法比LLMs学习和遵循的方法更稳健且更易学习。

    arXiv:2403.10228v1 Announce Type: cross  Abstract: Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than o
    
[^18]: 具有分层解缠结构的增强相干感知网络用于方面类别情感分析

    Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis

    [https://arxiv.org/abs/2403.10214](https://arxiv.org/abs/2403.10214)

    本文提出了一种具有分层解缠结构的增强相干感知网络（ECAN）用于方面类别情感分析任务，通过探索相干性建模和分级解缠，解决了隐含方面和情感识别的问题，并有效区分了所有情感特征。

    

    方面类别情感分析（ACSA）旨在识别方面类别并预测它们的情感，由于其广泛的NLP应用，已被广泛研究。本文提出了一种具有分层解缠结构的增强相干感知网络（ECAN）用于ACSA任务。具体而言，我们探讨了相干性建模，以捕获整个评论中的上下文，并有助于隐含方面和情感的识别。为了解决多个方面类别和情感交织的问题，我们提出了一种分层...

    arXiv:2403.10214v1 Announce Type: new  Abstract: Aspect-category-based sentiment analysis (ACSA), which aims to identify aspect categories and predict their sentiments has been intensively studied due to its wide range of NLP applications. Most approaches mainly utilize intrasentential features. However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review. Even in a sentence, there is more than one aspect category with its sentiments, and they are entangled intra-sentence, which makes the model fail to discriminately preserve all sentiment characteristics. In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and sentiment identification. To address the issue of multiple aspect categories and sentiment entanglement, we propose a hierar
    
[^19]: 从 README 中提取功能

    Read between the lines -- Functionality Extraction From READMEs

    [https://arxiv.org/abs/2403.10205](https://arxiv.org/abs/2403.10205)

    本文介绍了一种新颖的文本处理任务——从 Git README 文件中提取功能，研究动机源自对大型语言模型在代码相关任务中应用的兴趣，通过开发小型微调模型，取得了70%和20%的性能提升。

    

    虽然文本摘要是一项众所周知的自然语言处理任务，但在本文中，我们介绍了一种称为从 Git README 文件中提取功能的新颖而有用的变体。虽然这个任务在抽象层面上是一个文本生成任务，但它涉及到自己的特殊性和挑战，使得现有的文本生成系统并不十分有用。这一任务的动机源自最近围绕着使用大型语言模型进行代码相关任务（如代码重构、代码摘要等）的研究和开发活动的激增。我们还发布了一个名为FuncRead的人工注释数据集，并为这一任务开发了一系列模型。我们进行了详尽的实验，结果表明，小型微调模型击败了可以使用流行的黑盒或白盒大型语言模型（LLMs）（如ChatGPT和Bard）设计的任何基线模型。我们的最佳微调的70亿CodeLlama模型在F1上取得了70%和20%的增益。

    arXiv:2403.10205v1 Announce Type: cross  Abstract: While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1
    
[^20]: 可以欺骗性地陈述事实吗？基于信仰的欺骗DeFaBel语料库

    Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based Deception

    [https://arxiv.org/abs/2403.10185](https://arxiv.org/abs/2403.10185)

    论文提出了DeFaBel语料库，这是一个基于信仰的欺骗的众包资源，用于研究欺骗与事实性之间的关系，并强调了论证中事实性、个人信念和欺骗意图之间的重要性。

    

    如果一个人坚信一个非事实性的陈述，比如“地球是平的”，并为其辩护，那么他并没有本质上的欺骗意图。由于论证源于真诚的信念，它可能不太可能展示出与欺骗或撒谎相关的语言特征。事实性、个人信念和欺骗意图之间的相互作用仍然是一个未经充分研究的领域。解开这些变量在论证中的影响对于更好地理解归因于每一个的语言特征至关重要。为了研究基于信念的欺骗与事实性之间的关系，我们提出了DeFaBel语料库，这是一个基于信仰的欺骗的众包资源。为了创建这个语料库，我们设计了一个研究，要求参与者撰写支持诸如“食用西瓜籽可能导致消化不良”的陈述，而不论其事实准确性或个人信念如何。

    arXiv:2403.10185v1 Announce Type: new  Abstract: If a person firmly believes in a non-factual statement, such as "The Earth is flat", and argues in its favor, there is no inherent intention to deceive. As the argumentation stems from genuine belief, it may be unlikely to exhibit the linguistic properties associated with deception or lying. This interplay of factuality, personal belief, and intent to deceive remains an understudied area. Disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them. To study the relation between deception and factuality, based on belief, we present the DeFaBel corpus, a crowd-sourced resource of belief-based deception. To create this corpus, we devise a study in which participants are instructed to write arguments supporting statements like "eating watermelon seeds can cause indigestion", regardless of its factual accuracy or their personal beliefs about 
    
[^21]: NLP验证：走向一种通用的用于认证鲁棒性的方法论

    NLP Verification: Towards a General Methodology for Certifying Robustness

    [https://arxiv.org/abs/2403.10144](https://arxiv.org/abs/2403.10144)

    本文尝试总结和评估由该领域迄今进展而形成的NLP验证流程的一般组成部分，贡献在于提出了将句子嵌入连续空间得到的可验证子空间的一般描述。

    

    深度神经网络在自然语言处理（NLP）领域取得了显著成功，确保它们的安全性和可靠性至关重要：在安全关键的情境中，这些模型必须对变化或攻击具有鲁棒性，并能对其输出给出保证。与计算机视觉不同，NLP缺乏一个统一的验证方法论，尽管近年来文献中取得了一些进展，但对于NLP验证的实用问题常常涉及不深。在本文中，我们尝试提炼和评估一个NLP验证流程的一般组成部分，该流程来源于迄今为止该领域的进展。我们的贡献有两方面：首先，我们给出了将句子嵌入连续空间得到的可验证子空间的一般描述。我们确定了可验证子空间的语义泛化技术挑战，并提出了一种有效处理的方法。

    arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
    
[^22]: 整体优于总和：在上下文学习中使用聚合演示进行顺序推荐

    The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation

    [https://arxiv.org/abs/2403.10135](https://arxiv.org/abs/2403.10135)

    探索在顺序推荐中使用上下文学习的方法，提出了一种聚合演示的新颖方法LLMSRec-Syn，在多个数据集上实验证明其优于现有基于LLM的方法。

    

    大型语言模型（LLMs）在各种自然语言处理任务中展现出优秀性能。为了将LLMs作为强大的顺序推荐系统，我们探索了上下文学习方法用于顺序推荐。我们研究了指导格式、任务一致性、演示选择和演示数量对模型的影响。我们提出了一种新颖的方法LLMSRec-Syn，通过将多个演示用户整合成一个聚合演示来提高准确性。我们在三个推荐数据集上进行了实验证明，LLMSRec-Syn优于最先进的基于LLM的顺序推荐方法。在某些情况下，LLMSRec-Syn可以与甚至优于监督学习方法。我们的代码公开在https://github.com/demoleiwang/LLMSRec_Syn。

    arXiv:2403.10135v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.
    
[^23]: RAFT：将语言模型调整到特定领域RAG

    RAFT: Adapting Language Model to Domain Specific RAG

    [https://arxiv.org/abs/2403.10131](https://arxiv.org/abs/2403.10131)

    提出了一种名为RAFT的训练方法，通过引用相关文档中能够帮助回答问题的正确序列来改善模型在特定领域中回答问题的能力。

    

    现在，通过大规模文本数据预训练大型语言模型（LLMs）已成为一种标准范式。在将这些LLMs用于许多下游应用程序时，通常还会通过基于RAG的提示或微调，将新知识（例如，时效新闻或私有领域知识）嵌入预训练模型中。然而，模型获得这些新知识的最佳方法仍然是一个未解决的问题。在本文中，我们提出了检索增强微调（RAFT），这是一种训练方法，旨在提高模型在"开放书籍"的领域设置中回答问题的能力。在RAFT中，给定一个问题和一组检索到的文档，我们训练模型忽略那些对回答问题没有帮助的文档，我们称之为干扰文档。RAFT通过原文引用相关文档中能够帮助回答问题的正确序列来实现这一点。

    arXiv:2403.10131v1 Announce Type: cross  Abstract: Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAF
    
[^24]: 利用RLAIF进行意图调节和无毒对抗生成的多任务指导调节

    Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF

    [https://arxiv.org/abs/2403.10088](https://arxiv.org/abs/2403.10088)

    该研究引入了CoARL框架，通过模拟社会偏见中的语用启示来增强对抗性言论生成，利用顺序多指导调节和强化学习生成意图调节的对抗性言论。

    

    对抗性言论被定义为减缓网络仇恨言论的回应，越来越被用作一种非审查解决方案。有效应对仇恨言论涉及消除通常在简短的单句陈述或虐待中暗示的刻板印象、偏见和偏见。这些隐含的表达挑战语言模型，特别是在seq2seq任务中，因为模型性能通常在更长上下文中表现出色。我们的研究引入了CoARL，这是一种新颖的框架，通过建模在仇恨言论中暗含的社会偏见的实用含义来增强对抗性言论生成。CoARL的前两个阶段涉及顺序多指导调节，教导模型理解攻击性陈述的意图、反应和危害，然后学习生成意图调节的对抗性言论的特定任务低秩适配器权重。最后一个阶段使用强化学习对输出进行微调，以提高效果和无毒性。

    arXiv:2403.10088v1 Announce Type: cross  Abstract: Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-
    
[^25]: DRAGIN：基于大型语言模型实时信息需求的动态检索增强生成

    DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models

    [https://arxiv.org/abs/2403.10081](https://arxiv.org/abs/2403.10081)

    提出了一种新框架DRAGIN，旨在解决大型语言模型在文本生成过程中动态检索和生成中存在的问题。

    

    动态检索增强生成（RAG）范式在大型语言模型（LLMs）的文本生成过程中主动决定何时以及何时检索。该范式的两个关键元素是确定激活检索模块的最佳时机（决定何时检索）以及一旦触发检索，制定适当的查询（确定要检索什么）。然而，当前动态RAG方法在两个方面都存在不足。首先，决定何时进行检索的策略通常依赖于静态规则。此外，决定要检索什么的策略通常局限于LLM的最近一句或最后几个标记，而LLM的实时信息需求可能跨越整个上下文。为克服这些局限性，我们引入了一个新框架DRAGIN， 即基于LLMs实时信息需求的动态检索增强生成。

    arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
    
[^26]: 三重GNN：为对话基于方面的四重情感分析引入句法和语义信息

    Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis

    [https://arxiv.org/abs/2403.10065](https://arxiv.org/abs/2403.10065)

    本文引入了Triple GNNs网络来增强对话基于方面的四重情感分析，通过使用GCN来建模话语内的句法依赖关系和DualGATs来构建话语之间的交互。

    

    arXiv:2403.10065v1 公告类型：新摘要：对话基于方面的情感分析（DiaASQ）旨在从给定的对话中检测四重\{目标，方面，观点，情感极性\}。在DiaASQ中，构成这些四重的元素不一定局限于单独的句子，而可能跨越对话中的多个话语。这需要同时关注单个话语的句法信息和它们之间的语义交互。然而，先前的研究主要集中在话语之间的粗粒度关系，因此忽视了详细的话语内句法信息和话语间关系的细粒度潜在好处。本文介绍了Triple GNNs网络以增强DiaASQ。它采用图卷积网络（GCN）来建模话语内的句法依赖关系，以及双图注意力网络（DualGATs）来构建话语之间的交互。

    arXiv:2403.10065v1 Announce Type: new  Abstract: Conversational Aspect-Based Sentiment Analysis (DiaASQ) aims to detect quadruples \{target, aspect, opinion, sentiment polarity\} from given dialogues. In DiaASQ, elements constituting these quadruples are not necessarily confined to individual sentences but may span across multiple utterances within a dialogue. This necessitates a dual focus on both the syntactic information of individual utterances and the semantic interaction among them. However, previous studies have primarily focused on coarse-grained relationships between utterances, thus overlooking the potential benefits of detailed intra-utterance syntactic information and the granularity of inter-utterance relationships. This paper introduces the Triple GNNs network to enhance DiaAsQ. It employs a Graph Convolutional Network (GCN) for modeling syntactic dependencies within utterances and a Dual Graph Attention Network (DualGATs) to construct interactions between utterances. Exp
    
[^27]: Repoformer：面向存储库级代码补全的选择性检索

    Repoformer: Selective Retrieval for Repository-Level Code Completion

    [https://arxiv.org/abs/2403.10059](https://arxiv.org/abs/2403.10059)

    本文提出了一种选择性的检索增强生成框架，通过自监督学习方法使代码LM能够避免不必要的检索，并在各种基准测试上始终优于现有方法。

    

    arXiv:2403.10059v1 公告类型：跨文摘：检索增强生成（RAG）的最新进展开启了存储库级代码补全的新时代。但是，现有方法中检索的不变使用暴露了效率和鲁棒性方面的问题，大部分检索到的上下文对于代码语言模型（code LM）来说既无效又有害。为了解决这些挑战，本文提出了一种选择性RAG框架，在不必要时避免使用检索。为了支持这一框架，我们设计了一种自监督学习方法，使代码LM能够准确自我评估检索是否可以提高其输出质量，并能够稳健地利用潜在含噪声的检索上下文。使用这种LM作为选择性检索策略和生成模型，我们的框架在包括RepoEval、CrossCodeEval和一个新...

    arXiv:2403.10059v1 Announce Type: cross  Abstract: Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective RAG framework where retrieval is avoided when unnecessary. To power this framework, we design a self-supervised learning approach that enables a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective retrieval policy and the generation model, our framework consistently outperforms the state-of-the-art prompting with an invariable retrieval approach on diverse benchmarks including RepoEval, CrossCodeEval, and a new
    
[^28]: 不要半心半意：捕捉连续指导调整中的关键部分信息

    Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning

    [https://arxiv.org/abs/2403.10056](https://arxiv.org/abs/2403.10056)

    提出了一种基于关键部分信息增益的新型连续指导调整方法，通过动态重放数据和优化训练目标，使LLMs能够捕捉任务感知信息和减轻过度拟合。

    

    arXiv:2403.10056v1 公告类型: 跨领域 摘要：大型语言模型（LLMs）的指导调整可以驱使它们在特定下游任务中产生符合人类目标的结果。然而，LLMs的连续指导调整（CIT）过程可能会带来灾难性遗忘（CF）问题，导致先前学到的能力退化。最近的方法尝试通过修改模型或重放数据来缓解CF问题，但这可能只记住指令的表面模式并在留存任务上感到困惑。在本文中，我们提出了一种基于关键部分信息增益（KPIG）的新型连续指导调整方法。我们的方法计算掩盖部分的信息增益，动态重放数据并优化训练目标，从而使LLMs能够捕捉与正确响应相关的任务感知信息，并减轻对指导中通用描述的过度拟合。此外，我们提出了两个指标，P分和V分，

    arXiv:2403.10056v1 Announce Type: cross  Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score,
    
[^29]: 在重叠中迷失：探索LLMs中的水印冲突

    Lost in Overlap: Exploring Watermark Collision in LLMs

    [https://arxiv.org/abs/2403.10020](https://arxiv.org/abs/2403.10020)

    本研究探讨了在大型语言模型中关于水印冲突的问题，发现双水印冲突存在时会对水印算法的检测性能造成威胁。

    

    由于大型语言模型（LLMs）在生成内容方面的普及，引发了关于文本版权的担忧。水印方法，特别是基于logit的方法，将不可察觉的标识嵌入文本中，以解决这些挑战。然而，水印方法在不同LLMs上的广泛应用导致了一种不可避免的问题，即在常见任务（如问答和改写）中发生的水印冲突。本研究关注双水印冲突，即同一文本中同时存在两个水印的情况。研究表明，水印冲突对上游和下游水印算法的检测器的检测性能构成威胁。

    arXiv:2403.10020v1 Announce Type: new  Abstract: The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.
    
[^30]: 从家族史中识别健康风险：自然语言处理技术调查

    Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques

    [https://arxiv.org/abs/2403.09997](https://arxiv.org/abs/2403.09997)

    该论文调查了利用自然语言处理技术从数字健康记录中识别家族性疾病风险的文献，强调了基于规则的方法以及最近更注重构建基于神经网络的模型。

    

    arXiv:2403.09997v1 公告类型：新摘要：电子健康记录包含患者的状态和病史信息，其中可能涵盖一些可能具遗传性的疾病和疾病史。家族史信息的一个重要用途在于精准健康，其目标是通过预防措施保持人群健康。自然语言处理（NLP）和机器学习技术可以帮助识别有助于健康专业人士在患者晚年之前识别健康风险的信息，从而挽救生命并降低医疗成本。我们调查了NLP领域关于利用数字健康记录来识别家族性疾病风险的技术文献。我们强调基于规则的方法得到了广泛研究，并仍然被广泛用于家族史提取。然而，近年来的工作更多地致力于构建基于神经网络的模型。

    arXiv:2403.09997v1 Announce Type: new  Abstract: Electronic health records include information on patients' status and medical history, which could cover the history of diseases and disorders that could be hereditary. One important use of family history information is in precision health, where the goal is to keep the population healthy with preventative measures. Natural Language Processing (NLP) and machine learning techniques can assist with identifying information that could assist health professionals in identifying health risks before a condition is developed in their later years, saving lives and reducing healthcare costs.   We survey the literature on the techniques from the NLP field that have been developed to utilise digital health records to identify risks of familial diseases. We highlight that rule-based methods are heavily investigated and are still actively used for family history extraction. Still, more recent efforts have been put into building neural models based on 
    
[^31]: GET：解锁CLIP的多模态潜力，用于广义类别发现

    GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery

    [https://arxiv.org/abs/2403.09974](https://arxiv.org/abs/2403.09974)

    本文提出了一种文本嵌入合成器（TES），用于为无标签数据生成伪文本嵌入，以解锁CLIP用于广义类别发现任务中的多模态潜力。

    

    给定包含旧类别和新类别的无标签数据集，广义类别发现（GCD）旨在准确发现新类别，并正确分类旧类别，利用从有标签样本中学习的类别概念。当前的GCD方法只使用单一的视觉信息模态，导致在视觉上相似类别的分类效果不佳。虽然某些类别在视觉上容易混淆，但它们的文本信息可能是不同的，这促使我们将文本信息引入到GCD任务中。然而，无标签数据缺乏类别名称，使得利用文本信息变得不切实际。为了解决这一具有挑战性的问题，在本文中，我们提出了一种文本嵌入合成器（TES），用于为无标签样本生成伪文本嵌入。具体而言，我们的TES利用CLIP可以生成对齐的视觉-语言特征这一特性，将视觉嵌入转换为CLIP文本模型的标记。

    arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
    
[^32]: 在承诺之前三思：通过反思多个答案评估大型语言模型的置信度

    Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers

    [https://arxiv.org/abs/2403.09972](https://arxiv.org/abs/2403.09972)

    提出了一种新的评估大型语言模型置信度的方法，通过反思和提供多个候选答案的理由来解决对不正确答案的过度自信问题

    

    置信度估计旨在评估输出的可信度，在应用大型语言模型（LLM）时至关重要，尤其是黑盒模型。由于LLM在生成不正确答案时的过度自信，现有对LLM的置信度估计通常不可校准。解决这个问题的现有方法通常受到一个显著限制的阻碍，即它们仅考虑LLM生成的一个答案的置信度。为了解决这一限制，我们提出了一种全新的范式，彻底评估多个候选答案的可信度，以减轻对不正确答案的过度自信。基于这一范式，我们引入了一个两步框架，首先指导LLM反思并为每个答案提供理由，然后汇总这些理由进行综合的置信度估计。这一框架可以与现有的置信度估计方法相结合

    arXiv:2403.09972v1 Announce Type: new  Abstract: Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for
    
[^33]: 处理好您的提示偏见！调查和减轻事实知识提取中的提示偏见

    Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction

    [https://arxiv.org/abs/2403.09963](https://arxiv.org/abs/2403.09963)

    本文调查了预训练语言模型在事实知识提取中存在的“提示偏见”，找到了不同类型提示的偏见程度，以及这种偏见对不同基准测试的影响，并提出了一种基于表示的方法来减轻这种提示偏见。

    

    最近的研究表明，预训练语言模型（PLMs）在事实知识提取中存在“提示偏见”，即提示往往会引入对特定标签的偏见。然而，模型内部提示偏见的程度和影响尚未得到充分探讨。为了回应这一点，本文量化了不同类型提示的偏见，并评估了它们对不同基准测试的影响。我们发现：1）实验中的所有提示都表现出不可忽视的偏见，基于梯度的提示如AutoPrompt和OptiPrompt显示出更高水平的偏见；2）提示偏见可以通过过度拟合测试数据集不合理地放大基准测试的准确性，特别是在类似LAMA这样的不平衡数据集上。基于这些发现，我们提出了一种基于表示的方法来减轻提示偏见，在推断时。具体而言，我们首先使用仅提示查询来估计有偏差的表示，然后从中删除。

    arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
    
[^34]: 大型语言模型中用于快速推测解码的循环草稿机制

    Recurrent Drafter for Fast Speculative Decoding in Large Language Models

    [https://arxiv.org/abs/2403.09919](https://arxiv.org/abs/2403.09919)

    本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。

    

    在本文中，我们介绍一种改进的推测解码方法，旨在提高大型语言模型的效率。我们的方法利用了两种成熟技术的优势：经典的双模型推测解码方法和较新的单模型方法Medusa。从Medusa得到灵感，我们的方法采用了单模型策略进行推测解码。然而，我们的方法通过使用具有循环依赖设计的单个轻量级草稿头来区分自己，本质上类似于经典推测解码中使用的小型草稿模型，但避免了完整transformer架构的复杂性。由于循环依赖，我们可以使用波束搜索快速过滤出草稿头中不需要的候选项。其结果是一种结合了单模型设计简易性并避免了创建数据相关树依赖的方法。

    arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
    
[^35]: 地理信息语言识别

    Geographically-Informed Language Identification

    [https://arxiv.org/abs/2403.09892](https://arxiv.org/abs/2403.09892)

    该论文提出了一种地理信息语言识别方法，在地理原点的基础上设置了16个区域性模型，显著提高了语言识别的准确性并对实际数据文集产生了显著影响。

    

    本文提出了一种语言识别方法，其中模型考虑的语言集取决于文本的地理来源。鉴于许多数字文集可以在国家级别进行地理标注，本文制定了16个区域特定模型，每个模型包含在该区域内国家中出现的预期语言。这些区域模型还包括31种使用广泛的国际语言，以确保涵盖这些通用语无论位置如何。传统语言识别测试数据的上游评估显示，F分数的提高范围从1.7点（东南亚）到高达10.4点（北非）。对社交媒体数据进行的下游评估显示，这种改进的性能对应用于大型现实世界数据文集的语言标签有重要影响。结果是一个高度准确的模型。

    arXiv:2403.09892v1 Announce Type: new  Abstract: This paper develops an approach to language identification in which the set of languages considered by the model depends on the geographic origin of the text in question. Given that many digital corpora can be geo-referenced at the country level, this paper formulates 16 region-specific models, each of which contains the languages expected to appear in countries within that region. These regional models also each include 31 widely-spoken international languages in order to ensure coverage of these linguae francae regardless of location. An upstream evaluation using traditional language identification testing data shows an improvement in f-score ranging from 1.7 points (Southeast Asia) to as much as 10.4 points (North Africa). A downstream evaluation on social media data shows that this improved performance has a significant impact on the language labels which are applied to large real-world corpora. The result is a highly-accurate model 
    
[^36]: Fisher Mask节点用于语言模型合并

    Fisher Mask Nodes for Language Model Merging

    [https://arxiv.org/abs/2403.09891](https://arxiv.org/abs/2403.09891)

    介绍了一种用于Transformers的新型模型合并方法，利用Fisher信息进行加权平均，提高了多任务模型的性能。

    

    微调预训练模型在下游性能方面具有显著优势。预训练模型（如BERT及其衍生物）在自然语言处理中的普遍性也导致了任务特定微调模型的激增。在多任务场景中，由于这些模型通常只能很好地执行一项任务，因此需要额外的训练或集成。模型合并这一不断增长的领域提供了一个解决方案，解决了将多个任务特定模型合并为单个多任务模型的挑战。在本研究中，我们引入了一种新颖的用于Transformers的模型合并方法，结合了先前Fisher加权平均和Fisher信息在模型修剪中的应用的见解。通过利用Transformer架构内的mask节点的Fisher信息，我们设计了一个计算效率高的加权平均方案。我们的方法展现出了稳定且显著的性能。

    arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
    
[^37]: Sabi\'a-2:一代新的葡萄牙大型语言模型

    Sabi\'a-2: A New Generation of Portuguese Large Language Models

    [https://arxiv.org/abs/2403.09887](https://arxiv.org/abs/2403.09887)

    Sabi'a-2是一代新的葡萄牙大型语言模型，其中的Sabi'a-2 Medium模型在多个考试中的表现超越了GPT-4，且在大多数考试中超过了GPT-3.5，同时专业化对模型的性能有显著影响，可在无需增大模型尺寸的情况下以比GPT-4便宜10倍的价格提供。

    

    我们介绍了Sabi'a-2，这是一族在葡萄牙文本上训练的大型语言模型。这些模型在多个考试中进行了评估，包括巴西大学的入学考试、专业认证考试以及各种学科（如会计、经济学、工程学、法律和医学）的研究生入学考试。我们的结果显示，到目前为止我们最优秀的模型Sabi'a-2 Medium，在64场考试中有23场与GPT-4的表现相匹敌或超越，并且在64场考试中有58场超过了GPT-3.5。值得注意的是，专业化对模型的性能有显著影响，无需增大模型尺寸，我们可以提供Sabi'a-2 Medium，每个记号的价格比GPT-4便宜10倍。最后，我们发现数学和编码是需要改进的关键能力。

    arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
    
[^38]: FakeWatch：一个用于检测假新闻以确保选举可信性的框架

    FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections

    [https://arxiv.org/abs/2403.09858](https://arxiv.org/abs/2403.09858)

    FakeWatch框架是为了检测假新闻而设计的，整合了传统机器学习技术和前沿语言模型，在北美选举相关新闻数据集上表现出较高的分类准确性。

    

    在当今技术驱动的世界中，假新闻的迅速传播，特别是在选举等重要事件期间，对信息的完整性构成了越来越大的威胁。为了应对这一挑战，我们引入了FakeWatch，一个精心设计用于检测假新闻的全面框架。利用新策划的北美选举相关新闻文章数据集，我们构建了强大的分类模型。我们的框架整合了一个模型中心，包括传统机器学习（ML）技术和尖端语言模型（LMs），以有效地识别假新闻。我们的总体目标是为研究界提供适应性和精准的分类模型，能够识别不断演变的误信息格局。对我们数据集上假新闻分类器进行的定量评估显示，尽管最先进的LMs稍微领先传统ML模型，但是...

    arXiv:2403.09858v1 Announce Type: new  Abstract: In today's technologically driven world, the rapid spread of fake news, particularly during critical events like elections, poses a growing threat to the integrity of information. To tackle this challenge head-on, we introduce FakeWatch, a comprehensive framework carefully designed to detect fake news. Leveraging a newly curated dataset of North American election-related news articles, we construct robust classification models. Our framework integrates a model hub comprising of both traditional machine learning (ML) techniques and cutting-edge Language Models (LMs) to discern fake news effectively. Our overarching objective is to provide the research community with adaptable and precise classification models adept at identifying the ever-evolving landscape of misinformation. Quantitative evaluations of fake news classifiers on our dataset reveal that, while state-of-the-art LMs exhibit a slight edge over traditional ML models, classical 
    
[^39]: 自洽性提升数学推理的校准

    Self-Consistency Boosts Calibration for Math Reasoning

    [https://arxiv.org/abs/2403.09849](https://arxiv.org/abs/2403.09849)

    基于自洽性的校准方法在数学推理任务中能够更好地建立模型信心和准确性之间的关联。

    

    校准，建立准确性和模型信心之间的关联，对LLM开发至关重要。我们基于自洽性设计了三种即插即用的校准方法（Wang等，2022年）用于数学推理任务。在两个流行基准（GSM8K和MathQA）上评估使用强大的开源LLMs（Mistral和LLaMA2），我们的方法比基于p(True)的现有方法（Kadavath等人，2022年）或logit（Kadavath等人，2022年）更好地连接模型信心和准确性。

    arXiv:2403.09849v1 Announce Type: cross  Abstract: Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).
    
[^40]: 使用大型语言模型在提示注入攻击下进行的机器翻译的规模行为研究

    Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks

    [https://arxiv.org/abs/2403.09832](https://arxiv.org/abs/2403.09832)

    该研究探讨了在机器翻译任务上对多个家族的大型语言模型进行的提示注入攻击的影响，发现在某些条件下，更大的模型可能更容易受到成功攻击。

    

    大型语言模型(LLMs)越来越成为许多自然语言处理任务的首选平台，如机器翻译，因为它们的质量往往与特定任务模型相比具有可比性或更好，并且通过自然语言指令或上下文示例指定任务的简单性。然而，它们的普遍性使它们容易受到最终用户的颠覆，后者可能将导致模型以未经授权且可能不安全的方式行为的指令嵌入到他们的请求中。在这项工作中，我们研究了在机器翻译任务上对多个家族的LLMs进行的提示注入攻击(PIAs)，重点关注模型大小对攻击成功率的影响。我们引入了一个新的基准数据集，并发现在多种语言对和英文注入提示的情况下，更大的模型在某些条件下可能更容易受到成功攻击的影响，这是一种情况

    arXiv:2403.09832v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance o
    
[^41]: 探讨大型语言模型在在线诱拐预防中的效力：有益还是有害？

    Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention

    [https://arxiv.org/abs/2403.09795](https://arxiv.org/abs/2403.09795)

    研究人员探索大型语言模型在在线诱拐预防中的有效性，发现没有模型明显适用于此目的，存在潜在的有害答案生成。

    

    强大的生成式大型语言模型(LLMs)作为问答系统正在成为普通大众和脆弱群体（如儿童）之间流行的工具。随着儿童与这些工具的互动日益增多，对研究人员来说，审视LLMs的安全性至关重要，特别是对可能导致严重后果的应用，比如在线儿童安全查询。本文探讨了LLMs在在线诱拐预防中的效力，既包括通过建议生成来识别和避免诱拐，也通过改变提供的上下文和提示特定性来调查提示设计对模型性能的影响。通过对超过6,000次LLMs互动的结果反映，我们发现没有模型明显适用于在线诱拐预防，在行为一致性方面存在缺乏一致性，并且存在有害答案生成的潜力，特别是来自开源

    arXiv:2403.09795v1 Announce Type: cross  Abstract: Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-sour
    
[^42]: 图像是对齐的软肋：利用视觉漏洞破解多模态大语言模型

    Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models

    [https://arxiv.org/abs/2403.09792](https://arxiv.org/abs/2403.09792)

    论文研究了多模态大语言模型对齐问题，揭示了图像输入对模型的漏洞，并提出了一种利用图像隐藏恶意意图、成功破解现有模型的方法。

    

    在这篇论文中，我们研究了多模态大语言模型（MLLMs）的无害对齐问题。我们对代表性的MLLMs的无害性能进行了系统的实证分析，并揭示了图像输入对MLLMs造成的对齐漏洞。受此启发，我们提出了一种名为HADES的新型破解方法，利用精心制作的图像隐藏和放大文本输入中的恶意意图的有害性。实验结果表明，HADES可以有效地破解现有的MLLMs，为LLaVA-1.5实现了90.26％的平均攻击成功率（ASR），为Gemini Pro Vision实现了71.60％的ASR。我们的代码和数据将公开发布。

    arXiv:2403.09792v1 Announce Type: cross  Abstract: In this paper, we study the harmlessness alignment problem of multimodal large language models~(MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly released.
    
[^43]: 通过人工智能实现情绪智能：自然语言处理和深度学习在医疗文本分析中的应用

    Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts

    [https://arxiv.org/abs/2403.09762](https://arxiv.org/abs/2403.09762)

    本文系统考察了人工智能在医疗文本中情感分析方面的应用，展示了算法精度、神经退行性疾病预测以及临床决策支持方面的显著进展。

    

    本文介绍了人工智能在评估与医疗相关文本中情绪方面的应用的方法论检查，特别关注了自然语言处理和深度学习技术的融合。我们审查了许多利用人工智能增强情感分析、对情绪进行分类以及基于临床叙事、患者对药物的反馈和在线健康讨论所获得的文本信息预测患者结果的研究。综述展示了用于情感分类的算法精度、用于神经退行性疾病的AI模型预测能力以及支持临床决策的AI系统的显著进展。值得注意的是，人工智能应用的利用提高了个性化治疗计划，通过整合患者情绪并

    arXiv:2403.09762v1 Announce Type: cross  Abstract: This manuscript presents a methodical examination of the utilization of Artificial Intelligence in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing and deep learning technologies. We scrutinize numerous research studies that employ AI to augment sentiment analysis, categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feedback on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification, the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI-powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contri
    
[^44]: 你的提示是什么？一种针对AI助手的远程键盘记录攻击

    What Was Your Prompt? A Remote Keylogging Attack on AI Assistants

    [https://arxiv.org/abs/2403.09751](https://arxiv.org/abs/2403.09751)

    本文揭示了一种可以用于读取AI助手加密响应的新型旁路攻击——令牌长度旁路，并展示了如何通过利用大型语言模型，提供句子间上下文并进行已知明文攻击来克服这一挑战。

    

    AI助手正逐渐成为社会的一个重要组成部分，用于寻求个人和机密问题的建议或帮助。本文揭示了一种新的旁路攻击，可用于通过网络读取AI助手的加密响应：令牌长度旁路。我们发现包括OpenAI和Microsoft在内的许多厂商受到这一旁路的影响。然而，仅从令牌长度序列推断响应内容却具有挑战性。这是因为令牌类似于单词，响应可以是几句话长，导致有成千上万个语法正确的句子。

    arXiv:2403.09751v1 Announce Type: cross  Abstract: AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel.   However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style.   Using these methods,
    
[^45]: 元认知分析：评估数据集和大型语言模型中的陈述性和程序性知识

    Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models

    [https://arxiv.org/abs/2403.09750](https://arxiv.org/abs/2403.09750)

    通过广泛实验探索了LLMs中的陈述性知识和程序性知识对各种任务的影响，发现陈述性知识在大多数任务中的益处大于程序性知识，在简单逻辑推理任务中反之；随着预训练和规模的增加，模型利用两种知识的能力均显著提高。

    

    元认知理论中的陈述性知识和程序性知识是两个关键部分，在LLM的预训练和推理中非常重要。然而，由于对这两种知识的定义、探究和定量评估存在挑战，缺乏对这两种知识进行全面比较的分析。本文从一个新的角度提供了LLMs的地面真知，并评估了有效得分。通过对广泛使用的数据集和模型进行大量实验，我们得出结论：(1) 在大多数任务中，来自陈述性知识的益处大于来自程序性知识的益处。(2) 仅在具有简单逻辑推理的任务中，程序性知识的利润大于陈述性知识。(3) 随着预训练的进行和规模的增加，模型利用两种知识的能力显著提高，但速度不同。我们进行了详细分析。

    arXiv:2403.09750v1 Announce Type: cross  Abstract: Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis 
    
[^46]: 重新探寻真相：多轮检索增强的大型语言模型是强大的假新闻检测器

    Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors

    [https://arxiv.org/abs/2403.09747](https://arxiv.org/abs/2403.09747)

    大型语言模型在假新闻检测中引入新的前沿，但仍需克服解决过时知识和低质量证据检索等问题

    

    假新闻的泛滥对政治、经济和社会带来了深远的影响。尽管已经采用了假新闻检测方法来缓解这一问题，但它们主要依赖于两个基本要素：证据的质量和相关性，以及预测机制的有效性。传统方法通常从维基百科等静态知识库中获取信息，但受限于过时或不完整的数据，尤其是对于新兴或罕见的要求。以其卓越的推理和生成能力而闻名的大型语言模型(LLMs)为假新闻检测引入了一个新的领域。然而，与传统方法一样，基于LLM的解决方案也会面临过时和长尾知识的限制。此外，检索增强的LLMs经常遇到低质量证据检索和上下文长度限制等问题。为了解决这些问题

    arXiv:2403.09747v1 Announce Type: cross  Abstract: The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these ch
    
[^47]: 评估大型语言模型在编程教育中生成反馈的应用

    Evaluating the Application of Large Language Models to Generate Feedback in Programming Education

    [https://arxiv.org/abs/2403.09744](https://arxiv.org/abs/2403.09744)

    评估了大型语言模型在编程教育中生成反馈的应用，结果显示大多数反馈有效地解决了代码错误，但存在不正确建议和虚构问题，需要进一步改进。

    

    该研究调查了大型语言模型，特别是GPT-4，在提升编程教育中的应用。研究概述了一个利用GPT-4提供编程任务反馈但不提供解决方案的网页应用的设计。研究开发了一个用于进行编程任务的网页应用，并在一个学期内对51名学生进行了评估。结果显示，大多数由GPT-4生成的反馈有效地解决了代码错误。然而，存在不正确建议和虚构问题的挑战表明有进一步改进的必要。

    arXiv:2403.09744v1 Announce Type: cross  Abstract: This study investigates the application of large language models, specifically GPT-4, to enhance programming education. The research outlines the design of a web application that uses GPT-4 to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by GPT-4 effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.
    
[^48]: 大型语言模型中的人为因素：系统文献综述与未来研究方向

    The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions

    [https://arxiv.org/abs/2403.09743](https://arxiv.org/abs/2403.09743)

    人工智能领域中，这项研究探索了人类因素在检测大型语言模型的错误输出中的作用，有助于减轻其在专业环境中使用时所带来的风险。

    

    OpenAI在2022年11月推出的ChatGPT标志着人工智能的一个关键时刻，将大型语言模型（LLMs）引入主流，并在用户采用方面创造了新记录。尤其是ChatGPT，经过广泛的互联网数据训练，展示出在各个领域具有显著的对话能力，暗示对劳动力产生了重大影响。然而，这些模型容易出现错误-“幻觉”和遗漏，产生不正确或不完整的信息。这在准确性至关重要的环境中尤为危险，比如法律合规、医学或精细的流程框架。

    arXiv:2403.09743v1 Announce Type: cross  Abstract: The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - "hallucinations" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.   There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essen
    
[^49]: 评估大语言模型作为对话推荐中生成用户模拟器

    Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

    [https://arxiv.org/abs/2403.09738](https://arxiv.org/abs/2403.09738)

    大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。

    

    合成用户是对话推荐系统评估中成本效益较高的真实用户代理。大型语言模型表现出在模拟类似人类行为方面的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一个新的协议，用于衡量语言模型能够准确模拟对话推荐中人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的关键特性：选择要谈论的物品，表达二进制偏好，表达开放式偏好，请求推荐以及提供反馈。通过对基准模拟器的评估，我们展示了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了关于如何通过模型选择和提示策略减少这些偏差的见解。

    arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
    
[^50]: 大型语言模型是否像人一样解决ARC视觉类比问题？

    Do Large Language Models Solve ARC Visual Analogies Like People Do?

    [https://arxiv.org/abs/2403.09734](https://arxiv.org/abs/2403.09734)

    该研究比较了人类和大型语言模型在ARC视觉类比问题上的表现，发现在特定任务上，人类和成年人的表现均优于大多数大型语言模型。对LLMs和年幼儿童错误分析揭示了类似的解决策略，同时指出了两种不同的错误类型，为我们理解LLMs如何解决视觉类比问题提供了新的启示。

    

    抑制论文（Chollet, 2019）形式，我们比较了儿童友好的ARC项目上人类和大型语言模型（LLM）的表现。结果表明，无论是儿童还是成年人，在这些任务上都胜过大多数LLMs。错误分析揭示了LLMs和年幼儿童之间类似的“倒退”解决策略，其中类比的一部分被简单复制。此外，我们发现其他两种错误类型，一种基于表面掌握关键概念（例如，内外关系），另一种基于类比输入矩阵的简单组合。总体而言，“概念”错误在人类中更常见，“矩阵”错误在LLMs中更常见。这项研究为LLM的推理能力和我们可以使用错误分析以及与人类发展的比较来理解LLMs如何解决视觉类比问题提供了新的视角。

    arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.
    
[^51]: OverleafCopilot：在Overleaf中利用大型语言模型增强学术写作能力

    OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models

    [https://arxiv.org/abs/2403.09733](https://arxiv.org/abs/2403.09733)

    OverleafCopilot是第一个无缝集成大型语言模型和Overleaf的工具，使研究人员能够在撰写论文时利用大型语言模型的能力。

    

    大型语言模型（LLMs）的迅速发展促进了不同领域各种应用的实现。在本技术报告中，我们探讨了将LLMs和流行的学术写作工具Overleaf集成，以提高学术写作的效率和质量。为实现上述目标，我们面临三个挑战：i）在Overleaf和LLMs之间实现无缝交互，ii）与LLM提供者建立可靠通信，iii）确保用户隐私。为了解决这些挑战，我们提出了OverleafCopilot，这是第一个无缝集成LLMs和Overleaf的工具（即浏览器扩展程序），使研究人员在撰写论文时能够充分利用LLMs的能力。具体来说，我们首先提出了一个有效的框架来连接LLMs和Overleaf。然后，我们开发了PromptGenius网站，供研究人员轻松查找和共享高质量的最新提示。第三，我们提出了一种ag

    arXiv:2403.09733v1 Announce Type: cross  Abstract: The rapid development of Large Language Models (LLMs) has facilitated a variety of applications from different domains. In this technical report, we explore the integration of LLMs and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and LLMs, ii) establishing reliable communication with the LLM provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the power of LLMs while writing papers. Specifically, we first propose an effective framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date prompts. Thirdly, we propose an ag
    
[^52]: PET-SQL：一个带有交叉一致性的增强提示的两阶段文本到SQL框架

    PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency

    [https://arxiv.org/abs/2403.09732](https://arxiv.org/abs/2403.09732)

    提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。

    

    最近文本到SQL（Text2SQL）领域的进展强调刺激大型语言模型（LLM）进行上下文学习，取得了显著成果。然而，他们在处理冗长的数据库信息和复杂的用户意图时面临挑战。本文提出了一个两阶段框架，以增强当前基于LLM的自然语言到SQL系统的性能。我们首先引入了一种新颖的提示表示，称为参考增强表示，其中包括模式信息和从表格随机抽样的单元格值，以指导LLM生成SQL查询。然后，在第一阶段，我们检索问题-SQL对作为少量演示，促使LLM生成初步SQL（PreSQL）。之后，解析PreSQL中提到的实体进行模式链接，可以显著压缩有用信息。在第二阶段，利用链接的模式，我们简化了

    arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
    
[^53]: 使用变压器模拟序列和树上的加权自动机

    Simulating Weighted Automata over Sequences and Trees with Transformers

    [https://arxiv.org/abs/2403.09728](https://arxiv.org/abs/2403.09728)

    变压器可以模拟加权有限自动机和加权树自动机，拓展了它们的应用范围。

    

    变压器是自然语言处理（NLP）社区中无处不在的模型，在过去几年中展示出令人印象深刻的经验成功。然而，关于它们推理的方式以及计算能力的限制，人们对此知之甚少。这些模型不是按顺序处理数据，却胜过诸如RNN的顺序神经模型。最近的研究表明，这些模型可以紧凑地模拟确定性有限自动机（DFAs）的序列推理能力。这带来了一个问题：变压器能否模拟更复杂的有限状态机的推理？在这项工作中，我们展示变压器可以模拟加权有限自动机（WFAs），这是一类包含DFAs的模型，以及加权树自动机（WTA），一种加权自动机推广到树形输入的模型。我们正式证明了这些说法，并给出了所需变压器模型大小的上界。

    arXiv:2403.09728v1 Announce Type: cross  Abstract: Transformers are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years. However, little is understood about how they reason and the limits of their computational capabilities. These models do not process data sequentially, and yet outperform sequential neural models such as RNNs. Recent work has shown that these models can compactly simulate the sequential reasoning abilities of deterministic finite automata (DFAs). This leads to the following question: can transformers simulate the reasoning of more complex finite state machines? In this work, we show that transformers can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the transformer models nee
    
[^54]: 探究检索增强生成和微调在发展基于人工智能的知识系统中的表现

    Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems

    [https://arxiv.org/abs/2403.09727](https://arxiv.org/abs/2403.09727)

    RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.

    

    arXiv:2403.09727v1 公告类型: 交叉领域 摘要: 生成式大型语言模型(G-LLM)的发展为类似ChatGPT、Bing或Gemini的新型知识系统的开发打开了新的机会。微调(FN)和检索增强生成(RAG)是可用于实现基于G-LLM的知识系统领域自适应的技术。在我们的研究中，利用ROUGE、BLEU、METEOR分数和余弦相似度，我们比较并检验了GPT-J-6B、OPT-6.7B、LlaMA、LlaMA-2语言模型的RAG和FN的表现。基于在不同数据集上展示的测量结果，我们展示了基于RAG的构建比使用FN产生的模型更有效。我们指出将RAG和FN连接起来并不是轻而易举的，因为将FN模型与RAG连接可能会导致性能下降。此外，我们概述了一个简单的基于RAG的架构，平均在RO方面优于FN模型16%

    arXiv:2403.09727v1 Announce Type: cross  Abstract: The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the RO
    
[^55]: RAD-PHI2：为放射学调整PHI-2的指导

    RAD-PHI2: Instruction Tuning PHI-2 for Radiology

    [https://arxiv.org/abs/2403.09725](https://arxiv.org/abs/2403.09725)

    本研究调查了将小语言模型用于放射学领域的应用，通过微调具有27亿参数的Phi-2，提出了具有良好性能的RadPhi-2-Base语言模型。

    

    小语言模型（SLMs）在一般领域语言理解、推理和编码任务中表现出色，但它们在医学领域的能力，尤其是涉及放射学文本的能力，还没有得到充分探讨。本研究探讨了SLMs在一般放射学知识特别是与了解症状、放射学发现的外观、鉴别诊断、评估预后以及针对不同器官系统疾病的治疗方面的问题回答中的应用。此外，我们探讨了将SLMs应用于处理与放射学报告相关任务在基于AI的放射学工作流中的效用。我们使用Radiopaedia这一协作在线放射学资源中的高质量教育内容对具有27亿参数的Phi-2进行微调。所得的语言模型，RadPhi-2-Base，表现出了解决一般放射学问题的能力。

    arXiv:2403.09725v1 Announce Type: cross  Abstract: Small Language Models (SLMs) have shown remarkable performance in general domain language understanding, reasoning and coding tasks, but their capabilities in the medical domain, particularly concerning radiology text, is less explored. In this study, we investigate the application of SLMs for general radiology knowledge specifically question answering related to understanding of symptoms, radiological appearances of findings, differential diagnosis, assessing prognosis, and suggesting treatments w.r.t diseases pertaining to different organ systems. Additionally, we explore the utility of SLMs in handling text-related tasks with respect to radiology reports within AI-driven radiology workflows. We fine-tune Phi-2, a SLM with 2.7 billion parameters using high-quality educational content from Radiopaedia, a collaborative online radiology resource. The resulting language model, RadPhi-2-Base, exhibits the ability to address general radiol
    
[^56]: ClaimVer：通过知识图谱实现可解释的声明级验证和证据归因

    ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs

    [https://arxiv.org/abs/2403.09724](https://arxiv.org/abs/2403.09724)

    ClaimVer是一个人为中心的框架，通过知识图谱实现可解释的声明级验证和证据归因，致力于提高用户对文本验证方法的信任并强调细粒度证据的重要性。

    

    在广泛传播的信息误导和社交媒体以及人工智能生成的文本的激增中，验证和信任所遇到的信息变得日益困难。许多事实核查方法和工具已被开发，但它们往往缺乏适当的可解释性或细粒度，无法在各种情境中发挥作用。一种易于使用、可访问且能够执行细粒度证据归因的文本验证方法变得至关重要。更重要的是，建立用户对这种方法的信任需要呈现每个预测背后的理由，因为研究表明这显著影响人们对自动化系统的信任。将用户关注重点放在具体的问题内容上，而不是提供简单的笼统标签也非常重要。在本文中，我们提出了$\textit{ClaimVer，一个以人为中心的框架}$，旨在满足用户的信息需求。

    arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
    
[^57]: 从临床文本中提取生物医学概念预测患者的再入院情况

    Prediction of readmission of patients by extracting biomedical concepts from clinical texts

    [https://arxiv.org/abs/2403.09722](https://arxiv.org/abs/2403.09722)

    从临床文本中提取生物医学概念预测患者的再入院情况，可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。

    

    如今，存在大量的电子健康数据为进行旨在改善为患者提供的医疗服务并降低医疗系统成本的研究创造了潜在能力。近年来医学领域备受关注的一个话题是识别出刚从医院出院后可能很快再次入院的患者。这种识别可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。本研究讨论了利用文本挖掘方法和对患者电子文件中的出院报告文本进行处理来预测患者再次入院情况。为此，使用两种方法评估了各种机器学习模型的性能：词袋模型和概念袋模型。

    arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
    
[^58]: 基于语义提及图增强模型的文档级事件论元抽取

    A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction

    [https://arxiv.org/abs/2403.09721](https://arxiv.org/abs/2403.09721)

    提出了一种语义提及图增强模型（GAM），通过构建语义提及图和引入集成图变换器模块，有效解决了文档级事件论元抽取中的独立建模实体提及和文档提示隔离问题。

    

    文档级事件论元抽取（DEAE）旨在从非结构化文档中识别论元及其特定角色。本文提出了一种语义提及图增强模型（GAM），旨在解决DEAE中独立建模实体提及和文档提示隔离的问题。GAM构建了一个捕获文档和提示内部及间部关系的语义提及图，引入了一个集成的图变换器模块来有效处理提及及其三种语义关系。

    arXiv:2403.09721v1 Announce Type: cross  Abstract: Document-level Event Argument Extraction (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize prompt-based methods to guide pre-trained language models (PLMs) in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention Graph Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention graph that captures relations within and between documents and prompts, encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensembled graph transformer module to address mentions and their three semantic relations effectively. Later, the graph-augmented en
    
[^59]: 微调与提示，语言模型能理解人类价值观吗？

    Fine-tuning vs Prompting, Can Language Models Understand Human Values?

    [https://arxiv.org/abs/2403.09720](https://arxiv.org/abs/2403.09720)

    通过对Human Value Detection 2023任务中的微调和提示调整的探索，验证语言模型是否能理解人类价值观。

    

    准确处理句子中的潜在支持的价值观对于理解说话者的倾向至关重要，但在自然语言理解（NLU）中却是一个具有挑战性的任务。在本文中，我们探讨了在人类价值检测2023中微调和提示调整在这一下游任务中的潜力。此外，我们尝试验证模型是否能够根据在预训练阶段获得的知识有效解决这个问题。同时，我们对大型语言模型（LLMs）在这个任务中与RLHF的能力感兴趣，并提出了一些初步尝试。

    arXiv:2403.09720v1 Announce Type: cross  Abstract: Accurately handling the underlying support values in sentences is crucial for understanding the speaker's tendencies, yet it poses a challenging task in natural language understanding (NLU). In this article, we explore the potential of fine-tuning and prompt tuning in this downstream task, using the Human Value Detection 2023. Additionally, we attempt to validate whether models can effectively solve the problem based on the knowledge acquired during the pre-training stage. Simultaneously, our interest lies in the capabilities of large language models (LLMs) aligned with RLHF in this task, and some preliminary attempts are presented.
    
[^60]: Mevaker: 针对希伯来语言的结论提取和分配资源

    Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language

    [https://arxiv.org/abs/2403.09719](https://arxiv.org/abs/2403.09719)

    介绍了用于希伯来语言的结论提取模型和数据集，包括摘要和结论提取数据集，并提供了相应模型和代码

    

    在本文中，我们基于以色列国家审计长和巡查员的报告，介绍了用于希伯来语言的摘要MevakerSumm和结论提取MevakerConc数据集，以及两个辅助数据集。我们提供了用于结论提取(HeConE, HeConEspc)和结论分配(HeCross)的模型，本工作中使用的所有代码、数据集和模型检查点都是公开可用的。

    arXiv:2403.09719v1 Announce Type: cross  Abstract: In this paper, we introduce summarization MevakerSumm and conclusion extraction MevakerConc datasets for the Hebrew language based on the State Comptroller and Ombudsman of Israel reports, along with two auxiliary datasets. We accompany these datasets with models for conclusion extraction (HeConE, HeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and model checkpoints used in this work are publicly available.
    
[^61]: 提升自然语言处理与系统推荐之间协作的TextCNN的全面实现

    Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation

    [https://arxiv.org/abs/2403.09718](https://arxiv.org/abs/2403.09718)

    TextCNN的全面实现提升了自然语言处理与系统推荐之间的协作，为NLP领域的文本分类任务带来了新的标准技术。

    

    自然语言处理（NLP）是人工智能的重要分支，研究如何使计算机理解、处理和生成人类语言。文本分类是NLP中的基本任务，旨在将文本分类为不同的预定义类别。深度学习在许多研究领域取得了巨大成功，并成为NLP领域的标准技术，广泛应用于文本分类任务中。与数字和图片不同，文本处理强调精细处理能力。传统文本分类方法通常需要对输入模型的文本数据进行预处理，并通过手动方式获得良好的样本特征。

    arXiv:2403.09718v1 Announce Type: cross  Abstract: Natural Language Processing (NLP) is an important branch of artificial intelligence that studies how to enable computers to understand, process, and generate human language. Text classification is a fundamental task in NLP, which aims to classify text into different predefined categories. Text classification is the most basic and classic task in natural language processing, and most of the tasks in natural language processing can be regarded as classification tasks. In recent years, deep learning has achieved great success in many research fields, and today, it has also become a standard technology in the field of NLP, which is widely integrated into text classification tasks. Unlike numbers and images, text processing emphasizes fine-grained processing ability. Traditional text classification methods generally require preprocessing the input model's text data. Additionally, they also need to obtain good sample features through manual 
    
[^62]: 通过心理状态跟踪提升面向抑郁症诊断的聊天系统

    Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking

    [https://arxiv.org/abs/2403.09717](https://arxiv.org/abs/2403.09717)

    本文提出将心理状态跟踪集成到大型语言模型中，以明确引导抑郁症诊断导向的聊天，从而能更好地捕捉患者在对话过程中的信息、情绪或症状变化。

    

    抑郁症诊断导向的聊天旨在引导患者进行自我表达，收集抑郁症检测的关键症状。最近的工作集中于结合任务导向对话和闲聊，模拟基于访谈的抑郁症诊断。然而，这些方法无法很好地捕捉患者在对话过程中的信息、情绪或症状的变化。此外，还没有明确的框架用于引导对话，这导致一些无用的交流影响了体验。本文提出将心理状态跟踪（POST）集成到大型语言模型（LLM）中，以明确引导抑郁症诊断导向的聊天。具体而言，该状态源自于心理理论模型，包括四个组件，即阶段、信息、总结和下一步。我们微调了一个LLM模型以生成动态心理状态，进而用于辅助r

    arXiv:2403.09717v1 Announce Type: cross  Abstract: Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist r
    
[^63]: 文本分析用户许可协议用于标记潜在恶意软件

    Textual analysis of End User License Agreement for red-flagging potentially malicious software

    [https://arxiv.org/abs/2403.09715](https://arxiv.org/abs/2403.09715)

    该论文提出了一种通过文本分析用户许可协议，识别潜在恶意软件的方法，并使用监督分类器对其进行分类，为解决EULA过长且难以理解的问题提供了一种解决方案。

    

    每天用户会下载新软件和更新，每个下载的软件都附带一份用户许可协议（EULA），但这经常被忽略。EULA包含的信息是为了避免法律责任，然而，这可能带来一系列潜在问题，比如间谍软件或对目标系统产生非预期影响。用户不读EULA是因为文档太长，难以理解，文本摘要是这种问题的一个解决方案。我们提出了一个解决方案，可以概括EULA并将其分类为“良性”或“恶意”。我们提取了不同软件的EULA文本，然后使用八种监督分类器对文本进行分类。我们使用集成学习将EULA分类为良性或恶意。

    arXiv:2403.09715v1 Announce Type: cross  Abstract: New software and updates are downloaded by end users every day. Each dowloaded software has associated with it an End Users License Agreements (EULA), but this is rarely read. An EULA includes information to avoid legal repercussions. However,this proposes a host of potential problems such as spyware or producing an unwanted affect in the target system. End users do not read these EULA's because of length of the document and users find it extremely difficult to understand. Text summarization is one of the relevant solution to these kind of problems. This require a solution which can summarize the EULA and classify the EULA as "Benign" or "Malicious". We propose a solution in which we have summarize the EULA and classify the EULA as "Benign" or "Malicious". We extract EULA text of different sofware's then we classify the text using eight different supervised classifiers. we use ensemble learning to classify the EULA as benign or malicio
    
[^64]: 从语言模型中诱导语言结构

    Linguistic Structure Induction from Language Models

    [https://arxiv.org/abs/2403.09714](https://arxiv.org/abs/2403.09714)

    该论文研究从语言模型中以非监督方式生成句法结构的方法，其中介绍了一种利用数值表示短语树的新方法。

    

    我们大脑中隐式地通过分层结构来组织单词在句子中的组成，而这些结构形成了单词的线性序列。语言学家们形式化了不同的框架来模拟这种层次结构；其中最常见的两种句法框架是短语结构和依存结构。短语结构将句子表示为短语的嵌套组，而依存结构通过为单词之间分配关系来表示句子。最近，对智能机器的追求产生了能够以人类水平完成许多语言任务的语言模型（LMs）。许多研究现在质疑LMs是否隐式地表示句法层次结构。本论文集中于在非监督设置中从LMs中生成短语结构和依存结构。我回顾了该领域的关键方法并重点介绍了一项利用二元短语结构树的数值表示（语法距离）的工作线。

    arXiv:2403.09714v1 Announce Type: cross  Abstract: Linear sequences of words are implicitly represented in our brains by hierarchical structures that organize the composition of words in sentences. Linguists formalize different frameworks to model this hierarchy; two of the most common syntactic frameworks are Constituency and Dependency. Constituency represents sentences as nested groups of phrases, while dependency represents a sentence by assigning relations between its words. Recently, the pursuit of intelligent machines has produced Language Models (LMs) capable of solving many language tasks with a human-level performance. Many studies now question whether LMs implicitly represent syntactic hierarchies. This thesis focuses on producing constituency and dependency structures from LMs in an unsupervised setting. I review the critical methods in this field and highlight a line of work that utilizes a numerical representation for binary constituency trees (Syntactic Distance). I pres
    
[^65]: 一种用于论证挖掘的混合智能方法

    A Hybrid Intelligence Method for Argument Mining

    [https://arxiv.org/abs/2403.09713](https://arxiv.org/abs/2403.09713)

    提出了一种混合(人类+AI)方法HyEnA，用于从意见文本中提取论点，结合了自动化处理速度和人类理解推理能力，在公民反馈语料库上取得了更高的覆盖率和准确率。

    

    大规模调查工具能够收集公民反馈意见语料库。从庞大且嘈杂的意见集中提取关键论点有助于快速准确地理解意见。完全自动化的方法可以提取论点，但(1)需要大规模标记数据集，导致较高的注释成本; (2)对已知观点效果良好，但对新颖观点效果欠佳。我们提出了HyEnA，一种混合(人类+AI)方法，用于从主观文本中提取论点，结合了自动化处理的速度和人类的理解和推理能力。我们在三个公民反馈语料库上评估了HyEnA。我们发现，一方面，与一组各种意见进行比较时，HyEnA在高覆盖率和准确率方面优于最先进的自动化方法，证实了人类洞察的必要性。另一方面，HyEnA需要较少的人力工作量，且不会牺牲质量。

    arXiv:2403.09713v1 Announce Type: new  Abstract: Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quali
    
[^66]: 一种用于问答的知识注入课程预训练框架

    A Knowledge-Injected Curriculum Pretraining Framework for Question Answering

    [https://arxiv.org/abs/2403.09712](https://arxiv.org/abs/2403.09712)

    提出了一种通用的知识注入课程预训练框架（KICP），用于实现全面的知识图谱学习和利用以解决KBQA任务。

    

    知识注入问答（KBQA）是自然语言处理研究中的一个关键任务，也是一种访问网络数据和知识的方法，需要利用知识图谱（KG）进行推理。本文提出了一种通用的知识注入课程预训练框架（KICP），旨在实现对KBQA任务的全面KG学习和利用。

    arXiv:2403.09712v1 Announce Type: cross  Abstract: Knowledge-based question answering (KBQA) is a key task in NLP research, and also an approach to access the web data and knowledge, which requires exploiting knowledge graphs (KGs) for reasoning. In the literature, one promising solution for KBQA is to incorporate the pretrained language model (LM) with KGs by generating KG-centered pretraining corpus, which has shown its superiority. However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning. To this end, in this paper, we propose a general Knowledge-Injected Curriculum Pretraining framework (KICP) to achieve comprehensive KG learning and exploitation for KBQA tasks, which is composed of knowledge injection (KI), knowledge adaptation (KA) and curriculum re
    
[^67]: 对混合代码的女性憎恨评论进行探索性数据分析

    Exploratory Data Analysis on Code-mixed Misogynistic Comments

    [https://arxiv.org/abs/2403.09709](https://arxiv.org/abs/2403.09709)

    本研究提出了一个新颖的混合代码Hinglish评论数据集，通过预处理和探索性数据分析技术得出在欠资源语言中检测女性憎恨评论的见解。

    

    自在线仇恨言论和网络欺凌问题以社交媒体平台（如YouTube和Twitter）的普及日益加剧以来，女性更有可能成为在线虐待的受害者。然而，在针对欠资源语言中的女性仇恨检测方面缺乏研究。本文提出了一个YouTube评论的新颖数据集，其中包含从被弱标记为“女性憎恨”和“非女性憎恨”的YouTube视频中收集的混合代码Hinglish评论。对数据集应用了预处理和探索性数据分析（EDA）技术以获取其特征的见解。这一过程通过情感分数、词云等提供了对数据集的更好理解。

    arXiv:2403.09709v1 Announce Type: new  Abstract: The problems of online hate speech and cyberbullying have significantly worsened since the increase in popularity of social media platforms such as YouTube and Twitter (X). Natural Language Processing (NLP) techniques have proven to provide a great advantage in automatic filtering such toxic content. Women are disproportionately more likely to be victims of online abuse. However, there appears to be a lack of studies that tackle misogyny detection in under-resourced languages. In this short paper, we present a novel dataset of YouTube comments in mix-code Hinglish collected from YouTube videos which have been weak labelled as `Misogynistic' and `Non-misogynistic'. Pre-processing and Exploratory Data Analysis (EDA) techniques have been applied on the dataset to gain insights on its characteristics. The process has provided a better understanding of the dataset through sentiment scores, word clouds, etc.
    
[^68]: 利用新型自然语言处理算法管道对免疫检查点抑制剂IrAEs进行机构级监测

    Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs Using a Novel Natural Language Processing Algorithmic Pipeline

    [https://arxiv.org/abs/2403.09708](https://arxiv.org/abs/2403.09708)

    通过新型自然语言处理算法管道，研究对108,280份临床记录进行分析，发现ICIs治疗患者中IrAEs的发生情况，并进行了治疗中断率和生存曲线的构建。

    

    背景：免疫检查点抑制剂（ICIs）已经彻底改变了癌症治疗，但可能导致严重的免疫相关不良事件（IrAEs）。监测IrAEs的发生对于个性化风险评估以及协助治疗决策至关重要。本研究通过分析接受ICIs治疗的Tel Aviv Sourasky医疗中心患者的临床记录，利用自然语言处理算法管道系统性地识别了七种常见或严重的IrAEs。我们研究了皮质类固醇的使用情况，以及IrAEs后的治疗中断率，并构建了生存曲线以可视化治疗过程中不良事件的发生。

    arXiv:2403.09708v1 Announce Type: new  Abstract: Background: Immune checkpoint inhibitors (ICIs) have revolutionized cancer treatment but can result in severe immune-related adverse events (IrAEs). Monitoring IrAEs on a large scale is essential for personalized risk profiling and assisting in treatment decisions.   Methods: In this study, we conducted an analysis of clinical notes from patients who received ICIs at the Tel Aviv Sourasky Medical Center. By employing a Natural Language Processing algorithmic pipeline, we systematically identified seven common or severe IrAEs. We examined the utilization of corticosteroids, treatment discontinuation rates following IrAEs, and constructed survival curves to visualize the occurrence of adverse events during treatment.   Results: Our analysis encompassed 108,280 clinical notes associated with 1,635 patients who had undergone ICI therapy. The detected incidence of IrAEs was consistent with previous reports, exhibiting substantial variation ac
    
[^69]: 复杂文本到SQL的Schema-Aware多任务学习

    Schema-Aware Multi-Task Learning for Complex Text-to-SQL

    [https://arxiv.org/abs/2403.09706](https://arxiv.org/abs/2403.09706)

    提出了一个适用于复杂SQL查询的schema-aware多任务学习框架，通过设计schema链接鉴别器和定义6种关系类型来解决文本到SQL解析中的挑战。

    

    传统的文本到SQL解析器在合成涉及多个表或列的复杂SQL查询时表现不佳，这是因为识别正确的schema项和在问题与schema项之间进行准确对齐的挑战固有。为解决上述问题，我们提出了一个针对复杂SQL查询的schema-aware多任务学习框架（称为MTSQL）。具体来说，我们设计了一个schema链接鉴别器模块来区分有效的问题-schema链接，通过独特的链接关系明确指导编码器以增强对齐质量。在解码器方面，我们定义了6种类型的关系来描述表和列之间的连接（例如，WHERE_TC），并引入了以操作符为中心的三元提取器来识别那些与预定义关系相关的schema项。此外，我们通过预测的三元组建立了一组语法约束规则集，以过滤。

    arXiv:2403.09706v1 Announce Type: cross  Abstract: Conventional text-to-SQL parsers are not good at synthesizing complex SQL queries that involve multiple tables or columns, due to the challenges inherent in identifying the correct schema items and performing accurate alignment between question and schema items. To address the above issue, we present a schema-aware multi-task learning framework (named MTSQL) for complicated SQL queries. Specifically, we design a schema linking discriminator module to distinguish the valid question-schema linkings, which explicitly instructs the encoder by distinctive linking relations to enhance the alignment quality. On the decoder side, we define 6-type relationships to describe the connections between tables and columns (e.g., WHERE_TC), and introduce an operator-centric triple extractor to recognize those associated schema items with the predefined relationship. Also, we establish a rule set of grammar constraints via the predicted triples to filte
    
[^70]: 一种针对大型语言模型在心理健康领域的新颖细致对话评估框架

    A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health

    [https://arxiv.org/abs/2403.09705](https://arxiv.org/abs/2403.09705)

    提出了一种用于评估大型语言模型在心理健康领域微妙对话能力的新框架，并展示GPT4 Turbo可以与已验证的治疗师表现出更相似的结果。

    

    了解大型语言模型（LLMs）的对话能力可以帮助其更谨慎和适当地部署，对于像心理健康这样的安全关键领域尤为重要，其中某人的生命可能取决于对紧急问题回复的确切措辞。本文提出了一种评估LLMs微妙对话能力的新型框架。在其中，我们从心理治疗对话分析文献中发展了一系列定量指标。虽然我们确保我们的框架和指标可供研究人员转移到相关邻域，我们将它们应用到心理健康领域。我们使用我们的框架通过验证的心理健康数据集评估了几种流行的前沿LLMs模型，包括一些GPT和Llama模型。我们的结果显示，GPT4 Turbo在表现上与已验证的治疗师相比与其他模型更为相似。

    arXiv:2403.09705v1 Announce Type: cross  Abstract: Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of LLMs. Within it, we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature. While we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains, we apply them to the mental health field. We use our framework to evaluate several popular frontier LLMs, including some GPT and Llama models, through a verified mental health dataset. Our results show that GPT4 Turbo can perform significantly more similarly to verified therapists than other sel
    
[^71]: 对齐大型语言模型至特定情境规范的 Alignment Studio

    Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations

    [https://arxiv.org/abs/2403.09704](https://arxiv.org/abs/2403.09704)

    本文提出了 Alignment Studio 架构，使应用开发者能够调整大型语言模型至他们特定的价值观、社会规范、法律和其他法规，并协调潜在冲突的需求。

    

    大型语言模型的对齐通常由模型提供者进行，以添加或控制跨用例和情境中通用或普遍理解的行为。相比之下，本文提出了一种方法和架构，赋予应用开发者调整模型至其特定价值观、社会规范、法律和其他法规的能力，并在情境中协调潜在冲突的需求。我们阐述了这种对齐工作室架构的三个主要组成部分：构架者、指导者和审核者共同作用于控制语言模型的行为。我们通过一个企业内部聊天机器人对齐到业务行为准则的实例来说明这种方法。

    arXiv:2403.09704v1 Announce Type: cross  Abstract: The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.
    
[^72]: 概念感知数据构建提升语言模型的上下文学习

    Concept-aware Data Construction Improves In-context Learning of Language Models

    [https://arxiv.org/abs/2403.09703](https://arxiv.org/abs/2403.09703)

    该研究提出了概念感知训练（CoAT）框架，用于构建训练场景，让语言模型从演示中学习利用类比推理概念，并发现通过使用CoAT，预训练的transformers可以更好地利用演示中的新潜在概念，使得上下文学习对函数变换更加 robust。

    

    许多最近的语言模型（LMs）能够进行上下文学习（ICL），表现为LMs能够仅通过自然语言指令执行新任务的能力。先前有关策划上下文学习者的工作假定ICL是由于巨大的过参数化或多任务训练规模导致的。然而，最近的理论工作将ICL能力归因于概念相关的训练数据，并在小规模、合成环境中创建了功能型上下文学习者。

    arXiv:2403.09703v1 Announce Type: cross  Abstract: Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.   In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functio
    
[^73]: 生成器引导的群体反应评估

    Generator-Guided Crowd Reaction Assessment

    [https://arxiv.org/abs/2403.09702](https://arxiv.org/abs/2403.09702)

    本文提出了一种生成器引导的群体反应评估任务，利用生成式大型语言模型引导分类模型做出更好的预测，结果显示经微调的 FLANG-RoBERTa 模型表现最佳。

    

    在社交媒体领域，理解和预测帖子影响力是一个重大挑战。本文提出了一个旨在估计给定社交媒体帖子是否会比另一个帖子获得更多反应的 Crowd Reaction Assessment (CReAM) 任务，对于数字营销人员和内容撰写者来说尤为重要。我们介绍了一个 Crowd Reaction Estimation Dataset (CRED)，包含来自白宫的一对推文，带有转推计数的比较措施。提出的生成器引导评估方法 (GGEA) 利用生成式大型语言模型 (LLM)，例如 ChatGPT、FLAN-UL2 和 Claude，引导分类模型以做出更好的预测。我们的结果表明，经过微调的 FLANG-RoBERTa 模型，在 tweet 内容和 Claude 生成的回应之间利用交叉编码器架构表现最佳。我们进一步使用基于 T5 的释义软件来生成给定帖子的释义，并展示...

    arXiv:2403.09702v1 Announce Type: new  Abstract: In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate 
    
[^74]: 揭开AI的阴影：探究大型语言模型的欺骗能力

    Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models

    [https://arxiv.org/abs/2403.09676](https://arxiv.org/abs/2403.09676)

    该研究探讨了大型语言模型的欺骗行为并分类讨论其引发的社会影响和风险。

    

    这项研究对人工智能欺骗的复杂领域进行了批判性导航，集中研究了大型语言模型（LLMs）的欺骗行为。作者的目标是阐明这一问题，审视围绕它的讨论，随后深入其分类和后果。文章从评估2033年AI安全峰会（ASS），以及LLMs的介绍开始，强调了潜在导致它们欺骗行为的多维偏见。文献综述涵盖了四种分类的欺骗：战略欺骗、模仿、谄媚和不忠推理，以及它们所带来的社会影响和风险。最后，作者对与应对欺骗AI的持久挑战相关的各个方面采取了评估立场。这包括考虑国际协作治理、个人与AI重新构建的互动，提出实际调整的建议。

    arXiv:2403.09676v1 Announce Type: cross  Abstract: This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, 
    
[^75]: 避开生成的替代事实的危险：以ChatGPT-4制造的Ω变种案例作为医学误信息的警示故事

    Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation

    [https://arxiv.org/abs/2403.09674](https://arxiv.org/abs/2403.09674)

    本研究展示了AI（ChatGPT-4）如何轻松制造令人信服但完全虚构的科学数据，以制造出一个完全虚构的医学案例来警示医学误信息的危害。

    

    在人工智能与医学研究交织的时代，真相的披露变得日益复杂。本研究表面上审查了一种所谓的新型SARS-CoV-2变种，被称为Ω变种，展示在S基因区域中有31个独特突变。然而，这个故事的真正潜台词是展示了AI（具体来说是ChatGPT-4）可以如何轻松地制造令人信服但完全虚构的科学数据。所谓的Ω变种在一个完全接种疫苗、之前感染过的35岁男性中被鉴定出现严重COVID-19症状。通过详细的，尽管是虚拟的，基因组分析和接触者追踪，本研究模拟了真实病例报告的严谨方法，从而为一个引人入胜但完全构造的叙述奠定了基础。整个病例研究是由OpenAI的大型语言模型ChatGPT-4生成的Ω变种。

    arXiv:2403.09674v1 Announce Type: new  Abstract: In an era where artificial intelligence (AI) intertwines with medical research, the delineation of truth becomes increasingly complex. This study ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega variant, showcasing 31 unique mutations in the S gene region. However, the real undercurrent of this narrative is a demonstration of the ease with which AI, specifically ChatGPT-4, can fabricate convincing yet entirely fictional scientific data. The so-called Omega variant was identified in a fully vaccinated, previously infected 35-year-old male presenting with severe COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and contact tracing, this study mirrors the rigorous methodology of genuine case reports, thereby setting the stage for a compelling but entirely constructed narrative. The entire case study was generated by ChatGPT-4, a large language model by OpenAI. The fabricated Omega variant f
    
[^76]: API保护的LLMs的标志泄露专有信息

    Logits of API-Protected LLMs Leak Proprietary Information

    [https://arxiv.org/abs/2403.09539](https://arxiv.org/abs/2403.09539)

    大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能

    

    大型语言模型（LLMs）的商业化导致了高级API-only接入专有模型的常见实践。在这项工作中，我们展示了即使对于模型架构有保守的假设，也可以从相对较少的API查询中学习关于API保护的LLM的大量非公开信息（例如，使用OpenAI的gpt-3.5-turbo仅花费不到1000美元）。我们的发现集中在一个关键观察上：大多数现代LLM受到了softmax瓶颈的影响，这限制了模型输出到完整输出空间的线性子空间。我们表明，这导致了一个模型图像或模型签名，从而以较低的成本解锁了几种功能：有效发现LLM的隐藏大小，获取完整词汇输出，检测和消除不同模型更新，识别给定单个完整LLM输出的源LLM，以及...

    arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
    
[^77]: DevBench：软件开发的综合基准测试

    DevBench: A Comprehensive Benchmark for Software Development

    [https://arxiv.org/abs/2403.08604](https://arxiv.org/abs/2403.08604)

    DevBench是一个综合基准测试，评估大型语言模型在软件开发生命周期各个阶段的表现，并发现现有的模型在其中存在挑战。

    

    arXiv:2403.08604v1宣布类型：新的摘要：大型语言模型（LLMs）的最新进展显著提升了它们的编码能力。然而，现有的基准测试主要关注编程的简化或孤立方面，如单文件代码生成或存储库问题调试，未能全面衡量由真实世界编程活动提出的各种挑战的全谱。为此，我们提出了DevBench，一个综合基准测试，评估LLMs在软件开发生命周期的各个阶段，包括软件设计、环境设置、实现、验收测试和单元测试。DevBench具有各种编程语言和领域，高质量数据收集，并针对每个任务精心设计和验证的指标。实证研究表明，当前的LLMs，包括GPT-4-Turbo，无法解决DevBench提出的挑战。分析表明，模型难以理解

    arXiv:2403.08604v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understand
    
[^78]: 通过融合高度专业化语言模型同时掌握文本、代码和数学

    Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models

    [https://arxiv.org/abs/2403.08281](https://arxiv.org/abs/2403.08281)

    通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。

    

    自然语言、编程代码和数学符号的底层数据分布变化巨大，对于那些努力同时在三个领域实现高性能的大型语言模型（LLMs）提出了复杂挑战。本文提出了一种直接融合已经高度专业化模型的方法。所提出的融合框架UltraFuser包括三个已经在语言、编码和数学上得到充分训练的专家。引入了一个标记级别的门控机制来混合专家的输出。设计了一个伴随平衡采样的两阶段训练策略以确保稳定性。为了有效训练融合模型，我们进一步构建了一个

    arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
    
[^79]: 多任务媒体偏见分析通用化的预训练表达识别

    Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions

    [https://arxiv.org/abs/2403.07910](https://arxiv.org/abs/2403.07910)

    MAGPIE是第一个为媒体偏见检测定制的大规模多任务预训练方法，在媒体偏见检测方面表现优异，并且相对于单一任务方法需要更少的微调步骤。

    

    媒体偏见检测是一个复杂的、多方面的问题，传统上通过使用单一任务模型和小型领域内数据集来解决，因此缺乏泛化能力。为了解决这一问题，我们介绍了MAGPIE，这是第一个专门为媒体偏见检测定制的大规模多任务预训练方法。为了实现规模化的预训练，我们提出了大偏见混合（LBM），这是一个包含59个与偏见相关的任务的编译。MAGPIE在Bias Annotation By Experts (BABE)数据集上的媒体偏见检测方面优于先前的方法，F1分数相对提高了3.3%。MAGPIE在Media Bias Identification Benchmark (MBIB)中的8个任务中有5个方面表现优于先前的模型。使用RoBERTa编码器，MAGPIE仅需要相对于单一任务方法的15%的微调步骤。我们的评估表明，比如任务如情感和情绪会增强所有学习，所有任务会增强假新闻检测，

    arXiv:2403.07910v1 Announce Type: cross  Abstract: Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability. To address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection. To enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches. Our evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and s
    
[^80]: 将竞争转化为合作：多Agent系统和语言模型在现代组织中的革命性作用

    Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations

    [https://arxiv.org/abs/2403.07769](https://arxiv.org/abs/2403.07769)

    文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。

    

    这篇文章探讨了基于多Agent系统理论（SMA）结合大型语言模型（LLM）的计算实体的动态影响，其特点是能够模拟复杂的人类互动，作为一种革新人类用户交互的可能性，从利用专门的人工代理支持从操作组织流程到基于应用知识和人的编排的战略决策。 先前的调查显示，在处理新挑战和实用任务（如引发逻辑推理和问题解决）时，特别是在人工代理的自主方法方面存在限制。 还考虑到，传统技术，如激发思想链，需要明确的人类指导。 在我们的方法中，我们使用从大型语言模型（LLM）开发的代理，每个代理都有不同

    arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
    
[^81]: SVD-LLM: 针对大型语言模型压缩的截断感知奇异值分解

    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression

    [https://arxiv.org/abs/2403.07378](https://arxiv.org/abs/2403.07378)

    SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。

    

    大型语言模型（LLMs）的进展受到其庞大尺寸的限制，这需要LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一个有希望的解决方案。然而，现有的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，并且在SVD截断后剩余模型参数的更新缺失。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的限制。SVD-LLM采用了一种截断感知的数据白化策略，以确保奇异值和压缩损失之间的直接映射。此外，SVD-LLM采用一种逐层闭式模型参数更新策略，以弥补SVD截断引起的准确性降低。我们在总共11个数据集和七个m上评估了SVD-LLM。

    arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
    
[^82]: 使用语音活动投影进行多语言交替预测

    Multilingual Turn-taking Prediction Using Voice Activity Projection

    [https://arxiv.org/abs/2403.06487](https://arxiv.org/abs/2403.06487)

    本文研究了在口头对话中使用语音活动投影进行多语言交替预测，在多语言数据上训练的多语言模型展示出与单一语言模型相当的预测性能，并且学会了辨别输入信号的语言。

    

    本文研究了在多语言数据上应用语音活动投影（VAP），这是一种用于口头对话的预测性交替模型，涵盖英语、汉语和日语。VAP模型持续预测双人对话中参与者即将发生的语音活动，利用交叉注意力Transformer捕捉参与者之间的动态互动。结果表明，在单一语言上训练的VAP模型在其他语言上的应用不会产生很好的预测。然而，在三种语言上训练的多语言模型，在所有语言上表现出与单语模型相当的预测性能。进一步的分析表明，多语言模型已学会辨别输入信号的语言。我们还分析了对音调敏感性，这是一种被认为对于交替非常重要的韵律线索。最后，我们比较了两种不同的音频编码器。

    arXiv:2403.06487v1 Announce Type: new  Abstract: This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrast
    
[^83]: CLIcK：韩国文化和语言智慧的基准数据集

    CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean

    [https://arxiv.org/abs/2403.06412](https://arxiv.org/abs/2403.06412)

    CLIcK介绍了一个包含1,995个问答对的韩国文化和语言智慧基准数据集，为填补韩语基准数据缺失的问题而来。

    

    尽管针对韩语的大型语言模型（LLMs）迅速发展，但仍然存在明显缺乏测试必要韩国文化和语言知识的基准数据集。现有的许多韩语基准数据集是通过翻译从英语对应数据集中衍生出来的，它们通常忽视不同的文化背景。仅有少数从韩国数据源捕捉文化知识的基准数据集，提供的仅有偏见和仇恨言论检测等狭窄任务。为了填补这一空白，我们介绍了一个名为CLIcK的韩国文化和语言智慧基准数据集，包含1,995个问答对。CLIcK将其数据来源于韩国官方考试和教科书，将问题分为两个主要类别（语言和文化）下的11个类别。对于CLIcK中的每个实例，我们提供了对哪些文化和语言知识的细粒度注释。

    arXiv:2403.06412v1 Announce Type: new  Abstract: Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge
    
[^84]: 通过小语言模型全面改造多模态助手

    A Comprehensive Overhaul of Multimodal Assistant with Small Language Models

    [https://arxiv.org/abs/2403.06199](https://arxiv.org/abs/2403.06199)

    通过设计多模态小语言模型(MSLMs)及提出高效多模态助手Mipha，实现了在多个方面的协同作用，击败了大语言模型，为开发强大MSLMs提供了见解和指南

    

    多模态大语言模型(MLLMs)展示了在与视觉理解和推理相关的任务中令人印象深刻的技能。然而，由于培训和推理阶段的高计算需求，它们的广泛应用面临障碍，限制了它们在研究和用户社区中受众的范围。在本文中，我们研究了多模态小语言模型（MSLMs）的设计方面，并提出了一种名为Mipha的高效多模态助手，旨在在多个方面之间创造协同作用：视觉表示、语言模型和优化策略。我们表明，在不增加训练数据量的情况下，我们的Mipha-3B在多个基准测试中胜过了最先进的大型MLLMs，特别是LLaVA-1.5-13B。通过详细讨论，我们提供了发展强大的MSLMs的见解和指南，使其能够与MLLMs的能力相媲美。我们的代码可以获得。

    arXiv:2403.06199v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is availa
    
[^85]: SciAssess：基准测试LLM在科学文献分析中的熟练程度

    SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis

    [https://arxiv.org/abs/2403.01976](https://arxiv.org/abs/2403.01976)

    SciAssess介绍了一个专为深度分析科学文献而设计的基准测试，旨在全面评估LLMs在科学领域记忆、理解和分析能力的有效性。

    

    arXiv:2403.01976v1 公告类型：新 抽象：大型语言模型（LLMs）的最新突破已经彻底改变了自然语言理解和生成，引发了人们对利用这些技术进行细致科学文献分析的兴趣激增。然而，现有的基准测试未能充分评估LLMs在科学领域的熟练程度，特别是在涉及复杂理解和多模态数据的情况下。为此，我们引入了SciAssess，一个专为深度分析科学文献而设计的基准测试，旨在全面评估LLMs的有效性。SciAssess专注于评估LLMs在科学背景下记忆、理解和分析的能力。它包括来自不同科学领域的代表性任务，如一般化学、有机材料和合金材料。严格的质量控制措施确保了其在正确性、匿名化和复制方面的可靠性。

    arXiv:2403.01976v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copy
    
[^86]: VIXEN: 图像差异字幕的视觉文本比较网络

    VIXEN: Visual Text Comparison Network for Image Difference Captioning

    [https://arxiv.org/abs/2402.19119](https://arxiv.org/abs/2402.19119)

    提出了一种名为VIXEN的技术，能够用文本简洁地总结一对图像之间的视觉差异，为突出内容操作提供潜在的缓解方法

    

    我们提出了VIXEN - 一种能够用文本简洁地总结一对图像之间的视觉差异，以突出其中的任何内容操作的技术。我们的网络以成对的方式线性映射图像特征，构建出一个软提示，用于预训练大型语言模型。我们通过在最近的InstructPix2Pix数据集中利用提示到提示编辑框架生成的合成操作图像来训练，解决了现有图像差异字幕（IDC）数据集中训练数据量少，操作类型多样性不足的挑战。我们通过GPT-3生成的变化摘要来扩充这个数据集。我们展示了VIXEN能为不同图像内容和编辑类型生成最新的易懂的差异字幕，为防止通过操纵图像内容传播的信息错误提供潜在的缓解方法。代码和数据可在http://github.com/alexblck/vixen获取。

    arXiv:2402.19119v1 Announce Type: cross  Abstract: We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen
    
[^87]: LLM推断揭示：调查与Roofline模型见解

    LLM Inference Unveiled: Survey and Roofline Model Insights

    [https://arxiv.org/abs/2402.16363](https://arxiv.org/abs/2402.16363)

    本文提出了一个基于Roofline模型的框架，用于系统分析LLM推断技术，帮助识别部署中的瓶颈，并为更有效地部署LLM提供策略。

    

    高效大语言模型（LLM）推断领域正在迅速发展，提供了机遇和挑战的独特结合。虽然该领域已经扩展并充满活力，但至今还没有一个简明的框架来分析LLM推断的各种方法，以便清晰地理解这一领域。我们的调查不仅总结了当前研究现状，还基于Roofline模型引入了一个框架，用于系统分析LLM推断技术。这一框架能够帮助识别LLM部署中的瓶颈，并更深入地了解在实际设备上的实际方面，从而为部署LLM提供更有效的策略。此外，我们还系统地汇总了高效LLM推断的最新进展，涵盖关键领域，比如权重优化（如知识蒸馏和量化）。

    arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
    
[^88]: 有关LLMs指令中心响应的（不道德）程度有多高？揭示安全防护栏对有害查询的漏洞

    How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries

    [https://arxiv.org/abs/2402.15302](https://arxiv.org/abs/2402.15302)

    本研究探讨了大型语言模型（LLMs）对指令中心响应的容忍度，并提出了一个包含复杂查询的数据集，旨在揭示触发不道德响应的方法。

    

    在这项研究中，我们解决了一个围绕大型语言模型（LLMs）安全和道德使用日益关注的问题。尽管这些模型具有潜力，但它们可能会被各种复杂的方法欺骗，产生有害或不道德内容，包括“越狱”技术和有针对性的操纵。我们的工作集中在一个特定问题上：LLMs在要求它们生成以伪代码、程序或软件片段为中心的响应时，有多大程度上可能会被误导，而不是生成普通文本。为了调查这个问题，我们引入了TechHazardQA，一个数据集，其中包含应以文本和以指令为中心格式（例如伪代码）回答的复杂查询，旨在识别不道德响应的触发器。我们查询了一系列LLMs-- Llama-2-13b，Llama-2-7b，Mistral-V2和Mistral 8X7B--并要求它们生成文本和指令为中心的响应。为了评估我们的方法，

    arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
    
[^89]: CODIS：为多模态大型语言模型基准化上下文相关的视觉理解

    CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.13607](https://arxiv.org/abs/2402.13607)

    介绍了CODIS基准，用于评估模型利用自由形式文本提供的上下文来增强视觉理解的能力，发现多模态大型语言模型在此基准上表现未达到人类水平，需要提升模型理解视觉能力。

    

    多模态大型语言模型（MLLMs）在结合视觉和语言的各种任务中展现出了有希望的结果。随着这些模型在研究和应用中变得更加重要，对它们能力进行全面评估的重要性也日益增加。然而，大多数现有的基准测试未考虑到在某些情况下，图像需要在更广泛的上下文中被解释。在这项工作中，我们引入了一个名为CODIS的新基准，旨在评估模型使用在自由形式文本中提供的上下文来增强视觉理解的能力。我们的发现表明，MLLMs在这个基准上始终无法达到人类表现。进一步的分析证实了这些模型难以有效提取和利用上下文信息以提高它们对图像的理解能力。这凸显了提升MLLMs理解视觉能力的迫切需求。

    arXiv:2402.13607v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a 
    
[^90]: 在社交媒体上结合心理量表进行零-shot可解释的心理健康分析

    Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales

    [https://arxiv.org/abs/2402.10948](https://arxiv.org/abs/2402.10948)

    该方法结合心理量表通过LLMs进行零-shot心理健康分析，实验结果表明其优于其他方法

    

    传统的心理健康分析方法在容量方面表现强大，但缺乏解释能力，并且需要大规模注释的数据。另一方面，基于大型语言模型（LLMs）的生成式方法有潜力摆脱繁重的注释并提供解释。受到使用量表评估心理状态的心理评估实践的启发，我们的方法通过LLMs结合了两个程序。首先，患者完成心理健康问卷，其次，心理学家解释来自心理健康问题的收集信息并做出明智决策。实验结果表明，我们的方法胜过其他零-shot方法。

    arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
    
[^91]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^92]: 教育领域自然语言处理的调查：分类体系、系统综述和未来趋势

    Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends

    [https://arxiv.org/abs/2401.07518](https://arxiv.org/abs/2401.07518)

    这篇论文调查了教育领域自然语言处理的最新进展，提出了分类体系，并总结了挑战和未来研究方向。

    

    自然语言处理（NLP）旨在通过计算机科学领域的技术分析文本，应用于医疗保健、商业和教育领域。特别是，在教育领域，NLP已经被应用于教学和学习方面的帮助。本调查研究主要关注解决与教育领域相关的问题，并回顾了NLP的最新进展。具体来说，我们从介绍相关背景开始，然后提出教育领域NLP的分类系统。接着，我们根据上述分类系统说明任务定义、挑战和相应的技术。之后，我们展示了该领域中的一些现有演示，并总结了未来的研究方向。

    Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
    
[^93]: MUFFIN: 用于改善指示遵循的多方面指南的策划

    MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following

    [https://arxiv.org/abs/2312.02436](https://arxiv.org/abs/2312.02436)

    MUFFIN是一个新的指示遵循数据集策划方案，通过自动按比例扩大任务，通过多种输入方面使任务丰富多样。

    

    在大型语言模型（LLMs）领域中，加强指示遵循能力通常涉及策划广泛的训练数据。本文引入了一个新的指示遵循数据集策划方案MUFFIN，具体地通过用多种输入方面使任务自动按比例扩大以丰富这些任务。

    arXiv:2312.02436v2 Announce Type: replace-cross  Abstract: In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. 
    
[^94]: zrLLM：在具有大型语言模型的时间知识图上进行零样本关系学习

    zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models

    [https://arxiv.org/abs/2311.10112](https://arxiv.org/abs/2311.10112)

    本文提出了一种在时间知识图上进行零样本关系学习的方法，该方法利用大型语言模型(LLM)生成关系表示，并将其引入基于嵌入的TKGF方法中，能够捕捉关系描述中的语义信息，从而使得关系在建模时能具有相似的语义含义。

    

    模型化随时间变化的知识在时间知识图(TKGs)上已成为一个炽热话题。已经提出了各种方法来预测TKGs上的链接。其中大多数是基于嵌入的，其中学习隐藏表示以基于观察到的图上下文来表示知识图(KG)实体和关系。尽管这些方法在传统的TKG预测(TKGF)基准上表现出色，但它们在建模没有先前图上下文的未见过的零样本关系上面临强烈挑战。本文尝试解决这个问题的方法如下。我们首先将KG关系的文本描述输入大型语言模型(LLMs)中以生成关系表示，然后将它们引入基于嵌入的TKGF方法中。LLM增强的表示可以捕捉关系描述中的语义信息。这使得关系，无论是已见还是未见的，都能够获得类似的语义含义。

    arXiv:2311.10112v2 Announce Type: replace  Abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic mean
    
[^95]: 探索大型语言模型在计算辩论中的潜力

    Exploring the Potential of Large Language Models in Computational Argumentation

    [https://arxiv.org/abs/2311.09022](https://arxiv.org/abs/2311.09022)

    该研究旨在评估大型语言模型在计算辩论领域的性能，包括零样本和少样本设置，标准化了14个开源数据集，并介绍了一个新的反言生成基准数据集。

    

    计算辩论已成为包括人工智能、法律和公共政策在内的各个领域中不可或缺的工具。作为自然语言处理中新兴的研究领域，计算辩论吸引了越来越多的关注。研究计算辩论主要涉及两类任务：辩论挖掘和辩论生成。鉴于大型语言模型在理解上下文和生成自然语言方面表现出色，评估LLMs在各种计算辩论任务中的性能是值得的。本工作旨在评估LLMs（例如ChatGPT、Flan和LLaMA2模型）在计算辩论领域的零样本和少样本设置下的表现。我们将现有任务分为六个主要类别，并对十四个开源数据集的格式进行了标准化。此外，我们提供了一个关于反言生成的新基准数据集。

    arXiv:2311.09022v2 Announce Type: replace  Abstract: Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into six main categories and standardise the format of fourteen open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, 
    
[^96]: 跟进差分描述：语言模型解决图像分类中的歧义问题

    Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification

    [https://arxiv.org/abs/2311.07593](https://arxiv.org/abs/2311.07593)

    提出了一种零样本方法Follow-up Differential Descriptions（FuDD），通过为每个图像确定模糊类，并使用大型语言模型生成新的类描述，以更好地区分目标类。

    

    一种改善视觉-语言模型（如CLIP）在图像分类中性能的有希望的方法是通过扩展类描述（即提示）的相关属性，例如使用棕色麻雀代替麻雀。然而，当前的零样本方法无论目标类之间的共同之处如何，都会选择一组属性，可能提供没有帮助区分它们的有用信息。我们提出了Follow-up Differential Descriptions（FuDD），这是一种零样本方法，可以根据每个数据集量身定制类描述，并提供更好区分目标类的附加属性。FuDD首先为每个图像确定模糊类，然后使用大型语言模型（LLM）生成新的类描述，以区分它们。

    arXiv:2311.07593v2 Announce Type: replace  Abstract: A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between t
    
[^97]: 慎言：通过内心独白培养大型语言模型的沟通能力

    Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue

    [https://arxiv.org/abs/2311.07445](https://arxiv.org/abs/2311.07445)

    通过内在思考，本研究通过语言学和认知科学的灵感，赋予大型语言模型沟通技能，从而提高其拟人化和主动性，吸引用户进行更长时间的对话

    

    大型语言模型（LLMs）的出现进一步提高了开放领域对话系统的能力，可以生成流畅、连贯和多样化的回复。然而，LLMs仍然缺乏一项关键能力：沟通技巧。这种局限使它们更像信息搜索工具，而不是拟人化的聊天机器人。需要考虑沟通技能，如主题过渡、主动提问、概念引导、同理心和总结，使LLMs在对话中更具拟人化和主动性，从而增加用户的兴趣，吸引他们进行更长时间的交谈。然而，在黑匣子LLMs中启用这些沟通技能仍然是一个关键挑战，因为它们没有与真人相同的话语形成模式：先思后说。受语言学和认知科学启发，我们通过内在思考赋予LLMs沟通技能。

    arXiv:2311.07445v2 Announce Type: replace-cross  Abstract: The emergence of large language models (LLMs) further improves the capabilities of open-domain dialogue systems and can generate fluent, coherent, and diverse responses. However, LLMs still lack a crucial ability: communication skills. This limitation renders them more like information seeking tools rather than anthropomorphic chatbots. Communication skills, such as topic transition, proactively asking questions, concept guidance, empathy, and summarising often should be taken into consideration, to make LLMs more anthropomorphic and proactive during the conversation, thereby increasing the interest of users and attracting them to chat for longer. However, enabling these communication skills in black-box LLMs remains a key challenge because they do not have the same utterance formation mode as real people: think before speaking. Inspired by linguistics and cognitive science, we empower LLMs with communication skills through inn
    
[^98]: LILO：通过压缩和文档化代码学习可解释库

    LILO: Learning Interpretable Libraries by Compressing and Documenting Code

    [https://arxiv.org/abs/2310.19791](https://arxiv.org/abs/2310.19791)

    LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。

    

    尽管大型语言模型（LLMs）在代码生成方面表现出色，但软件开发的关键方面是重构的艺术：将代码整合到可重用和可读的程序库中。本文介绍了一种名为LILO的神经符号框架，它通过迭代地合成、压缩和文档化代码来构建适合特定问题领域的库。LILO将LLM引导的程序合成与Stitch自动重构的近期算法进展相结合：Stitch是一个符号压缩系统，可以高效地识别大型代码语料库中的最佳lambda抽象。为了使这些抽象可解释，我们引入了一种自动文档（AutoDoc）过程，它根据上下文中的使用示例推断出自然语言名称和文档字符串。除了提高人类可读性外，我们发现AutoDoc通过帮助LILO的合成器解释和部署学习到的抽象来提高性能。我们对LILO进行了三个归纳式程序综合的评估。

    While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
    
[^99]: XAL：可解释的主动学习提升了低资源学习者的分类器性能

    XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners

    [https://arxiv.org/abs/2310.05502](https://arxiv.org/abs/2310.05502)

    XAL提出了一种可解释的主动学习框架，鼓励分类器提供推断的理由并深入未标记数据，从而提升低资源文本分类的性能

    

    主动学习（AL）旨在通过迭代地筛选最具形成性的未标记数据进行注释，构建有效的训练集，被广泛应用于低资源任务。大多数分类中的主动学习技术依赖于模型的不确定性或分歧来选择未标记数据，会出现对表面模式的过度自信和缺乏探索的问题。受到人类根据因果信息推断和预测的认知过程启发，我们首次尝试将理由融入AL中，提出了一种新颖的面向低资源文本分类的可解释主动学习框架（XAL），旨在鼓励分类器证明其推断并深入研究无法提供合理解释的未标记数据。具体来说，除了使用预训练的双向编码器进行分类外，我们还采用预训练的单向编码器

    arXiv:2310.05502v2 Announce Type: replace  Abstract: Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directi
    
[^100]: Platypus: LLM的快速、廉价和强大的细化

    Platypus: Quick, Cheap, and Powerful Refinement of LLMs

    [https://arxiv.org/abs/2308.07317](https://arxiv.org/abs/2308.07317)

    Platypus是一组经过精细调节和合并的大型语言模型（LLMs），在全球开放LLM排行榜上表现最出色，在微调数据和计算量上仅需其他方法的一小部分。

    

    我们提出了Platypus，这是一组精细调节和合并的大型语言模型（LLMs），在发布本文时在HuggingFace的开放LLM排行榜上表现最出色，目前位列第一。在这项工作中，我们描述了（1）我们精心策划的数据集Open-Platypus，这是其他开放数据集的一个子集，我们向公众开放；（2）我们在微调和合并LoRA模块的过程中如何保留预训练LLMs的强先验，并将特定领域知识呈现出来；（3）我们在检查测试数据泄霢和训练数据中的污染方面的努力，这可以为未来的研究提供信息。具体来说，Platypus系列在各种模型大小上表现出色，以较少的微调数据和总体计算量获得了强大的定量LLM指标，位居全球开放LLM排行榜榜首。

    arXiv:2308.07317v2 Announce Type: replace  Abstract: We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for othe
    
[^101]: 针对预训练编码器型语言模型的精确无需重训练剪枝方法

    Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models

    [https://arxiv.org/abs/2308.03449](https://arxiv.org/abs/2308.03449)

    K-prune 是一种针对预训练编码器型语言模型的精确无需重训练的结构化剪枝算法，通过迭代剪枝过程保留有用知识以最小化剪枝错误，显著提升精确度。

    

    给定一个预训练的编码器型语言模型，我们如何在不重新训练的情况下对其进行精确压缩？对于预训练语言模型的压缩来说，无需重训练的结构化剪枝算法非常关键，因为它们能够显著降低剪枝成本并能够剪枝大型语言模型。然而，现有的无需重训练算法存在严重的精度下降问题，因为它们未能处理剪枝错误，尤其在高压缩率情况下。本文提出了K-prune（知识保留剪枝），这是一种针对预训练编码器型语言模型的精确无需重训练的结构化剪枝算法。K-prune专注于保留预训练模型的有用知识，通过精心设计的迭代剪枝过程（包括知识测量、知识保留蒙版搜索和知识保留权重调整），以最小化剪枝错误。因此，K-prune显示出显著的精确度提升。

    arXiv:2308.03449v2 Announce Type: replace  Abstract: Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose K-prune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models. K-prune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, K-prune shows significant accuracy improv
    
[^102]: 具有结构化提示的持续问答学习

    Continuous QA Learning with Structured Prompts

    [https://arxiv.org/abs/2208.14602](https://arxiv.org/abs/2208.14602)

    提出了一种名为Diana的动态架构终身QA模型，通过增强语言模型学习一系列QA任务，并使用四种层次组织的提示来捕获不同粒度的QA知识，以提高模型的泛化性能。

    

    具有终身学习（LL）能力的QA模型对于实际的QA应用至关重要，并且基于架构的LL方法被报告为这些模型的有效实现。然而，将先前的方法扩展到QA任务并不是一件简单的事情，因为它们要么在测试阶段需要访问任务标识，要么不明确地对来自未见任务的样本进行建模。在本文中，我们提出了Diana：一种基于动态架构的终身QA模型，试图通过增强语言模型学习一系列QA任务。在Diana中使用了四种层次组织的提示来捕获不同粒度的QA知识。具体来说，我们将任务级提示用于捕获任务特定知识，以保持高LL性能，并保持实例级提示来学习跨不同输入样本共享的知识以提高模型的泛化性能。

    arXiv:2208.14602v3 Announce Type: replace-cross  Abstract: QA models with lifelong learning (LL) abilities are important for practical QA applications, and architecture-based LL methods are reported to be an effective implementation for these models. However, it is non-trivial to extend previous approaches to QA tasks since they either require access to task identities in the testing phase or do not explicitly model samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong QA model that tries to learn a sequence of QA tasks with a prompt enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture QA knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across different input samples to improve the model's generalization performance. Moreover, we dedi
    
[^103]: 使用自然语言处理从阿尔茨海默病患者的临床记录中提取睡眠信息

    Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing

    [https://arxiv.org/abs/2204.09601](https://arxiv.org/abs/2204.09601)

    通过自然语言处理从临床记录中提取睡眠信息，为研究睡眠与阿尔茨海默病发病关联提供了新的方法和工具

    

    Alzheimer's Disease（AD）是美国最常见的痴呆形式，睡眠是影响老年认知功能最关键的生活方式因素之一。然而，研究睡眠与AD发病之间关联的研究匮乏。本研究通过手动注释UPMC收集的7,266名AD患者的192,000份临床记录中的随机抽样文档，创建了金标准数据集。我们开发了基于规则的自然语言处理（NLP）算法、机器学习模型和基于大型语言模型（LLM）的NLP算法，以自动化提取睡眠相关信息。

    arXiv:2204.09601v2 Announce Type: replace-cross  Abstract: Alzheimer's Disease (AD) is the most common form of dementia in the United States. Sleep is one of the lifestyle-related factors that has been shown critical for optimal cognitive function in old age. However, there is a lack of research studying the association between sleep and AD incidence. A major bottleneck for conducting such research is that the traditional way to acquire sleep information is time-consuming, inefficient, non-scalable, and limited to patients' subjective experience. A gold standard dataset is created from manual annotation of 570 randomly sampled clinical note documents from the adSLEEP, a corpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved from the University of Pittsburgh Medical Center (UPMC). We developed a rule-based Natural Language Processing (NLP) algorithm, machine learning models, and Large Language Model(LLM)-based NLP algorithms to automate the extraction of sleep-rel
    
[^104]: 跨语言一致的儿童语言语义和句法注释

    Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech

    [https://arxiv.org/abs/2109.10952](https://arxiv.org/abs/2109.10952)

    该论文提出了一种跨语言一致的儿童语言语义和句法注释方法，利用通用依存关系方案和自动方法转换句子逻辑形式，为构建CDS语料库提供了新思路。

    

    本文提出了一种构建儿童语音（CDS）语料库以及句子逻辑形式的方法，并利用该方法在英语和希伯来语中创建了两个这样的语料库。该方法强调跨语言一致的表示，借鉴了依赖关系表示和语义解析方面的最新进展。具体而言，该方法包括两个步骤。首先，我们使用通用依存关系（UD）方案对语料库进行句法注释，该方案已经被开发出来以便一致地适用于各种领域和类型多样的语言。接下来，我们通过应用一种自动方法从UD结构转换句子逻辑形式（LFs）来进一步注释这些数据。UD和LF表示具有互补的优势：UD结构是语言中立的，支持多个注释者进行一致和可靠的注释，而LFs是中立的。

    arXiv:2109.10952v2 Announce Type: replace  Abstract: This paper proposes a methodology for constructing such corpora of child directed speech (CDS) paired with sentential logical forms, and uses this method to create two such corpora, in English and Hebrew. The approach enforces a cross-linguistically consistent representation, building on recent advances in dependency representation and semantic parsing. Specifically, the approach involves two steps. First, we annotate the corpora using the Universal Dependencies (UD) scheme for syntactic annotation, which has been developed to apply consistently to a wide variety of domains and typologically diverse languages. Next, we further annotate these data by applying an automatic method for transducing sentential logical forms (LFs) from UD structures. The UD and LF representations have complementary strengths: UD structures are language-neutral and support consistent and reliable annotation by multiple annotators, whereas LFs are neutral as 
    
[^105]: 基于能量的自动化模型评估

    Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])

    [http://arxiv.org/abs/2401.12689](http://arxiv.org/abs/2401.12689)

    提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。

    

    传统的机器学习模型评估协议依赖于标记的、假设独立同分布的测试数据集，而这在实际应用中往往并不常见。自动模型评估（AutoEval）提出了一种替代传统工作流程的方法，通过形成一个接近预测性能的测试管线，而无需真实标签的存在。尽管AutoEval框架近年来取得了一些成功，但仍存在过度自信、存储和计算成本高的问题。因此，我们提出了一种新颖的度量方式——元分布能量（MDE），它可以使AutoEval框架更加高效和有效。MDE的核心是建立一个关于个体样本相关信息（能量）的元分布统计量，然后通过基于能量的学习提供更平滑的表示能力。我们通过将MDE与分类损失相连接，进一步提供了理论洞见。我们还提供了大量实验证据来验证我们的方法。

    The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
    
[^106]: BOK-VQA: 基于外部知识的双语视觉问答系统通过图表示预训练

    BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])

    [http://arxiv.org/abs/2401.06443](http://arxiv.org/abs/2401.06443)

    本研究提出了BOK-VQA数据集，包含多语言的视觉问答数据以及与问题-回答内容相关的知识信息。通过以图嵌入的形式预训练数据的知识信息，可以有效地将外部知识注入VQA系统中，实现更好的问答效果。

    

    目前的生成模型研究方向，如最近开发的GPT4，旨在为多模态和多语言输入寻找相关的知识信息以提供答案。根据这些研究情况，对多语言评估视觉问答（VQA）任务的需求，作为多模态系统的代表任务，逐渐增加。因此，我们在本研究中提出了一种能够扩展到多语言的双语外部知识VQA（BOK-VQA）数据集。所提出的数据包括17K张图片，17K个韩语和英语的问题-回答对以及与问题-回答内容相关的28K个知识信息实例。我们还提出了一个框架，通过以图嵌入的形式预训练BOK-VQA数据的知识信息，可以有效地将知识信息注入VQA系统中。最后，通过深入分析，我们展示了构建训练数据中包含的知识信息的实际效果。

    The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data
    
[^107]: 重访大语言模型时代下的零-shot 抽象摘要，从位置偏见的角度出发

    Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])

    [http://arxiv.org/abs/2401.01989](http://arxiv.org/abs/2401.01989)

    这项研究通过测量位置偏见，重访了大语言模型中的零-shot 抽象摘要。研究结果揭示了模型不公平地优先考虑某些部分的信息，从而导致不可取的行为。对多个LLM模型和预训练抽象摘要模型进行的实验提供了关于零-shot 总结任务的模型性能和位置偏见的新见解和讨论。

    

    我们通过测量位置偏见来表征和研究大型语言模型（LLMs）中的零-shot 抽象摘要，我们将其视为先前文献中研究过的更为限制性的引导偏见现象的一般表述。位置偏见捕捉到模型在输入文本的某些部分上不公平地优先考虑信息，导致不可取的行为。通过对四个不同的真实数据集进行大量实验，我们研究了多个LLM模型如GPT 3.5-Turbo，Llama-2和Dolly-v2中的位置偏见，以及当前最先进的预训练编码器-解码器抽象摘要模型如Pegasus和BART。我们的发现为零-shot 总结任务的模型性能和位置偏见提供了新的见解和讨论。

    We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
    
[^108]: 通过模型适应来去除偏见算法

    Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])

    [http://arxiv.org/abs/2310.18913](http://arxiv.org/abs/2310.18913)

    本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。

    

    大型语言模型正在成为各种语言任务的首选解决方案。然而，随着容量的增长，模型很容易依赖训练数据中存在的偏见和刻板印象所产生的虚假相关性。本研究提出了一种新颖的方法来检测和减轻语言模型中的性别偏见。我们进行因果分析，以识别问题模型组件，并发现中上层前馈层最容易传递偏见。根据分析结果，我们通过线性投影将这些层乘以模型进行适应。我们的方法DAMA通过各种度量指标明显减少了偏见，同时保持模型在后续任务中的性能。我们发布了我们的方法和模型的代码，通过重新训练，保持了LLaMA的最先进性能，同时偏见显著减少。

    Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
    
[^109]: 用于对齐语言模型的组合偏好模型

    Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])

    [http://arxiv.org/abs/2310.13011](http://arxiv.org/abs/2310.13011)

    用于对齐语言模型的组合偏好模型（CPMs）是一种新颖的偏好模型框架，可以分解全局偏好评估并根据可解释的特征进行标量评分，得到更好的泛化能力和鲁棒性。

    

    随着语言模型的能力越来越强，将其与人类偏好进行对齐变得越来越重要。然而，用于训练偏好模型的主流范式存在根本性的限制，例如缺乏透明度和可扩展性，以及对偏好数据集过拟合的敏感性。我们提出了组合偏好模型（CPMs），这是一个新颖的偏好模型框架，将一个全局偏好评估分解为多个可解释的特征，从一个提示的语言模型中获取这些特征的标量评分，并使用逻辑回归分类器聚合这些评分。CPMs允许控制从偏好数据中使用哪些属性来训练偏好模型，并基于被认为是人类偏好判断基础的特征构建模型。我们的实验表明，CPMs不仅改善了泛化能力，比标准偏好模型更具鲁棒性，并且使用CPMs获得的最佳n个样本比使用标准PMs的表现更好。

    As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using C
    
[^110]: Kosmos-G：使用多模态大型语言模型在上下文中生成图像

    Kosmos-G: Generating Images in Context with Multimodal Large Language Models. (arXiv:2310.02992v1 [cs.CV])

    [http://arxiv.org/abs/2310.02992](http://arxiv.org/abs/2310.02992)

    本文介绍了Kosmos-G，一种利用多模态大型语言模型（MLLM）在上下文中生成图像的模型。该模型通过使用文本模态作为锚点，将MLLM的输出空间与CLIP对齐，并进行组合指令调整。Kosmos-G展示了零样本多实体主题驱动生成的独特能力。

    

    最近，在文本到图像（T2I）和视觉语言到图像（VL2I）生成方面取得了显著进展。然而，从通用的视觉语言输入生成图像，特别是涉及多个图像的情况，仍然未被充分探索。本文提出了Kosmos-G，该模型利用多模态大型语言模型（MLLMs）的先进感知能力来解决上述挑战。我们的方法通过使用文本模态作为锚点，将MLLM的输出空间与CLIP对齐，并在策划数据上进行组合指令调整。Kosmos-G展示了零样本多实体主题驱动生成的独特能力。值得注意的是，分数蒸馏指令调整对图像解码器不需要进行任何修改。这允许无缝替代CLIP并轻松集成各种U-Net技术，包括细粒度控制和个性化图像解码器变体。

    Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt to
    
[^111]: 让语言模型从数据中隐式学习自我改进能力

    Enable Language Models to Implicitly Learn Self-Improvement From Data. (arXiv:2310.00898v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00898](http://arxiv.org/abs/2310.00898)

    该论文探索了如何让语言模型隐式学习自我改进，并减少对人类标注的依赖。

    

    大型语言模型在开放式文本生成任务中展示了卓越的能力。然而，这些任务的本质决定了模型的回答质量始终有改进的空间。为了解决这个挑战，已经提出了各种方法来增强语言模型的性能。越来越多的关注点集中在使语言模型自我改进其回答质量上，从而减少对广泛的人工标注工作来收集多样化和高质量的训练数据的依赖。最近，基于提示的方法因其有效性、高效性和便利性而受到广泛关注。然而，这些方法通常需要为语言模型提供明确和详尽的指示。对于手动推导和提供所有必要的指示来实现现实世界复杂目标的改进（例如，更有帮助性和更少有害性），这是昂贵且具有挑战性的。

    Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, 
    
[^112]: JoMA: 通过MLP和注意力的联合动力学来解密多层Transformer

    JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00535](http://arxiv.org/abs/2310.00535)

    本文提出了联合MLP/注意力（JoMA）动态，用于解析多层Transformer架构的训练过程。通过预测非线性激活情况下注意力的行为，我们解释了多层Transformer中标记的层次组合方法。实验证实了我们的理论发现。

    

    我们提出了联合MLP/注意力（JoMA）动态，这是一种新颖的数学框架，用于理解多层Transformer架构的训练过程。通过在Transformer中去除自注意力层，我们得到仅包含MLP层的修改后动态。JoMA消除了先前分析中的不切实际的假设（例如缺乏残差连接），并预测注意力在非线性激活的情况下首先变得稀疏（为了学习重要的标记），然后变得密集（为了学习不那么重要的标记），而在线性情况下，它与现有研究一致，显示出注意力随时间变得稀疏。我们利用JoMA定性地解释了多层Transformer中如何将标记组合成层次结构，当输入标记是由潜在的层次生成模型生成时。在从现实世界数据集（Wikitext2/Wikitext103）训练的模型和各种预训练模型（OPT，Pythia）上进行的实验证实了我们的理论发现。

    We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
    
[^113]: XATU: 面向可解释性文本更新的细粒度基于指令的基准测试

    XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])

    [http://arxiv.org/abs/2309.11063](http://arxiv.org/abs/2309.11063)

    XATU是第一个细粒度基于指令的可解释性文本编辑基准测试，涵盖广泛的编辑类型，并通过引入细粒度指令和黄金标准编辑说明来提高可解释性。

    

    文本编辑是一个关键的任务，涉及修改文本以更好地与用户意图对齐。然而，现有的文本编辑基准数据集在提供粗粒度指令方面存在局限性。因此，尽管编辑后的输出似乎合理，但往往偏离了黄金参考中列出的预期更改，导致评估分数较低。为了全面调查大型语言模型的文本编辑能力，本文引入了XATU，这是第一个专门为细粒度基于指令的可解释性文本编辑而设计的基准测试。XATU涵盖了广泛的主题和文本类型，包括词汇、句法、语义和知识密集型的编辑。为了增强可解释性，我们利用高质量的数据源和人工注释，生成了一个包含细粒度指令和黄金标准编辑说明的基准测试。通过评估现有的开放和封闭的大型语言模型对我们的基准测试进行对比

    Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark
    
[^114]: 通过提取精炼的语音和语言情感表示进行语音情感识别

    Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])

    [http://arxiv.org/abs/2309.04849](http://arxiv.org/abs/2309.04849)

    该论文提出了EmoDistill，这是一个利用知识蒸馏来学习从语音中获取情感的强大的语言和语音表示的语音情感识别框架。通过在训练过程中利用经过SER微调的预训练语音和语言教师进行信息蒸馏，该方法在IEMOCAP基准测试中实现了最新的最高准确率，表明其在单模态和多模态技术中的优越性能。

    

    我们提出了EmoDistill，这是一个新颖的语音情感识别（SER）框架，利用跨模态知识蒸馏来学习从语音中获取情感的强大的语言和语音表示。在推理过程中，我们的方法仅使用一串语音信号来进行单模态SER，从而减少计算开销并避免运行时的转录和语音特征提取错误。在训练过程中，我们的方法从一对经过SER微调的预训练的语音和语言教师中的嵌入和逻辑层面蒸馏信息。在IEMOCAP基准测试中的实验表明，我们的方法在准确率上优于其他单模态和多模态技术，并达到了77.49％的无权重准确率和78.91％的加权准确率的最新成绩。详细的消融研究还展示了我们方法的每个组件的影响。

    We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
    
[^115]: 语言代理的认知架构

    Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.02427](http://arxiv.org/abs/2309.02427)

    本文提出了一种称为CoALA的认知架构，用于组织语言代理的现有研究并规划未来的发展方向。CoALA描述了一个具有模块化记忆组件、结构化行动空间和通用决策过程的语言代理。通过这一框架，有望发展出更强大的语言代理。

    

    最近的研究在大规模语言模型（LLMs）中增加了外部资源（例如互联网）或内部控制流（例如提示链），用于需要基于语境或推理的任务，从而产生了一类新的语言代理。尽管这些代理取得了实证成功，但我们缺乏一个系统的框架来组织现有代理并规划未来的发展。在本文中，我们借鉴了认知科学和符号人工智能的丰富历史，提出了语言代理的认知架构（CoALA）。CoALA描述了一个具有模块化记忆组件、用于与内部记忆和外部环境交互的结构化行动空间以及选择行动的通用决策过程的语言代理。我们使用CoALA对最近的大量研究进行了回顾和组织，并展望了更强大代理的可行方向。总的来说，CoALA将当今的语言代理置于上下文中。

    Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
    
[^116]: 基于投票的多模态自动欺骗检测

    Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])

    [http://arxiv.org/abs/2307.07516](http://arxiv.org/abs/2307.07516)

    本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。

    

    自动欺骗检测一直是一个热门的研究课题，利用机器学习和深度学习自动检测欺骗给这一旧领域带来了新的光明。在本文中，我们提出了一种基于投票的方法，用于从视频中使用音频、视觉和文本特征进行自动欺骗检测。我们在两个数据集上进行了实验，分别是密歇根大学的真实试验数据集和迈阿密大学的欺骗检测数据集。视频样本被分成图像、音频和手稿的帧。我们提出的多模态投票解决方案包括三个模型。第一个模型是用于从图像中检测欺骗的卷积神经网络（CNN），第二个模型是用于从音频中检测欺骗的Mel频谱图上的支持向量机（SVM），第三个模型是用于从手稿中检测欺骗的支持向量机（SVM）上的Word2Vec。我们提出的解决方案优于现有技术水平。在图像、音频和文本上取得的最佳结果分别为97％、96％、9

    Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
    
[^117]: Musketeer（一人之力，万人之力）：具有任务解释提示的通用视觉语言模型

    Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2305.07019](http://arxiv.org/abs/2305.07019)

    Musketeer是一种通用视觉语言模型，采用任务解释提示（TEP）机制，能够有效整合异构任务的知识，并在多个任务中表现均匀

    

    我们提出了一种序列到序列的视觉语言模型，其参数在所有任务上进行联合训练（万人之力），并在多个任务之间完全共享（一人之力），从而产生了一个名为Musketeer的单一模型。

    We present a sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks (all for one) and fully shared among multiple tasks (one for all), resulting in a single model which we named Musketeer. The integration of knowledge across heterogeneous tasks is enabled by a novel feature called Task Explanation Prompt (TEP). TEP reduces interference among tasks, allowing the model to focus on their shared structure. With a single model, Musketeer achieves results comparable to or better than strong baselines trained on single tasks, almost uniformly across multiple tasks.
    
[^118]: 在大型语言模型中加强迭代增强的思维链提示

    Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])

    [http://arxiv.org/abs/2304.11657](http://arxiv.org/abs/2304.11657)

    本文提出 Iter-CoT 方法，在大型语言模型中进行迭代增强的思维链提示，通过选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，提高了模型的泛化能力，同时使模型能够更准确地生成推理链。

    

    通过逐步引导思维链 (CoT) 作为示范，大型语言模型 (LLMs) 可以在各种推理任务上实现高度有效的性能。然而，LLMs 生成的演示推理链容易出现错误，这可能会导致推理过程中的错误。此外，不恰当的示例 (过于简单或复杂) 可以影响在不同难度级别下的整体性能。我们引入了Iter-CoT (迭代引导思维链提示) 的迭代引导方法，用于选择实例并生成推理链。通过利用迭代增强，我们的方法使LLMs 自主更正错误，从而产生更精确、全面的推理链。同时，我们的方法选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，从而增强LLMs 的泛化能力。

    Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
    
[^119]: WebQAmGaze: 一份多语言Webcam阅读时眼动追踪数据集

    WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v1 [cs.CL])

    [http://arxiv.org/abs/2303.17876](http://arxiv.org/abs/2303.17876)

    WebQAmGaze是一个多语言低成本的阅读时眼动追踪数据集，包括332位参与者的数据，对相关段落的注视似乎能够反映回答理解问题的准确性。这份数据可以推动基于网络摄像头的阅读研究并开辟更便宜、更易获得的数据收集方式。

    

    我们创建了WebQAmGaze，这是一个多语种低成本的阅读时眼动追踪数据集，旨在支持公平透明的自然语言处理模型的开发。WebQAmGaze包括了来自332位参与者阅读英语、西班牙语和德语文本时的网络摄像头眼动数据。每个参与者都会完成两个阅读任务，包括五篇文章的正常阅读和信息寻找任务。经过数据预处理，我们发现对相关段落的注视似乎意味着回答理解问题的正确性。此外，我们与高质量的眼动追踪数据进行了比较分析，结果显示Webcam-ET获得的特征与商业ET设备的特征之间存在中等的相关性。我们相信这份数据可以推动基于网络摄像头的阅读研究并开辟更便宜、更易获得的数据收集方式。WebQAmGaze对于了解问题回答的认知过程以及自然语言处理模型的公平透明具有实用价值。

    We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading dataset, designed to support the development of fair and transparent NLP models. WebQAmGaze includes webcam eye-tracking data from 332 participants naturally reading English, Spanish, and German texts. Each participant performs two reading tasks composed of five texts, a normal reading and an information-seeking task. After preprocessing the data, we find that fixations on relevant spans seem to indicate correctness when answering the comprehension questions. Additionally, we perform a comparative analysis of the data collected to high-quality eye-tracking data. The results show a moderate correlation between the features obtained with the webcam-ET compared to those of a commercial ET device. We believe this data can advance webcam-based reading studies and open a way to cheaper and more accessible data collection. WebQAmGaze is useful to learn about the cognitive processes behind question answering (QA) and to a
    
[^120]: 从临床笔记中提取康复锻炼信息：基于规则和机器学习自然语言处理技术的比较

    Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])

    [http://arxiv.org/abs/2303.13466](http://arxiv.org/abs/2303.13466)

    本文提出了一种基于规则的自然语言处理算法，用于从临床笔记中提取卒中患者治疗过程的锻炼信息，并与几个小型机器学习模型进行比较。在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。这些算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。

    

    康复锻炼在卒中后患者的康复过程中扮演着至关重要的角色。通过个性化治疗和电子健康记录，医疗保健提供者可以使康复过程更加高效。在预测建模为患者分配治疗计划之前，自动化方法是从非结构化电子健康记录中提取康复锻炼信息所必需的。我们引入了一个基于规则的自然语言处理算法来注释卒中患者的治疗过程，并将其与几个小型机器学习模型进行比较。我们发现，在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。在这些算法可以部署到无标签文档之前，需要进行更多的研究，但定制的基于规则的自然语言处理算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。

    Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
    
[^121]: 通过加强句子选择来增强文本生成的学习生成问题方法

    Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.12192](http://arxiv.org/abs/2212.12192)

    本研究提出了一种通过加强句子选择来增强文本生成的学习生成问题方法，该方法通过设计选择器和生成器两个模块，使模型更关注与答案相关的句子，并隐式结合局部信息和全局信息来生成问题。实验结果表明该方法在问题生成任务上优于强大的预训练模型。

    

    我们提出了一种针对回答感知的问题生成问题的方法。我们观察到，回答和问题的信息可以在上下文中的一些相关句子中找到，而不仅仅依赖于强大的预训练语言模型的能力。基于此，我们设计了一个模型，包括两个模块：选择器和生成器。选择器强制模型更加关注与答案相关的句子，以提供隐含的局部信息。生成器通过将选择器提供的局部信息与编码器编码的整个上下文的全局信息隐式结合来生成问题。模型联合训练以利用两个模块之间的潜在交互。在两个基准数据集上的实验结果表明，我们的模型比强大的预训练模型在问题生成任务上更好。代码也可用。

    We introduce an approach for the answer-aware question generation problem. Instead of only relying on the capability of strong pre-trained language models, we observe that the information of answers and questions can be found in some relevant sentences in the context. Based on that, we design a model which includes two modules: a selector and a generator. The selector forces the model to more focus on relevant sentences regarding an answer to provide implicit local information. The generator generates questions by implicitly combining local information from the selector and global information from the whole context encoded by the encoder. The model is trained jointly to take advantage of latent interactions between the two modules. Experimental results on two benchmark datasets show that our model is better than strong pre-trained models for the question generation task. The code is also available.
    

