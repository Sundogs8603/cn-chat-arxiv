# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning and Verification of Task Structure in Instructional Videos.](http://arxiv.org/abs/2303.13519) | 该论文提出了一种新的预训练视频模型——VideoTaskformer。通过从整体上学习表示来验证视频的正确执行，并预测下一步骤。同时，论文也提出了两个新的检测教学视频中错误的基准。 |
| [^2] | [Learning Semantic Text Similarity to rank Hypernyms of Financial Terms.](http://arxiv.org/abs/2303.13475) | 本文介绍了一种能够提取和排序金融术语上义词的系统，使用神经网络学习不同术语间的语义相似度。该系统帮助用户更好地了解复杂金融术语，帮助投资者做出明智的决策。 |
| [^3] | [Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques.](http://arxiv.org/abs/2303.13466) | 本文提出了一种基于规则的自然语言处理算法，用于从临床笔记中提取卒中患者治疗过程的锻炼信息，并与几个小型机器学习模型进行比较。在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。这些算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。 |
| [^4] | [Deep RL with Hierarchical Action Exploration for Dialogue Generation.](http://arxiv.org/abs/2303.13465) | 本篇论文提出了一种新的方法，通过分层行为探索，从多个奖励函数中进行离线学习，并成功地解决了在对话生成中行为采样效率低下的问题，可以更好地识别人类情感细节。 |
| [^5] | [W2KPE: Keyphrase Extraction with Word-Word Relation.](http://arxiv.org/abs/2303.13463) | 本文提出了W2KPE算法用于关键词抽取。该算法在单一类别的命名实体识别任务中得分为45.04分，并使用了词-词关系、多类聚焦损失函数以及关键词评分等策略。 |
| [^6] | [CoBIT: A Contrastive Bi-directional Image-Text Generation Model.](http://arxiv.org/abs/2303.13455) | CoBIT是一种对比式双向图文生成模型，可以统一对比、图像到文本和文本到图像的预训练目标。它采用了一种新颖的unicoder-decoder结构，灵活性高，共享知识，对图像到文本和文本到图像生成任务都有益处。 |
| [^7] | [Development and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse.](http://arxiv.org/abs/2303.13451) | 该论文研究了如何解决临床报告的去识别化问题，提出了一种混合系统，能够将深度学习模型和手动规则的结果有效地合并，实现了对临床文档的高效伪名化，可以为研究目的提供数据，同时保护患者隐私。 |
| [^8] | [Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.](http://arxiv.org/abs/2303.13408) | 本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。 |
| [^9] | [Compositional Zero-Shot Domain Transfer with Text-to-Text Models.](http://arxiv.org/abs/2303.13386) | 提出了一种组合转移学习框架，用于专业领域中的零样本领域转移，使用未标记的领域自由文本进行领域和任务知识的共同学习，通过 NLGU 策略实现领域数据增强和标签预测，经实验证明在生物医学领域和放射学子领域具有优异的性能，胜过了现有的 SOTA。 |
| [^10] | [Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review.](http://arxiv.org/abs/2303.13379) | LLMs在教育中有自动生成和分析文本内容的潜力。然而，这些创新的实际性和伦理性存在担忧，需要考虑技术可行性、隐私、平等和善意等因素。 |
| [^11] | [Capabilities of GPT-4 on Medical Challenge Problems.](http://arxiv.org/abs/2303.13375) | 本论文对最先进的LLM——GPT-4在医学能力考试和基准数据集上进行了全面评估，结果显示其表现出色，有助于医学相关领域的研究和应用。 |
| [^12] | [Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks.](http://arxiv.org/abs/2303.13373) | 本文利用最新的自然语言处理技术，基于ClimaText数据集，微调ClimateBert transformer和BERT模型，成功检测气候变化相关句子，为金融监管机构和投资者提供更准确和一致的气候相关金融风险信息，从而有望支持更可持续的金融系统。 |
| [^13] | [ChatGPT and a New Academic Reality: AI-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing.](http://arxiv.org/abs/2303.13367) | 本文讨论了使用自然语言处理生成文本的ChatGPT模型，该技术被认为可以成为自动准备学术论文及手稿的潜在模型，然而，其与类似模型潜在的伦理问题需要考虑和解决。 |
| [^14] | [Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review.](http://arxiv.org/abs/2303.13365) | 本文调查了自然语言处理和机器学习在需求规范化中的应用现状，发现启发式NLP方法是自动RF中最常用的NLP技术，机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。 |
| [^15] | [Reevaluating Data Partitioning for Emotion Detection in EmoWOZ.](http://arxiv.org/abs/2303.13364) | 本文重新评估了EmoWOZ数据集的数据划分，在此基础上提出了一种新的分层抽样方法用于处理高度不平衡和不均匀分布的情感标签。使用这个方法建立在EmoWoz上的模型表现更好，未来研究者应该采取这种划分以确保一致和准确的性能评估。 |
| [^16] | [Towards the Scalable Evaluation of Cooperativeness in Language Models.](http://arxiv.org/abs/2303.13360) | 本论文旨在对基于语言模型的合作性评估的可扩展性进行研究，通过生成特定博弈论结构场景并进行评估，不过目前生成质量较一般。 |
| [^17] | [Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension.](http://arxiv.org/abs/2303.13355) | 本文通过机器阅读理解的无法回答问题的表现，揭示越南语言模型的弱点，并提出新方向。我们同时还发现现有越南机器阅读理解基准存在人工问题，需迫切寻求新的高质量基准评估进展。 |
| [^18] | [DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph.](http://arxiv.org/abs/2303.13351) | 这篇论文在DBLP学术知识图上创建了一个包含10000个问题-答案对的问答数据集，是最大的学术问答数据集。 |
| [^19] | [Leveraging Foundation Models for Clinical Text Analysis.](http://arxiv.org/abs/2303.13314) | 本研究提出了一个NLP框架，利用预训练的Transformer模型从临床数据中提取与传染病相关的关键信息，该方法在评估中优于标准方法。 |
| [^20] | [SwissBERT: The Multilingual Language Model for Switzerland.](http://arxiv.org/abs/2303.13310) | 该论文介绍了SwissBERT，它是一个专门为处理瑞士相关文本而创建的多语言语言模型，SwissBERT在与瑞士相关的自然语言理解任务上的效果优于以前的模型。 |
| [^21] | [GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering.](http://arxiv.org/abs/2303.13284) | 本论文提出了GETT-QA系统，该系统使用T5对自然语言问题生成简化的SPARQL查询，并使用截断的KG嵌入提高了知识图谱问答的性能。 |
| [^22] | [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization.](http://arxiv.org/abs/2303.13283) | 提出一种基于知识引导的上下文优化方法，通过降低可学习提示与手工提示之间的差异来解决具有特定文本知识的模型在未知类别上泛化能力差的问题。 |
| [^23] | [Parameter-Efficient Sparse Retrievers and Rerankers using Adapters.](http://arxiv.org/abs/2303.13220) | 本文研究了在信息检索中使用适配器的效果，特别是在SPLADE这种稀疏检索器上。研究表明，适配器-SPLADE只需优化2%的训练参数，但在效率和效果方面均优于完全微调的方法。 |
| [^24] | [Fairness-guided Few-shot Prompting for Large Language Models.](http://arxiv.org/abs/2303.13217) | 本文提出了一种新的搜索策略-FairPrompt，在保证公正性的前提下，通过评估提示预测偏差，确定近似最优的提示，从而改进大型语言模型的上下文学习性能，实验表明该方法在准确性和公正性方面均优于现有方法。 |
| [^25] | [A Simple Explanation for the Phase Transition in Large Language Models with List Decoding.](http://arxiv.org/abs/2303.13112) | 本文提供了一个对大语言模型中相变现象的简单解释，利用列表译码器建模，它能够保证在LLM低于临界阈值时错误候选序列数的期望保持有界，而在高于该阈值时呈指数增长。 |
| [^26] | [Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer.](http://arxiv.org/abs/2303.13099) | 本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。 |
| [^27] | [Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit.](http://arxiv.org/abs/2303.13072) | 本论文提出一种用于ASR中的Transformer模型的块重用策略并配以适配器模块，使得模型更加紧凑，可适应性更强，准确性更高。 |
| [^28] | [Retrieval-Augmented Classification with Decoupled Representation.](http://arxiv.org/abs/2303.13065) | 本文提出了一个混合粒度的中文BERT（MigBERT），利用同时考虑字符和词汇的表示方式，提高了中文PLMs的表现，并在各种中文NLP任务中取得了新的SOTA性能。单词比字符语义更丰富。数字也显示了MigBERT可以在日语中使用。 |
| [^29] | [SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization.](http://arxiv.org/abs/2303.13035) | 研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型. |
| [^30] | [GesGPT: Speech Gesture Synthesis With Text Parsing from GPT.](http://arxiv.org/abs/2303.13013) | GesGPT 是一种新的语音手势生成方法，它利用大型语言模型 GPT 的语义分析能力，从文本输入中提取与手势相关的信息，并使用手势库和集成模块生成意义丰富的语音手势。 |
| [^31] | [Is ChatGPT A Good Keyphrase Generator? A Preliminary Study.](http://arxiv.org/abs/2303.13001) | 本文对ChatGPT作为关键词生成器进行了初步研究，发现其在各个方面的性能表现良好，特别是在多领域关键词生成方面。ChatGPT仍面临生成缺失关键词的挑战。 |
| [^32] | [Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification.](http://arxiv.org/abs/2303.12936) | 本研究通过使用“交叉环境”设置，评估了ELMo和DistilBERT在新闻分类和情感分析任务上的监督学习性能表现。虽然两种模型超过传统基线，但在跨环境测试中表现较差，暴露了现代自然语言处理系统的代表能力极限。 |
| [^33] | [Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets.](http://arxiv.org/abs/2303.12898) | 本论文探索了医学文本到SQL生成的泛化问题，并展示了当前的数据集和模型准确率不足以满足临床需求，需要更进一步的研究和改进。 |
| [^34] | [A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification.](http://arxiv.org/abs/2303.12892) | 本研究提出了一个简化的Switch Transformer框架，并从头开始训练，取得了在小型法语临床文本分类任务中比预训练的BERT模型更好的效果，采用Switch Transformer的专家混合机制有助于提高识别准确度，最终在测试集上实现了87％的准确率、87％的精度和86％的召回率。 |
| [^35] | [JaCoText: A Pretrained Model for Java Code-Text Generation.](http://arxiv.org/abs/2303.12869) | JaCoText是一种使用Transformer神经网络从自然语言文本生成Java源代码的预训练模型。 |
| [^36] | [Salient Span Masking for Temporal Understanding.](http://arxiv.org/abs/2303.12860) | 本文介绍了一种用于时间理解的显著性跨度掩蔽技术，通过引入Temporal Span Masking中间训练并与Salient Span Masking结合使用，有效提高多个时间任务的性能及表示效果。 |
| [^37] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^38] | [Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs.](http://arxiv.org/abs/2303.12810) | 本文探究了大型语言模型(LLM)在不同领域推理任务上的表现，并发现LLM在类比和道德推理方面表现出色，在空间推理任务上表现较差。这对于LLM未来的发展具有重要意义。 |
| [^39] | [PACO: Provocation Involving Action, Culture, and Oppression.](http://arxiv.org/abs/2303.12808) | 该研究利用现有的印度WhatsApp帖子数据集，创造了一个可以从WhatsApp帖子中识别挑衅句子的模型PACO，并利用该模型可以防止可能的歧视或暴力事件。 |
| [^40] | [Features matching using natural language processing.](http://arxiv.org/abs/2303.12804) | 本文提出了一种使用自然语言处理进行特征匹配的新混合模型，它可以减少匹配不同数据集所需的时间。 |
| [^41] | [An Analysis of Abstractive Text Summarization Using Pre-trained Models.](http://arxiv.org/abs/2303.12796) | 本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明...... |
| [^42] | [Named Entity Recognition Based Automatic Generation of Research Highlights.](http://arxiv.org/abs/2303.12795) | 该研究使用命名实体识别技术自动生成研究亮点，探究其是否能提高生成亮点的质量。实验结果表明，增加命名实体信息可以提高生成亮点的性能。 |
| [^43] | [cTBL: Augmenting Large Language Models for Conversational Tables.](http://arxiv.org/abs/2303.12024) | 本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。 |
| [^44] | [Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes.](http://arxiv.org/abs/2303.09892) | Memotion 3是一个包含10,000个已注释模因的新数据集，引入了印度-英语混合模因，使其成为该领域内首个相应的数据集。此数据集可用于情感和情绪分析，并可用于对社交媒体上的虚假信息或仇恨内容进行研究。 |
| [^45] | [Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction.](http://arxiv.org/abs/2301.09209) | 本文提出了一种TransFusion架构，利用先前训练的图像字幕和视觉语言模型总结动作上下文，实现对多模态对象交互的预测，有效性得到验证。 |
| [^46] | [Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings.](http://arxiv.org/abs/2210.16848) | 本文提出了一种改进词嵌入的方法，分别为将更多上下文信息纳入Skip-gram框架和提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法。这两种方法经由外部和内部任务的检验，能够大幅度超越基准线。 |
| [^47] | [Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning.](http://arxiv.org/abs/2210.15387) | 该论文提出了一种使用自监督模型和多任务学习相结合的自动评估发音障碍严重程度的方法，在较少数据的情况下实现了向传统方法的优化，并且相对提高了1.25%的F1-score。 |
| [^48] | [Multi-lingual Evaluation of Code Generation Models.](http://arxiv.org/abs/2210.14868) | 本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。 |
| [^49] | [Focusing on Potential Named Entities During Active Label Acquisition.](http://arxiv.org/abs/2111.03837) | 本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。 |
| [^50] | [Graph-Based Decoding for Task Oriented Semantic Parsing.](http://arxiv.org/abs/2109.04587) | 本研究探索了一种替代语义解析任务的方法，将其作为依赖解析任务进行表述并应用了基于图的解码技术，有望提高部分注释数据的效率和数据使用效率。 |
| [^51] | [Discriminating Between Similar Nordic Languages.](http://arxiv.org/abs/2012.06431) | 本文提出了一种用于区分六种相似的北欧语言的机器学习方法，以解决现有工具的误分类问题。 |
| [^52] | [Offensive Language and Hate Speech Detection for Danish.](http://arxiv.org/abs/1908.04531) | 该论文提出了一个适用于检测攻击性语言的丹麦数据集，并为英语和丹麦语的恶意语言检测建立了最新技术性能。 |

# 详细

[^1]: 教学视频中任务结构的学习和验证

    Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])

    [http://arxiv.org/abs/2303.13519](http://arxiv.org/abs/2303.13519)

    该论文提出了一种新的预训练视频模型——VideoTaskformer。通过从整体上学习表示来验证视频的正确执行，并预测下一步骤。同时，论文也提出了两个新的检测教学视频中错误的基准。

    

    鉴于在线教学视频数量庞大，从视频中学习多步骤任务模型的多样性是一个诱人的目标。本文引入了一种新的预训练视频模型——VideoTaskformer，专注于表示教学视频的语义和结构。我们使用一种简单有效的目标来对VideoTaskformer进行预训练：从教学视频中随机屏蔽的步骤预测弱监督的文本标签（遮盖步骤建模）。与先前学习局部步骤表示的方法相比，我们的方法涉及全局学习，利用整个周围任务的视频作为上下文。从这些学习到的表示中，我们可以验证一个未见过的视频是否正确执行给定的任务，以及预测在给定步骤之后可能采取哪些步骤。我们引入了两个新的基准来检测教学视频中的错误，以验证是否存在异常步骤并检查步骤是否按正确的顺序执行。

    Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the rig
    
[^2]: 学习语义文本相似度来排序金融术语的上义词

    Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v1 [cs.CL])

    [http://arxiv.org/abs/2303.13475](http://arxiv.org/abs/2303.13475)

    本文介绍了一种能够提取和排序金融术语上义词的系统，使用神经网络学习不同术语间的语义相似度。该系统帮助用户更好地了解复杂金融术语，帮助投资者做出明智的决策。

    

    近年来，用户使用金融服务的方式发生了变革。随着数字化技术的发展，越来越多的用户更喜欢在线方式来执行金融活动。这导致了大量的金融内容的生成。大多数投资者在做出决策前都会阅读这些内容。每个行业都有特定于其运营领域的术语。银行和金融服务也不例外。为了完全理解这些内容，需要对金融术语有深入的了解。当用它所属的广义类别来说明时，一个术语的基本概念变得容易。这个广义类别被称为上义词。例如，“债券”是金融术语“替代债券”的上义词。在本文中，我们提出了一个系统，能够提取和排序给定金融术语的上义词。该系统经过金融文本训练，并使用神经网络学习不同金融术语之间的语义相似度。所提出的系统可用于提高对复杂金融术语的理解，帮助投资者做出明智的决策。

    Over the years, there has been a paradigm shift in how users access financial services. With the advancement of digitalization more users have been preferring the online mode of performing financial activities. This has led to the generation of a huge volume of financial content. Most investors prefer to go through these contents before making decisions. Every industry has terms that are specific to the domain it operates in. Banking and Financial Services are not an exception to this. In order to fully comprehend these contents, one needs to have a thorough understanding of the financial terms. Getting a basic idea about a term becomes easy when it is explained with the help of the broad category to which it belongs. This broad category is referred to as hypernym. For example, "bond" is a hypernym of the financial term "alternative debenture". In this paper, we propose a system capable of extracting and ranking hypernyms for a given financial term. The system has been trained with fin
    
[^3]: 从临床笔记中提取康复锻炼信息：基于规则和机器学习自然语言处理技术的比较

    Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])

    [http://arxiv.org/abs/2303.13466](http://arxiv.org/abs/2303.13466)

    本文提出了一种基于规则的自然语言处理算法，用于从临床笔记中提取卒中患者治疗过程的锻炼信息，并与几个小型机器学习模型进行比较。在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。这些算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。

    

    康复锻炼在卒中后患者的康复过程中扮演着至关重要的角色。通过个性化治疗和电子健康记录，医疗保健提供者可以使康复过程更加高效。在预测建模为患者分配治疗计划之前，自动化方法是从非结构化电子健康记录中提取康复锻炼信息所必需的。我们引入了一个基于规则的自然语言处理算法来注释卒中患者的治疗过程，并将其与几个小型机器学习模型进行比较。我们发现，在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。在这些算法可以部署到无标签文档之前，需要进行更多的研究，但定制的基于规则的自然语言处理算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。

    Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
    
[^4]: 使用分层行为探索的深度强化学习在对话生成中的应用

    Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])

    [http://arxiv.org/abs/2303.13465](http://arxiv.org/abs/2303.13465)

    本篇论文提出了一种新的方法，通过分层行为探索，从多个奖励函数中进行离线学习，并成功地解决了在对话生成中行为采样效率低下的问题，可以更好地识别人类情感细节。

    

    自然语言的行为空间极其庞大，因此在对话生成中，近似动态规划必须使用策略改进和行为采样。但是，由于有价值的回应非常稀疏，因此使用随机采样的贪心策略效率低下。本文提出了双粒度的 Q-function 并通过探索最有前途的回应类别来缓解这个局限性。该算法从识别人类情感细节的多个奖励函数中进行离线学习。实证研究表明，该算法优于基线方法。

    Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
    
[^5]: W2KPE：基于词-词关系的关键词抽取

    W2KPE: Keyphrase Extraction with Word-Word Relation. (arXiv:2303.13463v1 [cs.CL])

    [http://arxiv.org/abs/2303.13463](http://arxiv.org/abs/2303.13463)

    本文提出了W2KPE算法用于关键词抽取。该算法在单一类别的命名实体识别任务中得分为45.04分，并使用了词-词关系、多类聚焦损失函数以及关键词评分等策略。

    

    本文介绍了我们在ICASSP 2023 MUG Challenge Track 4——关键词提取方面的提交，旨在从会议资料中提取与会议主题最相关的关键词。我们将此挑战建模为单一类别的命名实体识别任务，并开发了更好的表现技术：对于数据预处理，我们在单词分割后对拆分的关键词进行编码。此外，我们通过将多个预处理句子融合为一个片段来增加模型可以同时接受的输入信息量。我们用多类聚焦损失函数替换损失函数，以解决关键词稀疏性问题。此外，我们对每个出现的关键词进行评分，并添加了额外的输出层以适应得分并排列关键词。我们进行了全面的评估，以找到单词分割工具、预训练嵌入模型和相应超参数的最佳组合。通过这些建议，我们在挑战中得分为45.04分。

    This paper describes our submission to ICASSP 2023 MUG Challenge Track 4, Keyphrase Extraction, which aims to extract keyphrases most relevant to the conference theme from conference materials. We model the challenge as a single-class Named Entity Recognition task and developed techniques for better performance on the challenge: For the data preprocessing, we encode the split keyphrases after word segmentation. In addition, we increase the amount of input information that the model can accept at one time by fusing multiple preprocessed sentences into one segment. We replace the loss function with the multi-class focal loss to address the sparseness of keyphrases. Besides, we score each appearance of keyphrases and add an extra output layer to fit the score to rank keyphrases. Exhaustive evaluations are performed to find the best combination of the word segmentation tool, the pre-trained embedding model, and the corresponding hyperparameters. With these proposals, we scored 45.04 on the
    
[^6]: CoBIT: 一种对比式双向图文生成模型

    CoBIT: A Contrastive Bi-directional Image-Text Generation Model. (arXiv:2303.13455v1 [cs.CV])

    [http://arxiv.org/abs/2303.13455](http://arxiv.org/abs/2303.13455)

    CoBIT是一种对比式双向图文生成模型，可以统一对比、图像到文本和文本到图像的预训练目标。它采用了一种新颖的unicoder-decoder结构，灵活性高，共享知识，对图像到文本和文本到图像生成任务都有益处。

    

    在视觉和语言领域中，已经出现了许多与对比性目标（如CLIP）、图像到文本生成目标（如PaLI）或文本到图像生成目标（如Parti）相对应的预训练基础模型。然而，这三个目标可以在相同的数据——图像文本对上预训练，并且相互补充，因为对比提供了全局对齐能力，生成也提供了细粒度理解。本文提出了一种对比式双向图文生成模型（CoBIT），试图将三个预训练目标统一到一个框架中。具体地，CoBIT采用了一种新颖的unicoder-decoder结构，包括图像unicoder、文本unicoder和跨模态decoder。图像/文本unicoders可以在不同任务中在编码和解码之间进行切换，从而实现了灵活性和共享知识，对图像到文本和文本到图像生成任务都有益处。

    The field of vision and language has witnessed a proliferation of pre-trained foundation models. Most existing methods are independently pre-trained with contrastive objective like CLIP, image-to-text generative objective like PaLI, or text-to-image generative objective like Parti. However, the three objectives can be pre-trained on the same data, image-text pairs, and intuitively they complement each other as contrasting provides global alignment capacity and generation grants fine-grained understanding. In this work, we present a Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts to unify the three pre-training objectives in one framework. Specifically, CoBIT employs a novel unicoder-decoder structure, consisting of an image unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders can switch between encoding and decoding in different tasks, enabling flexibility and shared knowledge that benefits both image-to-text and text-to-image gen
    
[^7]: 一种用于临床数据仓库中文档伪名化的自然语言处理算法的开发与验证

    Development and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse. (arXiv:2303.13451v1 [cs.CL])

    [http://arxiv.org/abs/2303.13451](http://arxiv.org/abs/2303.13451)

    该论文研究了如何解决临床报告的去识别化问题，提出了一种混合系统，能够将深度学习模型和手动规则的结果有效地合并，实现了对临床文档的高效伪名化，可以为研究目的提供数据，同时保护患者隐私。

    

    该研究的目的是解决临床报告的去识别化问题，以便为研究目的访问数据，同时确保患者隐私。研究突出了在此领域共享工具和资源所面临的困难，并介绍了大巴黎大学医院(AP-HP)在其临床数据仓库中实现文本文档系统伪名化的经验。我们根据12种识别实体注释了一组临床文档，并构建了一个混合系统，将深度学习模型和手动规则的结果合并。我们的结果显示了0.99的F1分数的整体性能。我们讨论了实施选择，并展示了更好地理解此任务所涉及的努力的实验，包括数据集大小、文档类型、语言模型或规则添加。我们在3条款BSD许可证下共享指南和代码。

    The objective of this study is to address the critical issue of de-identification of clinical reports in order to allow access to data for research purposes, while ensuring patient privacy. The study highlights the difficulties faced in sharing tools and resources in this domain and presents the experience of the Greater Paris University Hospitals (AP-HP) in implementing a systematic pseudonymization of text documents from its Clinical Data Warehouse. We annotated a corpus of clinical documents according to 12 types of identifying entities, and built a hybrid system, merging the results of a deep learning model as well as manual rules. Our results show an overall performance of 0.99 of F1-score. We discuss implementation choices and present experiments to better understand the effort involved in such a task, including dataset size, document types, language models, or rule addition. We share guidelines and code under a 3-Clause BSD license.
    
[^8]: 语义转换混淆AI生成文本检测，而检索是一种有效的防御方法

    Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])

    [http://arxiv.org/abs/2303.13408](http://arxiv.org/abs/2303.13408)

    本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。

    

    近期有多种方法被提出来用于识别恶意使用大型语言模型 (例如虚假内容创建或学术抄袭)中的AI生成文本，包括通过水印或统计异常点。本文探究这些文本检测算法对于AI生成文本的含义转换的鲁棒性。为了测试这些检测器的性能，我们首先训练了一个11B参数的语义转换生成模型(DIPPER)，该模型可以将段落进行语义转换，可选择利用周围文本(例如用户写的提示)作为上下文。DIPPER还使用标量旋钮来控制语义转换中词汇多样性和重新排列的程度。通过使用DIPPER来进行三种大型语言模型生成文本的语义转换，成功地混淆了多个文本检测器，包括水印检测、GPTZero、DetectGPT和OpenAI的文本分类器。例如，DIPPER将DetectGPT的检测准确率从70.3%降至4.6%（在恒定的1%误报率下）。

    To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
    
[^9]: 基于文本模型的组合零样本领域转移

    Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])

    [http://arxiv.org/abs/2303.13386](http://arxiv.org/abs/2303.13386)

    提出了一种组合转移学习框架，用于专业领域中的零样本领域转移，使用未标记的领域自由文本进行领域和任务知识的共同学习，通过 NLGU 策略实现领域数据增强和标签预测，经实验证明在生物医学领域和放射学子领域具有优异的性能，胜过了现有的 SOTA。

    

    在专业领域中，标签稀缺是提高任务性能的瓶颈。我们提出了一种新的组合转移学习框架（DoT5 领域组合零样本 T5），用于零样本领域转移。在没有访问领域标签的情况下，DoT5以多任务的方式共同学习领域知识（从未标记的领域自由文本的 MLM 中学习）和任务知识（从更容易获取的通用领域数据的任务训练中学习）。为了提高任务训练的可转移性，我们设计了一种名为 NLGU 的策略：我们同时为领域标签到数据生成训练 NLG，从而实现用于自我微调的数据增强和用于标签预测的 NLU 训练。我们在生物医学领域和放射学的资源贫乏子领域上评估了 DoT5，重点关注 NLI、文本摘要和嵌入学习。通过多任务学习，DoT5证明了组合转移学习的有效性，尤其是在零样本转移方面胜过了现有的 SOTA。

    Label scarcity is a bottleneck for improving task performance in specialised domains. We propose a novel compositional transfer learning framework (DoT5 domain compositional zero-shot T5) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train NLG for in-domain label-to-data generation which enables data augmentation for self-finetuning and NLU for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current SOTA in zero-shot transfer b
    
[^10]: 大语言模型在教育中的实际和伦理挑战：一项系统文献综述

    Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])

    [http://arxiv.org/abs/2303.13379](http://arxiv.org/abs/2303.13379)

    LLMs在教育中有自动生成和分析文本内容的潜力。然而，这些创新的实际性和伦理性存在担忧，需要考虑技术可行性、隐私、平等和善意等因素。

    

    基于大语言模型（LLMs）开发的教育技术创新显示出自动生成和分析文本内容的潜力。虽然已经开发了各种创新来自动化各种教育任务（例如，生成问题、提供反馈和评分），但对这些创新的实际性和伦理性存在担忧。这些担忧可能会阻碍未来研究和在真实教育环境中采用基于LLMs的创新。为了解决这个问题，我们对118篇自2017年以来发表的同行评议论文进行了系统的文献综述，以确定使用LLMs自动化和支持教育任务的当前研究状态。通过评估其技术可行性、模型性能、可复制性、系统透明度、隐私、平等和善意，还确定了LLMs创新的实际和伦理挑战。

    Educational technology innovations that have been developed based on large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic literature review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The practical and ethical challenges of LLMs-based innovations were also identified by assessing their technological readiness, model performance, replicability, system transparency, privacy, equality, and beneficence. The findings 
    
[^11]: GPT-4在医学挑战问题上的能力

    Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])

    [http://arxiv.org/abs/2303.13375](http://arxiv.org/abs/2303.13375)

    本论文对最先进的LLM——GPT-4在医学能力考试和基准数据集上进行了全面评估，结果显示其表现出色，有助于医学相关领域的研究和应用。

    

    大型语言模型（LLMs）已经在各个领域展示了惊人的自然语言理解和生成能力，包括医学。我们对一项最先进的LLM——GPT-4在医学能力考试和基准数据集上进行了全面评估。GPT-4是一个通用模型，没有经过针对医学问题的训练或设计用于解决临床任务。我们的分析涵盖了美国临床能力评估和授权考核计划（USMLE）的两组官方练习材料。我们还评估了在MultiMedQA基准数据集上的表现。除了测量模型的性能，还进行了实验来研究包含文本和图像的测试问题对模型性能的影响，探索训练期间内容记忆的可能性，并研究概率校准在高风险应用中的重要性。

    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications 
    
[^12]: 用ClimaText微调ClimateBERT transformer解析气候相关金融风险披露

    Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks. (arXiv:2303.13373v1 [cs.CL])

    [http://arxiv.org/abs/2303.13373](http://arxiv.org/abs/2303.13373)

    本文利用最新的自然语言处理技术，基于ClimaText数据集，微调ClimateBert transformer和BERT模型，成功检测气候变化相关句子，为金融监管机构和投资者提供更准确和一致的气候相关金融风险信息，从而有望支持更可持续的金融系统。

    

    近年来，金融机构，特别是个人和机构投资者，对公司报告气候相关金融风险的需求不断增长。为了识别这些类型的风险，公司可以在财务和非财务报告中短期内披露大量文本信息，特别是为了响应不断通过的规定。为此，本文应用最先进的自然语言处理技术来实现气候变化在文本语料库中的检测。我们使用迁移学习来微调两个变换器模型，BERT和ClimateBert，这是一个最近发布的基于DistillRoBERTa模型的模型，专门为气候文本分类量身定制。这两个算法基于变换器架构，能够学习文本中单词之间的上下文关系。我们在新的ClimaText数据集上对两个模型进行微调，该数据集由超过2000家公司披露其气候相关风险的报告组成。我们的实验表明，微调模型在识别ClimaText中与气候变化相关的句子方面优于原始模型。本研究的结果有潜力支持金融监管机构、投资者和其他利益相关者，为他们提供更准确、一致的关于气候相关金融风险的信息，从而为一个更可持续的金融系统做出贡献。

    In recent years there has been a growing demand from financial agents, especially from particular and institutional investors, for companies to report on climate-related financial risks. A vast amount of information, in text format, can be expected to be disclosed in the short term by firms in order to identify these types of risks in their financial and non financial reports, particularly in response to the growing regulation that is being passed on the matter. To this end, this paper applies state-of-the-art NLP techniques to achieve the detection of climate change in text corpora. We use transfer learning to fine-tune two transformer models, BERT and ClimateBert -a recently published DistillRoBERTa-based model that has been specifically tailored for climate text classification-. These two algorithms are based on the transformer architecture which enables learning the contextual relationships between words in a text. We carry out the fine-tuning process of both models on the novel Cl
    
[^13]: ChatGPT和新的学术现实：AI撰写的研究论文及大语言模型在学术出版中的伦理道德

    ChatGPT and a New Academic Reality: AI-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing. (arXiv:2303.13367v1 [cs.CL])

    [http://arxiv.org/abs/2303.13367](http://arxiv.org/abs/2303.13367)

    本文讨论了使用自然语言处理生成文本的ChatGPT模型，该技术被认为可以成为自动准备学术论文及手稿的潜在模型，然而，其与类似模型潜在的伦理问题需要考虑和解决。

    

    本文探讨了OpenAI的ChatGPT，这是一个使用自然语言处理来满足基于文本的用户请求（即聊天机器人）的生成式预训练转换器。讨论了ChatGPT及类似模型背后的历史和原则。然后讨论了这种技术对学术和学术研究出版物可能产生的影响。ChatGPT被视为自动准备论文和其他类型学术手稿的潜在模型。讨论了可能随着大型语言模型（如ChatGPT背后的基础技术GPT-3）的出现和其被学术界和研究人员使用而出现的伦理问题，将其置于人工智能、机器学习和自然语言处理在研究和学术出版方面的更广泛进展的背景下。

    This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer, which uses natural language processing to fulfill text-based user requests (i.e., a chatbot). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT-3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.
    
[^14]: 自然语言处理和机器学习在需求规范化中的应用：一篇系统综述

    Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])

    [http://arxiv.org/abs/2303.13365](http://arxiv.org/abs/2303.13365)

    本文调查了自然语言处理和机器学习在需求规范化中的应用现状，发现启发式NLP方法是自动RF中最常用的NLP技术，机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。

    

    软件开发方法的改进吸引了开发人员在需求工程领域自动化需求规范化（RF）中应用自然语言处理（NLP）和机器学习（ML），报告了应用NLP和ML在减少自然语言编写的需求不确定性和不完整性方面的潜在优势。本文的目标是调查和分类现有的NLP和ML在RF上的工作，识别该领域的挑战并提供有前途的未来研究方向。为了实现这一目标，我们进行了系统文献综述，选取了来自常用库的257篇论文。通过定义包含和排除标准来过滤搜索结果，并选择了47项相关研究，时间跨度在2012年至2022年之间。我们发现启发式NLP方法是自动RF中最常用的NLP技术，主要应用于结构化数据，而机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。

    Improvement of software development methodologies attracts developers to automatic Requirement Formalisation (RF) in the Requirement Engineering (RE) field. The potential advantages by applying Natural Language Processing (NLP) and Machine Learning (ML) in reducing the ambiguity and incompleteness of requirement written in natural languages is reported in different studies. The goal of this paper is to survey and classify existing work on NLP and ML for RF, identifying challenges in this domain and providing promising future research directions. To achieve this, we conducted a systematic literature review to outline the current state-of-the-art of NLP and ML techniques in RF by selecting 257 papers from common used libraries. The search result is filtered by defining inclusion and exclusion criteria and 47 relevant studies between 2012 and 2022 are selected. We found that heuristic NLP approaches are the most common NLP techniques used for automatic RF, primary operating on structured 
    
[^15]: 重新评估EmoWOZ中针对情感检测的数据划分

    Reevaluating Data Partitioning for Emotion Detection in EmoWOZ. (arXiv:2303.13364v1 [cs.CL])

    [http://arxiv.org/abs/2303.13364](http://arxiv.org/abs/2303.13364)

    本文重新评估了EmoWOZ数据集的数据划分，在此基础上提出了一种新的分层抽样方法用于处理高度不平衡和不均匀分布的情感标签。使用这个方法建立在EmoWoz上的模型表现更好，未来研究者应该采取这种划分以确保一致和准确的性能评估。

    

    本文聚焦于EmoWOZ数据集，该数据集是MultiWOZ数据集的扩展，提供了对话的情感标签。与原始的MultiWOZ数据集因其它目的被划分不同，EmoWOZ中情感标签高度不平衡，分布在不同划分中也不均匀，导致模型比较效果欠佳。为了解决这个问题、改善数据集的分布并减少数据集分布偏差，我们提出了一种基于情感标签的分层抽样方法，并引入了一种特殊技术来处理有多个情感标签的对话（序列）数据。使用我们提出的抽样方法，建立在EmoWoz上的模型可以表现更好，使它成为训练具有情感智能的对话代理更为可靠的资源。我们推荐未来的研究者使用这个新的数据集划分来确保一致和准确的性能评估。

    This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that provides emotion labels for the dialogues. MultiWOZ was partitioned initially for another purpose, resulting in a distributional shift when considering the new purpose of emotion recognition. The emotion tags in EmoWoz are highly imbalanced and unevenly distributed across the partitions, which causes sub-optimal performance and poor comparison of models. We propose a stratified sampling scheme based on emotion tags to address this issue, improve the dataset's distribution, and reduce dataset shift. We also introduce a special technique to handle conversation (sequential) data with many emotional tags. Using our proposed sampling method, models built upon EmoWoz can perform better, making it a more reliable resource for training conversational agents with emotional intelligence. We recommend that future researchers use this new partitioning to ensure consistent and accurate performance evaluations.
    
[^16]: 基于语言模型的合作性评估的可扩展性研究

    Towards the Scalable Evaluation of Cooperativeness in Language Models. (arXiv:2303.13360v1 [cs.CL])

    [http://arxiv.org/abs/2303.13360](http://arxiv.org/abs/2303.13360)

    本论文旨在对基于语言模型的合作性评估的可扩展性进行研究，通过生成特定博弈论结构场景并进行评估，不过目前生成质量较一般。

    

    预训练的语言模型（PLMs）驱动的AI系统可能越来越多地用于辅助人类进行涉及其他代理人的高 stakes 交互，例如协商或冲突解决。符合合作的AI的目标，我们希望以亲社会的方式理解和塑造PLM的多代理行为。一个重要的第一步是对模型在各种合作问题上行为的评估。由于交互中期望的行为取决于精确的博弈结构，我们专注于使用众包工人和语言模型生成特定结构的场景。我们的工作如下。首先，我们讨论了生成特定博弈论结构场景的关键方法问题。其次，我们使用众包工人和语言模型来生成这些场景。我们发现两种情况下的生成质量往往是中等水平。此外，我们获得了以下结论：

    It is likely that AI systems driven by pre-trained language models (PLMs) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. Consistent with the goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and shape the multi-agent behaviors of PLMs in a pro-social manner. An important first step is the evaluation of model behaviour across diverse cooperation problems. Since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. Our work proceeds as follows. First, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. Second, we employ both crowdworkers and a language model to generate such scenarios. We find that the quality of generations tends to be mediocre in both cases. We additionally get 
    
[^17]: 通过机器阅读理解的无法回答问题揭示越南语言模型的弱点

    Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension. (arXiv:2303.13355v1 [cs.CL])

    [http://arxiv.org/abs/2303.13355](http://arxiv.org/abs/2303.13355)

    本文通过机器阅读理解的无法回答问题的表现，揭示越南语言模型的弱点，并提出新方向。我们同时还发现现有越南机器阅读理解基准存在人工问题，需迫切寻求新的高质量基准评估进展。

    

    尽管多语言模型在单语言环境下的语言能力受到了很大限制，但研究人员仍然不得不依靠多语言模型来开发越南文机器阅读理解的最先进系统。这种研究困难是由于越南语言模型开发的高质量作品数量有限。为了鼓励在这个研究领域中进行更多的工作，我们使用机器阅读理解的下游任务，对当前越南单语言模型的语言弱点和优势进行了全面分析。从分析结果中，我们提出了发展越南语言模型的新方向。除了这个主要贡献外，我们还成功地揭示了越南机器阅读理解基准中存在的人工问题，并建议迫切需要新的高质量基准来跟踪越南机器阅读理解的进展。此外，我们还演示了越南机器阅读理解中无法回答的问题如何暴露语言建模的弱点，并评估模型的性能。

    Although the curse of multilinguality significantly restricts the language abilities of multilingual models in monolingual settings, researchers now still have to rely on multilingual models to develop state-of-the-art systems in Vietnamese Machine Reading Comprehension. This difficulty in researching is because of the limited number of high-quality works in developing Vietnamese language models. In order to encourage more work in this research field, we present a comprehensive analysis of language weaknesses and strengths of current Vietnamese monolingual models using the downstream task of Machine Reading Comprehension. From the analysis results, we suggest new directions for developing Vietnamese language models. Besides this main contribution, we also successfully reveal the existence of artifacts in Vietnamese Machine Reading Comprehension benchmarks and suggest an urgent need for new high-quality benchmarks to track the progress of Vietnamese Machine Reading Comprehension. Moreov
    
[^18]: DBLP-QuAD：DBLP学术知识图上的问答数据集

    DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])

    [http://arxiv.org/abs/2303.13351](http://arxiv.org/abs/2303.13351)

    这篇论文在DBLP学术知识图上创建了一个包含10000个问题-答案对的问答数据集，是最大的学术问答数据集。

    

    本文在DBLP学术知识图上创建了一个问答数据集。DBLP是一个在线计算机科学主要出版物的参考文献信息索引，索引了超过440万篇论文，由220万多位作者发表。我们的数据集包含了10000个问题-答案对以及相应的SPARQL查询，可以在DBLP KG上执行以获得正确的答案。DBLP-QuAD是最大的学术问答数据集。

    In this work we create a question answering dataset over the DBLP scholarly knowledge graph (KG). DBLP is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding SPARQL queries which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD is the largest scholarly question answering dataset.
    
[^19]: 利用基础模型进行临床文本分析

    Leveraging Foundation Models for Clinical Text Analysis. (arXiv:2303.13314v1 [cs.CL])

    [http://arxiv.org/abs/2303.13314](http://arxiv.org/abs/2303.13314)

    本研究提出了一个NLP框架，利用预训练的Transformer模型从临床数据中提取与传染病相关的关键信息，该方法在评估中优于标准方法。

    

    传染病是全球重要的公共卫生问题，从科学文献中提取相关信息可以促进有效的预防和治疗策略的制定。然而，大量的临床数据可用性会对信息抽取构成挑战。为了解决这个问题，本研究提出了一个自然语言处理（NLP）框架，使用预训练的Transformer模型在特定数据上进行微调，从自由文本临床数据中提取与传染病相关的关键信息。提出的框架包括三个组件：数据层用于从临床文本准备数据集，基础模型层用于实体提取，评估层用于性能分析。评估的结果表明，所提出的方法优于标准方法，并通过预训练的Transformer模型利用先前的知识使其有助于研究其他传染病。

    Infectious diseases are a significant public health concern globally, and extracting relevant information from scientific literature can facilitate the development of effective prevention and treatment strategies. However, the large amount of clinical data available presents a challenge for information extraction. To address this challenge, this study proposes a natural language processing (NLP) framework that uses a pre-trained transformer model fine-tuned on task-specific data to extract key information related to infectious diseases from free-text clinical data. The proposed framework includes three components: a data layer for preparing datasets from clinical texts, a foundation model layer for entity extraction, and an assessment layer for performance analysis. The results of the evaluation indicate that the proposed method outperforms standard methods, and leveraging prior knowledge through the pre-trained transformer model makes it useful for investigating other infectious disea
    
[^20]: SwissBERT：瑞士的多语言语言模型

    SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v1 [cs.CL])

    [http://arxiv.org/abs/2303.13310](http://arxiv.org/abs/2303.13310)

    该论文介绍了SwissBERT，它是一个专门为处理瑞士相关文本而创建的多语言语言模型，SwissBERT在与瑞士相关的自然语言理解任务上的效果优于以前的模型。

    

    我们介绍了SwissBERT，这是一个专门为处理与瑞士相关的文本而创建的掩码语言模型。 SwissBERT是一种预训练模型，我们将其调整为能够处理瑞士国家语言 -德语、法语、意大利语和罗曼什语的新闻文章。我们评估了SwissBERT在与瑞士相关的自然语言理解任务上的效果，发现它在这些任务上的表现往往优于以前的模型，特别是在处理当代新闻和/或罗曼什语格里斯昆时。由于SwissBERT使用语言适配器，因此未来的工作可能将其扩展到瑞士德语方言中。该模型和我们的开源代码公开发布在https://github.com/ZurichNLP/swissbert。

    We present SwissBERT, a masked language model created specifically for processing Switzerland-related text. SwissBERT is a pre-trained model that we adapted to news articles written in the national languages of Switzerland -German, French, Italian, and Romansh. We evaluate SwissBERT on natural language understanding tasks related to Switzerland and find that it tends to outperform previous models on these tasks, especially when processing contemporary news and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be extended to Swiss German dialects in future work. The model and our open-source code are publicly released at https://github.com/ZurichNLP/swissbert.
    
[^21]: GETT-QA：基于图嵌入的知识图谱问答中的T2T Transformer

    GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])

    [http://arxiv.org/abs/2303.13284](http://arxiv.org/abs/2303.13284)

    本论文提出了GETT-QA系统，该系统使用T5对自然语言问题生成简化的SPARQL查询，并使用截断的KG嵌入提高了知识图谱问答的性能。

    

    本文提出了一个名为GETT-QA的端到端知识图谱问答系统。GETT-QA使用了T5，这是一种热门的文本到文本预训练语言模型。该模型以自然语言形式的问题作为输入并生成所需SPARQL查询的简化形式。在简化形式中，模型不直接生成实体和关系ID，而是产生相应的实体和关系标签。标签在随后的步骤中与KG实体和关系ID联系起来。为了进一步改进结果，我们指导模型为每个实体生成KG嵌入的截断版本。截断的KG嵌入使得更精细的搜索从而更有效进行消歧。我们发现，T5能够在不改变损失函数的情况下学习截断的KG嵌入，提高了KGQA的性能。因此，我们在Wikidata的LC-QuAD 2.0和SimpleQuestions-Wikidata数据集上报告了端到端KGQA的强大结果。

    In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.
    
[^22]: 基于知识引导的上下文优化的视觉语言提示调整

    Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. (arXiv:2303.13283v1 [cs.CV])

    [http://arxiv.org/abs/2303.13283](http://arxiv.org/abs/2303.13283)

    提出一种基于知识引导的上下文优化方法，通过降低可学习提示与手工提示之间的差异来解决具有特定文本知识的模型在未知类别上泛化能力差的问题。

    

    提示调整是将预训练的视觉语言模型(VLM)适应于下游任务的有效方法，使用与任务相关的文本标记。然而，由于缺少强大的泛化能力的通用文本知识，具有特定文本知识的代表性CoOp工作在未见过的类别上的泛化能力较差。为了解决这个问题，我们引入了一种新的基于知识引导的上下文优化(KgCoOp)来增强对未知类别的适应能力。KgCoOp的关键见解是，可以通过降低可学习提示与手工提示之间的差异来缓解遗忘重要知识的问题。特别是，KgCoOp最小化通过学习提示生成的文本嵌入和手工制作提示之间的差异。最后，在对比损失上添加KgCoOp。

    Prompt tuning is an effective way to adapt the pre-trained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is the worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that forgetting about essential knowledge can be alleviated by reducing the discrepancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss ca
    
[^23]: 使用适配器的参数高效稀疏检索器和重排器

    Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])

    [http://arxiv.org/abs/2303.13220](http://arxiv.org/abs/2303.13220)

    本文研究了在信息检索中使用适配器的效果，特别是在SPLADE这种稀疏检索器上。研究表明，适配器-SPLADE只需优化2%的训练参数，但在效率和效果方面均优于完全微调的方法。

    

    使用适配器的参数高效迁移学习已在自然语言处理（NLP）中研究作为完全微调的替代方法。适配器是内存高效的，并通过在变压器层之间添加小瓶颈层进行训练，同时保持大型预训练语言模型（PLMs）冻结来与下游任务良好地进行缩放。尽管在NLP中表现出有希望的结果，但这些方法在信息检索方面尚未得到充分探索。本文旨在完善适配器在IR中的使用情况。首先，我们研究了适配器对于SPLADE（一种稀疏检索器）的应用，适配器不仅保留了通过完全微调实现的效率和效果，而且内存高效，训练轻量级。我们观察到，适配器-SPLADE仅优化2％的训练参数，但胜过完全微调的对应物以及已有的最佳稀疏检索器。

    Parameter-Efficient transfer learning with Adapters have been studied in Natural Language Processing (NLP) as an alternative to full fine-tuning. Adapters are memory-efficient and scale well with downstream tasks by training small bottle-neck layers added between transformer layers while keeping the large pretrained language model (PLMs) frozen. In spite of showing promising results in NLP, these methods are under-explored in Information Retrieval. While previous studies have only experimented with dense retriever or in a cross lingual retrieval scenario, in this paper we aim to complete the picture on the use of adapters in IR. First, we study adapters for SPLADE, a sparse retriever, for which adapters not only retain the efficiency and effectiveness otherwise achieved by finetuning, but are memory-efficient and orders of magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes just 2\% of training parameters, but outperforms fully fine-tuned counterpart and exis
    
[^24]: 大型语言模型的公正引导少样本提示

    Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])

    [http://arxiv.org/abs/2303.13217](http://arxiv.org/abs/2303.13217)

    本文提出了一种新的搜索策略-FairPrompt，在保证公正性的前提下，通过评估提示预测偏差，确定近似最优的提示，从而改进大型语言模型的上下文学习性能，实验表明该方法在准确性和公正性方面均优于现有方法。

    

    大型语言模型已经表现出惊人的能力，能够通过几个输入输出示例构建的提示进行直接应用来解决众多下游任务。但是，先前的研究表明，由于训练示例，示例顺序和提示格式的变化导致上下文学习容易出现高度不稳定性。因此，构建适当的提示对于改进上下文学习的性能至关重要。在这篇文章中，我们从预测偏差的角度重新探讨了这个问题。具体而言，我们引入了一个指标来评估固定提示相对于标签或给定属性的预测偏差。然后我们通过实验证明了预测偏差较大的提示总是导致不令人满意的预测质量。基于这个观察，我们提出了一种新的搜索策略，基于贪婪搜索来确定近似最优的提示，从而改进上下文学习的性能。我们提出的方法叫做"公正提示"，其中融入了公平性约束，以指导搜索不展现出对某些人群的偏见。我们在多种少样本分类任务上证明了FairPrompt的有效性，并展示了它在准确性和公正性方面均优于现有的最先进方法。

    Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
    
[^25]: 利用列表译码解释大语言模型中的相变现象

    A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])

    [http://arxiv.org/abs/2303.13112](http://arxiv.org/abs/2303.13112)

    本文提供了一个对大语言模型中相变现象的简单解释，利用列表译码器建模，它能够保证在LLM低于临界阈值时错误候选序列数的期望保持有界，而在高于该阈值时呈指数增长。

    

    最近的实验结果表明，大语言模型（LLM）呈现出小模型所没有的突出能力。当模型达到一定的规模关键点时，系统性能得到了极大地提高。在这篇文章中，我们提供了一个简单的解释，并将LLM建模为一个序列到序列的随机函数。我们使用列表译码器代替每个步骤的即时生成，该译码器在每个步骤保留一个候选序列列表，并在结束时推迟输出序列的生成。我们表明，存在一个临界阈值，当LLM低于此阈值时，期望的错误候选序列数保持有界，当LLM高于此阈值时，期望错误序列数呈指数增长。这样的阈值与传染病的基本繁殖数有关。

    Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function. Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of erroneous candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.
    
[^26]: 多视角的零样本开放意图归纳：多领域批处理和代理梯度转移

    Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])

    [http://arxiv.org/abs/2303.13099](http://arxiv.org/abs/2303.13099)

    本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。

    

    在任务导向的对话系统中，检测和诱导新的意图是将该系统应用于实际应用的两个主要挑战。本文提出了语义多视角模型来解决这两个难题：（1）用于一般嵌入的SBERT（2）多领域批处理（MDB）用于对话领域知识，以及（3）用于集群专业语义的代理梯度转移（PGT）。 MDB一次向模型提供多种对话数据集，通过学习多领域知识来解决多领域问题。我们引入了一种新的方法PGT，它采用Siamese网络直接使用聚类方法微调模型。我们的模型可以学习如何使用PGT聚类对话语句。实验结果表明，与基线系统相比，我们的多视角模型与MDB和PGT显着提高了Open Intent Induction的性能。

    In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
    
[^27]: 超越通用Transformer：自适应模块在ASR中的Transformer模型中的模块重用

    Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit. (arXiv:2303.13072v1 [cs.SD])

    [http://arxiv.org/abs/2303.13072](http://arxiv.org/abs/2303.13072)

    本论文提出一种用于ASR中的Transformer模型的块重用策略并配以适配器模块，使得模型更加紧凑，可适应性更强，准确性更高。

    

    基于Transformer的模型在端到端的自动语音识别（ASR）应用中取得了重要进展。 Transformer模型使得能够在智能设备上部署端到端的ASR系统。但是，这些模型仍有一个缺点，即需要大量的模型参数。为了克服通用Transformer模型在边缘设备上应用ASR的缺点，我们提出了一种解决方案，可以重用Transformer模型中的块作为小尺寸ASR系统使用的设计策略，既满足资源限制的目标，又不会影响识别准确率。具体来说，我们设计了一种新的用于语音Transformer（BRST）的块重用策略来增强参数的有效性，并提出了一个适配器模块（ADM），可以生成一个只需要少量可训练参数的紧凑和可适应的模型来确保每个重用块的陪伴。我们在Aishell-1和CommonVoice检验数据集上进行了实验，并证明了BRST对准确性、可扩展性等方面的优越性。

    Transformer-based models have recently made significant achievements in the application of end-to-end (E2E) automatic speech recognition (ASR). It is possible to deploy the E2E ASR system on smart devices with the help of Transformer-based models. While these models still have the disadvantage of requiring a large number of model parameters. To overcome the drawback of universal Transformer models for the application of ASR on edge devices, we propose a solution that can reuse the block in Transformer models for the occasion of the small footprint ASR system, which meets the objective of accommodating resource limitations without compromising recognition accuracy. Specifically, we design a novel block-reusing strategy for speech Transformer (BRST) to enhance the effectiveness of parameters and propose an adapter module (ADM) that can produce a compact and adaptable model with only a few additional trainable parameters accompanying each reusing block. We conducted an experiment with the
    
[^28]: 利用解耦表示的检索增强分类

    Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v1 [cs.CL])

    [http://arxiv.org/abs/2303.13065](http://arxiv.org/abs/2303.13065)

    本文提出了一个混合粒度的中文BERT（MigBERT），利用同时考虑字符和词汇的表示方式，提高了中文PLMs的表现，并在各种中文NLP任务中取得了新的SOTA性能。单词比字符语义更丰富。数字也显示了MigBERT可以在日语中使用。

    

    预训练语言模型（PLM）在各种NLP任务中显示出显着的改进。大多数中文PLM将输入文本视为字符序列，并完全忽略词信息。虽然整词屏蔽可以缓解这一问题，但词汇中的语义仍然没有得到很好的表示。在本文中，我们重新审视了中文PLM的分词粒度。我们通过同时考虑字符和词汇，提出了一个混合粒度的中文BERT（MigBERT）。为了实现这一点，我们设计了用于学习字符和单词级表示的目标函数。我们对各种中文NLP任务进行了广泛的实验，以评估现有PLM以及所提出的MigBERT。实验结果表明，MigBERT在所有这些任务上均实现了新的SOTA性能。进一步的分析表明，单词比字符语义更丰富。更有趣的是，我们展示了MigBERT也可以与日语一起使用。

    Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code has been released here~\footnote{\url{https://g
    
[^29]: SPeC：软提示校准在临床笔记摘要中降低性能变异的研究

    SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])

    [http://arxiv.org/abs/2303.13035](http://arxiv.org/abs/2303.13035)

    研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型.

    

    电子健康记录（EHR）存储着包括病历、诊断、治疗和检测结果在内的大量患者信息。这些记录对于医疗保健专业人员做出明智的患者护理决策非常关键。摘要临床笔记可以帮助医疗保健专业人员更好地发现潜在健康风险，以及做出更好的决策。这一过程通过确保医疗保健专业人员可以访问最相关和最新的患者数据，有助于减少错误并提高患者的护理效果。最近的研究表明，将提示与大语言模型（LLM）相结合可以显著提高摘要任务的效率。然而，我们发现这种方法也会导致输出方差增加，即使提示意义相似，输出也会有明显的差异。为了解决这一挑战，我们引入了一个模型无关的软提示校准（SPeC）流程，该流程采用软提示嵌入来减轻输入变量对输出多样性的影响。我们的实验表明，SPeC不仅可以降低LLM的性能变异，而且在临床笔记摘要任务上优于现有的最先进模型。

    Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
    
[^30]: GesGPT: 基于 GPT 文本解析的语音手势合成

    GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])

    [http://arxiv.org/abs/2303.13013](http://arxiv.org/abs/2303.13013)

    GesGPT 是一种新的语音手势生成方法，它利用大型语言模型 GPT 的语义分析能力，从文本输入中提取与手势相关的信息，并使用手势库和集成模块生成意义丰富的语音手势。

    

    手势合成作为重要的研究领域，旨在生成上下文相关和自然的手势来对应语音或文本输入。尽管基于深度学习的方法取得了显著进展，但它们通常忽略了文本中丰富的语义信息，导致手势不够表达和意义不够丰富。我们提出了 GesGPT，这是一种利用大型语言模型（LLM），如 GPT 的语义分析能力进行手势生成的新方法。通过利用 LLM 在文本分析方面的强大能力，我们设计提示来从文本输入中提取与手势相关的信息。我们的方法包括开发提示原则，将手势生成转化为基于 GPT 的意图分类问题，并利用精心策划的手势库和集成模块生成语义丰富的语音手势。实验结果表明，GesGPT 可以有效地针对语音或文本输入生成上下文相关的、有意义的手势。

    Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates conte
    
[^31]: ChatGPT是一款好的关键词生成器吗？初步研究。

    Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])

    [http://arxiv.org/abs/2303.13001](http://arxiv.org/abs/2303.13001)

    本文对ChatGPT作为关键词生成器进行了初步研究，发现其在各个方面的性能表现良好，特别是在多领域关键词生成方面。ChatGPT仍面临生成缺失关键词的挑战。

    

    ChatGPT的出现引起了计算语言学界的重视。为了展示其作为关键词生成器的能力，我们对ChatGPT进行了初步评估以用于关键词生成任务。我们评估了其在各个方面的性能，包括关键词生成提示，关键词生成多样性，多领域关键词生成和长文本理解。我们的评估基于六个基准数据集，并采用OpenAI建议的提示，并将其扩展为六个候选提示。我们发现ChatGPT在所有六个候选提示上表现出色，在不同数据集之间观察到了轻微的性能差异。基于我们的发现，我们得出结论，ChatGPT有很大的关键词生成潜力。此外，我们发现ChatGPT在生成缺失关键词方面仍面临挑战。最后，在最后一节中，我们还介绍了一些限制和未来的研究方向。

    The emergence of ChatGPT has recently garnered significant attention from the computational linguistics community. To demonstrate its capabilities as a keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the keyphrase generation task. We evaluate its performance in various aspects, including keyphrase generation prompts, keyphrase generation diversity, multi-domain keyphrase generation, and long document understanding. Our evaluation is based on six benchmark datasets, and we adopt the prompt suggested by OpenAI while extending it to six candidate prompts. We find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets. Based on our findings, we conclude that ChatGPT has great potential for keyphrase generation. Moreover, we discover that ChatGPT still faces challenges when it comes to generating absent keyphrases. Meanwhile, in the final section, we also present some limitations and futu
    
[^32]: 深度上下文化语言表示对文本分类可泛化性的分析

    Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification. (arXiv:2303.12936v1 [cs.CL])

    [http://arxiv.org/abs/2303.12936](http://arxiv.org/abs/2303.12936)

    本研究通过使用“交叉环境”设置，评估了ELMo和DistilBERT在新闻分类和情感分析任务上的监督学习性能表现。虽然两种模型超过传统基线，但在跨环境测试中表现较差，暴露了现代自然语言处理系统的代表能力极限。

    

    本研究评估了两种最先进的深度上下文语言表示——ELMo和DistilBERT在二元抗议新闻分类和产品评论情感分析的监督学习中的稳健性，采用了“交叉环境”设置，使用测试集不同于训练数据。具体来说，在新闻分类任务中，这些模型是在印度本地新闻上开发的，并在中国本地新闻上进行测试。在情感分析任务中，这些模型是在电影评论方面进行训练的，而在客户评论方面进行测试的。本次比较旨在探索当今自然语言处理系统的代表能力极限，以达到通用于真实场景的系统。模型经过微调，使用前馈神经网络和双向长短期记忆网络。多项式朴素贝叶斯和线性支持向量机用作传统基线。结果表明，ELMo和DistilBERT表现优于传统基线，但在跨环境测试中的表现较差。

    This study evaluates the robustness of two state-of-the-art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification and sentiment analysis of product reviews. A "cross-context" setting is enabled using test sets that are distinct from the training data. Specifically, in the news classification task, the models are developed on local news from India and tested on the local news from China. In the sentiment analysis task, the models are trained on movie reviews and tested on customer reviews. This comparison is aimed at exploring the limits of the representative power of today's Natural Language Processing systems on the path to the systems that are generalizable to real-life scenarios. The models are fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector Machine are used as traditional baselines. The results show that, i
    
[^33]: 探索医学文本到SQL模型和数据集泛化的理解

    Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets. (arXiv:2303.12898v1 [cs.CL])

    [http://arxiv.org/abs/2303.12898](http://arxiv.org/abs/2303.12898)

    本论文探索了医学文本到SQL生成的泛化问题，并展示了当前的数据集和模型准确率不足以满足临床需求，需要更进一步的研究和改进。

    

    电子病历（EMR）存储在关系型数据库中。如果用户不熟悉数据库架构或数据库基础知识，则访问所需信息可能具有挑战性。因此，研究人员探索了文本到SQL生成方法，为医疗保健专业人员提供直接访问EMR数据，而无需数据库专家。然而，当前可用的数据集已被“解决”，最先进的模型已获得了大于或接近90%的准确度。本文中，我们展示了在医学领域解决文本到SQL生成仍有很长的路要走。为了证明这一点，我们创建了现有医学文本到SQL数据集MIMICSQL的新数据分割，更好地衡量了结果模型的泛化能力。我们评估了最先进的语言模型在我们的新数据分割上的表现，结果表明准确度从最高达92%降至28%，因此表明有大量改进的空间。

    Electronic medical records (EMRs) are stored in relational databases. It can be challenging to access the required information if the user is unfamiliar with the database schema or general database fundamentals. Hence, researchers have explored text-to-SQL generation methods that provide healthcare professionals direct access to EMR data without needing a database expert. However, currently available datasets have been essentially "solved" with state-of-the-art models achieving accuracy greater than or near 90%. In this paper, we show that there is still a long way to go before solving text-to-SQL generation in the medical domain. To show this, we create new splits of the existing medical text-to-SQL dataset MIMICSQL that better measure the generalizability of the resulting models. We evaluate state-of-the-art language models on our new split showing substantial drops in performance with accuracy dropping from up to 92% to 28%, thus showing substantial room for improvement. Moreover, w
    
[^34]: 用于临床叙述分类的小规模交换变压器和基于NLP的模型

    A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])

    [http://arxiv.org/abs/2303.12892](http://arxiv.org/abs/2303.12892)

    本研究提出了一个简化的Switch Transformer框架，并从头开始训练，取得了在小型法语临床文本分类任务中比预训练的BERT模型更好的效果，采用Switch Transformer的专家混合机制有助于提高识别准确度，最终在测试集上实现了87％的准确率、87％的精度和86％的召回率。

    

    近年来，基于变压器的模型（如交换变压器）在自然语言处理任务中取得了显著的结果。然而，这些模型通常过于复杂并需要大量的预训练，这限制了它们在有限数据的小型临床文本分类任务中的有效性。在本研究中，我们提出了一个简化的Switch Transformer框架，并从头开始在CHU Sainte-Justine医院的小型法语临床文本分类数据集上进行了训练。我们的结果表明，简化的小规模变压器模型优于预训练的BERT模型，包括DistillBERT、CamemBERT、FlauBERT和FrALBERT。此外，使用Switch Transformer的专家混合机制有助于捕获多样的模式；因此，所提出的方法比具有自我注意机制的传统变压器获得更好的结果。最后，我们提出的框架在测试集上实现了87％的准确率，87％的精度和86％的召回率，突显了其在小型临床文本分类任务中的潜力。

    In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\%, precision at 87\%, and recall 
    
[^35]: JaCoText: 一种用于Java代码文本生成的预训练模型

    JaCoText: A Pretrained Model for Java Code-Text Generation. (arXiv:2303.12869v1 [cs.CL])

    [http://arxiv.org/abs/2303.12869](http://arxiv.org/abs/2303.12869)

    JaCoText是一种使用Transformer神经网络从自然语言文本生成Java源代码的预训练模型。

    

    基于预训练的transformer模型在自然语言生成任务中表现出高性能。然而，一股新的兴趣浪潮涌起：自动生成编程语言。这项任务包括将自然语言指令转换为编程代码。本文介绍了JaCoText，这是一种基于Transformers神经网络的模型，旨在从自然语言文本生成Java源代码。

    Pretrained transformer-based models have shown high performance in natural language generation task. However, a new wave of interest has surged: automatic programming language generation. This task consists of translating natural language instructions to a programming code. Despite the fact that well-known pretrained models on language generation have achieved good performance in learning programming languages, effort is still needed in automatic code generation. In this paper, we introduce JaCoText, a model based on Transformers neural network. It aims to generate java source code from natural language text. JaCoText leverages advantages of both natural language and code generation models. More specifically, we study some findings from the state of the art and use them to (1) initialize our model from powerful pretrained models, (2) explore additional pretraining on our java dataset, (3) carry out experiments combining the unimodal and bimodal data in the training, and (4) scale the i
    
[^36]: 用于时间理解的显著性跨度掩蔽

    Salient Span Masking for Temporal Understanding. (arXiv:2303.12860v1 [cs.CL])

    [http://arxiv.org/abs/2303.12860](http://arxiv.org/abs/2303.12860)

    本文介绍了一种用于时间理解的显著性跨度掩蔽技术，通过引入Temporal Span Masking中间训练并与Salient Span Masking结合使用，有效提高多个时间任务的性能及表示效果。

    

    显著性跨度掩蔽 (SSM) 已经被证明是提高封闭式问答性能的有效策略。 SSM通过创建额外的无监督训练句子对普通遮蔽语言模型进行扩展，这些句子屏蔽了一个实体或日期跨度，从而过度取样了事实信息。 尽管这种范式很成功，但跨度类型和采样策略相对任意，并且不被广泛研究用于其他任务。 因此，我们从时间任务的角度研究了SSM，在这些任务中学习各种时间表达的良好表示非常重要。 为此，我们引入了中间培训Temporal Span Masking (TSM)。首先，我们发现仅使用SSM就可以平均改善三个时间任务的下游性能5.8个点。此外，我们通过添加TSM任务能够实现额外的改进（平均+0.29个点）。这些是目标任务报告的新最佳结果。我们的分析表明，SSM和TSM策略的效果对于多个时间任务是通用的，并且可以相互补充。

    Salient Span Masking (SSM) has shown itself to be an effective strategy to improve closed-book question answering performance. SSM extends general masked language model pretraining by creating additional unsupervised training sentences that mask a single entity or date span, thus oversampling factual information. Despite the success of this paradigm, the span types and sampling strategies are relatively arbitrary and not widely studied for other tasks. Thus, we investigate SSM from the perspective of temporal tasks, where learning a good representation of various temporal expressions is important. To that end, we introduce Temporal Span Masking (TSM) intermediate training. First, we find that SSM alone improves the downstream performance on three temporal tasks by an avg. +5.8 points. Further, we are able to achieve additional improvements (avg. +0.29 points) by adding the TSM task. These comprise the new best reported results on the targeted tasks. Our analysis suggests that the effec
    
[^37]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^38]: LLM是万能的大师吗？探索LLM的领域不可知推理技能。

    Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])

    [http://arxiv.org/abs/2303.12810](http://arxiv.org/abs/2303.12810)

    本文探究了大型语言模型(LLM)在不同领域推理任务上的表现，并发现LLM在类比和道德推理方面表现出色，在空间推理任务上表现较差。这对于LLM未来的发展具有重要意义。

    

    大型语言模型(LLM)类似于人类推理的潜力一直是机器学习界争议最激烈的话题之一。然而，人类的推理能力是多方面的，可以通过各种形式进行体现，包括类比、空间和道德推理等。这一事实引发了一个问题，LLM能否在所有这些不同领域中同样表现出色。本研究旨在通过直接使用或从现有类比和空间推理数据集中汲取启示，对LLM在不同推理任务上的表现进行研究。此外，为了评估LLM像人类一样推理的能力，研究还对更开放、自然的语言问题进行了评估。我的研究结果表明，LLM在类比和道德推理方面表现出色，但在空间推理任务上表现得不够熟练。我认为这些实验对于推动LLM未来的发展，特别是在改进空间推理能力方面具有重要意义。

    The potential of large language models (LLMs) to reason like humans has been a highly contested topic in Machine Learning communities. However, the reasoning abilities of humans are multifaceted and can be seen in various forms, including analogical, spatial and moral reasoning, among others. This fact raises the question whether LLMs can perform equally well across all these different domains. This research work aims to investigate the performance of LLMs on different reasoning tasks by conducting experiments that directly use or draw inspirations from existing datasets on analogical and spatial reasoning. Additionally, to evaluate the ability of LLMs to reason like human, their performance is evaluted on more open-ended, natural language questions. My findings indicate that LLMs excel at analogical and moral reasoning, yet struggle to perform as proficiently on spatial reasoning tasks. I believe these experiments are crucial for informing the future development of LLMs, particularly 
    
[^39]: PACO: 包含行动、文化和压迫的挑衅

    PACO: Provocation Involving Action, Culture, and Oppression. (arXiv:2303.12808v1 [cs.CL])

    [http://arxiv.org/abs/2303.12808](http://arxiv.org/abs/2303.12808)

    该研究利用现有的印度WhatsApp帖子数据集，创造了一个可以从WhatsApp帖子中识别挑衅句子的模型PACO，并利用该模型可以防止可能的歧视或暴力事件。

    

    在印度，人们根据某些属性（如宗教）认同于特定群体，同一宗教群体经常相互挑衅。之前的研究表明，挑衅在增加印度两个主要宗教群体之间的紧张关系方面扮演着重要角色。随着互联网的出现，这种挑衅也出现在WhatsApp等社交媒体平台上。通过利用现有的印度WhatsApp帖子数据集，我们识别出了三种针对印度穆斯林的挑衅句子类别。此外，我们为三种挑衅类别标记了7000个句子，并将其称为PACO数据集。我们利用PACO来训练一个可以从WhatsApp帖子中识别挑衅句子的模型。我们的最佳模型是精调RoBERTa，并在五倍交叉验证中实现了0.851的平均AUC分数。自动识别挑衅句子可以阻止挑衅文本扩散到群众之间，可以防止可能的歧视或暴力事件。

    In India, people identify with a particular group based on certain attributes such as religion. The same religious groups are often provoked against each other. Previous studies show the role of provocation in increasing tensions between India's two prominent religious groups: Hindus and Muslims. With the advent of the Internet, such provocation also surfaced on social media platforms such as WhatsApp.  By leveraging an existing dataset of Indian WhatsApp posts, we identified three categories of provoking sentences against Indian Muslims. Further, we labeled 7,000 sentences for three provocation categories and called this dataset PACO. We leveraged PACO to train a model that can identify provoking sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and achieved a 0.851 average AUC score over five-fold cross-validation. Automatically identifying provoking sentences could stop provoking text from reaching out to the masses, and can prevent possible discrimination or viol
    
[^40]: 使用自然语言处理进行特征匹配

    Features matching using natural language processing. (arXiv:2303.12804v1 [cs.DB])

    [http://arxiv.org/abs/2303.12804](http://arxiv.org/abs/2303.12804)

    本文提出了一种使用自然语言处理进行特征匹配的新混合模型，它可以减少匹配不同数据集所需的时间。

    

    特征匹配是匹配不同数据集的基本步骤。本文提出了一种新的混合模型，该模型由预训练的基于自然语言处理（NLP）的BERT模型与基于Jaccard相似度的统计模型并行使用，以测量两个不同数据集中特征列表之间的相似性。这减少了搜索相关性或手动匹配每个数据集中的特征所需的时间。

    The feature matching is a basic step in matching different datasets. This article proposes shows a new hybrid model of a pretrained Natural Language Processing (NLP) based model called BERT used in parallel with a statistical model based on Jaccard similarity to measure the similarity between list of features from two different datasets. This reduces the time required to search for correlations or manually match each feature from one dataset to another.
    
[^41]: 使用预训练模型进行抽象文本摘要的分析

    An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])

    [http://arxiv.org/abs/2303.12796](http://arxiv.org/abs/2303.12796)

    本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明......

    

    人们现在使用像谷歌、雅虎和必应这样的搜索引擎来查找互联网上的信息。由于数据爆炸，如果为用户提供相关的搜索结果摘要而不仅仅是网页链接将会很有帮助。文本摘要已成为帮助用户迅速掌握大量信息的关键方法。在本文中，对不同的预训练模型进行了在不同数据集上的评估。具体来说，我们使用了三个不同的预训练模型，分别是google/pegasus-cnn-dailymail、T5-base、facebook/bart-large-cnn。我们考虑了三个不同的数据集，分别是CNN-dailymail、SAMSum和BillSum，以从上述三个模型中获取输出。通过ROUGH和BLEU指标，在这些不同的数据集上比较了这些预训练模型，每个数据集有2000个示例。

    People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.
    
[^42]: 基于命名实体识别的研究亮点自动生成技术研究

    Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])

    [http://arxiv.org/abs/2303.12795](http://arxiv.org/abs/2303.12795)

    该研究使用命名实体识别技术自动生成研究亮点，探究其是否能提高生成亮点的质量。实验结果表明，增加命名实体信息可以提高生成亮点的性能。

    

    传统科学论文摘要用于总结论文内容。近期，研究亮点作为摘要的补充，聚焦于论文的主要发现，但使用频率还不如摘要普遍。该研究旨在使用论文不同部分的输入，自动生成研究亮点。研究使用命名实体识别技术，探究它能否改进生成研究亮点的质量。研究使用两个深度学习模型：第一个是指针-生成器网络，第二个在第一个模型的基础上增加了覆盖机制。 然后将上述每个模型与命名实体识别特征相结合。该方法可用于为缺少亮点的论文生成亮点。实验结果显示，增加命名实体信息可以提高深度学习模型生成高质量研究亮点的性能。

    A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learn
    
[^43]: cTBL：增强大型语言模型用于对话表格

    cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])

    [http://arxiv.org/abs/2303.12024](http://arxiv.org/abs/2303.12024)

    本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。

    

    多模态对话人工智能中一个开放的挑战是如何从文本和非文本来源中增强大型语言模型以进行多轮对话。为了解决这个问题，本文引入了Conversation Table (cTBL)，这是一种三步编码器-解码器方法，用于检索表格信息并生成基于检索信息的对话响应。cTBL使用转换器编码器嵌入进行浓密表检索，并在HyrbiDialogue数据集Top-1和Top-3准确性上相对于稀疏检索提高了最多5%。此外，cTBL使用编码器和解码器模型进行表格知识检索，在HyrbiDialogue上产生了最高46%的ROUGE分数相对改进，并实现了更好的人工评估响应生成。

    An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
    
[^44]: Memotion 3: 代表印度-英语混合码的情感与情绪分析的互联网模因数据集

    Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])

    [http://arxiv.org/abs/2303.09892](http://arxiv.org/abs/2303.09892)

    Memotion 3是一个包含10,000个已注释模因的新数据集，引入了印度-英语混合模因，使其成为该领域内首个相应的数据集。此数据集可用于情感和情绪分析，并可用于对社交媒体上的虚假信息或仇恨内容进行研究。

    

    模因是现今社交媒体上传达幽默的新型机制。模因通常包含图片和一些文本。模因可被用于传播虚假信息或仇恨，因此对其进行详细的研究非常关键。我们介绍了Memotion 3，这是一个包含10,000个已注释模因的新数据集。与领域内其他普遍的数据集不同，包括之前的Memotion，Memotion 3引入了印度-英语混合模因，而之前的研究仅限于英语模因。我们描述了Memotion任务、数据收集和数据集创建方法。我们还为任务提供了一个基准。基准代码和数据集将在 https://github.com/Shreyashm16/Memotion-3.0 上提供。

    Memes are the new-age conveyance mechanism for humor on social media sites. Memes often include an image and some text. Memes can be used to promote disinformation or hatred, thus it is crucial to investigate in details. We introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other prevalent datasets in the domain, including prior iterations of Memotion, Memotion 3 introduces Hindi-English Codemixed memes while prior works in the area were limited to only the English memes. We describe the Memotion task, the data collection and the dataset creation methodologies. We also provide a baseline for the task. The baseline code and dataset will be made available at https://github.com/Shreyashm16/Memotion-3.0
    
[^45]: 总结过去以预测未来：自然语言对场景的描述促进多模态对象交互

    Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.09209](http://arxiv.org/abs/2301.09209)

    本文提出了一种TransFusion架构，利用先前训练的图像字幕和视觉语言模型总结动作上下文，实现对多模态对象交互的预测，有效性得到验证。

    

    本论文针对自我中心视频中的对象交互预测进行了研究。该任务需要理解先前对对象执行的动作所形成的时空上下文，称为动作上下文。我们提出了一种基于多模态transformer的架构TransFusion。它利用语言的表达能力，对动作上下文进行总结。TransFusion利用预先训练的图像字幕和视觉语言模型从过去的视频帧中提取动作上下文。将这个动作上下文与下一个视频帧一起经过多模态融合模块进行处理，从而预测下一个对象交互。我们的模型实现了更高效的端到端学习，大型预训练语言模型则增加了通用性和泛化能力。在Ego4D和EPIC-KITCHENS-100上的实验证实了我们的多模态融合模型的有效性。同时，也凸显了在一个视觉似乎足够的任务中使用基于语言的上下文摘要的好处。我们的方法胜过了现有的方法。

    We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatiotemporal context formed by past actions on objects, coined action context. We propose TransFusion, a multimodal transformer-based architecture. It exploits the representational power of language by summarising the action context. TransFusion leverages pre-trained image captioning and vision-language models to extract the action context from past video frames. This action context together with the next video frame is processed by the multimodal fusion module to forecast the next object interaction. Our model enables more efficient end-to-end learning. The large pre-trained language models add common sense and a generalisation capability. Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our multimodal fusion model. They also highlight the benefits of using language-based context summaries in a task where vision seems to suffice. Our method outperforms state-
    
[^46]: 使用上下文向量和图形装配改进词嵌入

    Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.16848](http://arxiv.org/abs/2210.16848)

    本文提出了一种改进词嵌入的方法，分别为将更多上下文信息纳入Skip-gram框架和提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法。这两种方法经由外部和内部任务的检验，能够大幅度超越基准线。

    

    尽管从大型预训练模型生成的上下文化嵌入在许多任务中表现良好，但由于计算成本低、部署便捷、稳定性高，传统的静态嵌入（例如Skip-gram、Word2Vec）仍在低资源和轻量级环境中发挥着重要作用。本文旨在通过以下方法改进词嵌入：1）将更多从现有预训练模型中获得的上下文信息纳入Skip-gram框架中，我们称之为Context-to-Vec；2）提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法，独立于训练。通过外部和内部任务，我们的方法被证明能够以大幅超越基准线的性能表现。

    Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.
    
[^47]: 自监督模型与多任务学习相结合的发音障碍自动严重程度评估方法

    Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15387](http://arxiv.org/abs/2210.15387)

    该论文提出了一种使用自监督模型和多任务学习相结合的自动评估发音障碍严重程度的方法，在较少数据的情况下实现了向传统方法的优化，并且相对提高了1.25%的F1-score。

    

    自动评估发音障碍的严重程度对于持续治疗和康复至关重要。然而，获取非典型发音的难度较大，往往会导致数据稀缺问题。为了应对这个问题，我们提出了一种新的自动评估发音障碍严重程度的方法，使用自监督模型与多任务学习相结合。我们联合训练Wav2vec 2.0 XLS-R进行两个不同的任务：严重程度分类和辅助自动语音识别（ASR）。对于基准实验，我们采用手工制作的声学特征和机器学习分类器，如SVM、MLP和XGBoost。在韩国发音障碍QoLT数据库上进行探究，我们的模型优于传统的基准方法，F1-score相对提高1.25%。此外，所提出的模型超过了没有ASR头训练的模型，实现了10.61%的相对百分比提高。此外，我们还展示了多任务学习如何影响障碍严重程度的评估结果。

    Automatic assessment of dysarthric speech is essential for sustained treatments and rehabilitation. However, obtaining atypical speech is challenging, often leading to data scarcity issues. To tackle the problem, we propose a novel automatic severity assessment method for dysarthric speech, using the self-supervised model in conjunction with multi-task learning. Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity classification and auxiliary automatic speech recognition (ASR). For the baseline experiments, we employ hand-crafted acoustic features and machine learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean dysarthric speech QoLT database, our model outperforms the traditional baseline methods, with a relative percentage increase of 1.25% for F1-score. In addition, the proposed model surpasses the model trained without ASR head, achieving 10.61% relative percentage improvements. Furthermore, we present how multi-task learning affects the seve
    
[^48]: 代码生成模型的多语言评估

    Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14868](http://arxiv.org/abs/2210.14868)

    本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。

    

    我们提出了新的基准测试，用于评估代码生成模型：MBXP和Multilingual HumanEval，以及MathQA-X。这些数据集涵盖了10种以上的编程语言，并使用可扩展的转换框架将原始Python数据集中的提示和测试用例转译成目标语言中的相应数据。利用这些基准测试，我们能够以多语言方式评估代码生成模型的性能，并发现了语言模型在跨领域语言上的泛化能力、多语言模型在单语言模型上的优势、少量提示教授模型新语言的能力，以及在单语言设置下的零-shot翻译能力。此外，我们使用我们的代码生成模型进行大规模引导，以获取多种语言的合成规范解，这些解可用于其他与代码相关的评估，如代码插入、鲁棒性或摘要任务。总的来说，

    We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
    
[^49]: 集中关注潜在命名实体的主动标注获取

    Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.03837](http://arxiv.org/abs/2111.03837)

    本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。

    

    命名实体识别(NER)旨在识别结构化文本中命名实体的提及并将其分类到预定义的命名实体类别中。虽然基于深度学习的预训练语言模型有助于在NER中实现良好的预测性能，但许多特定领域的NER应用仍需要大量标记数据。主动学习(AL)是解决标签获取问题的通用框架，已用于NER任务，以最小化注释成本而不牺牲模型性能。然而，标记的严重不均匀类分布引入了设计有效的NER主动学习查询方法的挑战。我们提出了几个AL句子查询评估函数，更多关注潜在的正面标记，并使用基于句子和标记成本评估策略来评估这些提议的函数。我们还提出了更好的数据驱动的正常化方法，以惩罚过长或过短的句子。

    Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
    
[^50]: 基于图解码技术的面向任务的语义解析

    Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2109.04587](http://arxiv.org/abs/2109.04587)

    本研究探索了一种替代语义解析任务的方法，将其作为依赖解析任务进行表述并应用了基于图的解码技术，有望提高部分注释数据的效率和数据使用效率。

    

    近年来，语义解析的主流范式是将解析作为序列到序列任务，使用自回归序列解码器生成预测值。本研究探讨了一种替代范式。我们将语义解析作为依赖解析任务进行了表述，应用了针对句法解析开发的基于图的解码技术。我们在 TOP 数据集上比较了给定相同预先训练的 Transformer 编码器的各种解码技术，包括训练数据有限或仅包含部分注释示例的设置。我们发现，我们基于图的方法在标准设置上与序列解码器相当竞争，并在数据效率和部分注释数据可用的设置中提供了显着的改进。

    The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where training data is limited or contains only partially-annotated examples. We find that our graph-based approach is competitive with sequence decoders on the standard setting, and offers significant improvements in data efficiency and settings where partially-annotated data is available.
    
[^51]: 区分相似的北欧语言

    Discriminating Between Similar Nordic Languages. (arXiv:2012.06431v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2012.06431](http://arxiv.org/abs/2012.06431)

    本文提出了一种用于区分六种相似的北欧语言的机器学习方法，以解决现有工具的误分类问题。

    

    自动语言识别是一个具有挑战性的问题，尤其是在区分密切相关语言方面更加困难。本文提出了一种用于北欧语言自动语言识别的机器学习方法，这些语言往往被现有的最先进工具误分类。具体而言，我们将重点关注六种北欧语言之间的区分：丹麦语、瑞典语、挪威语（尼诺斯克）、挪威语（博克马尔）、法罗语和冰岛语。

    Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\aa}l), Faroese and Icelandic.
    
[^52]: Offensive Language and Hate Speech Detection for Danish（丹麦语的恶意语言和仇恨言论检测）

    Offensive Language and Hate Speech Detection for Danish. (arXiv:1908.04531v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1908.04531](http://arxiv.org/abs/1908.04531)

    该论文提出了一个适用于检测攻击性语言的丹麦数据集，并为英语和丹麦语的恶意语言检测建立了最新技术性能。

    

    社交媒体平台上存在攻击性语言及其可能带来的影响正成为现代社会的一个主要问题。由于每天创造的内容量巨大，因此需要自动化方法来检测和处理此类内容。到目前为止，大部分研究都集中在解决英语语言问题上，而这个问题是多语言的。我们构建了一个包含Reddit和Facebook中用户生成的评论的丹麦数据集，其中包含各种社交媒体平台上的用户生成的评论，据我们所知，这是第一个这样的数据集。 我们的数据集进行了注释，以捕捉各种攻击性语言的类型和目标。我们开发了四个自动分类系统，每个系统都设计用于英语和丹麦语。在英语攻击性语言的检测中，最佳性能系统的平均宏F1得分为0.74，而对于丹麦语的最佳系统，其平均宏F1得分为0.67。我们的贡献有两个方面：首先，我们提出了一个适用于检测攻击性语言的丹麦数据集；其次，我们为英语和丹麦语的攻击性语言检测建立了最新技术性能。

    The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual.  We construct a Danish dataset containing user-generated comments from \textit{Reddit} and \textit{Facebook}. It contains user generated comments from various social media platforms, and to our knowledge, it is the first of its kind. Our dataset is annotated to capture various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1-score of $0.74$, and the best performing system for Dani
    

