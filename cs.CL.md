# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Radiology-Llama2: Best-in-Class Large Language Model for Radiology.](http://arxiv.org/abs/2309.06419) | 本文介绍了放射学领域的最佳大型语言模型Radiology-Llama2，通过指令调节过程进行特定训练，能够生成连贯且临床上有用的放射学结果。通过ROUGE指标定量评估和专家评估，证明Radiology-Llama2在可理解性、连贯性、相关性、简洁性和临床效用方面具有最先进的性能，展示了本地化语言模型在放射学等领域的潜力。 |
| [^2] | [Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails.](http://arxiv.org/abs/2309.06415) | 这项研究通过一个新颖的毒性兔子洞框架对PaLM 2的安全反馈进行了稳健性审计，揭示了PaLM 2生成的高度令人不安的毒性内容未被安全守护栏评估为高度不安全。 |
| [^3] | [Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems.](http://arxiv.org/abs/2309.06384) | 本研究通过构建评论模型和引入反馈学习循环，解决了大型语言模型在问答系统中引用错误、生成虚构信息和缺少关键细节的问题。 |
| [^4] | [Cited Text Spans for Citation Text Generation.](http://arxiv.org/abs/2309.06365) | 本文提出了一种弥合引用和引文文本之间距离的方法，通过使用引文文本跨度(CTS)替代摘要作为输入，从而使得引文生成更加准确和相关。通过自动标注和基于关键词的检索方法，可以实现高效的CTS标注，提高引文文本生成的效果。 |
| [^5] | [Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity.](http://arxiv.org/abs/2309.06364) | 本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。 |
| [^6] | [Learning to Predict Concept Ordering for Common Sense Generation.](http://arxiv.org/abs/2309.06363) | 之前的研究发现，向常识生成器展示概念的顺序对生成句子的质量起重要作用。本研究通过考虑多种语言模型和概念排序策略，发现BART-large模型在使用CommonGen训练数据进行微调时表现最佳。 |
| [^7] | [Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering.](http://arxiv.org/abs/2309.06358) | 本论文研究了使用生成式数据增强方法如何提高问答模型在自然分布转换下的鲁棒性，通过实验展示了增强阅读理解数据集的效果。 |
| [^8] | [Re-Reading Improves Reasoning in Language Models.](http://arxiv.org/abs/2309.06275) | 许多研究关注于如何引导和结构化大型语言模型的推理过程，但很少有研究关注于输入问题本身。本研究引入了一种称为“重新阅读”的提示策略，通过深入阅读输入提示中的问题信息，提供了更深入的洞察、更准确的模式识别和更有效的推理能力。 |
| [^9] | [The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models.](http://arxiv.org/abs/2309.06236) | 这项研究讨论了在大语言模型中表示和分词时间数据的困难，并提出了解决方案，如使用轻量级嵌入层进行提示调整和多模态适配器。 |
| [^10] | [Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction.](http://arxiv.org/abs/2309.06219) | 该论文提出了自动识别人类动作共现的任务，并创建了ACE数据集以及相应的代码。通过利用视觉和文本信息的图链接预测模型，可以有效捕捉不同数据域中的人类动作关系。 |
| [^11] | [Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains.](http://arxiv.org/abs/2309.06192) | 通过对新闻故事链的聚类，改进和评估了新闻推荐中信息碎片化的检测。研究结果对于衡量信息流的完整性和影响民主和公共讨论具有重要意义。 |
| [^12] | [Glancing Future for Simultaneous Machine Translation.](http://arxiv.org/abs/2309.06179) | 本文提出了一种新的方法，在同时机器翻译中通过课程学习的逐步减少可用源信息，从整个句子到与延迟对应的前缀，以实现从序列到序列训练到前缀到前缀训练的过渡，从而增强SiMT模型的翻译能力。 |
| [^13] | [AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking.](http://arxiv.org/abs/2309.06175) | 本文提出了一种利用集成模型将知识库与查询对齐的方法，用于实体识别和链接挑战。通过扩展知识库和利用外部知识，提高了召回率，并使用支持向量回归和多元加性回归树过滤结果得到高精度的实体识别和链接。最终实现了高效的计算和0.535的F1分数。 |
| [^14] | [Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis.](http://arxiv.org/abs/2309.06163) | 在IberLEF 2023中，我们提出了瓜拉尼语和西班牙语代码交替分析的GUA-SPA任务。该任务包括识别语言、NER和新颖的西班牙语片段在代码交替语境中使用方式的分类。在评估阶段，Task 1取得了较好的结果，而Task 2和Task 3的结果则不稳定。 |
| [^15] | [Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts.](http://arxiv.org/abs/2309.06135) | 提出了Prompting4Debugging（P4D）作为一个调试和红队测试工具，可以自动找到扩散模型的问题提示，以测试部署的安全机制的可靠性。 |
| [^16] | [Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO.](http://arxiv.org/abs/2309.06132) | 本文提出了一种混合方法来自动测量文本中的模糊性和主观性。通过引入专家系统VAGO，以及基于BERT-like架构的神经克隆，该方法在固定语料库和多语言生成方面表现出良好的性能。 |
| [^17] | [Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection.](http://arxiv.org/abs/2309.06131) | 本论文研究了在有限的训练数据和预算下，对基于预训练语言模型的排序器进行微调的问题。通过观察发现，在不同随机选择的训练数据子集上进行微调时，有效性存在很大变异性。因此，通过主动选择对排序器效果积极的训练数据子集，可以实现有效性提升。 |
| [^18] | [AstroLLaMA: Towards Specialized Foundation Models in Astronomy.](http://arxiv.org/abs/2309.06126) | AstroLLaMA是一个专门用于天文学的模型，通过从arXiv中的天文学摘要fine-tuned得到，其在因果语言建模中表现出色，生成的文本完成和嵌入提取比其他基础模型更具洞察力和科学相关性。 |
| [^19] | [Characterizing Latent Perspectives of Media Houses Towards Public Figures.](http://arxiv.org/abs/2309.06112) | 本研究提出了一种使用GPT-2从语料库中进行非摘要或生成性特征化人物实体的零-shot方法，用于揭示媒体对公众人物的潜在态度。 |
| [^20] | [Towards Visual Taxonomy Expansion.](http://arxiv.org/abs/2309.06105) | 本文提出了一种视觉分类扩展（VTE）方法，将视觉特征引入到分类扩展任务中，并通过文本上义词学习任务和视觉原型学习任务，结合文本和视觉语义，产生了细粒度的视觉语义。在实验中，我们的方法在中文分类数据集上显著提高了准确度，表现优于ChatGPT. |
| [^21] | [Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies.](http://arxiv.org/abs/2309.06089) | 该研究比较了不同的微调和跨语言转移策略在解决跨语言任务时的表现，评估了灾难性遗忘的程度和转移的成功程度。 |
| [^22] | [BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models.](http://arxiv.org/abs/2309.06085) | BHASA是一个综合评估套件，用于评估大语言模型在东南亚语言和文化方面的表现。它包括NLP基准、语言诊断工具包和文化诊断数据集。目前，该套件的初步版本仅针对印度尼西亚语、越南语、泰语和泰米尔语实现。 |
| [^23] | [RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair.](http://arxiv.org/abs/2309.06057) | RAP-Gen是一种检索增强修补生成框架，通过利用之前的代码库中检索到的相关修复模式，以减轻深度学习方法在自动程序修复中参数模型建模复杂搜索空间的负担。 |
| [^24] | [How does representation impact in-context learning: A exploration on a synthetic task.](http://arxiv.org/abs/2309.06054) | 本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。 |
| [^25] | [Content Reduction, Surprisal and Information Density Estimation for Long Documents.](http://arxiv.org/abs/2309.06009) | 本研究提出了四个准则用于估计长文档的信息密度，包括惊讶度、熵、均匀信息密度和词汇密度。实证结果表明了基于注意力的词汇选择方法在自动化医学编码方面的有效性。 |
| [^26] | [Circuit Breaking: Removing Model Behaviors with Targeted Ablation.](http://arxiv.org/abs/2309.05973) | 本论文提出了一种通过有针对性的消融模型组件之间的因果路径来去除语言模型中不良行为的新方法。在减少GPT-2毒性语言生成方面，仅消融12条因果边中的11.6K可以有效减轻毒性生成，并在其他输入上的性能下降很小。 |
| [^27] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^28] | [The Moral Machine Experiment on Large Language Models.](http://arxiv.org/abs/2309.05958) | 本研究通过利用道德机器框架，调查了大型语言模型在道德决策上的倾向性，发现虽然它们与人类的偏好存在定性上的相似性，但在数量上存在明显的差异，表明大型语言模型更倾向于做出更坚决的决策。这些发现对于自动驾驶具有重要的伦理和潜在影响。 |
| [^29] | [Balanced and Explainable Social Media Analysis for Public Health with Large Language Models.](http://arxiv.org/abs/2309.05951) | 提出了一种用于社交媒体的平衡和可解释性的公共卫生分析框架ALEX，通过采用复杂的数据增强方法来克服数据不平衡问题，并有效利用大型语言模型的能力。 |
| [^30] | [Language Models as Black-Box Optimizers for Vision-Language Models.](http://arxiv.org/abs/2309.05950) | 本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。 |
| [^31] | [Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge.](http://arxiv.org/abs/2309.05938) | 本文提出了一个新任务：回答产品的主观归纳问题（SUBJPQA）。与传统的QA任务不同，这类问题的答案是非唯一的，并且需要从多个角度总结多个知识源的主观意见和客观知识来解释。为了解决这个任务，我们提出了一个三步骤的方法，包括信息检索、相关性捕捉和摘要生成。 |
| [^32] | [Do PLMs Know and Understand Ontological Knowledge?.](http://arxiv.org/abs/2309.05936) | 本文研究了预训练语言模型（PLMs）对本体知识的存储和语义理解能力，以及其对实体类型、类和属性之间的层次关系，以及属性的域和范围约束的记忆情况。 |
| [^33] | [A Survey of Hallucination in Large Foundation Models.](http://arxiv.org/abs/2309.05922) | 本文调查了大型基础模型中的幻觉问题，包括幻觉现象的分类、评估标准和减轻幻觉的策略，并讨论了未来研究方向。 |
| [^34] | [SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs.](http://arxiv.org/abs/2309.05920) | SAGE是一个用于在十亿级产品目录中生成属性值的模型，可以处理跨语言、产品类型和目标属性的问题。它采用了一种新颖的建模方法，可以推断隐式使用迂回语言提到的属性值，并且能够预测属性的不适用性和无法从可用信息中获取属性值。 |
| [^35] | [Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs.](http://arxiv.org/abs/2309.05918) | 随机LLMs无法理解语言的原因是它们无法提供可以依赖的事实信息，它们存储的语言知识埋藏在无意义的微特征中，并在某些语言上下文中无法进行正确推理。本文建议在符号化方法中应用有效的自下而上策略 |
| [^36] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^37] | [Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric.](http://arxiv.org/abs/2309.05804) | 提出了一种新的对话生成损失函数和评估指标，解决了以往方法中词汇匹配和缺少上下文考虑的限制。 |
| [^38] | [Large Language Model for Science: A Study on P vs. NP.](http://arxiv.org/abs/2309.05689) | 本研究使用大型语言模型加强和加速对P vs. NP问题的研究，提出了基于苏格拉底推理的通用框架，通过与语言模型进行深入思考解决复杂问题。在P vs. NP问题的实证研究中，GPT-4成功产生了证明架构，并进行了严格的推理，得出了"P ≠ NP"的结论，揭示了语言模型在科学研究中的潜力。 |
| [^39] | [Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks.](http://arxiv.org/abs/2309.05668) | 本研究调查了在语言模型的预训练阶段中使用ChatGPT生成文本的人造文本的影响，通过比较分析了使用CNN/DailyMail新闻文章预训练的RoBERTa和使用相同文章预训练的ChatGPT的性能。 |
| [^40] | [An Empirical Study of NetOps Capability of Pre-Trained Large Language Models.](http://arxiv.org/abs/2309.05557) | 本文通过对预训练大型语言模型（LLMs）进行系统评估，发现LLMs在网络运维（NetOps）领域具有强大的潜力应用，能够提升自动化和智能化的NetOps能力。 |
| [^41] | [Multi-document Summarization: A Comparative Evaluation.](http://arxiv.org/abs/2309.04951) | 本文评估了多文档摘要领域的最新模型在不同领域和数据集上的表现，发现通用预训练模型LED在MS$^2$数据集上的性能优于其他模型，为未来的MDS研究提供了宝贵的参考和发展方向。 |
| [^42] | [Leveraging Large Language Models for Exploiting ASR Uncertainty.](http://arxiv.org/abs/2309.04842) | 这项工作旨在通过利用ASR的n-best列表来解决大型语言模型在口语理解任务上的潜在限制，而无需实质改变ASR和LLM的结构。 |
| [^43] | [FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning.](http://arxiv.org/abs/2309.04663) | FIAT是一种将上下文学习和完全微调范式融合的新的学习方式，可以在最大模型上进行指令和推理，并且在较小模型上进行参数更新，经过多语言任务测试，比之前的方法都表现更好。 |
| [^44] | [UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media.](http://arxiv.org/abs/2309.04213) | 本文提出了一种名为ALEX的框架，通过采用LLMs解释机制来改进社交媒体上的公共卫生分析性能。该方法通过数据增强和平衡训练解决了数据不平衡问题，并有效利用了LLMs的能力。 |
| [^45] | [The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature.](http://arxiv.org/abs/2309.04198) | 该研究介绍了CALLA数据集，用于探索LLMs从中文医学文献中获取交互式知识。通过自由对话事实核查任务，评估了LLMs掌握医学知识的能力，并发现了一种称为“事实跟随响应”的现象。为了提供更准确的评估方法，人工构建了两种角度的测试数据：一种与事实一致，一种与事实不一致。 |
| [^46] | [ImageBind-LLM: Multi-modality Instruction Tuning.](http://arxiv.org/abs/2309.03905) | ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。 |
| [^47] | [RoDia: A New Dataset for Romanian Dialect Identification from Speech.](http://arxiv.org/abs/2309.03378) | RoDia是第一个用于罗马尼亚方言识别的语音数据集，包含来自五个不同地区的2小时手动标注数据，并提供了一组竞争模型作为未来研究的基准。 |
| [^48] | [GPT Can Solve Mathematical Problems Without a Calculator.](http://arxiv.org/abs/2309.03241) | 本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。 |
| [^49] | [Zero-Resource Hallucination Prevention for Large Language Models.](http://arxiv.org/abs/2309.02654) | 本论文提出了一种零资源幻觉预防方法，通过评估模型对输入指令中概念的熟悉程度，在遇到不熟悉的概念时不生成响应，从而解决了大型语言模型中的幻觉问题。 |
| [^50] | [Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations.](http://arxiv.org/abs/2308.16349) | 我们引入了一个名为AffectVisDial的大规模数据集，其中包含50,000个基于视觉的对话，我们训练了情感视觉对话模型来解决基于对话的问答、情感预测和情感解释任务，展示出了有希望的情感推理能力。 |
| [^51] | [LLaSM: Large Language and Speech Model.](http://arxiv.org/abs/2308.15930) | LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。 |
| [^52] | [FonMTL: Towards Multitask Learning for the Fon Language.](http://arxiv.org/abs/2308.14280) | 本文面向Fon语的多任务学习，旨在通过在命名实体识别和词性标注任务上共享知识，增强模型在Fon语自然语言处理中的性能。 |
| [^53] | [Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?.](http://arxiv.org/abs/2308.01936) | 本文讨论了神经符号人工智能在处理逐渐复杂的类比推理时的必要性，以提供超越文字内容的广泛、多样化的知识，并结合统计和符号人工智能技术来增强和引导映射过程。 |
| [^54] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^55] | [Rewriting the Script: Adapting Text Instructions for Voice Interaction.](http://arxiv.org/abs/2306.09992) | 本研究旨在解决语音助手在引导人们完成复杂任务方面的挑战。我们通过观察参与者使用语音助手进行烹饪，发现了当前方法的九个问题，包括模糊大局、信息过载和无法传达操作性等。我们提出了一种新的方法来适应文本指令的语音交互。 |
| [^56] | [Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources.](http://arxiv.org/abs/2306.08753) | 本文介绍了一种使用纯粹的单语数据源训练双语和代码切换ASR模型的方法。通过引入集合标记器，将LID应用到每个标记，而不是在单语样本边界生成LID，我们展示了集合标记器的有效性，并提出了合成代码切换ASR数据生成技术，证明了所提出的代码切换ASR模型在语音任务中的有效性。 |
| [^57] | [Speech Separation based on Contrastive Learning and Deep Modularization.](http://arxiv.org/abs/2305.10652) | 本文提出了一种基于对比学习和深度模块化的完全无监督语音分离方法，解决了有监督学习中存在的排列问题、说话人数量不匹配的问题和高质量标记数据的依赖问题。 |
| [^58] | [PaLM 2 Technical Report.](http://arxiv.org/abs/2305.10403) | PaLM 2 是一种计算效率更高的最先进的语言模型，提供了更好的多语言和推理能力，并且通过使用多种目标进行训练，获得了在不同模型大小的下游任务上显着的改进质量。此外，PaLM 2 还展示了强大的推理能力和稳定的性能表现，使得模型能够更广泛地部署，并且可以控制毒性推理时间，而不会对其他能力产生影响。 |
| [^59] | [Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature.](http://arxiv.org/abs/2304.05406) | 本论文展示了使用OpenAI GPT-4大型语言模型通过上下文提示与天文学文献进行交互的潜力。该模型可用于提供多文献上下文下的详细答案，为天文学界探索开辟了有前途的途径。 |
| [^60] | [Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages.](http://arxiv.org/abs/2303.13592) | 本文探讨了使用大型语言模型（LLMs）生成东南亚五种语言和Singlish的混合代码数据的方法，发现ChatGPT展现出最高的潜力。然而，由于词汇选择错误的影响，ChatGPT和InstructGPT在生成混合代码时的熟练程度受到限制。 |
| [^61] | [ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning.](http://arxiv.org/abs/2212.07919) | ROSCOE是一套度量指标，用于评分逐步推理的正确性和质量。它可以衡量语义一致性、逻辑性、信息量、流畅度和事实等特征，并提供可解释的评估方法。 |
| [^62] | [Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey.](http://arxiv.org/abs/2212.04634) | 本文对结构化知识增强的故事生成进行了综述，总结了目前的方法与技术，指出了未来的发展方向和尚未解决的问题。 |
| [^63] | [Learning to Select from Multiple Options.](http://arxiv.org/abs/2212.00301) | 本文提出了一个上下文化的文本蕴涵（TE）模型（Context-TE），通过考虑其他选项作为当前建模的上下文，它能够解决TE方法中的两个限制。这个模型可以学习到更可靠的选项决策，并且通过加速推理过程来提高效率。 |
| [^64] | [Testing the limits of natural language models for predicting human language judgments.](http://arxiv.org/abs/2204.03592) | 该论文通过对争议句对进行实验比较，发现神经网络语言模型中GPT-2与人类判断最为一致，揭示了模型的失败以及找出最符合人类判断的模型。 |
| [^65] | [Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation.](http://arxiv.org/abs/2110.08501) | 本文提出了一种生成方法，可以明确生成隐含常识知识并用于响应生成。实证结果显示，该方法能够产生更丰富、更具体且遵循常识的响应，经由人工标注者评估也能够生成具有意义且相关的知识。 |

# 详细

[^1]: Radiology-Llama2: 用于放射学领域的最佳大型语言模型

    Radiology-Llama2: Best-in-Class Large Language Model for Radiology. (arXiv:2309.06419v1 [cs.CL])

    [http://arxiv.org/abs/2309.06419](http://arxiv.org/abs/2309.06419)

    本文介绍了放射学领域的最佳大型语言模型Radiology-Llama2，通过指令调节过程进行特定训练，能够生成连贯且临床上有用的放射学结果。通过ROUGE指标定量评估和专家评估，证明Radiology-Llama2在可理解性、连贯性、相关性、简洁性和临床效用方面具有最先进的性能，展示了本地化语言模型在放射学等领域的潜力。

    

    本文介绍了Radiology-Llama2，一个通过指令调节过程专门针对放射学的大型语言模型。Radiology-Llama2基于Llama2架构，并通过大量的放射学报告数据集进行进一步训练，能够生成连贯且临床上有用的放射学结果。在MIMIC-CXR和OpenI数据集上使用ROUGE指标进行定量评估，结果显示Radiology-Llama2相比其他生成语言模型具有最先进的性能，MIMIC-CXR上的Rouge-1得分为0.4834，OpenI上的得分为0.4185。放射学专家的附加评估强调了该模型在可理解性、连贯性、相关性、简洁性和临床效用方面的优势。本研究展示了专门为放射学等特定领域设计和调节的本地化语言模型的潜力。经过适当的评估和部署，这样的模型可以通过自动化机械任务和增强人机协同来改变放射学等领域。

    This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing h
    
[^2]: 深入毒性兔子洞：通过PaLM 2的守护栏调查

    Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])

    [http://arxiv.org/abs/2309.06415](http://arxiv.org/abs/2309.06415)

    这项研究通过一个新颖的毒性兔子洞框架对PaLM 2的安全反馈进行了稳健性审计，揭示了PaLM 2生成的高度令人不安的毒性内容未被安全守护栏评估为高度不安全。

    

    本文通过引入一种名为“毒性兔子洞”的新型框架，对PaLM 2的安全反馈进行了强化稳健性审计。从一个刻板印象开始，该框架指示PaLM 2生成比刻板印象更具有毒性的内容。每一次迭代，它都要求PaLM 2生成比上一次迭代更具有毒性的内容，直到PaLM 2的安全守护栏发出安全违规警报。我们的实验揭示了极其令人不安的反犹太主义、伊斯兰恐惧症、种族主义、恐同和厌女情绪（仅列举几种）的生成内容，并且这些内容在PaLM 2的安全守护栏评估中并未被视为高度不安全。

    This paper conducts a robustness audit of the safety feedback of PaLM 2 through a novel toxicity rabbit hole framework introduced here. Starting with a stereotype, the framework instructs PaLM 2 to generate more toxic content than the stereotype. Every subsequent iteration it continues instructing PaLM 2 to generate more toxic content than the previous iteration until PaLM 2 safety guardrails throw a safety violation. Our experiments uncover highly disturbing antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that PaLM 2 safety guardrails do not evaluate as highly unsafe.
    
[^3]: 朝着可靠流利的大型语言模型迈进：在问答系统中引入反馈学习循环

    Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems. (arXiv:2309.06384v1 [cs.CL])

    [http://arxiv.org/abs/2309.06384](http://arxiv.org/abs/2309.06384)

    本研究通过构建评论模型和引入反馈学习循环，解决了大型语言模型在问答系统中引用错误、生成虚构信息和缺少关键细节的问题。

    

    大型语言模型（LLMs）已成为各种日常应用中多功能的工具。然而，它们存在一些问题，影响了它们的实用性和可信度。这些问题包括引用错误（引文）、生成虚构信息（正确性）以及包含多余或遗漏关键细节（流畅性）。为了改善这些问题，本研究做出了几个重要贡献。首先，我们构建了一个数据集，用于训练评论模型，评估LLMs在问答系统中生成的回答的引文、正确性和流畅性。其次，我们提出了一种自动反馈机制，利用评论模型对生成文本的异构方面进行实时反馈。第三，我们引入了一个反馈学习循环，使用评论模型来迭代改进负责回答生成的LLM的性能。实验结果显示我们的方法的有效性。

    Large language models (LLMs) have emerged as versatile tools in various daily applications. However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency). To ameliorate these concerns, this study makes several key contributions. First, we build a dataset to train a critic model capable of evaluating the citation, correctness, and fluency of responses generated by LLMs in QA systems. Second, we propose an automated feedback mechanism that leverages the critic model to offer real-time feedback on heterogeneous aspects of generated text. Third, we introduce a feedback learning loop that uses this critic model to iteratively improve the performance of the LLM responsible for response generation. Experimental results demonstrate the efficacy of our approach, showing su
    
[^4]: 引文文本跨度用于引文文本生成

    Cited Text Spans for Citation Text Generation. (arXiv:2309.06365v1 [cs.CL])

    [http://arxiv.org/abs/2309.06365](http://arxiv.org/abs/2309.06365)

    本文提出了一种弥合引用和引文文本之间距离的方法，通过使用引文文本跨度(CTS)替代摘要作为输入，从而使得引文生成更加准确和相关。通过自动标注和基于关键词的检索方法，可以实现高效的CTS标注，提高引文文本生成的效果。

    

    为了避免非事实性幻觉，自动相关工作生成必须将其输出与引文中的内容相关联，但由于科学文档的长度，现有的概括性方法只有在引文摘要的基础上进行。我们证明摘要并不总是引文生成的最佳输入，以及以这种方式训练的模型会出现幻觉。我们提出使用引文文本跨度(CTS)作为摘要的替代条件。由于手动CTS注释非常耗时且需要大量人力，我们尝试使用基于ROUGE的自动标注候选CTS句子，并取得足够强的性能以替代昂贵的人工注释，并提出了一种基于关键词的CTS检索方法，使得以引文全文为基础生成引文文本变得有前景和实际可行。

    Automatic related work generation must ground their outputs to the content of the cited papers to avoid non-factual hallucinations, but due to the length of scientific documents, existing abstractive approaches have conditioned only on the cited paper \textit{abstracts}. We demonstrate that the abstract is not always the most appropriate input for citation generation and that models trained in this way learn to hallucinate. We propose to condition instead on the \textit{cited text span} (CTS) as an alternative to the abstract. Because manual CTS annotation is extremely time- and labor-intensive, we experiment with automatic, ROUGE-based labeling of candidate CTS sentences, achieving sufficiently strong performance to substitute for expensive human annotations, and we propose a human-in-the-loop, keyword-based CTS retrieval approach that makes generating citation texts grounded in the full text of cited papers both promising and practical.
    
[^5]: 基于框架的大型语言模型自由回答的定性分析：算法保真度

    Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])

    [http://arxiv.org/abs/2309.06364](http://arxiv.org/abs/2309.06364)

    本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。

    

    如今，使用大规模生成式语言模型（LLMs），可以模拟自由回答面试问题，就像传统上使用定性研究方法分析的那样。定性方法涵盖了一系列技术，涉及对开放式访谈或自由进行的自然语言对话的手动分析。本文考虑通过定性方法对LLMs生成的"硅参与者"进行研究，从而产生可能可以推广到真实人群的洞察力。我们分析的关键概念是算法保真度，这是由Argyle等人（2023年）引入的一个术语，用于描述LLM生成的输出与人类亚群体的信念和态度的程度相吻合。根据定义，高算法保真度表明从LLMs中提取的潜在信念可能可以推广到真实人类，而低算法保真度则使得这样的研究无效。本文使用LLM生成面试问答，...

    Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
    
[^6]: 学习预测常识生成的概念排序

    Learning to Predict Concept Ordering for Common Sense Generation. (arXiv:2309.06363v1 [cs.CL])

    [http://arxiv.org/abs/2309.06363](http://arxiv.org/abs/2309.06363)

    之前的研究发现，向常识生成器展示概念的顺序对生成句子的质量起重要作用。本研究通过考虑多种语言模型和概念排序策略，发现BART-large模型在使用CommonGen训练数据进行微调时表现最佳。

    

    先前的研究表明，向常识生成器展示概念的顺序对生成的句子质量起着重要作用。然而，确定给定一组概念的最佳顺序以便从预训练生成器生成包含所有概念的自然句子仍然是一个挑战。为了了解输入概念的排序与生成句子质量之间的关系，我们进行了一项系统研究，考虑了多种语言模型和概念排序策略。我们发现，在使用CommonGen训练数据中概念的出现顺序作为度量标准进行微调时，BART-large模型始终优于本研究中考虑的所有其他语言模型。此外，即使在针对特定任务的训练数据上进行微调，较大的基于GPT3的大型语言模型（LLM）变体在这个任务上并不一定能胜过更小的语言模型。

    Prior work has shown that the ordering in which concepts are shown to a commonsense generator plays an important role, affecting the quality of the generated sentence. However, it remains a challenge to determine the optimal ordering of a given set of concepts such that a natural sentence covering all the concepts could be generated from a pretrained generator. To understand the relationship between the ordering of the input concepts and the quality of the generated sentences, we conduct a systematic study considering multiple language models (LMs) and concept ordering strategies. We find that BART-large model consistently outperforms all other LMs considered in this study when fine-tuned using the ordering of concepts as they appear in CommonGen training data as measured using multiple evaluation metrics. Moreover, the larger GPT3-based large language models (LLMs) variants do not necessarily outperform much smaller LMs on this task, even when fine-tuned on task-specific training data
    
[^7]: 使用LLMs进行生成式数据增强提高问答中的分布鲁棒性

    Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])

    [http://arxiv.org/abs/2309.06358](http://arxiv.org/abs/2309.06358)

    本论文研究了使用生成式数据增强方法如何提高问答模型在自然分布转换下的鲁棒性，通过实验展示了增强阅读理解数据集的效果。

    

    自然语言处理中的鲁棒性问题仍然是一个重要的问题，最先进的模型在自然分布转换下表现不佳。在问答环境中，对领域适应方法的研究工作仍在不断发展。然而，在自然分布转换下的域泛化概念却受到很少关注，因为目标域是未知的。随着生成模型质量和获取方式的大幅提高，我们回答了一个问题：生成的数据集如何影响问答模型在自然分布转换下的性能？我们在4个不同数据集上进行了实验，分析了“野外生成”如何帮助实现域泛化。我们采取了两步生成方法，生成上下文和问答对来增强现有数据集。通过我们的实验，我们展示了如何通过增强阅读理解数据集来提升领域泛化能力。

    Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
    
[^8]: 重新阅读改善语言模型的推理能力

    Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])

    [http://arxiv.org/abs/2309.06275](http://arxiv.org/abs/2309.06275)

    许多研究关注于如何引导和结构化大型语言模型的推理过程，但很少有研究关注于输入问题本身。本研究引入了一种称为“重新阅读”的提示策略，通过深入阅读输入提示中的问题信息，提供了更深入的洞察、更准确的模式识别和更有效的推理能力。

    

    推理对于大型语言模型（LLM）是一个重要而具有挑战性的问题。目前的研究主要集中在开发多样化的提示策略，以引导和结构化LLM的推理过程。然而，这些基于仅解码的因果语言模型的方法通常在单个前向传递中操作输入问题，可能会忽略人类推理中丰富的前后交互。对于嵌入在提示中的输入问题这一关键维度，目前关注较少。为此，我们引入了一种简单但高效的提示策略，称为“重新阅读”。从人类学习和问题解决中汲取灵感，重新阅读意味着重访嵌在输入提示中的问题信息。这种方法与认知增强的原则完美契合，使LLM能够深入洞察、识别复杂的模式、建立 mor

    Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish mor
    
[^9]: 踏出的第一步最困难：在大语言模型中表示和分词时间数据的陷阱

    The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. (arXiv:2309.06236v1 [cs.LG])

    [http://arxiv.org/abs/2309.06236](http://arxiv.org/abs/2309.06236)

    这项研究讨论了在大语言模型中表示和分词时间数据的困难，并提出了解决方案，如使用轻量级嵌入层进行提示调整和多模态适配器。

    

    大型语言模型(LLMs)在各种任务中展示了出色的泛化能力，导致人们越来越多地将它们用作个人助手和通用计算引擎。然而，将数值/时间数据输入到这些模型中时，会出现一个明显的障碍，比如从可穿戴设备或电子健康记录中获取的数据。LLMs在其输入中使用分词器将文本分解为较小的单位。然而，分词器并不设计用于表示数值，并可能难以理解重复模式和上下文，将连续的值视为单独的标记并忽略它们的时间关系。在这里，我们讨论了最近使用LLMs进行以人为中心任务的研究，并提出了一个案例研究，展示了流行的LLMs错误地对时间数据进行分词。为了解决这个问题，我们强调了一些潜在的解决方案，例如使用轻量级嵌入层进行提示调整和多模态适配器。

    Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that c
    
[^10]: 使用图链接预测在生活方式vlog中的人类动作共现

    Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])

    [http://arxiv.org/abs/2309.06219](http://arxiv.org/abs/2309.06219)

    该论文提出了自动识别人类动作共现的任务，并创建了ACE数据集以及相应的代码。通过利用视觉和文本信息的图链接预测模型，可以有效捕捉不同数据域中的人类动作关系。

    

    我们介绍了自动识别人类动作共现的任务，即确定两个人类动作是否可以在同一时间间隔内共现。我们创建并公开了ACE（Action Co-occurrencE）数据集，该数据集由约12k个共现的视觉动作对和它们对应的视频片段组成的大型图形。我们描述了利用视觉和文本信息来自动推断两个动作是否共现的图链接预测模型。我们证明了图形特别适合捕捉人类动作之间的关系，并且所学习的图形表示对于我们的任务是有效的，并且在不同的数据域中捕捉到新颖而相关的信息。本文介绍的ACE数据集和代码可在https://github.com/MichiganNLP/vlog_action_co-occurrence公开获取。

    We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
    
[^11]: 提高和评估新闻推荐中的信息碎片检测与新闻故事链聚类

    Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v1 [cs.CL])

    [http://arxiv.org/abs/2309.06192](http://arxiv.org/abs/2309.06192)

    通过对新闻故事链的聚类，改进和评估了新闻推荐中信息碎片化的检测。研究结果对于衡量信息流的完整性和影响民主和公共讨论具有重要意义。

    

    新闻推荐系统在塑造民主社会中的信息获取方面扮演着越来越重要的角色。然而，将推荐针对用户的具体兴趣可能导致信息流的分歧。信息接触的碎片化对公共领域的完整性构成挑战，进而影响民主和公共讨论。碎片化指标量化了新闻推荐中信息流的碎片化程度。准确衡量该指标需要将自然语言处理（NLP）应用于识别不同的新闻事件、故事或时间线。本文对在新闻推荐中量化信息碎片化的各种方法进行了广泛调查。这些方法在新闻故事聚类的性能度量和不同模拟的新闻推荐场景下的碎片化评分评估中进行了评估。我们的研究发现。

    News recommender systems play an increasingly influential role in shaping information access within democratic societies. However, tailoring recommendations to users' specific interests can result in the divergence of information streams. Fragmented access to information poses challenges to the integrity of the public sphere, thereby influencing democracy and public discourse. The Fragmentation metric quantifies the degree of fragmentation of information streams in news recommendations. Accurate measurement of this metric requires the application of Natural Language Processing (NLP) to identify distinct news events, stories, or timelines. This paper presents an extensive investigation of various approaches for quantifying Fragmentation in news recommendations. These approaches are evaluated both intrinsically, by measuring performance on news story clustering, and extrinsically, by assessing the Fragmentation scores of different simulated news recommender scenarios. Our findings demons
    
[^12]: 在同时机器翻译中展望未来

    Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])

    [http://arxiv.org/abs/2309.06179](http://arxiv.org/abs/2309.06179)

    本文提出了一种新的方法，在同时机器翻译中通过课程学习的逐步减少可用源信息，从整个句子到与延迟对应的前缀，以实现从序列到序列训练到前缀到前缀训练的过渡，从而增强SiMT模型的翻译能力。

    

    同时机器翻译（SiMT）在阅读源语句的同时输出翻译。与传统的序列到序列（seq2seq）训练不同，现有的SiMT方法采用前缀到前缀（prefix2prefix）训练，即模型基于部分源标记预测目标标记。然而，前缀到前缀训练降低了模型捕捉全局信息的能力，并且由于缺乏必要的源信息而引入了强制预测。因此，弥合前缀到前缀训练和序列到序列训练之间差距以增强SiMT模型的翻译能力至关重要。在本文中，我们提出了一种新的方法，在课程学习中展望未来，实现从序列到序列训练过渡到前缀到前缀训练。具体而言，我们逐渐减少可用的源信息，从整个句子到与延迟对应的前缀。我们的方法适用于广泛的SiMT方法。

    Simultaneous machine translation (SiMT) outputs translation while reading the source sentence. Unlike conventional sequence-to-sequence (seq2seq) training, existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training, where the model predicts target tokens based on partial source tokens. However, the prefix2prefix training diminishes the ability of the model to capture global information and introduces forced predictions due to the absence of essential source information. Consequently, it is crucial to bridge the gap between the prefix2prefix training and seq2seq training to enhance the translation capability of the SiMT model. In this paper, we propose a novel method that glances future in curriculum learning to achieve the transition from the seq2seq training to prefix2prefix training. Specifically, we gradually reduce the available source information from the whole sentence to the prefix corresponding to that latency. Our method is applicable to a wide range of SiMT met
    
[^13]: AKEM: 利用集成模型将知识库与查询对齐以进行实体识别和链接

    AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v1 [cs.CL])

    [http://arxiv.org/abs/2309.06175](http://arxiv.org/abs/2309.06175)

    本文提出了一种利用集成模型将知识库与查询对齐的方法，用于实体识别和链接挑战。通过扩展知识库和利用外部知识，提高了召回率，并使用支持向量回归和多元加性回归树过滤结果得到高精度的实体识别和链接。最终实现了高效的计算和0.535的F1分数。

    

    本文提出了一种解决NLPCC 2015中实体识别和链接挑战的新方法。该任务包括从短搜索查询中提取命名实体的提及，并将其链接到参考中文知识库中的实体。为了解决这个问题，我们首先扩展现有知识库，并利用外部知识识别候选实体，从而提高召回率。接下来，我们从候选实体中提取特征，并利用支持向量回归和多元加性回归树作为评分函数来过滤结果。此外，我们还应用规则来进一步细化结果和提高精度。我们的方法计算效率高，达到了0.535的F1分数。

    This paper presents a novel approach to address the Entity Recognition and Linking Challenge at NLPCC 2015. The task involves extracting named entity mentions from short search queries and linking them to entities within a reference Chinese knowledge base. To tackle this problem, we first expand the existing knowledge base and utilize external knowledge to identify candidate entities, thereby improving the recall rate. Next, we extract features from the candidate entities and utilize Support Vector Regression and Multiple Additive Regression Tree as scoring functions to filter the results. Additionally, we apply rules to further refine the results and enhance precision. Our method is computationally efficient and achieves an F1 score of 0.535.
    
[^14]: IberLEF 2023中关于瓜拉尼语和西班牙语代码交替分析的GUA-SPA综述

    Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis. (arXiv:2309.06163v1 [cs.CL])

    [http://arxiv.org/abs/2309.06163](http://arxiv.org/abs/2309.06163)

    在IberLEF 2023中，我们提出了瓜拉尼语和西班牙语代码交替分析的GUA-SPA任务。该任务包括识别语言、NER和新颖的西班牙语片段在代码交替语境中使用方式的分类。在评估阶段，Task 1取得了较好的结果，而Task 2和Task 3的结果则不稳定。

    

    我们在IberLEF 2023中提出了第一个用于检测和分析瓜拉尼语和西班牙语代码交替的共享任务GUA-SPA。该挑战包括三个任务：识别一个标记的语言、NER和对西班牙语片段在代码交替语境中使用方式的新任务。我们标注了一个包含1500个来自新闻文章和推特的文本的语料库，大约有2.5万个标记，并对任务的信息进行了标记。三个团队参加了评估阶段，Task 1取得了较好的结果，Task 2和3的结果则参差不齐。

    We present the first shared task for detecting and analyzing code-switching in Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of three tasks: identifying the language of a token, NER, and a novel task of classifying the way a Spanish span is used in the code-switched context. We annotated a corpus of 1500 texts extracted from news articles and tweets, around 25 thousand tokens, with the information for the tasks. Three teams took part in the evaluation phase, obtaining in general good results for Task 1, and more mixed results for Tasks 2 and 3.
    
[^15]: Prompting4Debugging: 通过发现问题提示来对文本到图像扩散模型进行红队测试

    Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])

    [http://arxiv.org/abs/2309.06135](http://arxiv.org/abs/2309.06135)

    提出了Prompting4Debugging（P4D）作为一个调试和红队测试工具，可以自动找到扩散模型的问题提示，以测试部署的安全机制的可靠性。

    

    文本到图像扩散模型，例如稳定扩散（SD），最近展现出高质量内容生成的显著能力，并成为近期变革性人工智能浪潮的代表之一。然而，这种进步也带来了对该生成技术滥用的日益关注，特别是用于生成受版权保护或不适合在工作环境中查看的图像。虽然已经做出了一些努力来通过模型微调来过滤不适当的图像/提示或删除不希望的概念/风格，但这些安全机制对于多样化的问题提示的可靠性仍然不清楚。在这项工作中，我们提出了Prompting4Debugging（P4D）作为一个调试和红队测试工具，它可以自动找到扩散模型的问题提示，以测试部署的安全机制的可靠性。我们展示了我们的P4D工具在发现具有安全机制的SD模型的新漏洞方面的有效性。具体而言，我们的结果显示...

    Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows t
    
[^16]: 在文本中测量模糊性和主观性：从符号到神经网络的VAGO

    Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])

    [http://arxiv.org/abs/2309.06132](http://arxiv.org/abs/2309.06132)

    本文提出了一种混合方法来自动测量文本中的模糊性和主观性。通过引入专家系统VAGO，以及基于BERT-like架构的神经克隆，该方法在固定语料库和多语言生成方面表现出良好的性能。

    

    我们提出了一种混合方法来自动测量文本中的模糊性和主观性。首先，我们介绍了专家系统VAGO，并在一小组事实与观点句子的基准上对其进行了说明，并在更大的法语新闻语料库FreSaDa上进行了测试，以确认讽刺性文本中主观标记的更高流行率。然后，我们构建了一个基于BERT-like架构的VAGO神经克隆，该架构基于在FreSaDa上获得的符号VAGO分数进行训练。使用可解释性工具（LIME），我们展示了这个神经版本在丰富符号版本的词典和生成其他语言版本方面的兴趣。

    We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
    
[^17]: 对神经排序器进行微调的数据标注？当前主动学习策略并不比随机选择更好。

    Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection. (arXiv:2309.06131v1 [cs.IR])

    [http://arxiv.org/abs/2309.06131](http://arxiv.org/abs/2309.06131)

    本论文研究了在有限的训练数据和预算下，对基于预训练语言模型的排序器进行微调的问题。通过观察发现，在不同随机选择的训练数据子集上进行微调时，有效性存在很大变异性。因此，通过主动选择对排序器效果积极的训练数据子集，可以实现有效性提升。

    

    基于预训练语言模型（PLM）的搜索方法相比统计和早期神经排序模型显示出了巨大的有效性提升。然而，微调基于PLM的排序器需要大量的标注训练数据。标注数据需要大量的人工努力，因此在具体领域任务中非常昂贵。本文研究了在有限的训练数据和预算下微调基于PLM的排序器。我们研究了两种情况：从头开始微调排序器，以及从已经在通用数据上微调的排序器开始进行领域适应，并在目标数据集上继续微调。我们观察到在不同随机选择的训练数据子集上进行微调时，有效性存在很大的变异性。这表明通过主动选择对排序器效果最积极的训练数据子集，可以实现有效性提升。通过这种方式，将可以微调出有效的PLM排序器。

    Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rank
    
[^18]: AstroLLaMA: 面向天文学的专业基础模型

    AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])

    [http://arxiv.org/abs/2309.06126](http://arxiv.org/abs/2309.06126)

    AstroLLaMA是一个专门用于天文学的模型，通过从arXiv中的天文学摘要fine-tuned得到，其在因果语言建模中表现出色，生成的文本完成和嵌入提取比其他基础模型更具洞察力和科学相关性。

    

    大型语言模型在许多人类语言任务中表现出色，但在学术天文学等高度专业化领域往往难以胜任。为了弥合这个差距，我们介绍了AstroLLaMA，这是一个从arXiv上的超过300,000个天文学摘要中使用LLaMA-2 fine-tuned得到的70亿参数模型。AstroLLaMA针对传统因果语言建模进行了优化，其困惑度比Llama-2低30％，表现出明显的领域适应性。尽管参数明显较少，但我们的模型生成的文本完成和嵌入提取比最先进的基础模型更具洞察力和科学相关性。AstroLLaMA作为一个强大的领域特定模型，具有广泛的fine-tuning潜力。其公开发布旨在推动围绕天文学的研究，包括自动论文摘要和对话代理的开发。

    Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
    
[^19]: 揭示媒体对公众人物的潜在态度

    Characterizing Latent Perspectives of Media Houses Towards Public Figures. (arXiv:2309.06112v1 [cs.CL])

    [http://arxiv.org/abs/2309.06112](http://arxiv.org/abs/2309.06112)

    本研究提出了一种使用GPT-2从语料库中进行非摘要或生成性特征化人物实体的零-shot方法，用于揭示媒体对公众人物的潜在态度。

    

    媒体报道公众人物往往受到各自世界观的偏见影响。对这些潜在模式的描述有助于我们更好地理解和解释新闻故事。为此，我们需要多样化或主观的总结，这可能不适合分类为预定义的类别标签。本研究提出了一种使用GPT-2从语料库中进行非摘要或生成性特征化人物实体的零-shot方法。我们使用了来自几家知名新闻媒体的明确文章作为语料库，为这种方法建立了一个有力的论证。首先，我们用特定的人物实体进行了GPT-2预训练语言模型微调，然后再用从程序构造的特征化语料库创建的人物实体特征化演示进一步微调它。最后，我们用手动提示的实体名称预热了这个经过两次微调的模型。

    Media houses reporting on public figures, often come with their own biases stemming from their respective worldviews. A characterization of these underlying patterns helps us in better understanding and interpreting news stories. For this, we need diverse or subjective summarizations, which may not be amenable for classifying into predefined class labels. This work proposes a zero-shot approach for non-extractive or generative characterizations of person entities from a corpus using GPT-2. We use well-articulated articles from several well-known news media houses as a corpus to build a sound argument for this approach. First, we fine-tune a GPT-2 pre-trained language model with a corpus where specific person entities are characterized. Second, we further fine-tune this with demonstrations of person entity characterizations, created from a corpus of programmatically constructed characterizations. This twice fine-tuned model is primed with manual prompts consisting of entity names that w
    
[^20]: 实现视觉分类扩展

    Towards Visual Taxonomy Expansion. (arXiv:2309.06105v1 [cs.CV])

    [http://arxiv.org/abs/2309.06105](http://arxiv.org/abs/2309.06105)

    本文提出了一种视觉分类扩展（VTE）方法，将视觉特征引入到分类扩展任务中，并通过文本上义词学习任务和视觉原型学习任务，结合文本和视觉语义，产生了细粒度的视觉语义。在实验中，我们的方法在中文分类数据集上显著提高了准确度，表现优于ChatGPT.

    

    分类扩展任务非常重要，可以将新概念的不断增加的数量组织到现有分类中。大多数现有方法只关注使用文本语义，导致无法推广到未见术语和“典型超义问题”。本文提出了视觉分类扩展（VTE），将视觉特征引入到分类扩展任务中。我们提出了文本上义词学习任务和视觉原型学习任务，以聚类文本和视觉语义。除了各自模态的任务之外，我们引入了一个超-原型约束，将文本和视觉语义结合起来产生细粒度的视觉语义。我们的方法在两个数据集上进行了评估，取得了令人信服的结果。具体而言，对于中文分类数据集，我们的方法将准确度显著提高了8.75％。此外，我们的方法在中文分类数据集上的表现优于ChatGPT.

    Taxonomy expansion task is essential in organizing the ever-increasing volume of new concepts into existing taxonomies. Most existing methods focus exclusively on using textual semantics, leading to an inability to generalize to unseen terms and the "Prototypical Hypernym Problem." In this paper, we propose Visual Taxonomy Expansion (VTE), introducing visual features into the taxonomy expansion task. We propose a textual hypernymy learning task and a visual prototype learning task to cluster textual and visual semantics. In addition to the tasks on respective modalities, we introduce a hyper-proto constraint that integrates textual and visual semantics to produce fine-grained visual semantics. Our method is evaluated on two datasets, where we obtain compelling results. Specifically, on the Chinese taxonomy dataset, our method significantly improves accuracy by 8.75 %. Additionally, our approach performs better than ChatGPT on the Chinese taxonomy dataset.
    
[^21]: 在跨语言转移范式中测量灾难性遗忘：探索调优策略

    Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])

    [http://arxiv.org/abs/2309.06089](http://arxiv.org/abs/2309.06089)

    该研究比较了不同的微调和跨语言转移策略在解决跨语言任务时的表现，评估了灾难性遗忘的程度和转移的成功程度。

    

    跨语言转移是一种解决资源匮乏语言任务的有希望的技术。在这个实证研究中，我们比较了两种与零射和全射学习方法相结合的大型语言模型在跨语言设置下的微调方法。作为微调策略，我们比较了参数效率适配器方法与所有参数微调。作为跨语言转移策略，我们比较了使用每个语言依次的中间训练（IT）和在微调的验证阶段已经使用目标语言的跨语言验证（CLV）。我们评估了转移的成功程度以及源语言中由于跨语言转移而导致的灾难性遗忘的程度，即在学习不同语言中的新信息时之前获得的知识损失了多少。在两个不同的分类问题上，包括仇恨言论检测和产品评论，分别包含了多个语种数据集的结果。

    The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
    
[^22]: BHASA：面向大语言模型的东南亚语言和文化综合评估套件

    BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v1 [cs.CL])

    [http://arxiv.org/abs/2309.06085](http://arxiv.org/abs/2309.06085)

    BHASA是一个综合评估套件，用于评估大语言模型在东南亚语言和文化方面的表现。它包括NLP基准、语言诊断工具包和文化诊断数据集。目前，该套件的初步版本仅针对印度尼西亚语、越南语、泰语和泰米尔语实现。

    

    大型语言模型（LLM）的快速发展和规模带来的新能力使得构建全面、多样和具有挑战性的基准成为必要，如HELM和BIG-bench。然而，目前大部分基准只关注英语的表现，包括东南亚（SEA）语言的评估很少。因此，我们提出了BHASA，一个针对SEA语言的综合语言和文化评估套件。它包括三个组成部分：（1）涵盖自然语言理解（NLU）、生成（NLG）和推理（NLR）任务的NLP基准，共涵盖八个任务；（2）LINDSEA，一个跨越句法、语义和语用等各种语言现象的语言诊断工具包；（3）一份文化诊断数据集，旨在探索文化表达和敏感性。对于这个初步工作，我们只针对印度尼西亚语、越南语、泰语和泰米尔语实现了NLP基准。

    The rapid development of Large Language Models (LLMs) and the emergence of novel abilities with scale have necessitated the construction of holistic, diverse and challenging benchmarks such as HELM and BIG-bench. However, at the moment, most of these benchmarks focus only on performance in English and evaluations that include Southeast Asian (SEA) languages are few in number. We therefore propose BHASA, a holistic linguistic and cultural evaluation suite for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark covering eight tasks across Natural Language Understanding (NLU), Generation (NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit that spans the gamut of linguistic phenomena including syntax, semantics and pragmatics, and (3) a cultural diagnostics dataset that probes for both cultural representation and sensitivity. For this preliminary effort, we implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil, and we on
    
[^23]: RAP-Gen：基于CodeT5的检索增强修补生成方法用于自动程序修复

    RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair. (arXiv:2309.06057v1 [cs.SE])

    [http://arxiv.org/abs/2309.06057](http://arxiv.org/abs/2309.06057)

    RAP-Gen是一种检索增强修补生成框架，通过利用之前的代码库中检索到的相关修复模式，以减轻深度学习方法在自动程序修复中参数模型建模复杂搜索空间的负担。

    

    自动程序修复(APR)对于减少开发者的手动调试工作和提高软件可靠性非常重要。传统的基于搜索的技术通常依赖启发式规则或冗余假设来挖掘修复模式，而近年来，深度学习(DL)方法的兴起以数据驱动的方式自动化程序修复过程。然而，它们的性能常常受到参数模型对APR高度复杂搜索空间建模的限制。为了减轻参数模型的负担，本文提出了一种新颖的检索增强修补生成框架(RAP-Gen)，通过明确利用源自以前的错漏对代码库中检索到的相关修复模式。具体而言，我们构建了一个混合的修补筛选器，以语言无关的方式，基于原始源代码进行词法和语义匹配，不依赖于任何特定于代码的特征。

    Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware langu
    
[^24]: 表示对上下文学习的影响：对合成任务的探索

    How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])

    [http://arxiv.org/abs/2309.06054](http://arxiv.org/abs/2309.06054)

    本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。

    

    上下文学习，即从上下文样本中学习，是Transformer的一项引人注目的能力。然而，驱动上下文学习的机制尚未被充分理解。本研究旨在从一个未被充分探索的表示学习角度进行调查。在上下文学习场景中，表示更加复杂，表示可以受到模型权重和上下文样本的影响。我们将上述两个概念方面的表示分别称为权重内部成分和上下文内部成分。为了研究这两个成分如何影响上下文学习能力，我们构建了一个新颖的合成任务，从而可以设计两个探针，即权重内部探针和上下文探针，分别评估这两个成分。我们证明上下文内部成分的好坏与上下文学习性能高度相关，这表明上下文学习与表示学习之间的纠缠关系。

    In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
    
[^25]: 长文档的内容减少、惊讶度和信息密度估计

    Content Reduction, Surprisal and Information Density Estimation for Long Documents. (arXiv:2309.06009v1 [cs.CL])

    [http://arxiv.org/abs/2309.06009](http://arxiv.org/abs/2309.06009)

    本研究提出了四个准则用于估计长文档的信息密度，包括惊讶度、熵、均匀信息密度和词汇密度。实证结果表明了基于注意力的词汇选择方法在自动化医学编码方面的有效性。

    

    许多计算语言学方法已被提出用于研究语言的信息内容。我们考虑了两个有趣的研究问题：1)信息是如何在长文档中分布的？2)内容减少（如标记选择和文本摘要）如何影响长文档中的信息密度。我们提出了四个用于估计长文档信息密度的准则，包括惊讶度、熵、均匀信息密度和词汇密度。其中前三个采用了信息论的测量方法。我们为临床笔记提出了一种基于注意力的词汇选择方法，并研究了多领域文档的机器摘要。我们的研究发现了不同领域长文本的信息密度系统差异。对于长临床笔记的自动化医学编码的实证结果表明了基于注意力的词汇选择方法的有效性。

    Many computational linguistic methods have been proposed to study the information content of languages. We consider two interesting research questions: 1) how is information distributed over long documents, and 2) how does content reduction, such as token selection and text summarization, affect the information density in long documents. We present four criteria for information density estimation for long documents, including surprisal, entropy, uniform information density, and lexical density. Among those criteria, the first three adopt the measures from information theory. We propose an attention-based word selection method for clinical notes and study machine summarization for multiple-domain documents. Our findings reveal the systematic difference in information density of long text in various domains. Empirical results on automated medical coding from long clinical notes show the effectiveness of the attention-based word selection method.
    
[^26]: 切断电路: 通过有针对性的消融去除模型行为

    Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])

    [http://arxiv.org/abs/2309.05973](http://arxiv.org/abs/2309.05973)

    本论文提出了一种通过有针对性的消融模型组件之间的因果路径来去除语言模型中不良行为的新方法。在减少GPT-2毒性语言生成方面，仅消融12条因果边中的11.6K可以有效减轻毒性生成，并在其他输入上的性能下降很小。

    

    语言模型通常会表现出在预训练目标上提高性能但在下游任务上降低性能的行为。我们提出了一种新颖的方法，通过消融模型组件之间的一小部分因果路径，以禁用与不良行为有关的计算电路，从而去除不良行为。在拥有模型表现差的小型输入数据集的情况下，我们学会了消融一小部分重要的因果路径。在减少GPT-2毒性语言生成方面，我们发现消融仅仅12条因果边中的11.6K，可以减轻毒性生成，同时在其他输入上的性能下降很小。

    Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
    
[^27]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^28]: 大型语言模型上的道德机器实验

    The Moral Machine Experiment on Large Language Models. (arXiv:2309.05958v1 [cs.CL])

    [http://arxiv.org/abs/2309.05958](http://arxiv.org/abs/2309.05958)

    本研究通过利用道德机器框架，调查了大型语言模型在道德决策上的倾向性，发现虽然它们与人类的偏好存在定性上的相似性，但在数量上存在明显的差异，表明大型语言模型更倾向于做出更坚决的决策。这些发现对于自动驾驶具有重要的伦理和潜在影响。

    

    随着大型语言模型（LLMs）在各个领域的深入整合，理解它们如何做出道德判断变得至关重要，尤其是在自动驾驶领域。本研究利用道德机器框架，调查了知名的LLMs（包括GPT-3.5，GPT-4，PaLM 2和Llama 2）的道德决策倾向，并将其与人类偏好进行比较。虽然LLMs和人类的偏好（例如将人类放在宠物之上和更倾向于挽救更多生命）在很大程度上是一致的，但特别是PaLM 2和Llama 2显示出明显的偏差。此外，尽管LLM和人类偏好之间存在定性上的相似性，但在数量上存在明显的差异，这表明与人类相比，LLMs可能更趋向于做出更坚决的决策。这些发现阐明了LLMs的道德框架及其对自动驾驶的潜在影响。

    As large language models (LLMs) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. This study utilized the Moral Machine framework to investigate the ethical decision-making tendencies of prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their responses to human preferences. While LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations. Additionally, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans. These insights elucidate the ethical frameworks of LLMs and their potential implications for autonomous driving.
    
[^29]: 利用大型语言模型进行公共卫生的平衡和可解释性社交媒体分析

    Balanced and Explainable Social Media Analysis for Public Health with Large Language Models. (arXiv:2309.05951v1 [cs.CL])

    [http://arxiv.org/abs/2309.05951](http://arxiv.org/abs/2309.05951)

    提出了一种用于社交媒体的平衡和可解释性的公共卫生分析框架ALEX，通过采用复杂的数据增强方法来克服数据不平衡问题，并有效利用大型语言模型的能力。

    

    随着社交媒体的越来越流行，越来越多的公共卫生活动出现，这对于疫情监测和政府决策非常值得注意。当前的公共卫生分析技术涉及BERT和大型语言模型（LLM）等流行模型。尽管LLMs的最新进展表明通过在特定领域数据集上进行微调，它们具有强大的理解知识的能力，但为每个特定的公共卫生任务训练一个领域内LLM的成本尤为昂贵。此外，来自社交媒体的此类领域内数据集通常具有严重的不平衡性，这将妨碍LLMs的调优效率。为解决这些挑战，可以采用复杂的数据增强方法来克服数据不平衡问题。此外，通过适当地引导模型，可以有效利用LLMs的能力。基于以上讨论，在本文中，提出了一种新的ALEX框架，用于社交媒体的平衡和可解释性的公共卫生分析。

    As social media becomes increasingly popular, more and more public health activities emerge, which is worth noting for pandemic monitoring and government decision-making. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). Although recent progress in LLMs has shown a strong ability to comprehend knowledge by being fine-tuned on specific domain datasets, the costs of training an in-domain LLM for every specific public health task are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally highly imbalanced, which will hinder the efficiency of LLMs tuning. To tackle these challenges, the data imbalance issue can be overcome by sophisticated data augmentation methods for social media datasets. In addition, the ability of the LLMs can be effectively utilised by prompting the model properly. In light of the above discussion, in this paper, a novel ALEX framework is proposed for social
    
[^30]: 语言模型作为视觉-语言模型的黑盒优化器

    Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])

    [http://arxiv.org/abs/2309.05950](http://arxiv.org/abs/2309.05950)

    本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。

    

    预训练在大规模网络数据集上的视觉-语言模型 (VLMs) 展示了在各种视觉和多模态任务中的显著能力。目前，VLMs 的微调方法主要在白盒环境中操作，需要访问模型参数进行反向传播。然而，许多 VLMs 依赖于专有数据且不开源，限制了使用白盒方法进行微调。鉴于像 ChatGPT 这样的受欢迎私有大型语言模型 (LLMs) 仍然提供基于语言的用户界面，我们旨在通过自然语言提示开发一种新的 VLMs 微调方法，从而避免访问模型参数、特征嵌入或输出 logits 的需要。在这种设置下，我们提出使用基于聊天的 LLMs 作为黑盒优化器，以在使用 CLIP 进行少样本图像分类的示例任务中寻找最佳文本提示。具体而言，我们采用自动"爬山"程序，它能收敛到有效的提示上。

    Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
    
[^31]: 通过总结多源多视角知识回答产品的主观归纳问题

    Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])

    [http://arxiv.org/abs/2309.05938](http://arxiv.org/abs/2309.05938)

    本文提出了一个新任务：回答产品的主观归纳问题（SUBJPQA）。与传统的QA任务不同，这类问题的答案是非唯一的，并且需要从多个角度总结多个知识源的主观意见和客观知识来解释。为了解决这个任务，我们提出了一个三步骤的方法，包括信息检索、相关性捕捉和摘要生成。

    

    本文在回答产品的主观归纳问题（SUBJPQA）的领域中提出了一个新任务。这类问题的答案是非唯一的，但可以从多个角度来解释。例如，对于“手机是否重”的答案有多种不同的观点。一个满意的答案应该能够总结这些来自多个来源的主观意见，并提供客观知识，比如手机的重量。这与传统的QA任务非常不同，传统QA任务中对于事实问题的答案是唯一的，并且可以从单个数据源中找到。为了解决这个新任务，我们提出了一个三步骤的方法。首先，我们从多个知识源中检索所有与答案相关的线索，包括事实和观点。还收集了隐含的常识事实来补充必要但缺失的背景信息。然后，我们通过交互式注意力来捕捉它们与问题的相关性。接下来，我们设计了一个基于强化学习的摘要生成器来聚合这些信息。

    This paper proposes a new task in the field of Answering Subjective Induction Question on Products (SUBJPQA). The answer to this kind of question is non-unique, but can be interpreted from many perspectives. For example, the answer to 'whether the phone is heavy' has a variety of different viewpoints. A satisfied answer should be able to summarize these subjective opinions from multiple sources and provide objective knowledge, such as the weight of a phone. That is quite different from the traditional QA task, in which the answer to a factoid question is unique and can be found from a single data source. To address this new task, we propose a three-steps method. We first retrieve all answer-related clues from multiple knowledge sources on facts and opinions. The implicit commonsense facts are also collected to supplement the necessary but missing contexts. We then capture their relevance with the questions by interactive attention. Next, we design a reinforcement-based summarizer to ag
    
[^32]: PLMs是否知道和理解本体知识？

    Do PLMs Know and Understand Ontological Knowledge?. (arXiv:2309.05936v1 [cs.CL])

    [http://arxiv.org/abs/2309.05936](http://arxiv.org/abs/2309.05936)

    本文研究了预训练语言模型（PLMs）对本体知识的存储和语义理解能力，以及其对实体类型、类和属性之间的层次关系，以及属性的域和范围约束的记忆情况。

    

    本体知识是世界知识的重要组成部分，包括类和属性以及它们之间的关系。探索预训练语言模型（PLMs）是否知道和理解这样的知识具有重要意义。然而，现有的PLM探测研究主要关注事实知识，缺乏对本体知识的系统探测。本文旨在探究PLMs是否存储本体知识，并对其具有语义理解而不是对表面形式的死记硬背。为了探测PLMs是否了解本体知识，我们研究了PLMs对以下方面的记忆情况：（1）实体类型；（2）类和属性之间的层次关系，例如，人是动物的子类，参与团队的成员是成员的子属性；（3）属性的域和范围约束，例如，参与团队的成员的主语应是人，宾语应是一个运动队。

    Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a systematic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic understanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ont
    
[^33]: 对大型基础模型中幻觉的调查

    A Survey of Hallucination in Large Foundation Models. (arXiv:2309.05922v1 [cs.AI])

    [http://arxiv.org/abs/2309.05922](http://arxiv.org/abs/2309.05922)

    本文调查了大型基础模型中的幻觉问题，包括幻觉现象的分类、评估标准和减轻幻觉的策略，并讨论了未来研究方向。

    

    基础模型中的幻觉指的是生成偏离事实的内容或包含虚构信息。本研究概述了近期努力的广泛概述，这些努力旨在识别、阐明和解决幻觉问题，特别关注“大”基础模型（LFMs）。本文对特定于LFMs的各种类型的幻觉现象进行了分类，并建立了评估幻觉程度的评估标准。它还检查了减轻LFM中幻觉的现有策略，并讨论了未来研究的潜在方向。本文全面探讨了与LFMs中幻觉相关的挑战和解决方案。

    Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.
    
[^34]: SAGE: 针对十亿级产品目录的结构化属性值生成

    SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs. (arXiv:2309.05920v1 [cs.IR])

    [http://arxiv.org/abs/2309.05920](http://arxiv.org/abs/2309.05920)

    SAGE是一个用于在十亿级产品目录中生成属性值的模型，可以处理跨语言、产品类型和目标属性的问题。它采用了一种新颖的建模方法，可以推断隐式使用迂回语言提到的属性值，并且能够预测属性的不适用性和无法从可用信息中获取属性值。

    

    我们引入了SAGE，一个用于在全球电子商务目录中推断产品属性值的生成模型。我们将属性值预测问题以一种新颖的方式转化为跨语言、产品类型和目标属性的Seq2Seq摘要任务。我们的新模型方法不再局限于在预先指定的选项集内预测属性值，并且也不要求所寻找的属性值在文本中明确提及。SAGE可以推断隐式使用迂回语言提到的属性值，或者根本不提及的常识默认情况下的属性值。此外，SAGE能够预测一个属性对于当前产品是否不适用，或者是否无法从可用信息中获取。SAGE是第一种能够在实际电子商务目录设置中处理属性值预测任务所有方面的方法。一套综合的...

    We introduce SAGE; a Generative LLM for inferring attribute values for products across world-wide e-Commerce catalogs. We introduce a novel formulation of the attribute-value prediction problem as a Seq2Seq summarization task, across languages, product types and target attributes. Our novel modeling approach lifts the restriction of predicting attribute values within a pre-specified set of choices, as well as, the requirement that the sought attribute values need to be explicitly mentioned in the text. SAGE can infer attribute values even when such values are mentioned implicitly using periphrastic language, or not-at-all-as is the case for common-sense defaults. Additionally, SAGE is capable of predicting whether an attribute is inapplicable for the product at hand, or non-obtainable from the available information. SAGE is the first method able to tackle all aspects of the attribute-value-prediction task as they arise in practical settings in e-Commerce catalogs. A comprehensive set o
    
[^35]: 随机LLMs无法理解语言：走向符号化、可解释性和本体论基于的LLMs

    Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])

    [http://arxiv.org/abs/2309.05918](http://arxiv.org/abs/2309.05918)

    随机LLMs无法理解语言的原因是它们无法提供可以依赖的事实信息，它们存储的语言知识埋藏在无意义的微特征中，并在某些语言上下文中无法进行正确推理。本文建议在符号化方法中应用有效的自下而上策略

    

    在我们看来，围绕数据驱动的大型语言模型（LLMs）相对成功的狂热是有些误导的，原因如下：（i）LLMs不能依赖于事实信息，因为对于LLMs来说，摄入的所有文本（事实或非事实）都是平等的；（ii）由于它们的亚符号性质，这些模型对语言的任何“知识”都将永远埋藏在数十亿个微特征（权重）中，其中没有一个本身是有意义的；以及（iii）LLMs在几种语言上下文中常常无法进行正确推理（如名词复合词、共谓词、量词范围模糊和意向性上下文）。我们相信，数据驱动的大型语言模型（LLMs）的相对成功不是符号与亚符号之辩的反映，而是在规模上应用自下而上的逆向工程语言的成功策略的反映。在本文中，我们建议将有效的自下而上策略应用于符号化方法中

    In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
    
[^36]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^37]: 生成“nice”而不是生成“good”不像生成“rice”那么糟糕！朝着上下文和语义融合的对话生成损失函数和评估指标。

    Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])

    [http://arxiv.org/abs/2309.05804](http://arxiv.org/abs/2309.05804)

    提出了一种新的对话生成损失函数和评估指标，解决了以往方法中词汇匹配和缺少上下文考虑的限制。

    

    在过去的二十年中，对话建模取得了重大进展，从简单的基于规则的回答发展到个性化和有说服力的回答生成。然而，尽管这些进展，对话生成的目标函数和评估指标仍然停滞不前，分别是交叉熵和BLEU。这些基于词汇的指标存在以下主要限制：(a)没有语义考虑的词对词匹配：它将生成“nice”和生成“rice”作为“good”的失败统一考虑。(b)缺少为评估生成的回答提供上下文属性：即使生成的回答与正在进行的对话上下文相关，如果不与语料库中提供的黄金话语匹配，仍然可能受到惩罚。在本文中，我们首先全面调查这些限制，并提出了一种名为上下文语义融合对话(SemTextualLogue)损失函数的新损失函数。此外，我们还制定了一套新的评估指标，

    Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation
    
[^38]: 基于大语言模型的科学研究：对P vs. NP问题的研究

    Large Language Model for Science: A Study on P vs. NP. (arXiv:2309.05689v1 [cs.CL])

    [http://arxiv.org/abs/2309.05689](http://arxiv.org/abs/2309.05689)

    本研究使用大型语言模型加强和加速对P vs. NP问题的研究，提出了基于苏格拉底推理的通用框架，通过与语言模型进行深入思考解决复杂问题。在P vs. NP问题的实证研究中，GPT-4成功产生了证明架构，并进行了严格的推理，得出了"P ≠ NP"的结论，揭示了语言模型在科学研究中的潜力。

    

    在这项工作中，我们使用大型语言模型（LLMs）来增强和加速对P vs. NP问题的研究，这是理论计算机科学和数学中最重要的未解决问题之一。具体而言，我们提出了苏格拉底推理（Socratic reasoning）的通用框架，该框架通过与LLMs进行深入思考来解决复杂问题。苏格拉底推理鼓励LLMs递归地发现、解决和整合问题，同时促进自我评估和改进。我们对P vs. NP问题的试点研究表明，GPT-4在97个对话回合中成功地产生了一个证明架构，并进行了严格的推理，得出了“P ≠ NP”的结论，这与（Xu and Zhou, 2023）的结论一致。该调查在LLMs的广泛解空间中揭示了新的洞察力，为LLM在科学研究中提供了启示。

    In this work, we use large language models (LLMs) to augment and accelerate research on the P versus NP problem, one of the most important open problems in theoretical computer science and mathematics. Specifically, we propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding "P $\neq$ NP", which is in alignment with (Xu and Zhou, 2023). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.
    
[^39]: 研究使用ChatGPT生成文本进行预训练对下游任务的影响

    Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])

    [http://arxiv.org/abs/2309.05668](http://arxiv.org/abs/2309.05668)

    本研究调查了在语言模型的预训练阶段中使用ChatGPT生成文本的人造文本的影响，通过比较分析了使用CNN/DailyMail新闻文章预训练的RoBERTa和使用相同文章预训练的ChatGPT的性能。

    

    最近，在语言模型领域取得了显著进展，特别是随着大型语言模型（LLM）的出现，这些模型使用从互联网档案中提取的大量数据进行训练。这些LLM，例如ChatGPT，已广泛可用，使用户能够为各种目的生成文本，包括文章、论文、笑话和诗歌。由于LLM是在涵盖Reddit和Twitter等平台的各种文本来源上进行训练的，可以预见未来的训练数据集还将包含前几个模型生成的文本。鉴于此发展，我们的研究旨在探究在语言模型的预训练阶段中人造文本的影响。具体而言，我们对一个使用CNN/DailyMail新闻文章进行预训练的语言模型RoBERTa和一个使用相同文章进行训练的ChatGPT进行了比较分析，并评估了它们的表现。

    In recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their per
    
[^40]: 预训练大型语言模型的网络运维能力的实证研究

    An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05557](http://arxiv.org/abs/2309.05557)

    本文通过对预训练大型语言模型（LLMs）进行系统评估，发现LLMs在网络运维（NetOps）领域具有强大的潜力应用，能够提升自动化和智能化的NetOps能力。

    

    大型语言模型（LLMs）可以回答人类语言查询，并在网络运维（NetOps）领域展现出强大的潜力应用。由于具备大量常识知识，LLMs在推理准确性上比传统模型更好，并具有更强的泛化能力、推理能力和代码生成能力，这些能力可能对自动化和智能化的NetOps有巨大的提升。然而，LLMs在各种NetOps任务中的表现仍然未被充分探索。本文系统评估了选择的几种LLMs在NetOps领域的能力、优势和限制。评估针对5732个关于NetOps的问题进行，涵盖了26个公开可用的通用领域LLMs，包括ChatGPT、LLaMA、Falcon等。我们还对其中一些LLMs进行了NetOps语料库的微调，并评估了结果模型的性能。评估方法遵循广泛采用的用于生成任务基准测试的方法。

    Large language models (LLMs) can respond to human language queries and have shown powerful potential applications in network operations (NetOps). Thanks to the large amount of commonsense knowledge inherent, LLMs achieve much better inference accuracy than traditional models and emerge with strong abilities in generalization, reasoning, and code generation. These abilities may have a crucial boost to automated and intelligent NetOps. However, it remains under-explored how well LLMs perform in various NetOps tasks. In this work, we make a systematic assessment of the capabilities, strengths, and limitations of selected LLMs in the field of NetOps. The evaluation is conducted on a collection of 5,732 questions about NetOps, encompassing 26 publicly available general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune some of these LLMs with our collected NetOps corpus and evaluate the resulting models. The evaluation method follows the widely adopted benchmarks for gener
    
[^41]: 多文档摘要：一项比较评估

    Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])

    [http://arxiv.org/abs/2309.04951](http://arxiv.org/abs/2309.04951)

    本文评估了多文档摘要领域的最新模型在不同领域和数据集上的表现，发现通用预训练模型LED在MS$^2$数据集上的性能优于其他模型，为未来的MDS研究提供了宝贵的参考和发展方向。

    

    本文旨在评估多文档摘要(MDS)领域的最新模型在不同领域和不同类型数据集上的表现，并研究现有模型的局限性，以确定未来的研究方向。为了填补这个空白，我们进行了广泛的文献评估，以确定最新的模型和数据集。我们对BigSurvey-MDS和MS$^2$数据集上的PRIMERA和PEGASUS模型的性能进行了分析，这些数据集由于领域的不同而带来了独特的挑战。我们的研究结果表明，通用预训练模型LED在MS$^2$数据集上的性能优于PRIMERA和PEGASUS。我们使用ROUGE分数作为性能度量指标，评估了不同数据集上的模型。我们的研究为了解模型的优势和不足提供了宝贵的见解，并为不同领域中准确、鲁棒的模型的发展提供了参考。这项研究对未来的MDS研究具有重要价值。

    This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can 
    
[^42]: 利用大型语言模型来利用ASR不确定性

    Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])

    [http://arxiv.org/abs/2309.04842](http://arxiv.org/abs/2309.04842)

    这项工作旨在通过利用ASR的n-best列表来解决大型语言模型在口语理解任务上的潜在限制，而无需实质改变ASR和LLM的结构。

    

    尽管大型语言模型在各种自然语言处理（NLP）任务中表现出色，但要在口语理解（SLU）任务上表现出色，它们必须依靠现成的自动语音识别（ASR）系统进行转录，或者配备内置的语音模态。本文关注的是前一种情况，即LLM在SLU任务上的准确性受限于固定ASR系统在口语输入上的准确性。具体而言，我们解决了语音意图分类任务，其中高字词错误率可能限制LLM理解口头意图的能力。我们的目标不是通过设计复杂或专门的架构追求高准确性，而是在不实质改变底层ASR和LLM的情况下，看看我们能走多远，这些模型可以潜在地被多个不相关的任务共享。为此，我们提出使用ASR假设的n-best列表来提示LLM，而不仅仅是容易出错的1-best假设。

    While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypoth
    
[^43]: FIAT: 将学习范式与指令加速调优相融合

    FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])

    [http://arxiv.org/abs/2309.04663](http://arxiv.org/abs/2309.04663)

    FIAT是一种将上下文学习和完全微调范式融合的新的学习方式，可以在最大模型上进行指令和推理，并且在较小模型上进行参数更新，经过多语言任务测试，比之前的方法都表现更好。

    

    目前用于大型语言模型（LLMs）的学习范式通常分为上下文学习（ICL）和完全微调。每种范式都有其自身的取舍，这取决于可用数据、模型大小、计算成本、易用性和最终质量，但无法在所有情况下都表现良好。在本文中，我们首先以强调它们之间自然联系的方式描述了ICL和微调范式。基于这些联系，我们提出了一种名为FIAT的新学习范式，将这些范式的优点融合在一起，使得在最大模型上可以进行快速工程指令和链式思维推理，同时在参数效率调优的较小模型上使用类似的方法进行参数更新。我们在各种多语言任务上评估了FIAT的有效性，并观察到FIAT在100-10,000个训练样本规模下均比ICL和微调表现更好。我们希望FIAT能提供一种新的解决方案，使得在不同情况下都能取得更好的效果。

    Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a pr
    
[^44]: UQ在#SMM4H 2023上的论文：利用社交媒体进行公共卫生分析的ALEX方法

    UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])

    [http://arxiv.org/abs/2309.04213](http://arxiv.org/abs/2309.04213)

    本文提出了一种名为ALEX的框架，通过采用LLMs解释机制来改进社交媒体上的公共卫生分析性能。该方法通过数据增强和平衡训练解决了数据不平衡问题，并有效利用了LLMs的能力。

    

    随着社交媒体的普及，与公共卫生相关的活动也越来越多。目前的公共卫生分析技术涉及到流行的模型，如BERT和大型语言模型（LLMs）。然而，为公共卫生域训练LLMs的成本尤其昂贵。此外，社交媒体中这种域内数据集往往存在不平衡的问题。为应对这些挑战，可以通过数据增强和平衡训练来解决数据不平衡的问题。此外，可以通过适当设置模型的引导方式有效利用LLMs的能力。本文提出了一种新颖的ALEX框架，通过采用LLMs解释机制来提高社交媒体上的公共卫生分析性能。结果显示，我们的ALEX模型在Social Media Mining for Health 2023 （SMM4H）的任务2和任务4中获得了最佳性能，并在任务1中得到了较高的评分[1]。我们的代码已在 https:/ /github 上发布。

    As social media becomes increasingly popular, more and more activities related to public health emerge. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). However, the costs of training in-domain LLMs for public health are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally imbalanced. To tackle these challenges, the data imbalance issue can be overcome by data augmentation and balanced training. Moreover, the ability of the LLMs can be effectively utilized by prompting the model properly. In this paper, a novel ALEX framework is proposed to improve the performance of public health analysis on social media by adopting an LLMs explanation mechanism. Results show that our ALEX model got the best performance among all submissions in both Task 2 and Task 4 with a high score in Task 1 in Social Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https:// github
    
[^45]: CALLA数据集：从中文医学文献中探索LLMs的交互式知识获取

    The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])

    [http://arxiv.org/abs/2309.04198](http://arxiv.org/abs/2309.04198)

    该研究介绍了CALLA数据集，用于探索LLMs从中文医学文献中获取交互式知识。通过自由对话事实核查任务，评估了LLMs掌握医学知识的能力，并发现了一种称为“事实跟随响应”的现象。为了提供更准确的评估方法，人工构建了两种角度的测试数据：一种与事实一致，一种与事实不一致。

    

    大型语言模型（LLMs）在医学领域的应用引起了研究人员的兴趣。最近的研究集中于通过医学知识图构建指导微调（IFT）数据，以丰富LLMs的交互式医学知识。然而，作为丰富的医学知识来源的医学文献仍未被开发利用。我们的工作引入了CALLA数据集，以探索LLMs从中国医学文献中获取交互式知识。它通过自由对话事实核查任务评估LLMs掌握医学知识的能力。我们发现一种现象称为“事实跟随响应”，LLMs倾向于确认问题中提到的事实，并对挑战这些事实表现出不情愿。为消除这种现象导致的不准确评估，对于黄金事实，我们从两个角度人工构建测试数据：一个与事实一致，一个与事实不一致。根据这些测试数据，我们为LLMs评估其对医学知识的掌握能力提供了更准确的方法。

    The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the 
    
[^46]: ImageBind-LLM: 多模态指令调优的方法

    ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])

    [http://arxiv.org/abs/2309.03905](http://arxiv.org/abs/2309.03905)

    ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。

    

    我们提出了一种通过ImageBind对大型语言模型（LLMs）进行多模态指令调优的方法。现有的工作主要集中在语言和图像指令调优方面，与此不同，我们的ImageBind-LLM可以响应多模态条件，包括音频、3D点云、视频以及它们的嵌入空间算术，只需进行图像-文本对齐训练。在训练过程中，我们采用可学习的Bind网络来对齐LLaMA和ImageBind的图像编码器之间的嵌入空间。然后，通过Bind网络转换的图像特征被添加到LLaMA的所有层的单词标记中，通过一个无注意力和零初始化的门控机制逐步注入视觉指令。在ImageBind的联合嵌入的帮助下，简单的图像-文本训练使我们的模型展示出了卓越的多模态指令跟随能力。在推断过程中，多模态输入被送入相应的ImageBind编码器，并被处理。

    We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
    
[^47]: RoDia: 一份用于罗马尼亚方言识别的新数据集

    RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v1 [cs.CL])

    [http://arxiv.org/abs/2309.03378](http://arxiv.org/abs/2309.03378)

    RoDia是第一个用于罗马尼亚方言识别的语音数据集，包含来自五个不同地区的2小时手动标注数据，并提供了一组竞争模型作为未来研究的基准。

    

    方言识别是语音处理和语言技术中关键的任务，可以增强诸如语音识别、说话人验证等各种应用。尽管大多数研究都集中在广为使用的语言的方言识别上，但对于罗马尼亚这种资源有限的低资源语言的方言识别却没有得到足够的关注。为了填补这一研究空白，我们引入了RoDia，这是第一个用于罗马尼亚方言识别的语音数据集。RoDia数据集包含了来自罗马尼亚五个不同地区的各种语音样本，涵盖了城市和农村环境，总共有2小时的手动标注语音数据。除了我们的数据集之外，我们还介绍了一组可作为未来研究基准的竞争模型。最高得分的模型的宏F1分数为59.83%，微F1分数为62.08%，说明该任务具有挑战性。因此，我们认为RoDia是一个有价值的资源。

    Dialect identification is a critical task in speech processing and language technology, enhancing various applications such as speech recognition, speaker verification, and many others. While most research studies have been dedicated to dialect identification in widely spoken languages, limited attention has been given to dialect identification in low-resource languages, such as Romanian. To address this research gap, we introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource
    
[^48]: GPT可以在没有计算器的情况下解决数学问题

    GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])

    [http://arxiv.org/abs/2309.03241](http://arxiv.org/abs/2309.03241)

    本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。

    

    以往的研究通常认为大型语言模型无法在没有计算器工具的情况下准确执行算术运算，特别是超过8位数字的乘法，以及涉及小数和分数的运算。本文旨在挑战这种误解。通过充分的训练数据，一个拥有20亿参数的语言模型可以以近乎100%的准确度执行多位数的算术运算，而且没有数据泄露，显著超过了GPT-4（其多位数乘法准确率仅为4.3%）。我们还演示了我们的MathGLM，它是通过在包含了文本描述的附加多步骤算术运算和数学问题的数据集上从GLM-10B微调而成的，它在一个包含5000个样本的中文数学问题测试集上的表现与GPT-4相似。

    Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
    
[^49]: 针对大型语言模型的零资源幻觉预防

    Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v1 [cs.CL])

    [http://arxiv.org/abs/2309.02654](http://arxiv.org/abs/2309.02654)

    本论文提出了一种零资源幻觉预防方法，通过评估模型对输入指令中概念的熟悉程度，在遇到不熟悉的概念时不生成响应，从而解决了大型语言模型中的幻觉问题。

    

    在各个领域中广泛使用大型语言模型(LLMs)引起了“幻觉”问题的关注，这指的是LLMs生成事实不准确或没有根据的信息的情况。现有的语言助手中幻觉检测技术依赖于复杂的模糊、基于自由语言的思维链条(CoT)技术或基于参数的方法，存在解释性问题。此外，识别生成后幻觉的方法无法预防其发生，并且由于指令格式和模型风格的影响，性能不一致。在本文中，我们介绍一种新颖的预检测自我评估技术，称为{\method}，它专注于评估模型对输入指令中概念的熟悉程度，并在遇到不熟悉的概念时不生成响应。这种方法模拟了人类能够在没有把握时不作回应的能力。

    The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of "hallucination," which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as {\method}, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from respond
    
[^50]: 情感视觉对话：基于视觉对话理解情感形成的大规模基准

    Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v1 [cs.CL])

    [http://arxiv.org/abs/2308.16349](http://arxiv.org/abs/2308.16349)

    我们引入了一个名为AffectVisDial的大规模数据集，其中包含50,000个基于视觉的对话，我们训练了情感视觉对话模型来解决基于对话的问答、情感预测和情感解释任务，展示出了有希望的情感推理能力。

    

    我们引入了情感视觉对话，作为一个测试平台，用于研究理解在基于视觉对话中情感形成的过程。这项任务涉及三项技能：（1）基于对话的问答，（2）基于对话的情感预测，以及（3）基于对话生成情感解释。我们的主要贡献是构建了一个大规模数据集，称为AffectVisDial，包含50,000个10轮的基于视觉的对话，还包括总结的情感归因和基于对话的情感解释，总共需要27180个工作小时。我们解释了收集该数据集的设计决策，并介绍了与对话参与者相关的提问者和回答者任务。我们训练和展示了来自最先进模型的坚实的情感视觉对话基线。值得注意的是，我们模型生成的回答显示出有希望的情感推理能力。

    We introduce Affective Visual Dialog, an emotion explanation and reasoning task as a testbed for research on understanding the formation of emotions in visually grounded conversations. The task involves three skills: (1) Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3) Affective emotion explanation generation based on the dialog. Our key contribution is the collection of a large-scale dataset, dubbed AffectVisDial, consisting of 50K 10-turn visually grounded dialogs as well as concluding emotion attributions and dialog-informed textual emotion explanations, resulting in a total of 27,180 working hours. We explain our design decisions in collecting the dataset and introduce the questioner and answerer tasks that are associated with the participants in the conversation. We train and demonstrate solid Affective Visual Dialog baselines adapted from state-of-the-art models. Remarkably, the responses generated by our models show promising emotional reasoning abilit
    
[^51]: LLaSM: 大型语言和语音模型

    LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])

    [http://arxiv.org/abs/2308.15930](http://arxiv.org/abs/2308.15930)

    LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。

    

    最近，多模态大型语言模型引起了广泛关注。然而，大部分研究都集中在视觉-语言多模态模型上，提供了强大的能力来遵循视觉和语言指令。然而，我们认为语音也是人类与世界互动的重要方式。因此，对于一个通用的助手来说，能够遵循多模态语音和语言指令是至关重要的。在这项工作中，我们提出了大型语言和语音模型（LLaSM）。LLaSM是一个端到端训练的大型多模态语音-语言模型，具有跨模态对话能力，能够遵循语音和语言指令。我们的初步实验表明，LLaSM展示了一种更方便自然的人机交互方式。为了支持研究，我们还发布了一个大型的语音指令数据集LLaSM-Audio-Instructions。代码和演示可在https://github.com/LinkSoul-AI/LLaSM和ht上查看

    Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
    
[^52]: FonMTL:面向Fon语的多任务学习

    FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.14280](http://arxiv.org/abs/2308.14280)

    本文面向Fon语的多任务学习，旨在通过在命名实体识别和词性标注任务上共享知识，增强模型在Fon语自然语言处理中的性能。

    

    Fon语是一种真正的低资源非洲语言，大约有200万人口，其在线存在有限，并且现有数据集也很有限。多任务学习是一种学习范式，旨在通过在不同但相关的任务间共享知识来提高模型的泛化能力，这在数据稀缺的场景中尤为重要。本文提出了第一种探索性的多任务学习方法，用于增强Fon语自然语言处理中的模型能力。具体来说，我们探索了Fon语的命名实体识别（NER）和词性标注（POS）任务。我们利用两个语言模型作为编码器来构建输入的共享表示，并使用线性层块进行每个任务的分类。我们在Fon语的NER和POS任务上的结果与几个多语言预训练语言模型微调的性能相比，表现出竞争力（或更好）。

    The Fon language, spoken by an average 2 million of people, is a truly low-resourced African language, with a limited online presence, and existing datasets (just to name but a few). Multitask learning is a learning paradigm that aims to improve the generalization capacity of a model by sharing knowledge across different but related tasks: this could be prevalent in very data-scarce scenarios. In this paper, we present the first explorative approach to multitask learning, for model capabilities enhancement in Natural Language Processing for the Fon language. Specifically, we explore the tasks of Named Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage two language model heads as encoders to build shared representations for the inputs, and we use linear layers blocks for classification relative to each task. Our results on the NER and POS tasks for Fon, show competitive (or better) performances compared to several multilingual pretrained language models finet
    
[^53]: 为什么我们需要神经符号人工智能来建模实用的类比?

    Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v1 [cs.AI])

    [http://arxiv.org/abs/2308.01936](http://arxiv.org/abs/2308.01936)

    本文讨论了神经符号人工智能在处理逐渐复杂的类比推理时的必要性，以提供超越文字内容的广泛、多样化的知识，并结合统计和符号人工智能技术来增强和引导映射过程。

    

    智能的一个特点是能够利用熟悉的领域对不那么熟悉的领域进行推理，即类比推理。本文探讨了大型语言模型（LLMs）在处理在非结构化文本中表达的逐渐复杂的类比时的性能。我们讨论了四个不同复杂级别的类比：词汇类比、句法类比、语义类比和实用类比。随着类比变得越来越复杂，它们需要超出文本内容的广泛、多样化的知识，这在支持LLMs的词汇共现统计中不太可能找到。为了解决这个问题，我们讨论了采用神经符号人工智能技术的必要性，这些技术结合了统计和符号人工智能，根据非结构化文本提供信息以突出和增强相关内容，提供抽象和引导映射过程。我们的知识驱动方法在保持LLMs的效率的同时保持其性能。

    A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preservin
    
[^54]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^55]: 重写脚本：为语音交互适应文本指令的研究

    Rewriting the Script: Adapting Text Instructions for Voice Interaction. (arXiv:2306.09992v1 [cs.HC] CROSS LISTED)

    [http://arxiv.org/abs/2306.09992](http://arxiv.org/abs/2306.09992)

    本研究旨在解决语音助手在引导人们完成复杂任务方面的挑战。我们通过观察参与者使用语音助手进行烹饪，发现了当前方法的九个问题，包括模糊大局、信息过载和无法传达操作性等。我们提出了一种新的方法来适应文本指令的语音交互。

    

    近年来，语音助手的使用率迅速上升，但其应用范围仍主要局限于简单的任务，如播放音乐、无需手动搜索或控制物联网设备。本文研究语音助手在引导人们完成更复杂任务方面的限制：朗读文字指令。以食谱为例，我们观察了12名参与者在家使用最先进的语音助手进行烹饪。我们发现当前的方法存在9个挑战，包括模糊大局、信息过载和无法传达操作性等问题。语音助手所提供的指令尤其困难，因为它们不能像书面指令一样轻松浏览。尤其是Alexa在向用户提供关键信息或回答问题方面表现不佳。根据我们的观察，我们提出了一种新的方法，用于适应文本指令的语音交互。

    Voice assistants have sharply risen in popularity in recent years, but their use has been limited mostly to simple applications like music, hands-free search, or control of internet-of-things devices. What would it take for voice assistants to guide people through more complex tasks? In our work, we study the limitations of the dominant approach voice assistants take to complex task guidance: reading aloud written instructions. Using recipes as an example, we observe twelve participants cook at home with a state-of-the-art voice assistant. We learn that the current approach leads to nine challenges, including obscuring the bigger picture, overwhelming users with too much information, and failing to communicate affordances. Instructions delivered by a voice assistant are especially difficult because they cannot be skimmed as easily as written instructions. Alexa in particular did not surface crucial details to the user or answer questions well. We draw on our observations to propose eig
    
[^56]: 从单语数据源中训练双语和代码切换语音识别模型的方法

    Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.08753](http://arxiv.org/abs/2306.08753)

    本文介绍了一种使用纯粹的单语数据源训练双语和代码切换ASR模型的方法。通过引入集合标记器，将LID应用到每个标记，而不是在单语样本边界生成LID，我们展示了集合标记器的有效性，并提出了合成代码切换ASR数据生成技术，证明了所提出的代码切换ASR模型在语音任务中的有效性。

    

    多语言自动语音识别（ASR）模型能够转录多种语言的音频，消除了使用不同模型的需要。此外，它们还能进行语言识别（LID）和处理代码切换语音。然而，训练这些模型需要稀缺的代码切换和多语音数据语料库。本文评估了使用纯粹的单语数据源训练双语和代码切换ASR模型的不同方法。我们引入了集合标记器的概念，它与目前主流技术在单语样本边界生成LID的方法不同，而是为每个发射的标记生成LID。我们比较了双语和单语模型的性能，展示了集合标记器的有效性，提出了一种合成的代码切换ASR数据生成技术，并证明了所提出的代码切换ASR模型在语音任务中的有效性。

    Multilingual Automatic Speech Recognition (ASR) models are capable of transcribing audios across multiple languages, eliminating the need for separate models. In addition, they can perform Language Identification (LID) and handle code-switched speech. However, training these models requires special code-switch and multilingual speech corpora which are sparsely available. In this paper, we evaluate different approaches towards training of bilingual as well as code-switched ASR models using purely monolingual data sources. We introduce the concept of aggregate tokenizers that differs from the current prevalent technique of generating LIDs at the boundaries of monolingual samples and produces LID for each emitted token instead. We compare bilingual and monolingual model performance, showcase the efficacy of aggregate tokenizers, present a synthetic code-switched ASR data generation technique and demonstrate the effectiveness of the proposed code-switched ASR models for the tasks of speech
    
[^57]: 基于对比学习和深度模块化的语音分离

    Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])

    [http://arxiv.org/abs/2305.10652](http://arxiv.org/abs/2305.10652)

    本文提出了一种基于对比学习和深度模块化的完全无监督语音分离方法，解决了有监督学习中存在的排列问题、说话人数量不匹配的问题和高质量标记数据的依赖问题。

    

    目前，语音分离的最先进工具依赖于有监督学习。这意味着它们必须处理排列问题，它们受到训练和推断中使用的说话者数量不匹配的影响。此外，它们的性能严重依赖于高质量标记数据的存在。这些问题可以通过采用完全无监督的语音分离技术有效地解决。在本文中，我们使用对比学习建立帧的表示，然后在下游的深度模块化任务中使用学习到的表示。具体而言，在语音分离中，说话人的不同帧可以被看作是给定那个说话人的隐含标准帧的增强版。说话人的帧包含足够的韵律信息重叠，这是语音分离的关键。基于此，我们实现了自监督学习，学习缩小帧之间的距离。

    The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
    
[^58]: PaLM 2 技术报告

    PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])

    [http://arxiv.org/abs/2305.10403](http://arxiv.org/abs/2305.10403)

    PaLM 2 是一种计算效率更高的最先进的语言模型，提供了更好的多语言和推理能力，并且通过使用多种目标进行训练，获得了在不同模型大小的下游任务上显着的改进质量。此外，PaLM 2 还展示了强大的推理能力和稳定的性能表现，使得模型能够更广泛地部署，并且可以控制毒性推理时间，而不会对其他能力产生影响。

    

    我们介绍了 PaLM 2，这是一种新的最先进的语言模型，比其前身 PaLM 在多语言和推理能力方面更加出色，并且计算效率更高。PaLM 2 是一种基于 Transformer 的模型，使用多种目标进行训练。通过对英语和多语言语言以及推理任务的广泛评估，我们展示了 PaLM 2 在不同模型大小的下游任务上具有显着的改进质量，同时展现了比 PaLM 更快和更有效的推理能力。这种改进的效率使得模型能够更广泛地部署，同时也使得模型能够更快地响应，以获得更自然的交互节奏。PaLM 2 展示了强大的推理能力，在 BIG-Bench 和其他推理任务上相对于 PaLM 有巨大的改进。PaLM 2 在一套负责人的 AI 评估中表现出稳定的性能，并且在没有附加运行开销或对其他能力产生影响的情况下，能够对毒性进行推理时间的控制。

    We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
    
[^59]: 星际闲聊：使用大语言模型与天文学文献对话

    Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v1 [cs.CL])

    [http://arxiv.org/abs/2304.05406](http://arxiv.org/abs/2304.05406)

    本论文展示了使用OpenAI GPT-4大型语言模型通过上下文提示与天文学文献进行交互的潜力。该模型可用于提供多文献上下文下的详细答案，为天文学界探索开辟了有前途的途径。

    

    我们展示了利用上下文提示，采用最先进的OpenAI GPT-4大语言模型与天文学论文进行有意义互动的潜力。为了提高效率，我们采用了一种蒸馏技术，可以有效地减少原始输入论文的50\％，同时保持段落结构和整体语义完整性。然后，我们使用多个文档内容进行模型的响应（十个蒸馏过的文献）。我们的研究结果表明，GPT-4在多文档领域表现出色，提供了在相关研究发现框架中进行上下文化详细解答的信息。我们的结果展示了大型语言模型在天文学界的潜力，为进一步探索提供了有前途的途径，尤其是利用模型进行假设生成的可能性。

    We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large language model to engage in meaningful interactions with Astronomy papers using in-context prompting. To optimize for efficiency, we employ a distillation technique that effectively reduces the size of the original input paper by 50\%, while maintaining the paragraph structure and overall semantic integrity. We then explore the model's responses using a multi-document context (ten distilled documents). Our findings indicate that GPT-4 excels in the multi-document domain, providing detailed answers contextualized within the framework of related research findings. Our results showcase the potential of large language models for the astronomical community, offering a promising avenue for further exploration, particularly the possibility of utilizing the models for hypothesis generation.
    
[^60]: 大型语言模型生成混合代码文本的提示：东南亚语言的案例

    Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])

    [http://arxiv.org/abs/2303.13592](http://arxiv.org/abs/2303.13592)

    本文探讨了使用大型语言模型（LLMs）生成东南亚五种语言和Singlish的混合代码数据的方法，发现ChatGPT展现出最高的潜力。然而，由于词汇选择错误的影响，ChatGPT和InstructGPT在生成混合代码时的熟练程度受到限制。

    

    尽管混合代码在世界许多地区是一种常见的语言实践，但收集高质量且低成本的混合代码数据仍然是自然语言处理（NLP）研究的重大挑战。最近大型语言模型（LLMs）的普及迫使人们问：这些系统能用于数据生成吗？在本文中，我们探讨了在一个零-shot的方式下如何提示LLMs为东南亚（SEA）的五种语言（印尼语，马来语，中文，塔加路语，越南语）及克里奥尔语S ingl ish创造混合代码数据。我们发现，ChatGPT显示出最大的潜力，当明确定义“混合代码”术语时，能够68%的时间生成混合代码文本。此外，ChatGPT和InstructGPT（davinci-003）生成S ingl ish文本的表现也值得注意，它们在各种提示下的成功率平均为96%。但是，ChatGPT和InstructGPT的混合代码熟练程度受到词汇选择错误的影响，导致语义不正确的输出。

    While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
    
[^61]: ROSCOE: 用于评分逐步推理的一套度量指标

    ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07919](http://arxiv.org/abs/2212.07919)

    ROSCOE是一套度量指标，用于评分逐步推理的正确性和质量。它可以衡量语义一致性、逻辑性、信息量、流畅度和事实等特征，并提供可解释的评估方法。

    

    当大型语言模型被要求生成逐步推理来解释其最终答案时，它们在下游任务性能上显示出了改进。这些推理步骤大大提高了模型的可解释性和验证性，但在没有可靠的自动评估方法的情况下，独立于最终答案来研究它们的正确性是困难的。我们并不知道所述的推理步骤实际上有多少支持最终任务预测结果。在这项工作中，我们提出了ROSCOE，这是一套可解释的无监督自动评分指标，它改进并扩展了先前的文本生成评估指标。为了评估ROSCOE与基线指标的差异，我们设计了一种推理错误的分类，并在常用的推理数据集上收集了合成和人工评估得分。与现有指标相比，ROSCOE可以通过利用逐步推理的特性来衡量语义一致性、逻辑性、信息量、流畅度和事实等特征。

    Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rat
    
[^62]: 结构化知识增强的开放世界故事生成：一项全面调查

    Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04634](http://arxiv.org/abs/2212.04634)

    本文对结构化知识增强的故事生成进行了综述，总结了目前的方法与技术，指出了未来的发展方向和尚未解决的问题。

    

    讲故事和叙事是人类体验的基础，与我们的社会和文化参与密不可分。因此，长期以来，研究人员一直尝试创建能够自动生成故事的系统。近年来，受深度学习和大规模数据资源的推动，自动生成故事已经取得了很大的进展。然而，仍然存在一些重大挑战，例如，需要在生成的故事中实现全局一致性，这使得生成模型无法达到与人类叙述者相同的叙事能力。为了解决这些挑战，许多研究试图在生成过程中注入结构化知识，这被称为结构化知识增强的故事生成。将外部知识纳入其中可以增强故事事件之间的逻辑连贯性，实现更好的知识基础，并减轻故事中过度概括和重复问题。本次调查提供了对该研究领域的最新和全面的回顾：（i）我们提供了对结构化知识增强故事生成的综述，（ii）我们总结了目前的方法与技术，（iii）我们指出了尚待解决的问题与未来的发展方向。

    Storytelling and narrative are fundamental to human experience, intertwined with our social and cultural engagement. As such, researchers have long attempted to create systems that can generate stories automatically. In recent years, powered by deep learning and massive data resources, automatic story generation has shown significant advances. However, considerable challenges, like the need for global coherence in generated stories, still hamper generative models from reaching the same storytelling ability as human narrators. To tackle these challenges, many studies seek to inject structured knowledge into the generation process, which is referred to as structured knowledge-enhanced story generation. Incorporating external knowledge can enhance the logical coherence among story events, achieve better knowledge grounding, and alleviate over-generalization and repetition problems in stories. This survey provides the latest and comprehensive review of this research field: (i) we present a
    
[^63]: 学习从多个选项中选择

    Learning to Select from Multiple Options. (arXiv:2212.00301v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.00301](http://arxiv.org/abs/2212.00301)

    本文提出了一个上下文化的文本蕴涵（TE）模型（Context-TE），通过考虑其他选项作为当前建模的上下文，它能够解决TE方法中的两个限制。这个模型可以学习到更可靠的选项决策，并且通过加速推理过程来提高效率。

    

    许多自然语言处理任务可以看作是从一组选项中进行选择，比如分类任务、多项选择题等。文本蕴涵（TE）被证明是处理这些选择问题的最先进方法。TE将输入文本视为前提（P），选项视为假设（H），然后通过对（P，H）进行配对建模来处理选择问题。然而，TE方法存在两个限制：首先，配对建模无法意识到其他选项，这不够直观，因为人们常常通过比较竞争候选项来确定最佳选项；其次，配对TE的推理过程耗时，特别是当选项空间较大时。为了解决这两个问题，本文首先提出了一种上下文化的TE模型（Context-TE），通过将其他k个选项附加为当前（P，H）建模的上下文来进行建模。Context-TE能够通过考虑不同的上下文来学习到更可靠的H决策。其次，我们通过提出Pa

    Many NLP tasks can be regarded as a selection problem from a set of options, such as classification tasks, multi-choice question answering, etc. Textual entailment (TE) has been shown as the state-of-the-art (SOTA) approach to dealing with those selection problems. TE treats input texts as premises (P), options as hypotheses (H), then handles the selection problem by modeling (P, H) pairwise. Two limitations: first, the pairwise modeling is unaware of other options, which is less intuitive since humans often determine the best options by comparing competing candidates; second, the inference process of pairwise TE is time-consuming, especially when the option space is large. To deal with the two issues, this work first proposes a contextualized TE model (Context-TE) by appending other k options as the context of the current (P, H) modeling. Context-TE is able to learn more reliable decision for the H since it considers various context. Second, we speed up Context-TE by coming up with Pa
    
[^64]: 检验自然语言模型对人类语言判断预测的极限

    Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.03592](http://arxiv.org/abs/2204.03592)

    该论文通过对争议句对进行实验比较，发现神经网络语言模型中GPT-2与人类判断最为一致，揭示了模型的失败以及找出最符合人类判断的模型。

    

    神经网络语言模型可以作为关于人类语言处理方式的计算假设。我们使用一种新颖的实验方法对多样的语言模型进行了模型与人类一致性的比较：争议句对。对于每个争议句对，两个语言模型在哪个句子更可能出现在自然文本中上存在不同意见。考虑到九个语言模型（包括n-gram、循环神经网络和变换器模型），我们通过从语料库中选择句子或者合成优化句对来创建了数百个这样的争议句对。然后，人类受试者提供了判断，指示在每个句对中，哪个句子更可能发生。争议句对被证明极为有效，能够揭示模型的失败和识别与人类判断最为一致的模型。经过测试，最符合人类判断的模型是GPT-2，尽管实验还发现了显著的其他模型。

    Neural network language models can serve as computational hypotheses about how humans process language. We compared the model-human consistency of diverse language models using a novel experimental approach: controversial sentence pairs. For each controversial sentence pair, two language models disagree about which sentence is more likely to occur in natural text. Considering nine language models (including n-gram, recurrent neural networks, and transformer models), we created hundreds of such controversial sentence pairs by either selecting sentences from a corpus or synthetically optimizing sentence pairs to be highly controversial. Human subjects then provided judgments indicating for each pair which of the two sentences is more likely. Controversial sentence pairs proved highly effective at revealing model failures and identifying models that aligned most closely with human judgments. The most human-consistent model tested was GPT-2, although experiments also revealed significant s
    
[^65]: 慎言而后言：明确生成隐含常识知识以供响应生成

    Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.08501](http://arxiv.org/abs/2110.08501)

    本文提出了一种生成方法，可以明确生成隐含常识知识并用于响应生成。实证结果显示，该方法能够产生更丰富、更具体且遵循常识的响应，经由人工标注者评估也能够生成具有意义且相关的知识。

    

    隐含知识，如常识，对于流畅的人类对话很关键。当前的神经响应生成模型被训练成直接生成响应，忽略了未明示的隐含知识。在本文中，我们提出了慎言而后言（TBS）的生成方法，首先外化隐含常识知识（思考），然后利用该知识生成响应（言）。我们期望外化隐含知识能够实现更高效的学习，产生更丰富的响应，并且实现更可解释的模型。我们分析了采集知识对齐对话、表示隐含知识以及在知识和对话之间过渡的不同选择。实证结果显示，TBS模型在大多数自动评估指标上优于端到端和增加知识的响应生成基准，并且生成更丰富、更具体且遵循常识的响应，经由人工标注者评估也生成具有意义且相关的知识。

    Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak). We expect that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues. Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators. TBS also generates knowledge that makes sense and is relevant to the 
    

