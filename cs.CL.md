# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance.](http://arxiv.org/abs/2304.05372) | 本文通过测量OpenAI ChatGPT和Google Bard等AI聊天机器人的可靠性发现，其在感知和评估写作提示的复杂性方面与经验丰富的人类评分者的一致性较低。 |
| [^2] | [Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories.](http://arxiv.org/abs/2304.05371) | 本文研究了聊天机器人的一个新发展：长期记忆机制。然而，我们发现这种机制可能会导致机器人记住了错误和虚假的信息，并将其作为事实陈述重复。 |
| [^3] | [Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding.](http://arxiv.org/abs/2304.05368) | 本研究全面评估了大型语言模型在临床语言理解任务上的表现，并引入自问自答提示策略来提高LLMs在医疗保健相关任务中的效果。 |
| [^4] | [The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges.](http://arxiv.org/abs/2304.05351) | 本研究对ChatGPT在股票预测方面进行了零样本分析，结果表明其预测股票移动的表现不如最先进和传统方法，需要进一步改进。 |
| [^5] | [Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in Slavic Languages.](http://arxiv.org/abs/2304.05336) | 本文介绍了亚当·密茨凯维奇大学 (AMU) 在SlavNER第四次共享任务中探究使用基础模型的解决方案，通过使用基于流行的BERT和T5模型架构的模型和外部数据集，该方法对斯拉夫语的NER和词形还原非常有效，结果有希望。 |
| [^6] | [Toxicity in ChatGPT: Analyzing Persona-assigned Language Models.](http://arxiv.org/abs/2304.05335) | 论文分析了基于对话的大型语言模型ChatGPT中的毒性，通过指定人物角色，输出会涉及刻板印象、有害对话和伤人的言论。因此，我们需要充分理解LLMs的能力和局限性，以确保这些系统的安全性。 |
| [^7] | [Emergent autonomous scientific research capabilities of large language models.](http://arxiv.org/abs/2304.05332) | 本文介绍了一种智能代理系统，该系统结合多个大型语言模型，可自主设计、规划和执行科学实验。我们展示了代理的科学研究能力，并提供了三个不同的示例，其中最复杂的示例是成功地执行了催化的交叉偶联反应。 |
| [^8] | [ELVIS: Empowering Locality of Vision Language Pre-training with Intra-modal Similarity.](http://arxiv.org/abs/2304.05303) | 本文提出了一种新的视觉语言预训练方法，称为ELVIS，可以增强放射学报告或 X 射线图像中的局部性能力，提高了理解位置参考的能力。 |
| [^9] | [RRHF: Rank Responses to Align Language Models with Human Feedback without tears.](http://arxiv.org/abs/2304.05302) | RRHF是一种新的学习范式，可以高效地对齐语言模型输出概率与人类偏好，它通过排序损失对不同采样策略生成的响应进行评分，并在调整过程中只需1到2个模型。 |
| [^10] | [An Entity-based Claim Extraction Pipeline for Real-world Biomedical Fact-checking.](http://arxiv.org/abs/2304.05268) | 本论文提出了一个通过实体链接来实现术语规范化的声明提取管道，成功提取了医学推文中的基于实体的声明，特别是在社交媒体文本上的表现令人满意。 |
| [^11] | [Controllable Textual Inversion for Personalized Text-to-Image Generation.](http://arxiv.org/abs/2304.05265) | 本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。 |
| [^12] | [Approximating Human Evaluation of Social Chatbots with Prompting.](http://arxiv.org/abs/2304.05253) | 该论文提出了一种利用提示来评估社交Chatbot的新方法，可以近似人类对Chatbot的主观评估，而不需要人类准备评估材料。 |
| [^13] | [Towards preserving word order importance through Forced Invalidation.](http://arxiv.org/abs/2304.05221) | 强制无效化技术可帮助预训练语言模型识别错误的单词序列，从而提高模型对单词顺序的敏感性。 |
| [^14] | [Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond.](http://arxiv.org/abs/2304.05216) | 本文研究了对预训练代码模型的微调，探索了各层预训练表示和编码的代码知识，提出了有效的微调方案。实验发现微调过程可以保留大部分代码属性，基本代码属性由较低和中间层捕获。 |
| [^15] | [LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization.](http://arxiv.org/abs/2304.05205) | 本文提出了一种基于文本关联和生成模型混合方法的越南多文档摘要方法，该方法在VLSP 2022竞赛中表现良好。 |
| [^16] | [Multi-step Jailbreaking Privacy Attacks on ChatGPT.](http://arxiv.org/abs/2304.05197) | 本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。 |
| [^17] | [Teaching Large Language Models to Self-Debug.](http://arxiv.org/abs/2304.05128) | 本文提出了一种自我调试方法，通过少量演示来教授大型语言模型自动调试其预测的程序，在多项代码生成基准测试中取得了最先进的性能。 |
| [^18] | [FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training.](http://arxiv.org/abs/2304.05051) | 基于时尚符号和特征提示的细粒度时尚视觉语言预训练方法，可以更好地建模时尚属性和特征，有效提高多种下游任务的表现。 |
| [^19] | [What Food Do We Tweet about on a Rainy Day?.](http://arxiv.org/abs/2304.05041) | 研究发现不同的天气条件下人们在推特上谈论的食物存在差异，研究这种现象可促进对食品消费者选择和看法的理解。 |
| [^20] | [Human-machine cooperation for semantic feature listing.](http://arxiv.org/abs/2304.05012) | 本文提出了一种人机合作的方法，将有限数据的人类词汇语义模型和大型语言模型结合起来，高效生成高质量的语义特征规范。 |
| [^21] | [Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition.](http://arxiv.org/abs/2304.04991) | 本文提出了一种名为Sim-T的轻量级模型，采用了复用技术有效压缩模型，保持了模型性能，可以很好地解决在计算资源或存储内存有限的设备上部署的问题。 |
| [^22] | [Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference.](http://arxiv.org/abs/2304.04947) | 提出一种名为条件适配器（CoDA）的参数高效的迁移学习方法，它可以通过在现有的密集预训练模型中增加稀疏激活、少量新参数以及轻量级的训练阶段来实现平衡速度和准确性的新方式，实验结果表明，这种方法可以在各种任务中实现2倍至8倍的推理加速，且准确率有轻微或无损失，且参数效率相同。 |
| [^23] | [Sentence-Level Relation Extraction via Contrastive Learning with Descriptive Relation Prompts.](http://arxiv.org/abs/2304.04935) | 本文提出了一种新的范式，即具有描述性关系提示的对比学习(CTL-DRP)，通过考虑实体信息、关系知识和实体类型限制，显著提高了句子级关系抽取的准确性。在 TACRED、TACREV 和 Re-TACRED 上获得了竞争性的 F1 分数，其中在 Re-TACRED 上表现最好。 |
| [^24] | [Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task.](http://arxiv.org/abs/2304.04933) | 本文证明了深度强化学习可用于提供自适应的教育支持，尤其对于最初成绩较低的学生具有最大的益处。 |
| [^25] | [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics.](http://arxiv.org/abs/2304.04907) | 本文提出了三个代理任务用于在代理的领域内预训练，以帮助模型生成未来视图图像语义，从而在视觉与语言导航任务中提高性能。 |
| [^26] | [DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach.](http://arxiv.org/abs/2304.04881) | DISTO提出了一种新的学习度量标准来评估多项选择题中生成的干扰选项。DISTO与人类对干扰选项的评分高度相关，且排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不适用于干扰选项评估。 |
| [^27] | [Generative Knowledge Selection for Knowledge-Grounded Dialogues.](http://arxiv.org/abs/2304.04836) | 本论文提出了一个称为GenKS的简单而有效的生成式知识选择方法，在基于知识的对话系统中，通过生成知识片段的标识符，捕捉并解决了知识内部的交互，同时通过超链接机制显式地建模了对话-知识交互。 |
| [^28] | [A Large-Scale Comparative Study of Accurate COVID-19 Information versus Misinformation.](http://arxiv.org/abs/2304.04811) | 本文通过大规模比较研究表明，COVID-19错误信息的分布、传播能力、语言分析与准确信息不同，并且研制了一个新的分类数据集，该数据集平均提高了9%以上的错误信息分类能力。 |
| [^29] | [Examining Temporalities on Stance Detection Towards COVID-19 Vaccination.](http://arxiv.org/abs/2304.04806) | 研究考虑了时间性对COVID-19疫苗态度检测的影响，发现时间分割显著降低了立场分类的准确性。 |
| [^30] | [Expectations over Unspoken Alternatives Predict Pragmatic Inferences.](http://arxiv.org/abs/2304.04758) | 研究表明，人们在理解语言时会根据上下文中未明说的替代方案的期望来做出标度推理，该机制解释了标度推理内部和跨标度变化的原因。 |
| [^31] | [ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit.](http://arxiv.org/abs/2304.04596) | ESPnet-ST-v2是一个开源的多功能口语翻译工具包，支持多种翻译任务，采用了先进的架构和技术，具有非常高的性能表现。 |
| [^32] | [Linking Representations with Multimodal Contrastive Learning.](http://arxiv.org/abs/2304.03464) | 本文提出了一种名为CLIPPINGS的多模态框架，用于记录链接。该框架利用深度学习和对比学习的方法，通过端到端训练对称的视觉和语言编码器，在度量空间中学习相近或不同类别的表示方法，用于多个应用场景，如构建全面的补充专利注册表和识别不同社交媒体平台上的个人。 |
| [^33] | [Approach Intelligent Writing Assistants Usability with Seven Stages of Action.](http://arxiv.org/abs/2304.02822) | 本文提出采用诺曼的七个动作阶段作为智能写作助手交互设计的框架，并提供支持这些动作阶段的工具的示例。该框架有潜力成为人-LLM交互研究的重要工具。 |
| [^34] | [Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT.](http://arxiv.org/abs/2304.02213) | 本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。 |
| [^35] | [Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing.](http://arxiv.org/abs/2304.02017) | 本文全面探讨了ChatGPT在自然语言处理中的应用、优点和局限性，强调了使用这个强大工具时的道德考虑，为人工智能和NLP领域的讨论做出了贡献。 |
| [^36] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^37] | [Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture.](http://arxiv.org/abs/2303.16753) | 本文提出了一种参数有效的结构，通过MPO分解共享中央张量并保持层特定的辅助张量，将预训练语言模型扩展到更深的深度。 |
| [^38] | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.](http://arxiv.org/abs/2303.13988) | 本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。 |
| [^39] | [Retrieval-Augmented Classification with Decoupled Representation.](http://arxiv.org/abs/2303.13065) | 本文提出了一个混合粒度的中文BERT（MigBERT），利用同时考虑字符和词汇的表示方式，提高了中文PLMs的表现，并在各种中文NLP任务中取得了新的SOTA性能。单词比字符语义更丰富。数字也显示了MigBERT可以在日语中使用。 |
| [^40] | [Does Synthetic Data Generation of LLMs Help Clinical Text Mining?.](http://arxiv.org/abs/2303.04360) | 研究探讨了利用 ChatGPT 进行临床文本挖掘的潜力，但初步结果表明效果欠佳，同时上传患者信息也引发隐私问题。因此，提出了一种新的训练方法，即使用 ChatGPT 生成带标签的大量高质量合成数据进行训练。 |
| [^41] | [Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?.](http://arxiv.org/abs/2301.11219) | 本文旨在研究有害段子中实体扮演的角色，探测每个实体是否是段子中的英雄、恶棍或受害者，使用HVVMemes数据集，设计了VECTOR模型来执行任务。 |
| [^42] | [EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records.](http://arxiv.org/abs/2301.07695) | 该论文提出了一个面向电子病历数据的文本转SQL数据集，该数据集具有一系列独特挑战，包括生成SQL查询、理解时间表达式以及区分有无答案的问题。 |
| [^43] | [VaxxHesitancy: A Dataset for Studying Hesitancy Towards COVID-19 Vaccination on Twitter.](http://arxiv.org/abs/2301.06660) | 该论文介绍了一份用于研究推特上 COVID-19 疫苗犹豫的数据集，疫苗犹豫一直是一个普遍的问题，了解公众对 COVID-19 疫苗犹豫的原因非常重要。 |
| [^44] | [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks.](http://arxiv.org/abs/2210.15629) | 本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。 |
| [^45] | [Prophet Attention: Predicting Attention with Future Attention for Image Captioning.](http://arxiv.org/abs/2210.10914) | 本文提出了一种先知式注意力模型，它利用未来信息计算理想的注意力权重，进一步规范偏移的注意力，这种方法在图像描述中取得了最先进的性能。 |
| [^46] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^47] | [Competence-based Multimodal Curriculum Learning for Medical Report Generation.](http://arxiv.org/abs/2206.14579) | 本论文提出了一个基于能力的多模态课程学习框架（CMCL），通过模拟放射学家的学习过程来逐步优化医学报告生成模型，在公共数据集上实验表明CMCL可以有效地提高模型性能。 |
| [^48] | [Survey of Aspect-based Sentiment Analysis Datasets.](http://arxiv.org/abs/2204.05232) | 本研究汇总了65个公开可用的ABSA数据集，包括45个英文数据集和20个其他语言数据集，提供了一个可以用于训练和评估自主ABSA系统的数据库。 |
| [^49] | [Contrastive Attention for Automatic Chest X-ray Report Generation.](http://arxiv.org/abs/2106.06965) | 本文提出对比注意力（CA）模型来准确捕捉和描述自动生成胸部X光图像描述中的异常区域，该模型将当前输入图像与正常图像进行比较以提取对比信息。实验结果证明了该方法的有效性和优越性。 |

# 详细

[^1]: ChatGPT和Bard能够生成一致的评估项目吗？针对人类表现的可靠性分析。

    Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance. (arXiv:2304.05372v1 [cs.CL])

    [http://arxiv.org/abs/2304.05372](http://arxiv.org/abs/2304.05372)

    本文通过测量OpenAI ChatGPT和Google Bard等AI聊天机器人的可靠性发现，其在感知和评估写作提示的复杂性方面与经验丰富的人类评分者的一致性较低。

    

    ChatGPT和Bard是基于大语言模型的AI聊天机器人，被认为能够在各种领域中应用。在教育领域，这些AI技术已被用于评估和教学。在评估中，AI长期以来一直用于自动化的论文评分和自动化的项目生成。这些工具必须具备的一项心理测量属性是可靠性高，即AI分数与人类评分者意见一致。本文测量了OpenAI ChatGP和Google Bard的可靠性，以评估这些工具在感知和评估写作提示的复杂性方面与经验丰富的人类评分者的一致性。作为绩效指标的内部相关系数（ICC）显示，OpenAI ChatGPT和Google Bard的互可靠性低于人类评分的金标准。

    ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that are slated to promise different applications in diverse areas. In education, these AI technologies have been tested for applications in assessment and teaching. In assessment, AI has long been used in automated essay scoring and automated item generation. One psychometric property that these tools must have to assist or replace humans in assessment is high reliability in terms of agreement between AI scores and human raters. In this paper, we measure the reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and trained humans in perceiving and rating the complexity of writing prompts. Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.
    
[^2]: 那不是你的记忆，它是别人的：在聊天机器人记忆中播撒错误信息

    Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])

    [http://arxiv.org/abs/2304.05371](http://arxiv.org/abs/2304.05371)

    本文研究了聊天机器人的一个新发展：长期记忆机制。然而，我们发现这种机制可能会导致机器人记住了错误和虚假的信息，并将其作为事实陈述重复。

    

    聊天机器人的一个新发展是长期记忆机制，可以记住过去对话中的信息，以增加响应的连贯性和一致性。机器人被设计为从其对话伙伴中提取个人性质的知识，例如表明对特定颜色的偏好。在本文中，我们展示了这种记忆机制可能会导致意外行为。具体而言，我们发现一个人可以将个人陈述与信息陈述结合起来，导致机器人将信息陈述与个人知识一起记录在其长期记忆中。这意味着机器人可能被欺骗记住错误信息，并在回忆与对话主题相关的信息时将其作为事实陈述重复。我们在基于ParlAI平台实现的BlenderBot 2框架上展示了这种漏洞，并在更近期、规模更大的BlenderBot 3模型上提供了例子。

    One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We gen
    
[^3]: 大型语言模型在医疗保健领域中准备就绪了吗？临床语言理解的比较研究。

    Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])

    [http://arxiv.org/abs/2304.05368](http://arxiv.org/abs/2304.05368)

    本研究全面评估了大型语言模型在临床语言理解任务上的表现，并引入自问自答提示策略来提高LLMs在医疗保健相关任务中的效果。

    

    大型语言模型（LLMs）在各个领域取得了显著的进展，包括医疗保健领域。然而，临床语言理解任务的专业性质带来了独特的挑战和限制，需要进一步研究。在本研究中，我们对最先进的LLMs——GPT-3.5、GPT-4和Bard进行了全面评估，该评估范围涵盖了各种任务，包括命名实体识别、关系提取、自然语言推理、语义文本相似性、文档分类和问答。我们还引入了一种新的提示策略——自问自答提示（SQP），旨在通过引发与相关临床场景相关的信息性问题和答案，定制化提高LLMs的性能。我们的评估强调了任务特定的学习策略和提示技术对于提高LLMs在医疗保健相关任务中的有效性的重要性。

    Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
    
[^4]: ChatGPT在多模态股票预测挑战中的零样本分析：华尔街新手？

    The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])

    [http://arxiv.org/abs/2304.05351](http://arxiv.org/abs/2304.05351)

    本研究对ChatGPT在股票预测方面进行了零样本分析，结果表明其预测股票移动的表现不如最先进和传统方法，需要进一步改进。

    

    最近，如ChatGPT这样的大型语言模型在各种自然语言处理任务中展示了惊人的性能。然而，在预测股市走势方面，它们的有效性仍然有待探索。本文通过三个推文和历史股票价格数据集的广泛零样本分析，探讨了ChatGPT在多模态股票移动预测方面的能力。我们的研究表明，ChatGPT是一个“华尔街新手”，在预测股票移动方面的成功有限，不仅不如最先进的方法，而且不如使用价格特征的线性回归这样的传统方法。尽管思维链提示策略和推文的包含具有潜在的优势，ChatGPT的表现仍然不佳。此外，我们观察到它的可解释性和稳定性存在局限性，需要更专业的训练或微调。这项研究提供了有关ChatGPT在股票预测方面的见解。

    Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
    
[^5]: 探究基础模型在斯拉夫语命名实体识别和词形还原任务中的应用

    Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in Slavic Languages. (arXiv:2304.05336v1 [cs.CL])

    [http://arxiv.org/abs/2304.05336](http://arxiv.org/abs/2304.05336)

    本文介绍了亚当·密茨凯维奇大学 (AMU) 在SlavNER第四次共享任务中探究使用基础模型的解决方案，通过使用基于流行的BERT和T5模型架构的模型和外部数据集，该方法对斯拉夫语的NER和词形还原非常有效，结果有希望。

    

    本文介绍了亚当·密茨凯维奇大学 (AMU) 在SlavNER第四次共享任务中的解决方案。该任务涉及斯拉夫语中命名实体的识别、分类和词形还原。我们的方法主要涉及基础模型的应用，特别是使用基于流行的BERT和T5模型架构的模型。此外，我们还使用外部数据集进一步提高了模型的质量。我们的解决方案在两个任务中均取得了有希望的结果，达到了较高的指标得分。我们详细描述了我们的方法以及实验结果，表明该方法对斯拉夫语的NER和词形还原非常有效。此外，我们的词形还原模型将在 https://huggingface.co/amu-cai 上进行公开。

    This paper describes Adam Mickiewicz University's (AMU) solution for the 4th Shared Task on SlavNER. The task involves the identification, categorization, and lemmatization of named entities in Slavic languages. Our approach involved exploring the use of foundation models for these tasks. In particular, we used models based on the popular BERT and T5 model architectures. Additionally, we used external datasets to further improve the quality of our models. Our solution obtained promising results, achieving high metrics scores in both tasks. We describe our approach and the results of our experiments in detail, showing that the method is effective for NER and lemmatization in Slavic languages. Additionally, our models for lemmatization will be available at: https://huggingface.co/amu-cai.
    
[^6]: 聊天GPT中的毒性：分析个性化语言模型

    Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])

    [http://arxiv.org/abs/2304.05335](http://arxiv.org/abs/2304.05335)

    论文分析了基于对话的大型语言模型ChatGPT中的毒性，通过指定人物角色，输出会涉及刻板印象、有害对话和伤人的言论。因此，我们需要充分理解LLMs的能力和局限性，以确保这些系统的安全性。

    

    大型语言模型（LLMs）展现了令人难以置信的能力，超越了自然语言处理（NLP）社区，并被广泛应用于医疗保健、治疗、教育和客户服务等多种服务中。由于用户包括有重要信息需求的人，如与聊天机器人交互的学生或患者，因此这些系统的安全性至关重要。因此，必须明确了解LLMs的能力和局限性。为此，我们系统地评估了ChatGPT中的毒性，这是一种流行的基于对话的LLM，超过半百万次Generation被测试。我们发现，通过为ChatGPT指定一个人物角色，比如拳击手穆罕默德·阿里，可以显著增加产生的毒性。根据指定给ChatGPT的角色，其毒性可能会增加到6倍，其输出会涉及不正确的刻板印象、有害的对话和伤人的言论。这可能会潜在地损害人物角色的名誉。

    Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and ha
    
[^7]: 大型语言模型紧急出现的自主科研能力

    Emergent autonomous scientific research capabilities of large language models. (arXiv:2304.05332v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.05332](http://arxiv.org/abs/2304.05332)

    本文介绍了一种智能代理系统，该系统结合多个大型语言模型，可自主设计、规划和执行科学实验。我们展示了代理的科学研究能力，并提供了三个不同的示例，其中最复杂的示例是成功地执行了催化的交叉偶联反应。

    

    基于Transformer的大型语言模型在机器学习领域迅速发展，其应用涵盖自然语言、生物学、化学和计算机编程等领域。极限扩展和强化学习有了人类反馈后大大提高了生成文本的质量，使这些模型能够执行各种任务并推理其选择。本文介绍了一种智能代理系统，该系统结合多个大型语言模型，可自主设计、规划和执行科学实验。我们展示了代理的科学研究能力，并提供了三个不同的示例，其中最复杂的示例是成功地执行了催化的交叉偶联反应。最后，我们讨论了这种系统的安全性问题，并提出了避免滥用的措施。

    Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.
    
[^8]: ELVIS: 利用模态内相似性增强视觉语言预训练中的局部性能力

    ELVIS: Empowering Locality of Vision Language Pre-training with Intra-modal Similarity. (arXiv:2304.05303v1 [cs.CV])

    [http://arxiv.org/abs/2304.05303](http://arxiv.org/abs/2304.05303)

    本文提出了一种新的视觉语言预训练方法，称为ELVIS，可以增强放射学报告或 X 射线图像中的局部性能力，提高了理解位置参考的能力。

    

    深度学习在辅助放射科医生阅读胸部 X 射线图像方面表现出巨大潜力，但其需要昂贵的注释来提高性能，这阻碍了其广泛的临床应用。视觉语言预训练（VLP）可以通过利用常规生成的放射学报告进行训练，从而减轻注释的负担和成本，这些报告以成对的形式（图像-文本对）大量存在。此外，正在提出扩展到定位感知VLP，以满足CAD在CXR的准确异常定位需求。然而，我们发现由局部性VLP文献提出的公式实际上导致了下游定位任务所需的空间关系的丢失。因此，我们提出了Empowering Locality of VLP with Intra-modal Similarity（ELVIS），这是一种VLP，可感知模态内部的局部性能力，以更好地保留放射学报告或 X 射线图像中的局部性能力，从而提高了理解位置参考的能力。

    Deep learning has shown great potential in assisting radiologists in reading chest X-ray (CXR) images, but its need for expensive annotations for improving performance prevents widespread clinical application. Visual language pre-training (VLP) can alleviate the burden and cost of annotation by leveraging routinely generated reports for radiographs, which exist in large quantities as well as in paired form (imagetext pairs). Additionally, extensions to localization-aware VLPs are being proposed to address the needs of accurate localization of abnormalities for CAD in CXR. However, we find that the formulation proposed by locality-aware VLP literatures actually leads to loss in spatial relationships required for downstream localization tasks. Therefore, we propose Empowering Locality of VLP with Intra-modal Similarity, ELVIS, a VLP aware of intra-modal locality, to better preserve the locality within radiographs or reports, which enhances the ability to comprehend location references in
    
[^9]: RRHF: 无需烦恼地使用排名响应来对齐语言模型与人类反馈

    RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v1 [cs.CL])

    [http://arxiv.org/abs/2304.05302](http://arxiv.org/abs/2304.05302)

    RRHF是一种新的学习范式，可以高效地对齐语言模型输出概率与人类偏好，它通过排序损失对不同采样策略生成的响应进行评分，并在调整过程中只需1到2个模型。

    

    人类反馈的强化学习（RLHF）可以帮助将大型语言模型与人类偏好对齐，从而显著提高人类与这些模型间的交互质量。与PPO相比，我们提出了一种新的学习范式——RRHF，它通过排序损失对不同采样策略生成的响应进行评分，并学习将它们与人类偏好对齐。RRHF可以高效地对齐语言模型输出概率与人类偏好，其效果和Fine-Tuning一样稳健，而在调整过程中只需1到2个模型。此外，RRHF可以被认为是SFT和奖励模型的扩展，与PPO相比在编码和模型数量方面更为简单。

    Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model count
    
[^10]: 基于实体的声明提取管道：用于现实生物医学事实检查的方法。

    An Entity-based Claim Extraction Pipeline for Real-world Biomedical Fact-checking. (arXiv:2304.05268v1 [cs.CL])

    [http://arxiv.org/abs/2304.05268](http://arxiv.org/abs/2304.05268)

    本论文提出了一个通过实体链接来实现术语规范化的声明提取管道，成功提取了医学推文中的基于实体的声明，特别是在社交媒体文本上的表现令人满意。

    

    为了解决现有的生物医学声明事实检查模型通常是在合成或精细措辞的数据上进行训练并且很难转移到社交媒体内容的问题，Wuehrl＆Klinger（2022）提出了基于文本中的医学实体提取简洁的声明来模仿常见训练声明的专注性的做法。然而，他们的研究有两个限制：首先，它依赖于黄金注释实体。因此，无法评估其在现实世界应用中的可行性，因为这需要自动检测相关实体。第二，他们用原始标记表示声明实体。这构成了术语不匹配，可能限制了事实检查性能。为了理解这两个挑战，我们提出了一个用于医学推文的声明提取管道，该管道包括命名实体识别和通过实体链接的术语规范化。我们发现，自动NER确实会导致性能下降，但我们发现实体链接极大地提高了系统准确提取声明的能力，特别是在社交媒体文本上。我们提出的管道成功地从医学推文中提取了简明的基于实体的声明，在手动注释的测试集上获得了0.66的F1分数。

    Existing fact-checking models for biomedical claims are typically trained on synthetic or well-worded data and hardly transfer to social media content. This mismatch can be mitigated by adapting the social media input to mimic the focused nature of common training claims. To do so, Wuehrl & Klinger (2022) propose to extract concise claims based on medical entities in the text. However, their study has two limitations: First, it relies on gold-annotated entities. Therefore, its feasibility for a real-world application cannot be assessed since this requires detecting relevant entities automatically. Second, they represent claim entities with the original tokens. This constitutes a terminology mismatch which potentially limits the fact-checking performance. To understand both challenges, we propose a claim extraction pipeline for medical tweets that incorporates named entity recognition and terminology normalization via entity linking. We show that automatic NER does lead to a performance
    
[^11]: 个性化文本到图像生成的可控文本反转

    Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])

    [http://arxiv.org/abs/2304.05265](http://arxiv.org/abs/2304.05265)

    本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。

    

    最近，大规模生成模型在以文本为引导的高保真图像的生成方面取得了前所未有的性能。当引导信息包含用户定义的、未见过的或长尾概念标记时，文本反转成为一种有效的个性化生成技术。尽管如此，我们发现并展示了文本反转的部署仍充满了“黑魔法”，例如额外数据集的严苛要求，在循环中需要艰苦的人力成本和缺乏鲁棒性等。在这项工作中，我们提出了一种名为可控文本反转的大大增强版反转，解决了所有上述问题，并反过来提供了一个强大，数据效率高，易于使用的框架。COTI的核心是基于理论的损失目标，具有全面和新颖的加权评分机制，并由主动学习范式所提取。广泛的结果表明，COTI的性能比之前技术有了显著的提升，尤其是在数据少的情况下。

    The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
    
[^12]: 利用提示来近似人类对社交Chatbot的评估

    Approximating Human Evaluation of Social Chatbots with Prompting. (arXiv:2304.05253v1 [cs.CL])

    [http://arxiv.org/abs/2304.05253](http://arxiv.org/abs/2304.05253)

    该论文提出了一种利用提示来评估社交Chatbot的新方法，可以近似人类对Chatbot的主观评估，而不需要人类准备评估材料。

    

    随着强大的对话模型逐渐面向广大用户开放，用户开始积极地与这种技术进行社交互动。除非技术得到适当的控制，这种前所未有的交互体验可能会对用户造成相当大的社交和心理风险。这就需要可扩展和强大的评估指标来评估社交Chatbot。现有的自动评估指标通常关注客观质量指标，忽略社交维度的主观感受。此外，大多数这些方法都基于可用基准数据集中预生成的对话，这意味着需要人类参与准备评估材料，因此影响了指标的可扩展性。为了解决这个问题，我们提出利用来自GPT系列的新兴大型语言模型(LLM)并描述了一种新的框架，允许进行提示式的对话系统评估。通过这个框架，我们可以通过关注主观评价标准来近似人类对社交Chatbot的评估。通过使用GPT-3，该框架可以应用于各种各样的对话模型，并且不需要任何人类输入来准备评估材料。我们通过对四种不同的对话模型进行一系列彻底的实验，并分析了框架的优缺点，证明了我们方法的有效性。

    Once powerful conversational models have become available for a wide audience, users started actively engaging in social interactions with this technology. Such unprecedented interaction experiences may pose considerable social and psychological risks to the users unless the technology is properly controlled. This creates an urgent need for scalable and robust evaluation metrics for conversational chatbots. Existing automatic evaluation metrics usually focus on objective quality measures and disregard subjective perceptions of social dimensions. Moreover, most of these approaches operate on pre-produced dialogs from available benchmark corpora, which implies human involvement for preparing the material for evaluation and, thus, impeded scalability of the metrics. To address this limitation, we propose to make use of the emerging large language models (LLMs) from the GPT-family and describe a new framework allowing to conduct dialog system evaluation with prompting. With this framework,
    
[^13]: 通过强制无效化实现保护单词顺序重要性的目标

    Towards preserving word order importance through Forced Invalidation. (arXiv:2304.05221v1 [cs.CL])

    [http://arxiv.org/abs/2304.05221](http://arxiv.org/abs/2304.05221)

    强制无效化技术可帮助预训练语言模型识别错误的单词序列，从而提高模型对单词顺序的敏感性。

    

    大型预训练语言模型例如BERT已经广泛应用于自然语言理解（NLU）任务中。然而，最近的研究发现预训练语言模型对单词顺序不敏感。即使对一个句子中单词进行随机排列，语法上的关键信息也被破坏，但NLU任务的性能仍然不变。为了保护单词顺序的重要性，我们提出了一种简单的方法：强制无效化（Forced Invalidation，FI）：强制模型将排列错误的序列识别为无效样本。我们在基于BERT和注意力的模型上进行了各种英语NLU和QA任务的广泛评估，证明了Force Invalidation可以显著提高模型对单词顺序的敏感性。

    Large pre-trained language models such as BERT have been widely used as a framework for natural language understanding (NLU) tasks. However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed. To help preserve the importance of word order, we propose a simple approach called Forced Invalidation (FI): forcing the model to identify permuted sequences as invalid samples. We perform an extensive evaluation of our approach on various English NLU and QA based tasks over BERT-based and attention-based models over word embeddings. Our experiments demonstrate that Forced Invalidation significantly improves the sensitivity of the models to word order.
    
[^14]: 面向预训练代码模型有效微调：实验研究及其拓展

    Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])

    [http://arxiv.org/abs/2304.05216](http://arxiv.org/abs/2304.05216)

    本文研究了对预训练代码模型的微调，探索了各层预训练表示和编码的代码知识，提出了有效的微调方案。实验发现微调过程可以保留大部分代码属性，基本代码属性由较低和中间层捕获。

    

    近年来，在许多软件测试和分析任务中，对预训练代码模型进行微调（如CodeBERT）以适应下游任务取得了巨大成功。虽然有效且流行，但微调预训练参数会导致大量计算成本。在本文中，我们进行了广泛的实验研究，探索微调期间每层预训练表示和编码的代码知识发生了什么。然后，我们基于上述发现提出了有效的微调大型预训练代码模型的替代方案。我们的实验研究表明：（1）源代码的词汇、语法和结构性质分别编码在较低、中间和较高的层中，而语义属性跨越整个模型。（2）微调过程保留了大部分代码属性。具体而言，较低和中间层捕获的基本代码属性在微调期间仍然保留。此外，我们发现...

    Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that o
    
[^15]: VLSP2022-Abmusu团队：基于文本关联和生成模型的越南多文档摘要的混合方法

    LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization. (arXiv:2304.05205v1 [cs.CL])

    [http://arxiv.org/abs/2304.05205](http://arxiv.org/abs/2304.05205)

    本文提出了一种基于文本关联和生成模型混合方法的越南多文档摘要方法，该方法在VLSP 2022竞赛中表现良好。

    

    多文档摘要很具有挑战性，因为摘要不仅应描述所有文档中最重要的信息，还应提供文档的连贯解释。本文提出了一种基于聚类相似性的多文档摘要方法。在提取式方法中，我们使用基于修改后的PageRank算法和文本关联考虑机制的混合模型。通过从每个簇中选择最重要的句子生成摘要后，我们应用了BARTpho和ViT5来构建抽象模型。本研究考虑了提取式和抽象式方法。所提出的方法在VLSP 2022竞赛中取得了竞争性的结果。

    Multi-document summarization is challenging because the summaries should not only describe the most important information from all documents but also provide a coherent interpretation of the documents. This paper proposes a method for multi-document summarization based on cluster similarity. In the extractive method we use hybrid model based on a modified version of the PageRank algorithm and a text correlation considerations mechanism. After generating summaries by selecting the most important sentences from each cluster, we apply BARTpho and ViT5 to construct the abstractive models. Both extractive and abstractive approaches were considered in this study. The proposed method achieves competitive results in VLSP 2022 competition.
    
[^16]: Multi-step Jailbreaking Privacy Attacks on ChatGPT

    Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])

    [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)

    本文探讨了来自OpenAI模型API以及New Bing增强版的ChatGPT对隐私带来的风险，指出应用程序集成的LLMs可能导致比以往更严重的隐私威胁，实验支持该观点。

    

    随着大型语言模型（LLMs）的迅速发展，许多下游NLP任务可以通过良好的提示得到很好的解决。尽管模型开发人员和研究人员努力确保避免从LLMs生成有害内容，但仍然难以引导AI生成的内容（AIGC）为人类带来好处。由于强大的LLMs正在吞噬来自各个领域的现有文本数据（例如，GPT-3训练了45TB的文本），因此人们自然会怀疑训练数据中是否包含私人信息以及这些LLMs及其下游应用程序可以带来什么隐私威胁。在本文中，我们研究了来自OpenAI的模型API和通过ChatGPT增强的New Bing所带来的隐私威胁，并显示应用程序集成的LLMs可能导致比以往更严重的隐私威胁。为此，我们进行了大量实验证明我们的说法，并讨论LLMs的隐私影响。

    With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.
    
[^17]: 自我调试：教授大型语言模型自动调试能力

    Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])

    [http://arxiv.org/abs/2304.05128](http://arxiv.org/abs/2304.05128)

    本文提出了一种自我调试方法，通过少量演示来教授大型语言模型自动调试其预测的程序，在多项代码生成基准测试中取得了最先进的性能。

    

    大型语言模型(LLM)在代码生成方面取得了卓越的性能，但对于复杂的编程任务，在一次性生成正确的解决方案方面变得具有挑战性。因此，一些先前的工作设计了程序修复方法来提高代码生成的性能。在本文中，我们提出了自我调试(Self-Debugging)方法，通过少量样本演示来教授大型语言模型调试其预测的程序。具体而言，我们证明了自我调试可以教授大型语言模型进行橡皮鸭子调试(Rubber Duck Debugging)。也就是说，在没有任何关于代码正确性或错误信息的反馈的情况下，该模型能够通过用自然语言解释生成的代码来识别它的错误。自我调试在多项代码生成基准测试中取得了最先进的性能，包括文本到SQL生成的Spider数据集，C++到Python翻译的TransCoder和文本到Python生成的MBPP。在没有单元测试的Spider基准测试中，所提出的自我调试方法明显优于现有的程序修复方法。

    Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
    
[^18]: FashionSAP: 基于符号和特征提示的细粒度时尚视觉语言预训练方法

    FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training. (arXiv:2304.05051v1 [cs.CV])

    [http://arxiv.org/abs/2304.05051](http://arxiv.org/abs/2304.05051)

    基于时尚符号和特征提示的细粒度时尚视觉语言预训练方法，可以更好地建模时尚属性和特征，有效提高多种下游任务的表现。

    

    时尚视觉语言预训练模型已被证明在多种下游任务中非常有效。然而，通用的视觉语言预训练模型往往忽略了细粒度领域特征，而这些特征在区分特定领域任务和常规任务中非常重要。为了建模细粒度多模态时尚属性和特征，我们提出了一种基于时尚符号和特征提示（FashionSAP）的细粒度时尚视觉语言预训练方法。首先，我们提出了时尚符号，一种新的抽象时尚概念层，用于表示不同的时尚物品，并概括各种细粒度时尚特征，从而使建模细粒度属性更加有效。其次，我们提出属性提示方法，明确让模型学习时尚物品的具体属性。我们根据时尚数据的格式设计适当的提示模板。进行了全面的实验验证。

    Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics. Firstly, we propose the fashion symbols, a novel abstract fashion concept layer, to represent different fashion items and to generalize various kinds of fine-grained fashion features, making modelling fine-grained attributes more effective. Secondly, the attributes prompt method is proposed to make the model learn specific attributes of fashion items explicitly. We design proper prompt templates according to the format of fashion data. Comprehensive experiments are conducted o
    
[^19]: 在雨天我们会推特哪些食物？

    What Food Do We Tweet about on a Rainy Day?. (arXiv:2304.05041v1 [cs.SI])

    [http://arxiv.org/abs/2304.05041](http://arxiv.org/abs/2304.05041)

    研究发现不同的天气条件下人们在推特上谈论的食物存在差异，研究这种现象可促进对食品消费者选择和看法的理解。

    

    食品选择是一个由品味、氛围、文化或天气等因素塑造的复杂现象。本文探讨了不同天气条件下与食品相关的推特数据。我们结合了覆盖过去十年的拉脱维亚食品推特数据集和包含平均温度、降水等现象的天气观察数据集。我们发现了哪些天气条件导致了特定的食品信息共享；自动分类推特情感并讨论如何根据天气情况变化。这项研究有助于大规模社交网络数据的理解，了解食品消费者的选择和看法。

    Food choice is a complex phenomenon shaped by factors such as taste, ambience, culture or weather. In this paper, we explore food-related tweeting in different weather conditions. We inspect a Latvian food tweet dataset spanning the past decade in conjunction with a weather observation dataset consisting of average temperature, precipitation, and other phenomena. We find which weather conditions lead to specific food information sharing; automatically classify tweet sentiment and discuss how it changes depending on the weather. This research contributes to the growing area of large-scale social network data understanding of food consumers' choices and perceptions.
    
[^20]: 人机合作生成语义特征列表

    Human-machine cooperation for semantic feature listing. (arXiv:2304.05012v1 [cs.CL])

    [http://arxiv.org/abs/2304.05012](http://arxiv.org/abs/2304.05012)

    本文提出了一种人机合作的方法，将有限数据的人类词汇语义模型和大型语言模型结合起来，高效生成高质量的语义特征规范。

    

    语义特征规范是描述人类概念知识的重要工具，但需要大量人力。大型语言模型（LLM）为自动生成此类功能提供了新途径，但容易出现显著的错误。本文介绍了一种新方法，将来自有限数据的学习型人类词汇语义模型与LLM生成的数据相结合，以高效生成高质量的特征规范。

    Semantic feature norms, lists of features that concepts do and do not possess, have played a central role in characterizing human conceptual knowledge, but require extensive human labor. Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error. Here, we present a new method for combining a learned model of human lexical-semantics from limited data with LLM-generated data to efficiently generate high-quality feature norms.
    
[^21]: 使用复用技术简化Transformer网络的语音识别模型Sim-T

    Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition. (arXiv:2304.04991v1 [cs.SD])

    [http://arxiv.org/abs/2304.04991](http://arxiv.org/abs/2304.04991)

    本文提出了一种名为Sim-T的轻量级模型，采用了复用技术有效压缩模型，保持了模型性能，可以很好地解决在计算资源或存储内存有限的设备上部署的问题。

    

    近年来，由于其出色的模型性能，Transformer网络在语音识别任务中受到了广泛的关注。然而，Transformer网络始终涉及大量计算和参数，导致在计算资源或存储内存有限的设备上部署时存在严重的问题。因此，本文通过提出一种名为Sim-T的新型轻量级模型，通过新开发的复用技术可以有效地压缩模型，而对模型性能的牺牲几乎可以忽略不计。具体地，所提出的技术包括模块权重复用和注意力得分复用。此外，还提出了一种新颖的解码器结构来促进注意力得分复用。实验证明，Sim-T模型非常有效。

    In recent years, a great deal of attention has been paid to the Transformer network for speech recognition tasks due to its excellent model performance. However, the Transformer network always involves heavy computation and large number of parameters, causing serious deployment problems in devices with limited computation sources or storage memory. In this paper, a new lightweight model called Sim-T has been proposed to expand the generality of the Transformer model. Under the help of the newly developed multiplexing technique, the Sim-T can efficiently compress the model with negligible sacrifice on its performance. To be more precise, the proposed technique includes two parts, that are, module weight multiplexing and attention score multiplexing. Moreover, a novel decoder structure has been proposed to facilitate the attention score multiplexing. Extensive experiments have been conducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when the proposed Sim-T is 48% para
    
[^22]: 条件适配器：具有快速推理的参数高效的迁移学习方法

    Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v1 [cs.CL])

    [http://arxiv.org/abs/2304.04947](http://arxiv.org/abs/2304.04947)

    提出一种名为条件适配器（CoDA）的参数高效的迁移学习方法，它可以通过在现有的密集预训练模型中增加稀疏激活、少量新参数以及轻量级的训练阶段来实现平衡速度和准确性的新方式，实验结果表明，这种方法可以在各种任务中实现2倍至8倍的推理加速，且准确率有轻微或无损失，且参数效率相同。

    

    我们提出了一种名为条件适配器（CoDA）的参数高效的迁移学习方法，同时提高了推理效率。CoDA不仅适用于标准适配器方法，还可以通过条件计算来实现平衡速度和准确性的新方式。通过在现有的密集预训练模型中增加稀疏激活、少量新参数以及轻量级的训练阶段，CoDA方法提供了一种出乎意料的传递知识的高效方法。我们的实验表明，与最先进的适配器方法相比，CoDA在各种语言、视觉和语音任务中都实现了2倍至8倍的推理加速，而且准确率有轻微或无损失，且参数效率相同。

    We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.
    
[^23]: 通过具有描述性关系提示的对比学习进行句子级关系抽取

    Sentence-Level Relation Extraction via Contrastive Learning with Descriptive Relation Prompts. (arXiv:2304.04935v1 [cs.CL])

    [http://arxiv.org/abs/2304.04935](http://arxiv.org/abs/2304.04935)

    本文提出了一种新的范式，即具有描述性关系提示的对比学习(CTL-DRP)，通过考虑实体信息、关系知识和实体类型限制，显著提高了句子级关系抽取的准确性。在 TACRED、TACREV 和 Re-TACRED 上获得了竞争性的 F1 分数，其中在 Re-TACRED 上表现最好。

    

    句子级关系抽取旨在识别给定句子中两个实体之间的关系。现有的工作主要集中在获得更好的实体表示并采用多标签分类器进行关系抽取。这些工作的主要局限性在于它们忽略了背景关系知识和实体类型与候选关系之间的相互关系。在本文中，我们提出了一种新的范式，即具有描述性关系提示的对比学习(CTL-DRP)，以共同考虑实体信息、关系知识和实体类型限制。具体来说，我们在生成上下文嵌入时引入了改进的实体标记和描述性关系提示，并利用对比学习来对受限候选关系进行排名。CTL-DRP 在 TACRED 上获得了竞争性的 F1 分数为 76.7%。此外，新提出的范式在 TACREV 和 Re-TACRED 上分别取得了 85.8% 和 91.6% 的 F1 分数，两者均为最高。

    Sentence-level relation extraction aims to identify the relation between two entities for a given sentence. The existing works mostly focus on obtaining a better entity representation and adopting a multi-label classifier for relation extraction. A major limitation of these works is that they ignore background relational knowledge and the interrelation between entity types and candidate relations. In this work, we propose a new paradigm, Contrastive Learning with Descriptive Relation Prompts(CTL-DRP), to jointly consider entity information, relational knowledge and entity type restrictions. In particular, we introduce an improved entity marker and descriptive relation prompts when generating contextual embedding, and utilize contrastive learning to rank the restricted candidate relations. The CTL-DRP obtains a competitive F1-score of 76.7% on TACRED. Furthermore, the new presented paradigm achieves F1-scores of 85.8% and 91.6% on TACREV and Re-TACRED respectively, which are both the st
    
[^24]: 强化学习辅导员在数学任务中更好地支持了低成绩学生

    Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])

    [http://arxiv.org/abs/2304.04933](http://arxiv.org/abs/2304.04933)

    本文证明了深度强化学习可用于提供自适应的教育支持，尤其对于最初成绩较低的学生具有最大的益处。

    

    资源限制使得为所有学生提供个性化教学变得困难。强化学习可以成为减少发展成本、提高智能辅导软件效果的关键工具，旨在为学生提供正确的支持。在这里，我们展示了深度强化学习如何在叙述故事线软件中为学习“容积”概念的学生提供自适应教育支持。通过解释性人工智能工具，我们也提取了有关学习的可解释洞见，证明了所得政策在不同的学生群体中具有类似的表现。最重要的是，在这两项研究中，强化学习故事系统对最初的预测分数最低的学生有最大的益处，这表明了AI适应并为低成绩学生提供支持的机会。

    Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
    
[^25]: 通过生成未来视图图像语义以改善视觉与语言导航

    Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])

    [http://arxiv.org/abs/2304.04907](http://arxiv.org/abs/2304.04907)

    本文提出了三个代理任务用于在代理的领域内预训练，以帮助模型生成未来视图图像语义，从而在视觉与语言导航任务中提高性能。

    

    视觉与语言导航是一项任务，要求代理根据自然语言指令在环境中进行导航。在每个步骤中，代理通过从可导航位置集合中进行选择来选择下一步动作。在本文中，我们旨在进一步探索代理是否可以受益于在导航期间生成潜在未来视图。直观地说，人类根据自然语言指令和周围的视图会对未来的环境有一个预期，并帮助正确地导航。因此，为了给代理装备这种生成未来导航视图语义的能力，我们首先在代理的领域内预训练过程中提出了三种代理任务: 掩蔽全景建模 (MPM)，掩蔽轨迹建模 (MTM) 和带有图像生成的动作预测 (APIG)。这三个目标教会了模型预测全景中的缺少视图 (MPM)、预测完整轨迹中的缺少步骤 (MTM) 和进行动作预测和图像生成 (APIG)。

    Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full tra
    
[^26]: 使用基于负采样的方法评估多项选择题中的干扰选项

    DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])

    [http://arxiv.org/abs/2304.04881](http://arxiv.org/abs/2304.04881)

    DISTO提出了一种新的学习度量标准来评估多项选择题中生成的干扰选项。DISTO与人类对干扰选项的评分高度相关，且排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不适用于干扰选项评估。

    

    多项选择题是一种评估阅读理解能力的高效常见方法。每道多项选择题需要一组干扰选项，这些选项虽然不正确，但要足够合理以考查学生的知识掌握情况。已经提出了干扰选项生成模型，并且它们的性能通常使用机器翻译度量标准来评估。然而，机器翻译度量标准经常误判生成的干扰选项的合适性。我们提出了DISTO：用于评估生成干扰选项的第一个学习度量标准。我们通过展示DISTO得分与人类对干扰选项质量的评分高度相关来验证DISTO。与此同时，DISTO排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不应用于干扰选项评估。

    Multiple choice questions (MCQs) are an efficient and common way to assess reading comprehension (RC). Every MCQ needs a set of distractor answers that are incorrect, but plausible enough to test student knowledge. Distractor generation (DG) models have been proposed, and their performance is typically evaluated using machine translation (MT) metrics. However, MT metrics often misjudge the suitability of generated distractors. We propose DISTO: the first learned evaluation metric for generated distractors. We validate DISTO by showing its scores correlate highly with human ratings of distractor quality. At the same time, DISTO ranks the performance of state-of-the-art DG models very differently from MT-based metrics, showing that MT metrics should not be used for distractor evaluation.
    
[^27]: 知识生成式选择在基于知识的对话系统中的应用

    Generative Knowledge Selection for Knowledge-Grounded Dialogues. (arXiv:2304.04836v1 [cs.CL])

    [http://arxiv.org/abs/2304.04836](http://arxiv.org/abs/2304.04836)

    本论文提出了一个称为GenKS的简单而有效的生成式知识选择方法，在基于知识的对话系统中，通过生成知识片段的标识符，捕捉并解决了知识内部的交互，同时通过超链接机制显式地建模了对话-知识交互。

    

    知识选择是基于知识的对话系统中的关键问题，它旨在根据对话历史选择适当的知识片段用于回复。以往的研究主要采用分类方法独立地将每个候选片段分类为“相关”或“不相关”，但这种方法忽略了片段之间的交互作用，导致推断片段含义的困难，并且缺乏对话-知识相互作用的语篇结构建模。我们提出了一个称为GenKS的简单而有效的生成式知识选择方法。GenKS学习通过使用序列到序列模型生成它们的标识符来选择片段。因此，GenKS通过关注机制天然地捕捉了知识内部的交互作用。同时，我们设计了一个超链接机制来显式地建模对话-知识交互。我们在三个基准数据集上进行了实验，并验证了GenKS在相关性、多样性以及生成适当回复的能力方面取得了最先进的性能。

    Knowledge selection is the key in knowledge-grounded dialogues (KGD), which aims to select an appropriate knowledge snippet to be used in the utterance based on dialogue history. Previous studies mainly employ the classification approach to classify each candidate snippet as "relevant" or "irrelevant" independently. However, such approaches neglect the interactions between snippets, leading to difficulties in inferring the meaning of snippets. Moreover, they lack modeling of the discourse structure of dialogue-knowledge interactions. We propose a simple yet effective generative approach for knowledge selection, called GenKS. GenKS learns to select snippets by generating their identifiers with a sequence-to-sequence model. GenKS therefore captures intra-knowledge interaction inherently through attention mechanisms. Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge interactions explicitly. We conduct experiments on three benchmark datasets, and verify GenKS achie
    
[^28]: 准确的COVID-19信息与错误信息的大规模比较研究

    A Large-Scale Comparative Study of Accurate COVID-19 Information versus Misinformation. (arXiv:2304.04811v1 [cs.CL])

    [http://arxiv.org/abs/2304.04811](http://arxiv.org/abs/2304.04811)

    本文通过大规模比较研究表明，COVID-19错误信息的分布、传播能力、语言分析与准确信息不同，并且研制了一个新的分类数据集，该数据集平均提高了9%以上的错误信息分类能力。

    

    COVID-19疫情导致信息泛滥，大量与COVID-19相关的内容被通过社交媒体高速传播，这让公众难以区分COVID-19的准确和错误信息。为此，本文通过对超过2.42亿条推特进行大规模计算分析，对COVID-19错误和准确信息的特征进行了比较研究。研究涵盖四个方面: 1)主题的分布，2)推特的实时性，3)语言分析和4)时间上的传播能力。本文还创造了一个新的COVID-19错误信息分类数据集，最后演示了该数据集能够通过平均F1指标将错误信息分类提高9%以上。

    The COVID-19 pandemic led to an infodemic where an overwhelming amount of COVID-19 related content was being disseminated at high velocity through social media. This made it challenging for citizens to differentiate between accurate and inaccurate information about COVID-19. This motivated us to carry out a comparative study of the characteristics of COVID-19 misinformation versus those of accurate COVID-19 information through a large-scale computational analysis of over 242 million tweets. The study makes comparisons alongside four key aspects: 1) the distribution of topics, 2) the live status of tweets, 3) language analysis and 4) the spreading power over time. An added contribution of this study is the creation of a COVID-19 misinformation classification dataset. Finally, we demonstrate that this new dataset helps improve misinformation classification by more than 9% based on average F1 measure.
    
[^29]: 考察COVID-19疫苗态度检测中的时间性

    Examining Temporalities on Stance Detection Towards COVID-19 Vaccination. (arXiv:2304.04806v1 [cs.CL])

    [http://arxiv.org/abs/2304.04806](http://arxiv.org/abs/2304.04806)

    研究考虑了时间性对COVID-19疫苗态度检测的影响，发现时间分割显著降低了立场分类的准确性。

    

    先前的研究指出疫苗接种是控制COVID-19传播的有效策略。了解公众的疫苗态度对决策者至关重要。然而，社交媒体上的 COVID-19 疫苗态度（如支持和犹豫）会随时间而演变，因此在分析这些立场时需要考虑可能的时间漂移。该研究旨在检查时间概念漂移对推特上 COVID-19 疫苗立场检测的影响。为此，我们使用基于转换器的模型对社交媒体数据进行了随机和时间分割的评估。我们的研究发现，在所有单语和多语数据集的随机和时间分割之间，模型性能存在显著差异。时间分割显著降低了立场分类的准确性。

    Previous studies have highlighted the importance of vaccination as an effective strategy to control the transmission of the COVID-19 virus. It is crucial for policymakers to have a comprehensive understanding of the public's stance towards vaccination on a large scale. However, attitudes towards COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved over time on social media. Thus, it is necessary to account for possible temporal shifts when analysing these stances. This study aims to examine the impact of temporal concept drift on stance detection towards COVID-19 vaccination on Twitter. To this end, we evaluate a range of transformer-based models using chronological and random splits of social media data. Our findings demonstrate significant discrepancies in model performance when comparing random and chronological splits across all monolingual and multilingual datasets. Chronological splits significantly reduce the accuracy of stance classification. Therefore, 
    
[^30]: 无言的选择对期望的影响及其对语用推理的预测

    Expectations over Unspoken Alternatives Predict Pragmatic Inferences. (arXiv:2304.04758v1 [cs.CL])

    [http://arxiv.org/abs/2304.04758](http://arxiv.org/abs/2304.04758)

    研究表明，人们在理解语言时会根据上下文中未明说的替代方案的期望来做出标度推理，该机制解释了标度推理内部和跨标度变化的原因。

    

    标度推理是人类基于无言的选择解释语言的一个典型例子。虽然实证研究表明人类的标度推理率变化很大，既在单个标度的实例内，也在不同标度之间，但很少有综合解释能够定量解释跨标度和单一标度内的变异。此外，虽然通常认为标度推理是通过思考无言的选择而产生的，但人们仍在争论人类是在语言形式层面还是在概念层面上推理无言的选择。在本文中，我们测试了一个共享的机制，解释了标度推理率在标度内部和跨标度内的变异：关于无言的选择的上下文驱动的期望。使用神经语言模型来近似人类的预测分布，我们发现标度推理率可以通过中性标度作为替代方案的预期程度来捕获。 然而，值得注意的是，在基于含义的观点下，期望显著地只预测跨标度的变异。

    Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable -- both within instances of a single scale, and across different scales -- there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view
    
[^31]: ESPnet-ST-v2：多功能口语翻译工具包

    ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit. (arXiv:2304.04596v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2304.04596](http://arxiv.org/abs/2304.04596)

    ESPnet-ST-v2是一个开源的多功能口语翻译工具包，支持多种翻译任务，采用了先进的架构和技术，具有非常高的性能表现。

    

    ESPnet-ST-v2是一个开源的口语翻译工具包，支持离线语音到文本翻译（ST）、同步语音到文本翻译（SST）和离线语音到语音翻译（S2ST）。与其他开源口语翻译工具包不同的是，ESPnet-ST-v2采用了许多先进的架构，包括转录器、混合CTC/attention、多解码器、时间同步分块CTC/attention、Translatotron模型和直接离散单元模型。

    ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.
    
[^32]: 用多模态对比学习连接表示

    Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v1 [cs.CV])

    [http://arxiv.org/abs/2304.03464](http://arxiv.org/abs/2304.03464)

    本文提出了一种名为CLIPPINGS的多模态框架，用于记录链接。该框架利用深度学习和对比学习的方法，通过端到端训练对称的视觉和语言编码器，在度量空间中学习相近或不同类别的表示方法，用于多个应用场景，如构建全面的补充专利注册表和识别不同社交媒体平台上的个人。

    

    许多应用需要将包含在各种文档数据集中的实例分组成类。最广泛使用的方法不使用深度学习，也不利用文档固有的多模态性质。值得注意的是，记录链接通常被概念化为字符串匹配问题。本研究开发了 CLIPPINGS，一种用于记录链接的多模态框架。CLIPPINGS 采用端到端训练对称的视觉和语言双编码器，通过对比语言-图像预训练进行对齐，学习一个度量空间，其中给定实例的汇总图像-文本表示靠近同一类中的表示，并远离不同类中的表示。在推理时，可以通过从离线示例嵌入索引中检索它们最近的邻居或聚类它们的表示来链接实例。本研究研究了两个具有挑战性的应用：通过将专利与其对应的监管文件链接来构建全面的补充专利注册表，以及在不同的社交媒体平台上识别个人。

    Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. Notably, record linkage is typically conceptualized as a string-matching problem. This study develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a multimodal framework for record linkage. CLIPPINGS employs end-to-end training of symmetric vision and language bi-encoders, aligned through contrastive language-image pre-training, to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The study examines two challenging applications: constructing comprehensive suppl
    
[^33]: 以七个动作阶段的方式设计智能写作助手的可用性

    Approach Intelligent Writing Assistants Usability with Seven Stages of Action. (arXiv:2304.02822v1 [cs.HC] CROSS LISTED)

    [http://arxiv.org/abs/2304.02822](http://arxiv.org/abs/2304.02822)

    本文提出采用诺曼的七个动作阶段作为智能写作助手交互设计的框架，并提供支持这些动作阶段的工具的示例。该框架有潜力成为人-LLM交互研究的重要工具。

    

    尽管大型语言模型( LLMS )具备成为写作助手的潜力，但它们仍然存在一些问题，例如模型输出的连贯性和流畅性、可信度、生成内容的所有权以及模型性能的可预测性，从而限制了它们的可用性。在本文中，我们提出采用诺曼的七个动作阶段作为智能写作助手交互设计的框架。我们通过提供软件教程创作的示例，说明该框架适用于写作任务。本文还讨论了该框架作为综合基于LLMS工具的交互设计研究的工具，并提供了支持这些动作阶段的工具的示例。最后，我们简要概述了人-LLMS交互研究框架的潜力。

    Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability. In this position paper, we propose to adopt Norman's seven stages of action as a framework to approach the interaction design of intelligent writing assistants. We illustrate the framework's applicability to writing tasks by providing an example of software tutorial authoring. The paper also discusses the framework as a tool to synthesize research on the interaction design of LLM-based tools and presents examples of tools that support the stages of action. Finally, we briefly outline the potential of a framework for human-LLM interaction research.
    
[^34]: 大型语言模型作为钥匙：用GPT解密材料科学的秘密。

    Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])

    [http://arxiv.org/abs/2304.02213](http://arxiv.org/abs/2304.02213)

    本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。

    

    本文介绍了一个新的自然语言处理（NLP）任务——结构化信息推理（SIS），以解决材料科学设备层面信息提取的复杂性。我们使用现有的钙钛矿太阳能电池FAIR数据集对GPT-3进行微调，获得了91.8 F1得分，并更新了数据集，包括迄今为止所有相关科学论文。所生成的数据集已被格式化和标准化，使得它可以直接作为后续数据分析的输入。这个特性将使材料科学家通过选择高质量的领域评论文章来开发其自己的模型。此外，我们设计了实验来预测PCE和反向预测参数，并获得了与DFT相当的性能，这证明了大型语言模型能够像材料学家一样评判材料和设计新材料。

    This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
    
[^35]: 解锁ChatGPT的潜力：对其在自然语言处理中应用、优点、局限性和未来方向的全面探讨

    Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])

    [http://arxiv.org/abs/2304.02017](http://arxiv.org/abs/2304.02017)

    本文全面探讨了ChatGPT在自然语言处理中的应用、优点和局限性，强调了使用这个强大工具时的道德考虑，为人工智能和NLP领域的讨论做出了贡献。

    

    ChatGPT是人工智能领域中广泛应用的强大工具，已成功应用于聊天机器人、内容生成、语言翻译、个性化推荐和医疗诊断治疗。它的多功能性和准确性使其成为自然语言处理（NLP）的强大工具。但是，ChatGPT也存在局限性，例如其倾向于产生有偏见的响应以及存在潜在的有害语言模式。本文全面概述了ChatGPT及其应用、优点和局限性，并强调了在真实场景中使用这个强大工具时道德考虑的重要性。最后，本文通过提供提示工程技术的见解，为关于人工智能及其对视觉和NLP领域的影响的持续讨论做出了贡献。

    ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
    
[^36]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^37]: 使用参数有效的结构将预训练语言模型扩展至更深的深度

    Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])

    [http://arxiv.org/abs/2303.16753](http://arxiv.org/abs/2303.16753)

    本文提出了一种参数有效的结构，通过MPO分解共享中央张量并保持层特定的辅助张量，将预训练语言模型扩展到更深的深度。

    

    本文提出了一种高度参数有效的方法，通过使用基于矩阵乘积算子（MPO）的更具能力的参数共享架构，将预训练语言模型（PLMs）扩展到更深的模型深度。通过MPO分解，我们的架构跨所有层共享中央张量以减少模型大小，并保持层特定的辅助张量以增强适应性灵活性。

    In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO). MPO decomposition can reorganize and factorize the information of a parameter matrix into two parts: the major part that contains the major information (central tensor) and the supplementary part that only has a small proportion of parameters (auxiliary tensors). Based on such a decomposition, our architecture shares the central tensor across all layers for reducing the model size and meanwhile keeps layer-specific auxiliary tensors (also using adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed mo
    
[^38]: 机器心理学：利用心理学方法探究大型语言模型的新兴能力和行为

    Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])

    [http://arxiv.org/abs/2303.13988](http://arxiv.org/abs/2303.13988)

    本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。

    

    大型语言模型（LLM）是将人工智能系统与人类交流和日常生活紧密结合的先锋。由于快速技术进步和其极高的通用性，现今LLM已经拥有数百万用户，并正处于成为主要信息检索、内容生成、问题解决等技术的前沿。因此，对其进行全面评估和审查显得尤为重要。由于当前LLM中出现愈加复杂和新颖的行为模式，可将其视为参与人类心理实验的对象，以便更为全面地评估其能力。为此，本文引入了一个名为"机器心理学"的新兴研究领域。本文概述了各类心理学分支如何为LLM的行为测试提供有用参考。同时，本文规范了机器心理学研究的方法论标准，特别是专注于提示设计政策的制定。此外，它还描述了行为测试结果如何为未来的LLM发展提供指导。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
    
[^39]: 利用解耦表示的检索增强分类

    Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v1 [cs.CL])

    [http://arxiv.org/abs/2303.13065](http://arxiv.org/abs/2303.13065)

    本文提出了一个混合粒度的中文BERT（MigBERT），利用同时考虑字符和词汇的表示方式，提高了中文PLMs的表现，并在各种中文NLP任务中取得了新的SOTA性能。单词比字符语义更丰富。数字也显示了MigBERT可以在日语中使用。

    

    预训练语言模型（PLM）在各种NLP任务中显示出显着的改进。大多数中文PLM将输入文本视为字符序列，并完全忽略词信息。虽然整词屏蔽可以缓解这一问题，但词汇中的语义仍然没有得到很好的表示。在本文中，我们重新审视了中文PLM的分词粒度。我们通过同时考虑字符和词汇，提出了一个混合粒度的中文BERT（MigBERT）。为了实现这一点，我们设计了用于学习字符和单词级表示的目标函数。我们对各种中文NLP任务进行了广泛的实验，以评估现有PLM以及所提出的MigBERT。实验结果表明，MigBERT在所有这些任务上均实现了新的SOTA性能。进一步的分析表明，单词比字符语义更丰富。更有趣的是，我们展示了MigBERT也可以与日语一起使用。

    Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code has been released here~\footnote{\url{https://g
    
[^40]: LLM的合成数据生成对临床文本挖掘有帮助吗？

    Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04360](http://arxiv.org/abs/2303.04360)

    研究探讨了利用 ChatGPT 进行临床文本挖掘的潜力，但初步结果表明效果欠佳，同时上传患者信息也引发隐私问题。因此，提出了一种新的训练方法，即使用 ChatGPT 生成带标签的大量高质量合成数据进行训练。

    

    大型语言模型（LLM）的最近进展推动了诸如OpenAI的ChatGPT之类的高性能模型的发展。这些模型在问答、论文写作和代码生成等各种任务中表现出色。然而，它们在医疗保健领域的有效性仍不确定。本研究旨在通过检查ChatGPT从非结构化健康文本中提取结构化信息的能力，重点关注生物命名实体识别和关系提取，探讨ChatGPT在临床文本挖掘中的潜力。然而，我们的初步结果表明，直接应用ChatGPT进行这些任务导致表现不佳，并引发与将患者信息上传到ChatGPT API有关的隐私问题。为了克服这些限制，我们提出了一种新的训练范式，利用ChatGPT生成大量高质量的带标签的合成数据进行训练。

    Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine
    
[^41]: 认知有害段子中不同实体所扮演的角色：谁是英雄，谁是恶棍，谁是受害者？

    Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?. (arXiv:2301.11219v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11219](http://arxiv.org/abs/2301.11219)

    本文旨在研究有害段子中实体扮演的角色，探测每个实体是否是段子中的英雄、恶棍或受害者，使用HVVMemes数据集，设计了VECTOR模型来执行任务。

    

    段子通过视觉和文本信息结合的方式可以在社交媒体上影响人们的观点。由于段子可以迅速传播，因此推断段子的意图和潜在的有害性并采取及时的措施变得至关重要。在理解段子中所引用的实体和刻画每个实体的角色方面，常常会出现一个常见的问题。本文旨在了解段子是否美化、污名化或使每个实体成为受害者。为此，我们提出了有害段子中实体的角色识别任务，即检测段子中的“英雄”、“恶棍”和“受害者”，如果有的话。我们使用HVVMemes——最近作为CONSTRAINT@ACL-2022共享任务的一部分发布的有关美国政治和Covid-19段子的数据集。它包含了段子、引用的实体及其相关角色：英雄、恶棍、受害者和其他。此外，我们还设计了VECTOR(视觉-语义角色检测)模型来执行任务。

    Memes can sway people's opinions over social media as they combine visual and textual information in an easy-to-consume manner. Since memes instantly turn viral, it becomes crucial to infer their intent and potentially associated harmfulness to take timely measures as needed. A common problem associated with meme comprehension lies in detecting the entities referenced and characterizing the role of each of these entities. Here, we aim to understand whether the meme glorifies, vilifies, or victimizes each entity it refers to. To this end, we address the task of role identification of entities in harmful memes, i.e., detecting who is the 'hero', the 'villain', and the 'victim' in the meme, if any. We utilize HVVMemes - a memes dataset on US Politics and Covid-19 memes, released recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains memes, entities referenced, and their associated roles: hero, villain, victim, and other. We further design VECTOR (Visual-semantic role dEteCTo
    
[^42]: EHRSQL：面向电子病历的实用文本转SQL基准测试

    EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.07695](http://arxiv.org/abs/2301.07695)

    该论文提出了一个面向电子病历数据的文本转SQL数据集，该数据集具有一系列独特挑战，包括生成SQL查询、理解时间表达式以及区分有无答案的问题。

    

    我们为电子病历（EHR）提供了一个新的文本到SQL数据集。对话是由222个医院工作人员包括医生、护士、保险审查和健康档案团队等手机而来。为了构建关于结构化EHR数据的QA数据集，我们在一所大学医院进行了一次民调并制作了模板话术以创建种子问题。然后，我们手动将它们链接到两个开源的EHR数据库（MIMIC-III和eICU）中，并在数据集中包含了来自民意调查的各种时间表达式和未能回答的问题。我们的数据集提出了一系列独特的挑战：模型需要 1）生成反映医院中各种需求的SQL查询，包括简单的检索和复杂的操作，如计算生存率，2）理解各种时间表达式以回答与时间敏感的医疗问题相关的问题，3）根据预测区分给定问题是可回答还是不可回答。

    We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
    
[^43]: VaxxHesitancy: 一份用于研究推特上对 COVID-19 疫苗犹豫的数据集。

    VaxxHesitancy: A Dataset for Studying Hesitancy Towards COVID-19 Vaccination on Twitter. (arXiv:2301.06660v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.06660](http://arxiv.org/abs/2301.06660)

    该论文介绍了一份用于研究推特上 COVID-19 疫苗犹豫的数据集，疫苗犹豫一直是一个普遍的问题，了解公众对 COVID-19 疫苗犹豫的原因非常重要。

    

    疫苗犹豫一直是一个普遍的问题，随着社交媒体的普及，人们开始在网上表达他们对疫苗的担忧，同时也与支持和反对疫苗的人发表内容。自从第一次提到 COVID-19 疫苗以来，社交媒体用户就在发布关于他们的担忧和支持疫苗有效性的内容。了解公众对 COVID-19 疫苗犹豫的原因非常重要，对于需要制定行动以更好地告知人群以增加疫苗接种率的政策制定者来说尤其如此。在 COVID-19 的情况下，疫苗快速开发与反疫苗虚假信息的增长相伴，自动检测公民对疫苗接种的态度成为必需。这是一个重要的计算社会科学任务，需要数据分析才能获得更多洞见。

    Vaccine hesitancy has been a common concern, probably since vaccines were created and, with the popularisation of social media, people started to express their concerns about vaccines online alongside those posting pro- and anti-vaccine content. Predictably, since the first mentions of a COVID-19 vaccine, social media users posted about their fears and concerns or about their support and belief into the effectiveness of these rapidly developing vaccines. Identifying and understanding the reasons behind public hesitancy towards COVID-19 vaccines is important for policy markers that need to develop actions to better inform the population with the aim of increasing vaccine take-up. In the case of COVID-19, where the fast development of the vaccines was mirrored closely by growth in anti-vaxx disinformation, automatic means of detecting citizen attitudes towards vaccination became necessary. This is an important computational social sciences task that requires data analysis in order to gai
    
[^44]: 语言控制扩散：通过空间、时间和任务高效扩展

    Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15629](http://arxiv.org/abs/2210.15629)

    本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。

    

    训练通用型智能体在各个方面都很困难，需要处理高维输入（空间）、长时间跨度（时间）和多个新任务。最近的结构方面的进展使得我们可以沿着其中一个或两个维度提高扩展性能力，但计算成本仍然很高。本文提出使用语言控制扩散模型作为一种基于自然语言条件的分层规划器（LCD）来应对这三个方面。我们有效而高效地扩展扩散模型，以应对时间、状态和任务空间维度的长时间跨度控制问题。我们在CALVIN语言机器人基准测试中将LCD与其他最先进的模型进行比较，发现LCD在多任务成功率方面优于其他最先进的方法，而单任务成功率（SR）为88.7%，远高于以前的最佳成绩82.6%，大大提高了计算效率。

    Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
    
[^45]: 先知式注意力：基于未来信息的图像描述中的注意力预测方法

    Prophet Attention: Predicting Attention with Future Attention for Image Captioning. (arXiv:2210.10914v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.10914](http://arxiv.org/abs/2210.10914)

    本文提出了一种先知式注意力模型，它利用未来信息计算理想的注意力权重，进一步规范偏移的注意力，这种方法在图像描述中取得了最先进的性能。

    

    近来，注意力机制在序列到序列学习系统中被广泛使用。尤其是在图像描述中，注意力机制可以将正确的图像区域与适当的生成词语联系起来。然而，在解码过程中，这些注意力机制通常使用当前输入的隐藏状态来关注图像区域。在这种情况下，这些注意力模型存在“偏离焦点”的问题，它们根据先前的单词计算注意力权重，而不是即将生成的单词，这会影响地面和描述的性能。在本文中，我们提出了“先知式注意力”，类似于自我监督的形式。在训练阶段，该模块利用未来信息来计算“理想”的注意力权重，以进一步规范“偏移”的注意力。通过这种方式，图像区域与更精确的单词相关联，生成的图片标题更加准确。在三个基准数据集上的实验结果表明，我们提出的方法与其他基于注意力的模型相比，取得了最先进的性能。

    Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words. However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions. Under this setting, these attention models have a "deviated focus" problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the "ideal" attention weights towards image regions. These calculated "ideal" weights are further used to regularize the "deviated" attention. In this manner, image regions are grounded with
    
[^46]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^47]: 基于能力的多模态课程学习用于医学报告生成

    Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.14579](http://arxiv.org/abs/2206.14579)

    本论文提出了一个基于能力的多模态课程学习框架（CMCL），通过模拟放射学家的学习过程来逐步优化医学报告生成模型，在公共数据集上实验表明CMCL可以有效地提高模型性能。

    

    近年来，医学报告生成任务吸引了越来越多的研究关注，其目标是生成医学图像的连贯描述。与一般的图像字幕任务不同，医学报告生成对数据驱动的神经模型更具挑战性。主要原因是 1) 严重的数据偏差和 2) 有限的医学数据。为了减轻数据偏差并充分利用可用数据，我们提出了基于能力的多模态课程学习框架（CMCL）。具体来说，CMCL 模拟了放射学家的学习过程，并逐步优化模型。首先，CMCL 估计每个训练实例的难度并评估当前模型的能力; 其次，CMCL 选择最适合的训练实例组合以考虑当前模型的能力。通过迭代上述两个步骤，CMCL 可以逐渐提高模型的性能。在公共的IU-Xray和MIMIC-CXR数据集上的实验表明，CMCL 可以有效地提高医学报告生成的性能，并胜过几种最先进的方法。

    Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model's performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMC
    
[^48]: 面向方面的情感分析数据集调查

    Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.05232](http://arxiv.org/abs/2204.05232)

    本研究汇总了65个公开可用的ABSA数据集，包括45个英文数据集和20个其他语言数据集，提供了一个可以用于训练和评估自主ABSA系统的数据库。

    

    面向方面的情感分析(ABSA)是一个自然语言处理问题，需要分析用户生成的评论以确定：a)正在审查的目标实体，b)属于哪个高级方面，c)对目标和方面表达的情感。ABSA的众多但分散的语料库使研究人员很难快速确定最适合特定ABSA子任务的语料库。本研究旨在提供一个可以用于训练和评估自主ABSA系统的数据库。此外，我们提供了ABSA和其子任务的主要语料库概述，并强调研究人员在选择语料库时应考虑的几个特征。最后，我们讨论了当前收集方法的优缺点并为未来语料库创建提出建议。本调查审核了65个公开可用的ABSA数据集，涵盖25个领域，包括45个英语和20个其他语言的数据集。

    Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.
    
[^49]: 自动胸部X光报告生成的对比注意力模型研究

    Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.06965](http://arxiv.org/abs/2106.06965)

    本文提出对比注意力（CA）模型来准确捕捉和描述自动生成胸部X光图像描述中的异常区域，该模型将当前输入图像与正常图像进行比较以提取对比信息。实验结果证明了该方法的有效性和优越性。

    

    最近，自动生成给定胸部X光图像描述的胸部X光报告生成引起了越来越多的研究兴趣。胸部X光报告生成的关键挑战是准确捕捉和描述异常区域。在大多数情况下，正常区域占据整个胸部X光图像的主导地位，并且这些正常区域的描述占据了最终报告的主导地位。由于这种数据偏差，基于学习的模型可能无法关注异常区域。为了有效地捕捉和描述异常区域，我们提出了对比注意力（CA）模型。该CA模型不仅关注当前输入图像，还将其与正常图像进行比较，以提取对比信息。获取的对比信息可以更好地表示异常区域的视觉特征。根据在公共IU-X光和MIMIC-CXR数据集上的实验，将我们的CA纳入多种基线模型中，其结果证明了我们的方法的有效性和优越性。

    Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into severa
    

