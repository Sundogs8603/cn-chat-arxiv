# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Do large language models solve verbal analogies like children do?.](http://arxiv.org/abs/2310.20384) | 本文研究了大型语言模型是否像孩子一样通过联想来解决语言类比问题，实验证明荷兰语母语和多语言LLMs的表现与儿童相当，但当控制联想过程时，模型的性能下降1-2年。 |
| [^2] | [AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction.](http://arxiv.org/abs/2310.20352) | AMERICANO是一个基于论述驱动的分解和代理交互的论证生成框架，在论证生成中，通过将生成过程分解为顺序动作并细化论证草稿，实现了更好的论证性论述生成性能。 |
| [^3] | [Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM.](http://arxiv.org/abs/2310.20347) | 使用Apache TVM自动生成一系列高性能的阻塞矩阵乘法算法，并且通过生成特定处理器的微内核提供了灵活性和可优化性。 |
| [^4] | [InstructCoder: Empowering Language Models for Code Editing.](http://arxiv.org/abs/2310.20329) | 本研究旨在探索使用大型语言模型（LLMs）进行代码编辑，并引入了InstructCoder数据集，该数据集包含多样性的代码编辑任务，为通用代码编辑提供支持。 |
| [^5] | [ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science.](http://arxiv.org/abs/2310.20328) | 这个论文介绍了ChiSCor，一份包含619个奇幻故事的语料库，由442名荷兰4-12岁的儿童自由讲述。ChiSCor的故事是在自然环境中产生的，并提供了文本、音频、注释和附加元数据。论文还展示了ChiSCor在研究儿童语言和认知领域的潜力，以及对Zipf定律的扩展研究。 |
| [^6] | [Erato: Automatizing Poetry Evaluation.](http://arxiv.org/abs/2310.20326) | Erato是一个旨在促进自动化评估诗歌的框架，它可以将人类创作的诗歌与自动生成的诗歌进行对比，并有效地识别出关键差异。 |
| [^7] | [FA Team at the NTCIR-17 UFO Task.](http://arxiv.org/abs/2310.20322) | FA团队参加了NTCIR-17 UFO任务，通过利用ELECTRA语言模型的增强技术，成功实现了对表格中有价值数据的提取，达到了93.43%的准确率，并在排行榜上获得第二名。在TTRE任务中，他们还提出了基于规则的方法来提取文本和表格之间的关系。 |
| [^8] | [Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests.](http://arxiv.org/abs/2310.20320) | 本研究通过将11种基础模型和调整指令的大型语言模型（LLMs）与7-10岁儿童在高级测试中进行比较，发现GPT系列的调整指令LLMs表现最佳，并且在某些任务上超过了儿童的表现。此外，基础LLMs大多无法解决ToM任务，而调整指令则通过奖励合作性沟通有助于提升LLM的性能。 |
| [^9] | [Extracting Entities of Interest from Comparative Product Reviews.](http://arxiv.org/abs/2310.20274) | 本文提出了一种基于深度学习的方法，用于从比较性产品评论中提取产品比较信息，并通过实验证明其在这个任务中优于现有的语义角色标注框架。 |
| [^10] | [Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis.](http://arxiv.org/abs/2310.20260) | 本文研究了使用棋局教程作为机器学习下棋的新知识来源，开发了LEAP语料库，其中包含了结构化和非结构化数据。实验证明了该方法的性能。 |
| [^11] | [PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection.](http://arxiv.org/abs/2310.20256) | 提出一种名为PsyCoT的新颖个性检测方法，将心理问卷作为思维链条进行个性识别，并利用大型语言模型来增强对个性的合理推断能力。 |
| [^12] | [Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations.](http://arxiv.org/abs/2310.20246) | 本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。 |
| [^13] | [Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning.](http://arxiv.org/abs/2310.20236) | 本文提出了一个以事件为中心的模型，动态更新事件表示，通过多类别学习解决了时间关系分类任务中的信息共享和数据利用问题。实验证明该方法在英文和日文数据上均优于现有模型和传递学习基线模型。 |
| [^14] | [Does GPT-4 Pass the Turing Test?.](http://arxiv.org/abs/2310.20216) | GPT-4通过了公开的在线图灵测试中的41%的游戏，在语言风格和社会情感特征方面表现较佳，但仍未能达到人类参与者的水平。图灵测试仍然是评估自然交流和欺骗的相关方法。 |
| [^15] | [General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History.](http://arxiv.org/abs/2310.20204) | 基于电子健康记录，我们提出了一种称为REMed的检索增强医学预测模型，通过无限评估临床事件并自动选择相关事件进行预测，消除了人工特征选择和观察窗口的限制，并在实验中表现出优异的效果。 |
| [^16] | [Video-Helpful Multimodal Machine Translation.](http://arxiv.org/abs/2310.20201) | 该论文介绍了一个名为EVA的多模式机器翻译（MMT）数据集，包含了来自电影和电视剧的视频片段和相应的日英和中英平行字幕对，旨在解决现有数据集中视觉信息无法生成适当翻译的问题。同时，提出了基于选择性注意模式的SAFA MMT模型。 |
| [^17] | [Generating Continuations in Multilingual Idiomatic Contexts.](http://arxiv.org/abs/2310.20195) | 本论文测试了生成性语言模型在多语种惯用语境中生成延续的能力，并发现模型在字面和惯用上下文中的表现相似，并且在两种语言中均具有鲁棒性。 |
| [^18] | [DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text.](http://arxiv.org/abs/2310.20170) | DIVKNOWQA提出了一个综合数据集，评估LLMs在连接异构知识源上的推理能力，填补了现有领域中的一个空白。 |
| [^19] | [GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval.](http://arxiv.org/abs/2310.20158) | 这项工作提出了一种新颖的GAR-meets-RAG范式，通过迭代改进检索和重写阶段，克服了零样本信息检索中现有范式的挑战。 |
| [^20] | [Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision.](http://arxiv.org/abs/2310.20153) | 该论文提出了一种交互多重保真度学习框架，用于在有限的注释预算下开发小型特定领域的语言模型。该方法通过平衡低保真度自动注释和高保真度人类注释，以最大化模型性能。同时，还提出了一种增强注释多样性和信息性的查询策略。 |
| [^21] | [Multi-Agent Consensus Seeking via Large Language Models.](http://arxiv.org/abs/2310.20151) | 本文研究了基于大型语言模型的多智能体系统中的一致性寻求问题。研究发现，在没有明确指导的情况下，智能体主要使用平均策略进行一致性寻求，同时还分析了智能体数量、智能体个性和网络拓扑对协商过程的影响。 |
| [^22] | [Unlearn What You Want to Forget: Efficient Unlearning for LLMs.](http://arxiv.org/abs/2310.20150) | 本论文提出了一种高效的遗忘框架来处理大语言模型（LLMs）中的隐私问题和数据保护违规。通过引入轻量级遗忘层到transformers中，并使用有选择的师生目标学习，我们能够在删除数据后有效地更新LLMs，而无需重新训练整个模型。实验证明了该方法的有效性。 |
| [^23] | [EELBERT: Tiny Models through Dynamic Embeddings.](http://arxiv.org/abs/2310.20144) | EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。 |
| [^24] | [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models.](http://arxiv.org/abs/2310.20138) | 本论文提出了一个框架 DEPN，用于检测和编辑预训练语言模型中的隐私神经元。通过引入隐私神经元检测器和隐私神经元聚合器，我们能够有效降低私人数据泄露的风险，并且不会影响模型的性能。 |
| [^25] | [Improving Prompt Tuning with Learned Prompting Layers.](http://arxiv.org/abs/2310.20127) | 这篇论文提出了一种新颖的框架，称为选择性提示调整（SPT），通过学习选择合适的提示层来改进提示调整的性能。作者还提出了一个双层优化框架SPT-DARTS，可以更好地优化可学习的门，并提高提示调整的效果。实验证明，SPT框架在多个基准数据集上表现出色。 |
| [^26] | [Ling-CL: Understanding NLP Models through Linguistic Curricula.](http://arxiv.org/abs/2310.20121) | Ling-CL通过开发基于数据和现有语言复杂性知识的语言课程，帮助理解NLP模型学习的潜在语言知识。此工作在多个NLP数据集上展示了课程学习方法的指标选择和任务挑战推理的能力。 |
| [^27] | [Making Large Language Models Better Data Creators.](http://arxiv.org/abs/2310.20111) | 本文提出了一种统一的数据生成流程，只需要一个格式化示例，可以应用于各种任务，包括传统问题场景。该方法旨在解决大型语言模型在下游应用中依赖于人工标注数据的问题。 |
| [^28] | [Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models.](http://arxiv.org/abs/2310.20105) | 本研究评估了GPT-3.5和GPT-4模型在分类编程课程中学生求助请求方面的性能，并发现它们可以通过大型语言模型的自动分类来提高教育系统的效能。 |
| [^29] | [Evaluating Neural Language Models as Cognitive Models of Language Acquisition.](http://arxiv.org/abs/2310.20093) | 本文评估了神经语言模型作为语言习得的认知模型的潜力。作者认为用于评估句法能力的基准不够严格，并提出了使用严选数据集来探索语法结构基础的建议。 |
| [^30] | [Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning.](http://arxiv.org/abs/2310.20089) | 提供了一种关键词优化模板插入方法（KOTI）以改善临床信息提取任务的性能，通过优化位置可以在零射和少射训练设置中提高模型性能。 |
| [^31] | [Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models.](http://arxiv.org/abs/2310.20081) | 个性化是NLP系统中用户体验的关键，本文通过大型语言模型将总结和检索整合，提出了一种利用任务感知的总结增强个性化方法。 |
| [^32] | [Partial Tensorized Transformers for Natural Language Processing.](http://arxiv.org/abs/2310.20077) | 论文研究了在自然语言处理中应用部分张量化的变压器架构，通过对BERT和ViT等神经网络的嵌入层压缩和部分张量化，显著提高了模型的准确性，并打破了张量分解领域的新局面。 |
| [^33] | [Automatic Evaluation of Generative Models with Instruction Tuning.](http://arxiv.org/abs/2310.20072) | 本论文提出了一种基于指导调整的学习度量方法，通过对预训练语言模型进行微调来实现对生成模型的自动评估。实验结果表明，这种方法在各种评估任务上取得了良好的性能，并且多任务联合训练可以进一步提高性能。 |
| [^34] | [Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection.](http://arxiv.org/abs/2310.20046) | 本文提出了一种模型自适应的无优化算法AdaICL，通过识别不确定的示例并进行基于语义多样性的选择，提高了上下文学习的效果和预算效率。 |
| [^35] | [Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization.](http://arxiv.org/abs/2310.20033) | 本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。 |
| [^36] | [Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023.](http://arxiv.org/abs/2310.20003) | 该论文讨论了在西班牙语中早期检测抑郁症和饮食障碍的问题，提出了基于Transformer模型的解决方法，并应用了早期检测框架中定义的决策策略。 |
| [^37] | [Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design.](http://arxiv.org/abs/2310.19998) | 本文探索了使用大型语言模型（LLMs）作为工具在材料工程分析中的应用。LLMs可以用作一组具有特定特征、能力和指令的人工智能代理，为分析和设计问题提供强大的问题解决策略。实验重点是使用经过微调的模型MechGPT，在材料力学领域进行训练。通过Fine-tuning重新调整参数，增强了LLMs的能力。 |
| [^38] | [BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing.](http://arxiv.org/abs/2310.19975) | BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。 |
| [^39] | [Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023.](http://arxiv.org/abs/2310.19970) | 该论文介绍了UNSL在eRisk 2023中的应用，其中涉及了对抑郁症症状、病理性赌博风险和饮食紊乱迹象的检测与估计，并提出了基于Transformer的解决方案。 |
| [^40] | [The Impact of Depth and Width on Transformer Language Model Generalization.](http://arxiv.org/abs/2310.19956) | 深层的transformer语言模型在组合式泛化能力上比浅层模型更好，但额外层数的相对收益会迅速减小。 |
| [^41] | [Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications.](http://arxiv.org/abs/2310.19942) | 本研究提出了一种名为Split-NER的系统，通过将命名实体识别问题分成提取实体提及跨度和跨度分类两个子任务，然后利用问答模型解决这两个子任务，实现了高效和准确的命名实体识别。 |
| [^42] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^43] | [Evaluating Large Language Models: A Comprehensive Survey.](http://arxiv.org/abs/2310.19736) | 本调查综述了对大型语言模型（LLMs）的评估，包括知识和能力评估、对齐评估和安全评估。对于充分利用LLMs的能力以及确保其安全和有益的发展至关重要。 |
| [^44] | [Combining Language Models For Specialized Domains: A Colorful Approach.](http://arxiv.org/abs/2310.19708) | 该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。 |
| [^45] | [MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks.](http://arxiv.org/abs/2310.19677) | 本研究旨在衡量大型语言模型（LLM）在因果和道德判断任务上与人类的一致性。通过收集24篇认知科学论文的故事数据集，并使用统计分析方法，发现LLMs在考虑因果和道德因素时与人类参与者存在差异。 |
| [^46] | [Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding.](http://arxiv.org/abs/2310.19671) | 当前的大型语言模型在生成文本方面表现出了出色的能力，但对其能力的辩论缺乏细致的考虑。这篇论文评估了三个常见批评观点，并提出了对LLMs理解和意图问题的务实观点。 |
| [^47] | [LLMaAA: Making Large Language Models as Active Annotators.](http://arxiv.org/abs/2310.19596) | LLMaAA是一种利用大型语言模型作为主动批注器的方法，通过在主动学习循环中使用LLM确定高效批注内容，以最大限度地利用LLM的潜力并利用大量未标记数据。 |
| [^48] | [Constituency Parsing using LLMs.](http://arxiv.org/abs/2310.19462) | 本文研究了使用大型语言模型（LLMs）进行成分句法分析的潜力，通过采用线性化策略将输出树结构转化为符号序列，进一步提高了任务的效果。实验结果对LLMs的性能、泛化能力和成分句法分析中的挑战进行了深入研究。 |
| [^49] | [BERT Lost Patience Won't Be Robust to Adversarial Slowdown.](http://arxiv.org/abs/2310.19152) | 本文评估了多出口语言模型对抗恶意减速的稳健性, 发现复杂的机制更易受到恶意减速的攻击。此外，对抗训练无效，但对话模型的输入清洗是有效的。 |
| [^50] | [TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.](http://arxiv.org/abs/2310.19019) | TeacherLM-7.1B是一个小型模型，通过给自然语言处理样本进行注释，教会其他模型“为什么”而不仅仅是“什么”。它在MMLU上取得了52.3的零样本得分，同时具有出色的数据增强能力。发布TeacherLM系列模型和增强的数据集作为开源项目。 |
| [^51] | [LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery.](http://arxiv.org/abs/2310.18356) | LoRAShear是一种高效的大型语言模型结构剪枝和知识恢复方法，通过逐步剪枝和动态微调，有效减少LLMs的占用空间并且保持性能。 |
| [^52] | [AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models.](http://arxiv.org/abs/2310.18331) | AllTogether是一个标准化的提示模板，通过增强任务背景表示，提高了大型语言模型（LLMs）在基于HTML的Web导航中的性能。研究结果显示，像GPT-4这样的模型在这类任务中优于较小的模型，并且HTML代码片段的长度和历史轨迹对性能有显著影响。同时，在实时环境反馈方面，优于之前的逐步指导。这项工作为未来LLM驱动的Web代理研究提供了宝贵的见解。 |
| [^53] | [TarGEN: Targeted Data Generation with Large Language Models.](http://arxiv.org/abs/2310.17876) | TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。 |
| [^54] | [Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting.](http://arxiv.org/abs/2310.17811) | 该论文提出了一种使用RadGraph和少样本提示的风格感知放射学报告生成的方法。通过将报告的内容和风格分开处理，可以避免生成临床不准确的报告。定量评估和人工评估结果均表明该方法表现出良好的性能，并生成与个体放射科医生风格完全相同的报告。 |
| [^55] | [CodeFusion: A Pre-trained Diffusion Model for Code Generation.](http://arxiv.org/abs/2310.17680) | CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。 |
| [^56] | [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language.](http://arxiv.org/abs/2310.17306) | FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。 |
| [^57] | [Correction with Backtracking Reduces Hallucination in Summarization.](http://arxiv.org/abs/2310.16176) | 本文介绍了一种简单而有效的技术，CoBa，用于减少摘要中的幻觉。该方法通过测量条件词概率和上下文词距离的统计信息进行幻觉检测，并通过直观的回溯法进行减轻。实验证明，CoBa在减少摘要幻觉方面是有效且高效的。 |
| [^58] | [FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions.](http://arxiv.org/abs/2310.15421) | FANToM是一个新的基准，用于通过问答在信息不对称的对话环境中压力测试机器的心智理论。这个基准对最先进的大型语言模型来说具有挑战性，即使是具有思维链推理和微调的模型也比人类表现得差。 |
| [^59] | [Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering.](http://arxiv.org/abs/2310.14602) | 本研究实现了用于越南社区的生成前训练转换器(GPT-2)，专注于COVID-19相关问答。实验结果表明，GPT-2模型在社区COVID-19问答数据集中表现出色，优于其他模型。 |
| [^60] | [StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models.](http://arxiv.org/abs/2310.13673) | StereoMap提出了一个理论基础的框架，通过使用刻板印象内容模型的维度来量化大型语言模型对社会各群体的感知。研究结果显示，大型语言模型对这些群体表现出多样化的感知，具有混合的特征。 |
| [^61] | [Quality-Diversity through AI Feedback.](http://arxiv.org/abs/2310.13032) | 基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。 |
| [^62] | [Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance.](http://arxiv.org/abs/2310.10385) | 该论文研究了零样本神经机器翻译性能变化的原因，发现目标语言的翻译质量、词汇重叠和语言特性是影响性能变化的关键因素。 |
| [^63] | [Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning.](http://arxiv.org/abs/2310.08166) | 本论文介绍了Ziya-VL系列，这是一组双语大规模视觉语言模型，旨在将视觉语义融入语言模型以进行多模态对话。模型采用了查询变换器和优化方案，如指令调整和多阶段训练，以实现视觉语言对齐。 |
| [^64] | [Teaching Language Models to Hallucinate Less with Synthetic Tasks.](http://arxiv.org/abs/2310.06827) | 通过设计合成任务，我们的研究表明减少合成任务上的幻觉可以帮助减少现实世界的抽象概括任务上的幻觉。 |
| [^65] | [Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments.](http://arxiv.org/abs/2310.05964) | 本论文通过使用嵌入方法和词语间的语义关系，研究了不同社交媒体平台上评论之间的情感和语义关系，并通过分析用户评论中的文本，让研究人员、政治家和商业代表能够追踪全球用户之间共享情感的路径。 |
| [^66] | [DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task.](http://arxiv.org/abs/2309.16844) | DeBERTinha是一种通过多步骤训练和微调适应DebertaV3 XSmall模型的方法，用于巴西葡萄牙语自然语言处理任务。它在命名实体识别、情感分析和判断句子相关性等任务上表现出优越性能。 |
| [^67] | [COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs.](http://arxiv.org/abs/2309.14356) | COCO-Counterfactuals是一个自动构建图像-文本对的反事实例的框架，通过使用文本到图像扩散模型来自动生成多模态反事实例。通过人工评估，我们验证了COCO-Counterfactuals的质量，并展示了其对于改善域外泛化能力的实用性。 |
| [^68] | [D-Separation for Causal Self-Explanation.](http://arxiv.org/abs/2309.13391) | 本研究提出了一种新的准则，最小条件依赖（MCD）准则，来揭示因果解释。通过最小化选择理由候选项上未选择部分与目标标签的依赖，强制选择所有的标签原因。 |
| [^69] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^70] | [Beyond Document Page Classification: Design, Datasets, and Challenges.](http://arxiv.org/abs/2308.12896) | 本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。 |
| [^71] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^72] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^73] | [Propagating Knowledge Updates to LMs Through Distillation.](http://arxiv.org/abs/2306.09306) | 本研究通过上下文蒸馏的方法成功将知识更新传播到语言模型中，实现了更广泛的推理能力。 |
| [^74] | [TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models.](http://arxiv.org/abs/2306.06815) | 本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。 |
| [^75] | [FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering.](http://arxiv.org/abs/2306.05523) | FACTIFY3M是一个以多模式虚假信息验证为目标的数据集。虚假信息如今已成为当下重大的社会问题，这一数据集旨在通过多模式验证来及时识别和缓解虚假信息。 |
| [^76] | [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.](http://arxiv.org/abs/2306.04751) | 本文探究了指令调优语言模型在一系列开放指令跟随数据集上的最新进展，提供了一组大型指令调优模型，并进行了系统评估。实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。 |
| [^77] | [Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning.](http://arxiv.org/abs/2306.04746) | 该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。 |
| [^78] | [Multimodal Fusion Interactions: A Study of Human and Automatic Quantification.](http://arxiv.org/abs/2306.04125) | 本文比较研究了两种人类注释者可以用于注释多模态交互的分类，并提出了一种基于信息分解的分类学。 |
| [^79] | [Structural Similarities Between Language Models and Neural Response Measurements.](http://arxiv.org/abs/2306.01930) | 本研究研究了语言模型和神经响应测量之间的结构相似性，发现神经语言模型越大，其表示越相似于脑成像的神经响应测量。 |
| [^80] | [Faith and Fate: Limits of Transformers on Compositionality.](http://arxiv.org/abs/2305.18654) | 研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。 |
| [^81] | [Understanding Emotion Valence is a Joint Deep Learning Task.](http://arxiv.org/abs/2305.17422) | 研究通过多任务学习方法，探索情绪价值与情绪载体之间的相互依赖关系，并在联合预测设置中使用判别性模型取得了最佳平衡。 |
| [^82] | [Fine-Tuning Language Models with Just Forward Passes.](http://arxiv.org/abs/2305.17333) | 本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。 |
| [^83] | [Are Diffusion Models Vision-And-Language Reasoners?.](http://arxiv.org/abs/2305.16397) | 本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。 |
| [^84] | [Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios.](http://arxiv.org/abs/2305.14987) | 本文研究了在真实世界的信息查找场景中，LLMs的表格生成能力。通过使用四个数据集，在包括数据洞察生成和基于查询生成的情境下进行了评估。实验结果表明，当前高性能的LLMs在表格生成方面表现良好。 |
| [^85] | [How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench.](http://arxiv.org/abs/2305.14947) | 本研究通过对BIG-bench实验记录的研究，发现大型语言模型（LLM）的能力具有可预测性，并提出了寻找信息丰富的子集以最大程度恢复整个集合性能的问题。 |
| [^86] | [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4.](http://arxiv.org/abs/2305.14928) | 本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。 |
| [^87] | [Enabling Large Language Models to Generate Text with Citations.](http://arxiv.org/abs/2305.14627) | 本文提出ALCE，是首个自动LLMs引文评估基准，实现大型语言模型生成带引文的文本，提高其事实正确性和可验证性；提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。 |
| [^88] | [Pre-training Language Models for Comparative Reasoning.](http://arxiv.org/abs/2305.14457) | 本文提出一种预训练语言模型的新框架，旨在增强其在比较推理方面的能力。通过使用可扩展的基于文本实体比较数据的方法和新的预训练任务，该框架得到了显著的结果。 |
| [^89] | [Counterfactual Augmentation for Multimodal Learning Under Presentation Bias.](http://arxiv.org/abs/2305.14083) | 本文提出了一种用于纠正表示偏见的新颖方法，即反事实增强。实证评估表明，反事实增强相比于未修正的模型和现有的偏见校正方法，可以获得更好的下游性能。模型分析进一步指出，在理想情况下，生成的反事实与真实反事实密切相关。 |
| [^90] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^91] | [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings.](http://arxiv.org/abs/2305.11554) | 本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。 |
| [^92] | [Adaptive loose optimization for robust question answering.](http://arxiv.org/abs/2305.03971) | 本论文提出了一种简单而有效的自适应宽松优化损失函数，用于为问答系统综合内外分布的最佳表现，并显示了对对抗攻击的强韧性。 |
| [^93] | [Unlimiformer: Long-Range Transformers with Unlimited Length Input.](http://arxiv.org/abs/2305.01625) | Unlimiformer是一种Transformer模型的通用方法，可以将所有层的注意计算卸载到单个k近邻索引上，从而可处理无限长度的输入，而不增加额外的学习负担。 |
| [^94] | [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.](http://arxiv.org/abs/2305.01210) | 本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。 |
| [^95] | [Learning to Reason and Memorize with Self-Notes.](http://arxiv.org/abs/2305.00833) | 该论文提出了一种学习使用自注记进行推理和记忆的方法，通过允许模型明确思考、记录自己的想法，并整合先前的推理步骤，从而提高了多步推理的能力。 |
| [^96] | [Controlling keywords and their positions in text generation.](http://arxiv.org/abs/2304.09516) | 本文研究了一种控制文本生成中关键词及其位置的新方法，通过使用特殊标记控制关键词相对位置，可以生成更符合用户意图的文本。 |
| [^97] | [OpenAssistant Conversations -- Democratizing Large Language Model Alignment.](http://arxiv.org/abs/2304.07327) | 释放了OpenAssistant Conversations，这是一个由全球超过1,000名参与者进行人工生成和人工注释的助手风格对话语料库，可以通过SFT和RLHF有效地用于LLM对齐，提高模型性能和可用性。 |
| [^98] | [From Retrieval to Generation: Efficient and Effective Entity Set Expansion.](http://arxiv.org/abs/2304.03531) | 本文提出了GenExpan，一种基于生成式预训练语言模型的实体集扩展框架，利用前缀树保证实体生成的有效性，采用自动生成的类名来引导模型生成同一类实体，从而提高了效率和可扩展性。 |
| [^99] | [On the Pareto Front of Multilingual Neural Machine Translation.](http://arxiv.org/abs/2304.03216) | 本研究针对多语言神经机器翻译的数据不平衡问题，提出双重幂律方法用于预测独特的性能权衡前沿，并建立基于该方法的样本比例选择优化问题，取得更好的结果。 |
| [^100] | [Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning.](http://arxiv.org/abs/2304.01295) | 本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。 |
| [^101] | [GLEN: General-Purpose Event Detection for Thousands of Types.](http://arxiv.org/abs/2303.09093) | 研究者建立了一个通用事件检测数据集GLEN，涵盖了超过3,465种不同的事件类型，利用现有的标注，他们提出了一种新的多阶段事件检测模型，展示了在大本体大小和部分标签的情况下，该模型具有优越的性能。 |
| [^102] | [Learning List-Level Domain-Invariant Representations for Ranking.](http://arxiv.org/abs/2212.10764) | 本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。 |
| [^103] | [A Vision-free Baseline for Multimodal Grammar Induction.](http://arxiv.org/abs/2212.10564) | 本论文研究了在多模式设置下，只使用文本进行训练的大型语言模型（LLMs）是否能够提供强大的辅助来进行语法归纳。结果显示，基于LLM的纯文本方法在多种多模式数据集上优于先前的方法，并且在性能、参数数量和训练速度方面取得了最先进的结果。 |
| [^104] | [Conceptor-Aided Debiasing of Large Language Models.](http://arxiv.org/abs/2211.11087) | 本论文提出一种基于概念器的大型语言模型去偏见方法。我们通过后处理和一种新架构CI-BERT将概念器投影纳入所有层中。概念器后处理方法取得了最先进的去偏见结果，同时保持或改善了模型的性能。 |
| [^105] | [EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics.](http://arxiv.org/abs/2209.09593) | EffEval是一种对机器翻译评价指标效率进行全面评估的方法，其中TinyBERT在质量和效率之间提供了最佳平衡，CPU加速比GPU更显著，WMD近似没有提高效率但降低了质量，适配器提高了训练效率并在某些情况下提高了指标的质量。 |
| [^106] | [How direct is the link between words and images?.](http://arxiv.org/abs/2206.15381) | 该研究调查了词语和图像之间的联系，实验结果表明存在直接的联系。研究探讨了除了图像模拟外，受试者可能采用的其他策略，并分析了任务解决是否依赖于视觉信息。 |
| [^107] | [Language with Vision: a Study on Grounded Word and Sentence Embeddings.](http://arxiv.org/abs/2206.08823) | 本研究通过提出一个简单但非常有效的计算基于预训练词嵌入的模型，有效平衡了文本表示和视觉感知之间的相互作用。 |
| [^108] | [Born for Auto-Tagging: Faster and better with new objective functions.](http://arxiv.org/abs/2206.07264) | 本论文提出了一种用于自动标签的新目标函数BAT，通过改进模型结构和学习率策略，在50个epochs内实现了更快、更好的收敛速度和F得分，为自动标记提供了更准确和高效的解决方案。 |
| [^109] | [The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer.](http://arxiv.org/abs/2204.06457) | 本文研究了跨语言调整上下文词表示在多种语言和任务中的影响，结论是该方法对多语言NLI有益，同时对NER、XSR和跨语言QA也有改进，尤其对某些语言更为明显。而单语QA性能没有改善，有时甚至下降。 |
| [^110] | [BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain.](http://arxiv.org/abs/2202.10101) | 研究提出了WEAVER方法，它可以将旧知识融入到新模型中，以有效降低灾难性遗忘，并实现生物医学领域基于Transformer的模型的终身学习。 |

# 详细

[^1]: 大型语言模型是否像孩子一样解决语言类比问题？

    Do large language models solve verbal analogies like children do?. (arXiv:2310.20384v1 [cs.CL])

    [http://arxiv.org/abs/2310.20384](http://arxiv.org/abs/2310.20384)

    本文研究了大型语言模型是否像孩子一样通过联想来解决语言类比问题，实验证明荷兰语母语和多语言LLMs的表现与儿童相当，但当控制联想过程时，模型的性能下降1-2年。

    

    类比思维是人类认知的核心。成年人通过映射关系并回答问题，如“马属于马厩，鸡属于...？”而解决语言类比问题。相反，孩子们经常使用联想作答，例如回答“蛋”。本文研究了大型语言模型（LLMs）是否像孩子一样通过联想来解决A:B::C:?形式的语言类比问题。我们使用从在线自适应学习环境中提取的语言类比问题，其中来自荷兰的14,002名7-12岁儿童解决了622个荷兰语的语言类比问题。六个测试的荷兰语母语和多语言LLMs的表现与儿童大致相当，MGPT表现最差，接近7岁水平，XLM-V和GPT-3表现最佳，略高于11岁水平。然而，当我们控制联想过程时，情况发生变化，每个模型的表现水平下降1-2年。进一步实验表明，联想过程的控制对模型的性能有显著影响。

    Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associ
    
[^2]: AMERICANO:基于论述驱动的分解和代理交互的论证生成

    AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction. (arXiv:2310.20352v1 [cs.CL])

    [http://arxiv.org/abs/2310.20352](http://arxiv.org/abs/2310.20352)

    AMERICANO是一个基于论述驱动的分解和代理交互的论证生成框架，在论证生成中，通过将生成过程分解为顺序动作并细化论证草稿，实现了更好的论证性论述生成性能。

    

    论证生成是自然语言处理中具有挑战性的任务，需要严格的推理和适当的内容组织。受最近的思维链提示的启发，该提示将复杂的任务分解为中间步骤，我们提出了AMERICANO，一个具有代理交互的新型论证生成框架。我们的方法将生成过程分解为基于论证论述的顺序动作，并在此基础上生成论证性论述组成部分，然后根据这些组成部分生成最终的论证。为了进一步模仿人类写作过程，并改进当前自回归语言模型的从左到右生成范式，我们引入了一个论证细化模块，根据接收到的反馈自动评估和完善论证草稿。我们使用Reddit/CMV数据集的一个子集对我们的框架在反驳生成任务上进行了评估。结果表明，我们的方法优于其他方法。

    Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method out
    
[^3]: 使用Apache TVM自动生成一系列矩阵乘法例程

    Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM. (arXiv:2310.20347v1 [cs.CL])

    [http://arxiv.org/abs/2310.20347](http://arxiv.org/abs/2310.20347)

    使用Apache TVM自动生成一系列高性能的阻塞矩阵乘法算法，并且通过生成特定处理器的微内核提供了灵活性和可优化性。

    

    我们探索了利用Apache TVM开源框架自动生成一系列算法，这些算法遵循了流行的线性代数库（如GotoBLAS2、BLIS和OpenBLAS）的方法，以获得高性能的阻塞通用矩阵乘法（GEMM）形式。此外，我们完全自动化了生成过程，还利用Apache TVM框架推导出了GEMM的完整的特定处理器微内核的各种变体。这与高性能库中使用汇编代码手动编码的单个微内核的做法有所不同。

    We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in order to obtain high-performance blocked formulations of the general matrix multiplication (GEMM). % In addition, we fully automatize the generation process, by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for GEMM. This is in contrast with the convention in high performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. % In global, the combination of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves portability, maintainability and, globally, streamlines the software life cycle; 2)~provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yieldin
    
[^4]: InstructCoder: 为代码编辑赋能的语言模型。

    InstructCoder: Empowering Language Models for Code Editing. (arXiv:2310.20329v1 [cs.CL])

    [http://arxiv.org/abs/2310.20329](http://arxiv.org/abs/2310.20329)

    本研究旨在探索使用大型语言模型（LLMs）进行代码编辑，并引入了InstructCoder数据集，该数据集包含多样性的代码编辑任务，为通用代码编辑提供支持。

    

    代码编辑涵盖了开发者日常处理的各种实用任务。尽管其相关性和实用性，但自动代码编辑仍然是深度学习模型演化中尚未充分探索的领域，部分原因是数据稀缺。在本研究中，我们探索了使用大型语言模型（LLMs）根据用户指令编辑代码的方法，涵盖了诸如注释插入，代码优化和代码重构等一系列隐含任务。为了实现这一目标，我们引入了InstructCoder，这是第一个专为通用代码编辑而设计的数据集，包含高多样性的代码编辑任务。该数据集包含超过114,000个指令-输入-输出三元组，并涵盖了多个不同的代码编辑场景。数据集通过一个迭代过程进行系统扩展，该过程从GitHub的提交中获取代码编辑数据作为种子任务。种子任务和生成的任务随后用于提示ChatGPT获取更多任务数据。

    Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our exper
    
[^5]: ChiSCor：一份由荷兰儿童自由讲述的奇幻故事的计算语言学和认知科学语料库

    ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science. (arXiv:2310.20328v1 [cs.CL])

    [http://arxiv.org/abs/2310.20328](http://arxiv.org/abs/2310.20328)

    这个论文介绍了ChiSCor，一份包含619个奇幻故事的语料库，由442名荷兰4-12岁的儿童自由讲述。ChiSCor的故事是在自然环境中产生的，并提供了文本、音频、注释和附加元数据。论文还展示了ChiSCor在研究儿童语言和认知领域的潜力，以及对Zipf定律的扩展研究。

    

    在这篇资源论文中，我们发布了一个新的语料库ChiSCor，其中包含了619个奇幻故事，由442名年龄在4-12岁的荷兰儿童自由讲述。ChiSCor被编译用于研究儿童如何展示人物的观点，以及揭示语言和认知在发展中的作用，利用计算工具。与现有资源不同，ChiSCor的故事是在自然环境中产生的，符合最近对更生态有效数据集的呼吁。ChiSCor提供了文本、音频和人物复杂性和语言复杂性的注释。还为三分之一的荷兰儿童提供了附加元数据（如监护人的教育背景）。ChiSCor还包括一小部分62个英文故事。本文详细介绍了ChiSCor的编制方式，并展示了它在未来工作中的潜力，通过三个简短的案例研究：i）我们展示故事的句法复杂度在儿童的年龄之间非常稳定；ii）我们扩展了关于自由言论中Zipfian分布的工作，并展示了ChiSCor服从Zipf定律。

    In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law c
    
[^6]: Erato:自动化诗歌评估

    Erato: Automatizing Poetry Evaluation. (arXiv:2310.20326v1 [cs.CL])

    [http://arxiv.org/abs/2310.20326](http://arxiv.org/abs/2310.20326)

    Erato是一个旨在促进自动化评估诗歌的框架，它可以将人类创作的诗歌与自动生成的诗歌进行对比，并有效地识别出关键差异。

    

    我们提出了Erato，这是一个旨在促进诗歌自动化评估的框架，包括由诗歌生成系统生成的诗歌。我们的框架采用了多样化的特征，我们简要介绍了Erato的能力和其潜力扩展。使用Erato，我们对比了人类创作的诗歌与自动生成的诗歌，展示了其在识别关键差异方面的有效性。我们的实现代码和软件在GNU GPLv3许可下免费提供。

    We present Erato, a framework designed to facilitate the automated evaluation of poetry, including that generated by poetry generation systems. Our framework employs a diverse set of features, and we offer a brief overview of Erato's capabilities and its potential for expansion. Using Erato, we compare and contrast human-authored poetry with automatically-generated poetry, demonstrating its effectiveness in identifying key differences. Our implementation code and software are freely available under the GNU GPLv3 license.
    
[^7]: NTCIR-17 UFO任务中的FA团队（arXiv:2310.20322v1 [cs.CL]）

    FA Team at the NTCIR-17 UFO Task. (arXiv:2310.20322v1 [cs.CL])

    [http://arxiv.org/abs/2310.20322](http://arxiv.org/abs/2310.20322)

    FA团队参加了NTCIR-17 UFO任务，通过利用ELECTRA语言模型的增强技术，成功实现了对表格中有价值数据的提取，达到了93.43%的准确率，并在排行榜上获得第二名。在TTRE任务中，他们还提出了基于规则的方法来提取文本和表格之间的关系。

    

    FA团队参加了NTCIR-17 UFO的表格数据提取（TDE）和文本到表格关系提取（TTRE）任务。本文报告了我们解决这些问题的方法，并讨论了官方结果。我们成功地利用基于ELECTRA语言模型的各种增强技术从表格中提取有价值的数据。我们的努力导致了93.43%的令人印象深刻的TDE准确率，并使我们在排行榜上排名第二。这一卓越成就证明了我们提出的方法的有效性。在TTRE任务中，我们提出了基于规则的方法来提取文本和表格之间的有意义的关系，并验证了性能。

    The FA team participated in the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of Non-Financial Objects in Financial Reports (UFO). This paper reports our approach to solving the problems and discusses the official results. We successfully utilized various enhancement techniques based on the ELECTRA language model to extract valuable data from tables. Our efforts resulted in an impressive TDE accuracy rate of 93.43 %, positioning us in second place on the Leaderboard rankings. This outstanding achievement is a testament to our proposed approach's effectiveness. In the TTRE task, we proposed the rule-based method to extract meaningful relationships between the text and tables task and confirmed the performance.
    
[^8]: 大型语言模型中的心灵理论：11种最新模型与7-10岁儿童在高级测试中的表现比较

    Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests. (arXiv:2310.20320v1 [cs.CL])

    [http://arxiv.org/abs/2310.20320](http://arxiv.org/abs/2310.20320)

    本研究通过将11种基础模型和调整指令的大型语言模型（LLMs）与7-10岁儿童在高级测试中进行比较，发现GPT系列的调整指令LLMs表现最佳，并且在某些任务上超过了儿童的表现。此外，基础LLMs大多无法解决ToM任务，而调整指令则通过奖励合作性沟通有助于提升LLM的性能。

    

    我们应该给予大型语言模型（LLM）多大的认知能力，例如理解意图和信念的理论心灵（ToM）能力？在这里，我们通过以下方式，为这场新兴辩论增加一些证据：（i）测试11个基础模型和调整指令的LLMs的ToM相关能力，超越主导的虚假信念范式，包括非文字的语言使用和递归的意图；（ii）使用新编写的标准化测试版本来评估LLMs的稳健性；（iii）提示并计分开放问题和封闭问题；（iv）将LLM的表现与7-10岁儿童在相同任务上的表现进行基准测试。我们发现，GPT系列的调整指令LLMs在其他模型中表现最佳，并且通常也超过了儿童的表现。基础LLMs大多无法解决ToM任务，即使使用了专门的提示。我们认为，语言和ToM的相互关联性可能有助于解释为什么调整指令会增加LLM的性能：奖励合作性沟通。

    To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that t
    
[^9]: 从比较性产品评论中提取感兴趣的实体

    Extracting Entities of Interest from Comparative Product Reviews. (arXiv:2310.20274v1 [cs.IR])

    [http://arxiv.org/abs/2310.20274](http://arxiv.org/abs/2310.20274)

    本文提出了一种基于深度学习的方法，用于从比较性产品评论中提取产品比较信息，并通过实验证明其在这个任务中优于现有的语义角色标注框架。

    

    本文提出了一种基于深度学习的方法，用于从各种电子商务网站的用户评论中提取产品比较信息。任何一个比较性产品评论都有三个重要的信息实体：被比较产品的名称，用户观点（谓词）以及被比较的特征或方面。所有这些信息实体彼此依赖并受到评论语言规则的约束。我们观察到，这些相互依赖关系可以很好地通过LSTM进行捕捉。我们在现有的手动标记数据集上评估了我们的系统，并观察到其在这个任务中表现优于现有的语义角色标注（SRL）框架。

    This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
    
[^10]: 从教程中学习下棋: 基于情感分析的棋局评估语料库

    Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis. (arXiv:2310.20260v1 [cs.CL])

    [http://arxiv.org/abs/2310.20260](http://arxiv.org/abs/2310.20260)

    本文研究了使用棋局教程作为机器学习下棋的新知识来源，开发了LEAP语料库，其中包含了结构化和非结构化数据。实验证明了该方法的性能。

    

    学习下棋策略已经被广泛研究，大多数研究关注于使用搜索算法从之前的棋局中学习。棋局教程包含大师的知识，解释下棋策略，并且相比传统的下棋方法需要更小的搜索空间。本文探讨了棋局教程作为一种新的知识来源，以便使机器学会下棋，这是以前未曾探索的资源。我们开发了LEAP语料库，这是第一个新颖且具有结构化（棋局走法和局面状态）和非结构化（文字描述）数据的异构数据集，收集自一本含有1164个句子的棋局教程，讨论了91场比赛的战略走法。我们首先根据句子的相关性对其进行标注，即它们是否讨论了一次走法。然后，我们根据描述的走法对每个相关句子进行情感标注。我们进行了实证实验，评估了各种训练方法的性能。

    Learning chess strategies has been investigated widely, with most studies focussing on learning from previous games using search algorithms. Chess textbooks encapsulate grandmaster knowledge, explain playing strategies and require a smaller search space compared to traditional chess agents. This paper examines chess textbooks as a new knowledge source for enabling machines to learn how to play chess -- a resource that has not been explored previously. We developed the LEAP corpus, a first and new heterogeneous dataset with structured (chess move notations and board states) and unstructured data (textual descriptions) collected from a chess textbook containing 1164 sentences discussing strategic moves from 91 games. We firstly labelled the sentences based on their relevance, i.e., whether they are discussing a move. Each relevant sentence was then labelled according to its sentiment towards the described move. We performed empirical experiments that assess the performance of various tra
    
[^11]: PsyCoT: 将心理问卷作为强大的思维链条用于个性检测

    PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection. (arXiv:2310.20256v1 [cs.CL])

    [http://arxiv.org/abs/2310.20256](http://arxiv.org/abs/2310.20256)

    提出一种名为PsyCoT的新颖个性检测方法，将心理问卷作为思维链条进行个性识别，并利用大型语言模型来增强对个性的合理推断能力。

    

    最近大型语言模型（LLM）的进展，如ChatGPT，展示了在各种NLP任务中remarkable的零-shot表现。然而，LLM在个性检测方面的潜力，即通过写作文本来识别个体的个性，仍然未被充分探索。受到心理问卷的启发，心理问卷由心理学家精心设计，通过一系列有针对性的项目评估个体的个性特征，我们认为这些项目可以被视为一组良好结构化的思维链条过程。通过整合这些过程，LLM可以增强其从文本输入中对个性的合理推断能力。基于此，我们提出了一种新颖的个性检测方法，称为PsyCoT，它模仿个体以多轮对话方式完成心理问卷的方式。具体而言，我们将LLM作为一个在文本处理方向上具有专长的AI助手使用。

    Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in tex
    
[^12]: 在多语言数学推理中打破语言障碍：见解与观察

    Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])

    [http://arxiv.org/abs/2310.20246](http://arxiv.org/abs/2310.20246)

    本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。

    

    现有研究主要集中在开发适用于单语言中的数学推理的强大语言学习模型（LLM），在多语言环境下保持效果的研究很少。为了弥补这一差距，本文首次探索和训练强大的多语言数学推理（xMR）LLM。首先，通过利用翻译，我们构建了第一个包含十种不同语言的多语言数学推理指导数据集MGSM8KInstruct，从而解决了xMR任务中训练数据稀缺的问题。根据收集的数据集，我们提出了不同的训练策略来构建强大的xMR LLMs，被命名为MathOctopus，在几次训练中表现出优于传统开源LLMs和ChatGPT的能力。值得注意的是，MathOctopus-13B在MGSM测试集上达到了47.6%的准确率，超过了ChatGPT的46.3%。除了显著的结果，我们还从大量的实验证实中发现了一些重要的观察和见解：（1）在多语言上进行训练时，最好将目标语言的翻译与原始语言的表示结合起来。 （2）交替训练和多语言模型的自举有助于提高模型的表现。 （3）模型对于低频词和长句子的处理是挑战的，需要进一步改进。

    Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
    
[^13]: 动态更新事件表示的多类别学习用于时间关系分类

    Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning. (arXiv:2310.20236v1 [cs.CL])

    [http://arxiv.org/abs/2310.20236](http://arxiv.org/abs/2310.20236)

    本文提出了一个以事件为中心的模型，动态更新事件表示，通过多类别学习解决了时间关系分类任务中的信息共享和数据利用问题。实验证明该方法在英文和日文数据上均优于现有模型和传递学习基线模型。

    

    时间关系分类是一种用于识别两个提及之间的时间链接（TLINK）关系的成对任务，包括事件、时间和文档创建时间（DCT）。它存在两个关键限制：1）涉及共同提及的两个TLINK不共享信息；2）现有模型使用独立分类器对每个TLINK类别（E2E，E2T和E2D）进行分类，从而无法使用全部数据。本文提出了一种以事件为中心的模型，允许在多个TLINK中管理动态事件表示。我们的模型使用多任务学习处理三个TLINK类别，以利用全部数据。实验结果表明，我们的方法在英文和日文数据上均优于现有最先进模型和两个传递学习基线模型。

    Temporal relation classification is a pair-wise task for identifying the relation of a temporal link (TLINK) between two mentions, i.e. event, time, and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two transfer learning baselines on both the English and Japanese data.
    
[^14]: GPT-4 是否通过图灵测试？

    Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])

    [http://arxiv.org/abs/2310.20216](http://arxiv.org/abs/2310.20216)

    GPT-4通过了公开的在线图灵测试中的41%的游戏，在语言风格和社会情感特征方面表现较佳，但仍未能达到人类参与者的水平。图灵测试仍然是评估自然交流和欺骗的相关方法。

    

    我们在一个公开的在线图灵测试中评估了 GPT-4。在表现最好的 GPT-4 提示中，在 41% 的游戏中通过了测试，超过了 ELIZA（27%）和 GPT-3.5（14%）设定的基准，但还不如人类参与者（63%）的机会和基准。参与者的决策主要基于语言风格（35%）和社会情感特征（27%），支持智能不足以通过图灵测试的观点。参与者的人口统计学特征，包括教育水平和对语言模型的熟悉度，并不能预测被识别率，这表明即使是深入了解系统并频繁与其交互的人，也会容易被欺骗。尽管图灵测试作为智能的测试具有已知的局限性，我们认为它在评估自然交流和欺骗方面仍然具有相关性。具有冒充人类能力的 AI 模型可能会对社会产生广泛的影响，我们分析了不同策略和标准的有效性。

    We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria fo
    
[^15]: 利用近无限历史的通用检索增强医学预测模型

    General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])

    [http://arxiv.org/abs/2310.20204](http://arxiv.org/abs/2310.20204)

    基于电子健康记录，我们提出了一种称为REMed的检索增强医学预测模型，通过无限评估临床事件并自动选择相关事件进行预测，消除了人工特征选择和观察窗口的限制，并在实验中表现出优异的效果。

    

    基于电子健康记录（EHRs）开发临床预测模型（例如死亡预测）通常依赖于专家意见进行特征选择和调整观测窗口大小。这给专家带来负担并在开发过程中造成瓶颈。我们提出了一种检索增强的医学预测模型（REMed），以应对这些挑战。REMed可以基本评估无限量的临床事件，选择相关的事件并进行预测。这种方法有效地消除了需要手动进行特征选择并实时观察的需要。我们通过对27个临床任务和两个公开可用的EHR数据集的独立队列实验验证了这些特性，结果显示REMed优于其他现代架构，它们旨在处理尽可能多的事件。值得注意的是，我们发现REMed的偏好与医学专家的偏好密切相似。我们期望我们的方法能显著加速该领域的发展。

    Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
    
[^16]: 视频有益的多模式机器翻译

    Video-Helpful Multimodal Machine Translation. (arXiv:2310.20201v1 [cs.CL])

    [http://arxiv.org/abs/2310.20201](http://arxiv.org/abs/2310.20201)

    该论文介绍了一个名为EVA的多模式机器翻译（MMT）数据集，包含了来自电影和电视剧的视频片段和相应的日英和中英平行字幕对，旨在解决现有数据集中视觉信息无法生成适当翻译的问题。同时，提出了基于选择性注意模式的SAFA MMT模型。

    

    现有的多模式机器翻译（MMT）数据集由图像和视频字幕或教学视频字幕组成，这些数据很少包含语言的歧义，使得视觉信息在生成适当的翻译方面不起作用。最近的工作构建了一个模糊字幕数据集来减轻这个问题，但仍然存在视频不一定有助于消除歧义的问题。我们介绍了EVA（用于模糊字幕翻译的广泛训练集和有助于视频的评估集），一个包含852k日英（Ja-En）平行字幕对、520k中英（Zh-En）平行字幕对和来自电影和电视剧的相应视频片段的MMT数据集。除了广泛的训练集外，EVA还包含一个视频有益的评估集，其中字幕是模糊的，并且视频能够保证有助于消除歧义。此外，我们提出了基于选择性注意模式的SAFA MMT模型。

    Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention mode
    
[^17]: 在多语种惯用语境下生成延续

    Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])

    [http://arxiv.org/abs/2310.20195](http://arxiv.org/abs/2310.20195)

    本论文测试了生成性语言模型在多语种惯用语境中生成延续的能力，并发现模型在字面和惯用上下文中的表现相似，并且在两种语言中均具有鲁棒性。

    

    处理惯用或字面多词表达是理解和生成任何语言的关键方面。为包含惯用（或字面）表达的叙述生成具有上下文相关的延续的任务可以让我们测试生成性语言模型（LMs）理解非组合性比喻文本的纤细语言能力。我们使用两种不同语言（英语和葡萄牙语）的数据集在三种不同的训练设置下（零样本、少样本和微调）进行了一系列实验。我们的结果表明，模型在生成字面上下文的延续时略优于惯用上下文，但差距很小。此外，本研究中研究的模型在两种语言中表现出同样出色的性能，表明生成模型在执行此任务时具有鲁棒性。

    The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
    
[^18]: DIVKNOWQA: 通过知识库和文本进行开放领域问答来评估LLMs的推理能力

    DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text. (arXiv:2310.20170v1 [cs.CL])

    [http://arxiv.org/abs/2310.20170](http://arxiv.org/abs/2310.20170)

    DIVKNOWQA提出了一个综合数据集，评估LLMs在连接异构知识源上的推理能力，填补了现有领域中的一个空白。

    

    大型语言模型（LLMs）展现了令人印象深刻的生成能力，但是当仅仅依赖其内部知识时，它们容易出现幻觉，尤其是在回答需要少见知识的问题时。基于检索的LLMs已经成为将LLMs牢固根植于外部知识的潜在解决方案。然而，最近的方法主要强调从非结构化文本语料库中检索，因为这种方法可以无缝集成到提示中。当使用结构化数据（如知识图谱）时，大多数方法会将其简化为自然文本，忽略了底层结构。此外，当前领域中存在一个重要的缺口，即缺乏一个真实的基准来评估将LLMs与异质知识源（如知识库和文本）连接起来的有效性。为了填补这个缺口，我们创建了一个综合数据集，提出了两个独特的挑战：（1）需要检索信息的两跳多源问题。

    Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving informat
    
[^19]: GAR-meets-RAG范式用于零样本信息检索

    GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. (arXiv:2310.20158v1 [cs.CL])

    [http://arxiv.org/abs/2310.20158](http://arxiv.org/abs/2310.20158)

    这项工作提出了一种新颖的GAR-meets-RAG范式，通过迭代改进检索和重写阶段，克服了零样本信息检索中现有范式的挑战。

    

    给定一个查询和一个文档语料库，信息检索(IR)任务是输出一个相关文档的排名列表。结合大语言模型(LLMs)和基于嵌入的检索模型，最近的研究在零样本检索问题上取得了有希望的结果，即无法访问目标领域的标记数据。其中两个流行的范式是生成增强检索(GAR)或GAR（为查询生成附加上下文，然后检索）和检索增强生成(RAG)或RAG（将相关文档作为上下文检索，然后生成答案）。这些范式的成功取决于(i)在零样本设定中很难获得的高召回检索模型和(ii)通常需要良好初始化的高精确度(重新)排序模型。在这项工作中，我们提出了一种新颖的GAR-meets-RAG循环公式，克服了现有范式的挑战。我们的方法通过GAR和RAG阶段的迭代改进检索(通过GAR)和重写(通过RAG)阶段来提高性能。

    Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the
    
[^20]: 与稀疏人类监督相适应的成本有效的交互多重保真度学习用于语言模型的适应性

    Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])

    [http://arxiv.org/abs/2310.20153](http://arxiv.org/abs/2310.20153)

    该论文提出了一种交互多重保真度学习框架，用于在有限的注释预算下开发小型特定领域的语言模型。该方法通过平衡低保真度自动注释和高保真度人类注释，以最大化模型性能。同时，还提出了一种增强注释多样性和信息性的查询策略。

    

    大型语言模型（LLMs）在各种任务中展示了卓越的能力。然而，由于它们在部署时的巨大规模、易受错误信息影响以及高昂的数据注释成本，它们在特定领域任务的适应性有限。我们提出了一种新颖的交互多重保真度学习（IMFL）框架，用于在有限的注释预算下开发小型特定领域的语言模型。我们的方法将领域特定的微调过程形式化为多重保真度学习问题，重点是确定平衡低保真度自动LLM注释和高保真度人类注释以最大化模型性能的最佳获取策略。我们进一步提出了一种增强注释多样性和信息性的探索-开发查询策略，结合了两个创新设计：1）从人类注释样本中选择上下文例子来改进LLM注释

    Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation
    
[^21]: 基于大型语言模型的多智能体一致性寻求

    Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])

    [http://arxiv.org/abs/2310.20151](http://arxiv.org/abs/2310.20151)

    本文研究了基于大型语言模型的多智能体系统中的一致性寻求问题。研究发现，在没有明确指导的情况下，智能体主要使用平均策略进行一致性寻求，同时还分析了智能体数量、智能体个性和网络拓扑对协商过程的影响。

    

    大型语言模型（LLM）驱动的多智能体系统在协作解决复杂任务方面展现出了令人期待的能力。本研究考虑了多智能体协作中的一个基本问题：一致性寻求。当多个智能体一起工作时，我们关注的是它们如何通过智能体间的协商达成一致。为此，本研究研究了一个一致性寻求任务，其中每个智能体的状态是一个数值，并且它们通过相互协商来达成一致值。研究发现，当没有明确指导应采用哪种策略时，LLM驱动的智能体主要使用平均策略进行一致性寻求，尽管它们可能偶尔会使用其他策略。此外，本研究还分析了智能体数量、智能体个性和网络拓扑对协商过程的影响。本研究的发现有望为理解LLM驱动的多智能体行为奠定基础。

    Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-
    
[^22]: 忘记你想忘记的：LLMs的高效遗忘方法

    Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])

    [http://arxiv.org/abs/2310.20150](http://arxiv.org/abs/2310.20150)

    本论文提出了一种高效的遗忘框架来处理大语言模型（LLMs）中的隐私问题和数据保护违规。通过引入轻量级遗忘层到transformers中，并使用有选择的师生目标学习，我们能够在删除数据后有效地更新LLMs，而无需重新训练整个模型。实验证明了该方法的有效性。

    

    大语言模型（LLMs）通过预训练和记忆各种文本数据取得了重大进展，但这个过程可能面临隐私问题和数据保护规定的违规。因此，在不损害预测质量的情况下，能够轻松地从这些模型中删除与个人用户相关的数据变得越来越重要。为解决这些问题，本文提出了一种高效的遗忘框架，通过引入学习有选择的师生目标的轻量级遗忘层到transformers中，能够在数据删除后有效地更新LLMs，而无需对整个模型进行重新训练。此外，我们还引入了一种融合机制，以有效地组合不同的遗忘层，以处理一系列的遗忘操作。分类和生成任务的实验证明了我们方法的有效性。

    Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our 
    
[^23]: EELBERT:通过动态嵌入实现微型模型

    EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])

    [http://arxiv.org/abs/2310.20144](http://arxiv.org/abs/2310.20144)

    EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。

    

    我们介绍了EELBERT，一种用于压缩基于Transformer的模型（例如BERT）的方法，对下游任务的准确性影响最小。这是通过将模型的输入嵌入层替换为动态的，即即时计算的嵌入实现来实现的。由于输入嵌入层占模型大小的重要部分，特别是对于较小的BERT变体，用嵌入计算函数替换该层有助于显著减小模型大小。在GLUE基准测试中的实证评估显示，我们的BERT变体（EELBERT）与传统BERT模型相比仅具有最小的回归。通过这种方法，我们能够开发出我们最小的模型UNO-EELBERT，其GLUE得分比完全训练的BERT-tiny高4％，同时体积小15倍（1.2MB）。

    We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
    
[^24]: DEPN: 检测和编辑预训练语言模型中的隐私神经元

    DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])

    [http://arxiv.org/abs/2310.20138](http://arxiv.org/abs/2310.20138)

    本论文提出了一个框架 DEPN，用于检测和编辑预训练语言模型中的隐私神经元。通过引入隐私神经元检测器和隐私神经元聚合器，我们能够有效降低私人数据泄露的风险，并且不会影响模型的性能。

    

    在大规模数据上预训练的语言模型可以捕捉到丰富的知识和信息，但先前的研究揭示了其对数据记忆和重复的能力带来了数据泄露的风险。为了有效降低这些风险，我们提出了一个名为DEPN的框架，用于检测和编辑预训练语言模型中的隐私神经元，部分受到知识神经元和模型编辑的启发。在DEPN中，我们引入了一种称为隐私神经元检测器的新方法，用于定位与隐私信息相关的神经元，然后通过将它们的激活设置为零来编辑这些检测到的隐私神经元。此外，我们提出了一种隐私神经元聚合器，以批处理方式去除隐私信息。实验结果表明，我们的方法能够显著有效地降低私人数据泄露的风险，而不会降低模型的性能。

    Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate th
    
[^25]: 使用学习的提示层改进提示调整

    Improving Prompt Tuning with Learned Prompting Layers. (arXiv:2310.20127v1 [cs.CL])

    [http://arxiv.org/abs/2310.20127](http://arxiv.org/abs/2310.20127)

    这篇论文提出了一种新颖的框架，称为选择性提示调整（SPT），通过学习选择合适的提示层来改进提示调整的性能。作者还提出了一个双层优化框架SPT-DARTS，可以更好地优化可学习的门，并提高提示调整的效果。实验证明，SPT框架在多个基准数据集上表现出色。

    

    提示调整通过在输入嵌入或隐藏状态之前添加软提示，只优化提示来适应预训练模型（PTM）到下游任务。先前的工作手动选择了远非最佳的提示层，并未利用提示调整的潜力。在本工作中，我们提出了一个新颖的框架，选择性提示调整（SPT），通过在每个中间层插入一个由可学习概率门控制的提示来学习选择合适的提示层。我们进一步提出了一种新颖的双层优化框架SPT-DARTS，可以更好地优化可学习门，并改进学习提示层设置的最终提示调整性能。我们在十个基准数据集上进行了广泛实验，包括全数据和少样本情景。结果表明，我们的SPT框架可以比之前的PETuning基准表现更好，且使用的提示层设置相当或更少。

    Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \underline{S}elective \underline{P}rompt \underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer t
    
[^26]: Ling-CL:通过语言课程理解NLP模型

    Ling-CL: Understanding NLP Models through Linguistic Curricula. (arXiv:2310.20121v1 [cs.CL])

    [http://arxiv.org/abs/2310.20121](http://arxiv.org/abs/2310.20121)

    Ling-CL通过开发基于数据和现有语言复杂性知识的语言课程，帮助理解NLP模型学习的潜在语言知识。此工作在多个NLP数据集上展示了课程学习方法的指标选择和任务挑战推理的能力。

    

    我们利用心理语言学和语言习得研究中的语言复杂性表征来开发数据驱动的课程，以理解模型在处理NLP任务时所学习的潜在语言知识。我们方法的创新之处在于通过分析数据、现有关于语言复杂性的知识以及模型训练过程中的行为来制定语言课程。通过分析多个基准NLP数据集，我们的课程学习方法找到了一组语言度量指标，这些指标可用来了解解决每个任务所需的挑战和推理。我们的工作将为未来所有NLP领域的研究提供指导，使语言复杂性能够在研究和开发过程中被早期考虑。此外，我们的工作还促使人们对NLP中的黄金标准和公平评估进行审视。

    We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.
    
[^27]: 提升大型语言模型的数据生成能力

    Making Large Language Models Better Data Creators. (arXiv:2310.20111v1 [cs.CL])

    [http://arxiv.org/abs/2310.20111](http://arxiv.org/abs/2310.20111)

    本文提出了一种统一的数据生成流程，只需要一个格式化示例，可以应用于各种任务，包括传统问题场景。该方法旨在解决大型语言模型在下游应用中依赖于人工标注数据的问题。

    

    尽管大型语言模型（LLM）在自然语言处理领域显著提升了技术水平，但在实际应用中，由于成本、响应速度、控制能力以及隐私和安全等方面的考虑，将它们用于下游任务仍然具有挑战性。因此，在某些情况下，可训练模型仍然是首选解决方案。然而，这些模型仍然需要人工标注的数据才能实现最佳性能，而这种数据获取工作成本高且耗时。为了解决这个问题，一些减少人力工作量的技术被提出，其中包括使用LLM进行数据标注或生成数据。虽然这些方法在某些应用中是有效的，但在实际场景中遇到了困难。数据标注需要仔细选择数据，而生成数据则需要特定任务的启示工程。在本文中，我们提出了一个统一的数据生成流程，只需要一个格式化示例，适用于包括传统问题场景在内的广泛任务范围。

    Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic o
    
[^28]: 使用大型语言模型有效分类编程课程中学生求助请求

    Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models. (arXiv:2310.20105v1 [cs.CY])

    [http://arxiv.org/abs/2310.20105](http://arxiv.org/abs/2310.20105)

    本研究评估了GPT-3.5和GPT-4模型在分类编程课程中学生求助请求方面的性能，并发现它们可以通过大型语言模型的自动分类来提高教育系统的效能。

    

    准确分类与所寻求的帮助类型相关的学生求助请求可以实现针对性的有效响应。自动分类这类请求是非平凡的，但大型语言模型（LLMs）似乎提供了一种易于使用、经济实惠的解决方案。本研究评估了GPT-3.5和GPT-4模型在初级编程课程中对学生求助请求进行分类的性能。在零-shot测试中，GPT-3.5和GPT-4在大多数类别上表现出可比性，而GPT-4在与调试相关的子类别的分类上胜过GPT-3.5。对GPT-3.5模型进行微调可以提高其性能，以至于它在各类别之间的准确性和一致性上接近于两位人类评分者之间的观察结果。总体而言，本研究证明了使用LLMs通过自动分类学生需求来增强教育系统的可行性。

    The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
    
[^29]: 评估神经语言模型作为语言习得的认知模型

    Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])

    [http://arxiv.org/abs/2310.20093](http://arxiv.org/abs/2310.20093)

    本文评估了神经语言模型作为语言习得的认知模型的潜力。作者认为用于评估句法能力的基准不够严格，并提出了使用严选数据集来探索语法结构基础的建议。

    

    尽管神经语言模型（LM）的训练方式与儿童语言习得存在明显的差异，但它们在许多技术任务上的成功为其作为语言科学理论的潜在相关性提供了支持。本文认为，评估LM的句法能力的一些主流基准可能不够严格。特别是，我们发现基于模板的基准缺乏语言理论和心理学研究中常见的结构多样性。当使用小规模数据来模拟儿童语言习得时，简单的基准模型可以轻松匹配LM。我们主张使用已经过大量母语者评估过梯度可接受性并设计用于探索语法结构基础的严选数据集。在其中一个这样的数据集LI-Adger上，LM评估句子的方式与人类语言处理不一致。

    The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with hu
    
[^30]: 基于提示式学习的临床信息提取中关键词优化模板插入

    Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning. (arXiv:2310.20089v1 [cs.CL])

    [http://arxiv.org/abs/2310.20089](http://arxiv.org/abs/2310.20089)

    提供了一种关键词优化模板插入方法（KOTI）以改善临床信息提取任务的性能，通过优化位置可以在零射和少射训练设置中提高模型性能。

    

    临床笔记分类是一项常见的临床自然语言处理任务。然而，带有注释的数据集很少。最近，基于提示式学习已成为一种有效的方法，通过仅使用少量训练样本就可以适应预训练模型进行文本分类。提示设计的关键组成部分是模板（即提示文本）的定义。然而，模板位置的影响尚未得到充分的研究。在临床环境中，这似乎尤为重要，因为临床笔记中的任务相关信息通常很少。在本研究中，我们开发了一种关键词优化模板插入方法（KOTI），并展示了优化位置如何在零射和少射训练设置中改善多个临床任务的性能。

    Clinical note classification is a common clinical NLP task. However, annotated data-sets are scarse. Prompt-based learning has recently emerged as an effective method to adapt pre-trained models for text classification using only few training examples. A critical component of prompt design is the definition of the template (i.e. prompt text). The effect of template position, however, has been insufficiently investigated. This seems particularly important in the clinical setting, where task-relevant information is usually sparse in clinical notes. In this study we develop a keyword-optimized template insertion method (KOTI) and show how optimizing position can improve performance on several clinical tasks in a zero-shot and few-shot training setting.
    
[^31]: 通过大型语言模型将总结和检索整合，增强个性化能力

    Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models. (arXiv:2310.20081v1 [cs.CL])

    [http://arxiv.org/abs/2310.20081](http://arxiv.org/abs/2310.20081)

    个性化是NLP系统中用户体验的关键，本文通过大型语言模型将总结和检索整合，提出了一种利用任务感知的总结增强个性化方法。

    

    个性化是自然语言处理(NLP)系统中用户体验的一个关键因素。我们提出了一种利用大型语言模型(LLM)来更好地个性化用户体验的方法。为了个性化语言模型的输出，一个直接的方法是将过去的用户数据并入语言模型的提示中，但这种方法会导致输入过长，超出输入长度限制，并且引起延迟和成本问题。现有方法通过选择性地提取相关的用户数据（即选择性检索）来解决这些挑战，以构建下游任务的提示。然而，基于检索的方法受限于潜在的信息丢失、缺乏更深入的用户理解和冷启动挑战。为了克服这些限制，我们提出了一种新颖的总结增强方法，通过扩展检索增强个性化与任务感知相结合。

    Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-awar
    
[^32]: 部分张量化变压器用于自然语言处理

    Partial Tensorized Transformers for Natural Language Processing. (arXiv:2310.20077v1 [cs.CL])

    [http://arxiv.org/abs/2310.20077](http://arxiv.org/abs/2310.20077)

    论文研究了在自然语言处理中应用部分张量化的变压器架构，通过对BERT和ViT等神经网络的嵌入层压缩和部分张量化，显著提高了模型的准确性，并打破了张量分解领域的新局面。

    

    变压器架构由于其前所未有的准确性在自然语言处理和其他机器学习任务中开创了新局面。然而，它们广泛的存储和参数需求通常阻碍了它们的实际应用。在本研究中，我们研究了张量列分解对提高BERT和ViT等变压器视觉语言神经网络的准确性和压缩性的影响。我们专注于嵌入层压缩和神经网络的部分张量化（PTNN）通过一种算法方法。我们的新颖PTNN方法在不需要后期调整的情况下显著提高了现有模型的准确性，打破了张量分解领域的新局面。

    The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
    
[^33]: 用指导调整实现对生成模型的自动评估

    Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])

    [http://arxiv.org/abs/2310.20072](http://arxiv.org/abs/2310.20072)

    本论文提出了一种基于指导调整的学习度量方法，通过对预训练语言模型进行微调来实现对生成模型的自动评估。实验结果表明，这种方法在各种评估任务上取得了良好的性能，并且多任务联合训练可以进一步提高性能。

    

    自动评估自然语言生成一直以来都是NLP领域一个难以达到的目标。最近的一种方法是通过对预训练语言模型进行微调，来模拟人类在特定任务和评估标准上的判断。受到指导调整模型的泛化能力的启发，我们提出了一种基于指导调整的学习度量方法。为了测试我们的方法，我们收集了HEAP数据集，其中包含了各种自然语言生成任务和评估标准的人类判断。我们的研究结果表明，在HEAP上通过指导调整语言模型可以取得良好的评估性能，尽管有些评估标准的学习并不那么容易。此外，多任务联合训练可以进一步提高性能，对于未来缺乏人工标注数据的任务是有益的。

    Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
    
[^34]: 哪些示例适合用于上下文学习？朝着有效和高效的选择方向。(arXiv:2310.20046v1 [cs.CL])

    Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection. (arXiv:2310.20046v1 [cs.CL])

    [http://arxiv.org/abs/2310.20046](http://arxiv.org/abs/2310.20046)

    本文提出了一种模型自适应的无优化算法AdaICL，通过识别不确定的示例并进行基于语义多样性的选择，提高了上下文学习的效果和预算效率。

    

    大规模语言模型（LLM）可以通过上下文学习（ICL）适应新任务。ICL是高效的，因为它不需要对训练过的LLM进行任何参数更新，只需要少量的标注示例作为LLM的输入。本文研究了一种用于ICL的主动学习方法，在标注示例的预算有限的情况下。我们提出了一种模型自适应的无优化算法，称为AdaICL，它可以识别模型不确定的示例，并进行基于语义多样性的示例选择。基于多样性的采样可以提高整体效果，而不确定性采样可以提高预算效率并帮助LLM学习新信息。此外，AdaICL将其采样策略建模为一个最大覆盖问题，根据模型的反馈动态调整，并可以通过贪婪算法进行近似求解。在九个数据集和七个LLM上进行的大量实验证明，AdaICL的性能提高了4.4%的准确度，比现有技术达到了7.7%。

    Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7%
    
[^35]: 用于临床总结中事实对齐的合成模仿编辑反馈

    Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])

    [http://arxiv.org/abs/2310.20033](http://arxiv.org/abs/2310.20033)

    本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。

    

    大型语言模型（LLMs）如GPT和LLaMA系列在捕捉和浓缩关键上下文信息及在总结任务中实现最先进的性能方面表现出了异常能力。然而，社区对这些模型的虚构问题的担忧仍在不断上升。LLMs有时会生成虚构的摘要，这在临床领域的NLP任务（例如临床笔记总结）中可能会导致严重错误的诊断。使用人类反馈对LLMs进行微调已经显示出在生成过程中实现事实一致性的承诺，但这种训练过程需要高质量的人工注释数据，而在临床领域获取这样的数据可能非常昂贵。在这项工作中，我们提出了一种新的管道，使用ChatGPT代替人类专家生成高质量的反馈数据，以改善临床笔记总结的事实一致性。

    Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
    
[^36]: 早期检测西班牙语中的抑郁症和饮食障碍：UNSL在MentalRiskES 2023中的表现

    Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023. (arXiv:2310.20003v1 [cs.CL])

    [http://arxiv.org/abs/2310.20003](http://arxiv.org/abs/2310.20003)

    该论文讨论了在西班牙语中早期检测抑郁症和饮食障碍的问题，提出了基于Transformer模型的解决方法，并应用了早期检测框架中定义的决策策略。

    

    MentalRiskES是一个新颖的挑战，旨在解决与西班牙语早期风险检测相关的问题。目标是尽早检测到显示出不同任务的心理障碍迹象的Telegram用户。任务1涉及饮食障碍的检测，任务2关注抑郁症的检测，任务3旨在检测一种未知的障碍。这些任务被分成子任务，每个子任务定义了一种解决方法。我们的研究小组参与了任务1和任务2的子任务A：一个二元分类问题，评估用户是积极还是消极。为了解决这些任务，我们提出了基于Transformer的模型，并根据早期检测框架定义的标准进行决策策略。其中一个模型呈现了每个任务解决的重要词汇的扩展词汇表。此外，我们应用了基于模型进行预测历史的决策策略。

    MentalRiskES is a novel challenge that proposes to solve problems related to early risk detection for the Spanish language. The objective is to detect, as soon as possible, Telegram users who show signs of mental disorders considering different tasks. Task 1 involved the users' detection of eating disorders, Task 2 focused on depression detection, and Task 3 aimed at detecting an unknown disorder. These tasks were divided into subtasks, each one defining a resolution approach. Our research group participated in subtask A for Tasks 1 and 2: a binary classification problem that evaluated whether the users were positive or negative. To solve these tasks, we proposed models based on Transformers followed by a decision policy according to criteria defined by an early detection framework. One of the models presented an extended vocabulary with important words for each task to be solved. In addition, we applied a decision policy based on the history of predictions that the model performs duri
    
[^37]: 通过生成检索增强本体图和多智能体策略，解释性基于大型语言模型的材料设计

    Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design. (arXiv:2310.19998v1 [cs.CL])

    [http://arxiv.org/abs/2310.19998](http://arxiv.org/abs/2310.19998)

    本文探索了使用大型语言模型（LLMs）作为工具在材料工程分析中的应用。LLMs可以用作一组具有特定特征、能力和指令的人工智能代理，为分析和设计问题提供强大的问题解决策略。实验重点是使用经过微调的模型MechGPT，在材料力学领域进行训练。通过Fine-tuning重新调整参数，增强了LLMs的能力。

    

    Transformer神经网络显示出非常有希望的能力，特别是在材料分析、设计和制造方面的应用，包括它们有效地处理人类语言、符号、代码和数值数据的能力。在本文中，我们探讨了使用大型语言模型（LLM）作为支持材料工程分析的工具，用于检索有关主题领域的关键信息、开发研究假设、发现知识不同领域之间的机制关系，并根据物理基本事实编写和执行仿真代码进行主动知识生成。当LLMs被用作具有特定特征、能力和指令的AI代理集合时，它们可以为分析和设计问题提供强大的问题解决策略。我们的实验重点是使用在材料力学领域的训练数据基础上开发的Fine-tuned模型MechGPT。我们首先证实了Fine-tuning如何赋予LLMs重新调整参数的能力。

    Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. When used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how finetuning endows LLMs with re
    
[^38]: BioInstruct:用于生物医学自然语言处理的大型语言模型指令调整

    BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])

    [http://arxiv.org/abs/2310.19975](http://arxiv.org/abs/2310.19975)

    BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。

    

    大型语言模型通过在大量数据上进行预训练，然后进行特定领域的指令调整，在许多自然语言处理任务中取得了巨大成功。然而，在生物医学领域只发表了很少的指令。为了解决这个问题，我们引入了BioInstruct，这是一个定制的任务特定指令数据集，包含超过25,000个示例。通过使用三个人工筛选的指令样本，以GPT-4语言模型作为提示，精调大型语言模型，我们旨在优化其在生物医学自然语言处理中的性能。我们对LLaMA LLMs (1&2,7B&13B)进行了指令调整，并在生物医学自然语言处理应用中进行了评估，包括信息提取、问答和文本生成。我们还评估了指令如何对模型性能的贡献，使用了多任务学习原则。

    Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
    
[^39]: 利用Transformer潜力的策略：UNSL在eRisk 2023中的应用

    Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023. (arXiv:2310.19970v1 [cs.CL])

    [http://arxiv.org/abs/2310.19970](http://arxiv.org/abs/2310.19970)

    该论文介绍了UNSL在eRisk 2023中的应用，其中涉及了对抑郁症症状、病理性赌博风险和饮食紊乱迹象的检测与估计，并提出了基于Transformer的解决方案。

    

    CLEF eRisk实验室探索与互联网风险检测相关的不同任务的解决方案。在2023年的比赛中，任务1是搜索抑郁症症状，其目标是根据与BDI问卷症状相关性提取用户的写作。任务2与早期发现病理性赌博风险的问题有关，参与者需要尽快检测到有风险的用户。最后，任务3是估计饮食紊乱迹象的严重程度。我们的研究小组参与了前两个任务，并提出了基于Transformer的解决方案。在任务1中，我们应用了不同的方法，这些方法可以在信息检索任务中很有趣。两个提案是基于语境化嵌入向量的相似性，另一个则基于提示，这是一种吸引人的当前机器学习技术。在任务2中，我们提出了三个经过微调的模型，其后跟决策策略。

    The CLEF eRisk Laboratory explores solutions to different tasks related to risk detection on the Internet. In the 2023 edition, Task 1 consisted of searching for symptoms of depression, the objective of which was to extract user writings according to their relevance to the BDI Questionnaire symptoms. Task 2 was related to the problem of early detection of pathological gambling risks, where the participants had to detect users at risk as quickly as possible. Finally, Task 3 consisted of estimating the severity levels of signs of eating disorders. Our research group participated in the first two tasks, proposing solutions based on Transformers. For Task 1, we applied different approaches that can be interesting in information retrieval tasks. Two proposals were based on the similarity of contextualized embedding vectors, and the other one was based on prompting, an attractive current technique of machine learning. For Task 2, we proposed three fine-tuned models followed by decision polic
    
[^40]: 深度和宽度对Transformer语言模型泛化能力的影响

    The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])

    [http://arxiv.org/abs/2310.19956](http://arxiv.org/abs/2310.19956)

    深层的transformer语言模型在组合式泛化能力上比浅层模型更好，但额外层数的相对收益会迅速减小。

    

    为了处理新的句子，语言模型（LMs）必须以组合的方式进行泛化 - 将熟悉的元素以新的方式结合起来。模型结构的哪些方面促进了组合式泛化？针对transformers，我们测试假设，即当transformers更深（具有更多层次）时，它们更容易进行组合式泛化，这个假设基于最近的理论和实证研究。由于简单地增加层数会增加总参数数量，混淆了深度和大小，我们构建了三类模型，通过以保持总参数数量恒定的方式来权衡深度和宽度（分别为41M、134M和374M个参数）。我们将所有模型预训练为LMS，然后在测试组合式泛化的任务上进行微调。我们得出了三个主要结论：（1）在微调后，深层模型在超出分布范围的情况下比浅层模型更好地进行泛化，但额外层次的相对收益迅速减小；（2）在每个模型组中，深层模型表现出更好的组合式泛化能力...（文本已截断）

    To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show bett
    
[^41]: Split-NER: 通过两个基于问答的分类解决命名实体识别问题

    Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications. (arXiv:2310.19942v1 [cs.CL])

    [http://arxiv.org/abs/2310.19942](http://arxiv.org/abs/2310.19942)

    本研究提出了一种名为Split-NER的系统，通过将命名实体识别问题分成提取实体提及跨度和跨度分类两个子任务，然后利用问答模型解决这两个子任务，实现了高效和准确的命名实体识别。

    

    本研究将命名实体识别问题分成两个逻辑子任务：（1）提取实体提及跨度，无论实体类型如何；（2）将跨度分类为实体类型。进一步，我们将这两个子任务都形式化为问答问题，并产生两个可以分别为每个子任务进行优化的更轻的模型。在四个跨领域数据集上的实验表明，这个两步法既有效又节省时间。我们的系统SplitNER在OntoNotes5.0、WNUT17和一个网络安全数据集上的性能超过了基线，并在BioNLP13CG上表现相当。在所有情况下，与QA基线对照相比，它在训练时减少了显著的时间。我们系统的有效性源于分别对跨度检测和分类进行BERT模型的微调。源代码可在https://github.com/c3sr/split-ner找到。

    In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
    
[^42]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^43]: 评估大型语言模型：一项全面调查

    Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19736](http://arxiv.org/abs/2310.19736)

    本调查综述了对大型语言模型（LLMs）的评估，包括知识和能力评估、对齐评估和安全评估。对于充分利用LLMs的能力以及确保其安全和有益的发展至关重要。

    

    大型语言模型（LLMs）在各种任务中展示了卓越的能力。它们吸引了广泛的关注，并在许多下游应用中得到了应用。然而，与双刃剑一样，LLMs也存在潜在风险。它们可能受到私人数据泄露，产生不适当、有害或误导性的内容。此外，LLMs的快速进展引发了对可能出现没有足够保障的超智能系统的担忧。为了有效利用LLMs的能力，并确保其安全和有益的发展，对LLMs进行严格和全面的评估至关重要。本调查旨在提供对LLMs评估的全面概述。我们将LLMs的评估分为三大类别：知识和能力评估，对齐评估和安全评估。除了全面回顾评估方法和技术之外，

    Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and
    
[^44]: 结合语言模型的领域专用方法：一种丰富多彩的途径

    Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19708](http://arxiv.org/abs/2310.19708)

    该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。

    

    通用目的的语言模型在处理领域特定术语和术语时遇到困难，这些术语经常在医学或工业领域等专业领域中使用。此外，他们通常很难解释将通用语言与专门术语混合使用的混合语音。这对于在这些特定领域内操作的自动语音识别系统构成了挑战。在这项工作中，我们介绍了一种新颖的方法，将领域特定或次级语言模型集成到通用的语言模型中。该策略涉及对每个单词进行标记或“上色”，以指示其与通用或领域特定的语言模型的关联。我们开发了一种优化算法，可增强波束搜索算法，以有效处理涉及上色单词的推理。我们的评估表明，这种方法在集成术语到语言任务中非常有效。值得注意的是，我们的方法显著降低了领域专用任务的错误率。

    General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
    
[^45]: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks - MoCa

    MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks. (arXiv:2310.19677v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19677](http://arxiv.org/abs/2310.19677)

    本研究旨在衡量大型语言模型（LLM）在因果和道德判断任务上与人类的一致性。通过收集24篇认知科学论文的故事数据集，并使用统计分析方法，发现LLMs在考虑因果和道德因素时与人类参与者存在差异。

    

    人类对物理和社会世界的常识理解是基于直觉理论的。这些理论支持进行因果和道德判断。当发生不好的事情时，我们自然会问：谁做了什么，为什么？认知科学的丰富文献研究了人们的因果和道德直觉。这项工作揭示了一些系统地影响人们判断的因素，如规范违反以及伤害是否可避免或不可避免。我们收集了24篇认知科学论文的故事数据集，并开发了一个系统来注释每个故事所研究的因素。利用这个数据集，我们测试了大型语言模型（LLM）在基于文本场景上是否与人类参与者的因果和道德判断相一致。在总体水平上，最近的LLM的一致性有所改善。然而，通过统计分析，我们发现LLM在考虑这些不同因素时与人类参与者存在较大差异。

    Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human partic
    
[^46]: 大型语言模型：对当前辩论的细微差别的需求和对理解的务实观点(arXiv:2310.19671v2 [cs.CL] 更新)

    Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding. (arXiv:2310.19671v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19671](http://arxiv.org/abs/2310.19671)

    当前的大型语言模型在生成文本方面表现出了出色的能力，但对其能力的辩论缺乏细致的考虑。这篇论文评估了三个常见批评观点，并提出了对LLMs理解和意图问题的务实观点。

    

    目前的大型语言模型（LLMs）在生成语法正确、流畅的文本方面无与伦比。LLMs正在快速出现，并且关于LLM能力的辩论已经开始，但反思滞后。因此，在这篇立场论文中，我们首先聚焦于辩论，并对LLM能力的三个重复出现的批评进行批判性评估：i) LLMs只是模仿训练数据中的统计模式；ii) LLMs掌握了形式但并非功能性语言能力；iii) LLMs中的语言学习不能为人类语言学习提供信息。通过实证和理论论证，我们展示了这些观点需要更多细微之处。其次，我们概述了一个对LLMs中“真正”的理解和意图问题的务实观点。理解和意图涉及到我们假设他人具有的不可观察的心理状态，因为它们具有实用价值：它们使我们能够抽象出复杂的底层机制并预测行为。

    Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviou
    
[^47]: LLMaAA: 将大型语言模型作为主动批注器

    LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19596](http://arxiv.org/abs/2310.19596)

    LLMaAA是一种利用大型语言模型作为主动批注器的方法，通过在主动学习循环中使用LLM确定高效批注内容，以最大限度地利用LLM的潜力并利用大量未标记数据。

    

    在自然语言处理（NLP）中，普遍的监督学习方法因需求大量高质量标注数据而声名狼藉。实际上，获取这样的数据是一项昂贵的事业。最近，大型语言模型（LLM）出色的少样本性能推动了数据集生成的发展，其中训练数据仅从LLM中合成。然而，这种方法通常存在质量低的问题，并且需要多个数量级的已标记数据才能实现令人满意的性能。为了充分发挥LLM的潜力并利用大量未标记数据，我们提出了LLMaAA，它将LLM作为批注器，并将它们放入主动学习循环中以高效确定批注内容。为了利用伪标签进行稳健学习，我们优化了批注和训练过程：（1）我们从小的示范池中抽取k-NN示例作为上下文示例，（2）我们采用示例加权技术。

    Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting techniqu
    
[^48]: 使用大型语言模型进行成分句法分析

    Constituency Parsing using LLMs. (arXiv:2310.19462v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19462](http://arxiv.org/abs/2310.19462)

    本文研究了使用大型语言模型（LLMs）进行成分句法分析的潜力，通过采用线性化策略将输出树结构转化为符号序列，进一步提高了任务的效果。实验结果对LLMs的性能、泛化能力和成分句法分析中的挑战进行了深入研究。

    

    成分句法分析是一个基础但尚未解决的自然语言处理任务。本文探索了最近大型语言模型（LLMs）在各个领域和任务中展现出的卓越性能在解决这一任务上的潜力。我们采用三种线性化策略将输出的树结构转化为符号序列，使得LLMs可以通过生成线性化树来解决成分句法分析。我们使用多种不同的LLMs进行实验，包括ChatGPT、GPT-4、OPT、LLaMA和Alpaca，并将它们的性能与最先进的成分句法分析器进行比较。我们的实验涵盖了零样本学习、少样本学习和全样本学习的不同设置，并在一个领域内和五个领域外的测试数据集上评估模型。我们的发现揭示了LLMs的性能、泛化能力和成分句法分析中的挑战。

    Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.
    
[^49]: BERT失去耐心对抗恶意减速不能保持稳健性

    BERT Lost Patience Won't Be Robust to Adversarial Slowdown. (arXiv:2310.19152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19152](http://arxiv.org/abs/2310.19152)

    本文评估了多出口语言模型对抗恶意减速的稳健性, 发现复杂的机制更易受到恶意减速的攻击。此外，对抗训练无效，但对话模型的输入清洗是有效的。

    

    本文系统评估多出口语言模型对抗恶意减速的稳健性。为了审核其稳健性，我们设计了一种减速攻击，生成绕过早期退出点的自然恶意文本。我们使用所得到的WAFFLE攻击作为工具，对三种多出口机制在GLUE基准测试中对抗恶意减速进行全面评估。然后，我们展示了我们的攻击显著降低了这三种方法在白盒和黑盒设置下提供的计算节省效果。机制越复杂，越容易受到恶意减速的攻击。我们还对扰动的文本输入进行了语言分析，识别出我们的攻击生成的常见扰动模式，并将其与标准的恶意文本攻击进行了比较。此外，我们证明对抗训练在打败我们的减速攻击方面是无效的，但使用对话模型（如ChatGPT）进行输入清洗是有效的。

    In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can r
    
[^50]: TeacherLM: 教人打鱼而不是给鱼，语言建模同理

    TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19019](http://arxiv.org/abs/2310.19019)

    TeacherLM-7.1B是一个小型模型，通过给自然语言处理样本进行注释，教会其他模型“为什么”而不仅仅是“什么”。它在MMLU上取得了52.3的零样本得分，同时具有出色的数据增强能力。发布TeacherLM系列模型和增强的数据集作为开源项目。

    

    大型语言模型(LLMs)在各种自然语言处理任务中展现了惊人的推理和数据增强能力。然而，小型模型呢？在这项工作中，我们提出了TeacherLM-7.1B，能够给大多数自然语言处理样本进行相关基础知识、思维链和常见错误的注释，使注释不仅仅是一个答案，而且使其他模型可以学习“为什么”，而不仅仅是“什么”。TeacherLM-7.1B模型在MMLU上实现了52.3的零样本得分，超过了拥有100B参数的大多数模型。更令人印象深刻的是其数据增强能力。基于TeacherLM-7.1B，我们在多任务设置中使用了来自OPT和BLOOM系列的不同参数的多个学生模型对58个自然语言处理数据集进行了增强。实验结果表明，TeacherLM提供的数据增强带来了显着的好处。我们将作为开源发布TeacherLM系列模型和增强的数据集。

    Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
    
[^51]: LoRAShear: 高效的大型语言模型结构剪枝和知识恢复

    LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18356](http://arxiv.org/abs/2310.18356)

    LoRAShear是一种高效的大型语言模型结构剪枝和知识恢复方法，通过逐步剪枝和动态微调，有效减少LLMs的占用空间并且保持性能。

    

    大型语言模型（LLMs）已经改变了人工智能的格局，但其庞大的规模在计算成本方面带来了重大挑战。我们引入了LoRAShear，一种新颖的高效方法，用于结构化剪枝LLMs并恢复知识。LoRAShear首先在LoRA模块上创建依赖图，以发现最小删除结构并分析知识分布。然后，它在LoRA适配器上进行渐进式结构剪枝，并实现内在的知识转移，以更好地保留冗余结构中的信息。为了恢复剪枝期间丢失的知识，LoRAShear仔细研究并提出了一种动态微调方案，使用动态数据适配器，以有效缩小与完整模型的性能差距。数值结果表明，只使用一块GPU在几天内，LoRAShear将LLMs的占用空间有效减少了20%，仅有1.0%的性能损失。

    Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance d
    
[^52]: AllTogether：使用大型语言模型对使用拼接提示进行Web导航的效果进行研究

    AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18331](http://arxiv.org/abs/2310.18331)

    AllTogether是一个标准化的提示模板，通过增强任务背景表示，提高了大型语言模型（LLMs）在基于HTML的Web导航中的性能。研究结果显示，像GPT-4这样的模型在这类任务中优于较小的模型，并且HTML代码片段的长度和历史轨迹对性能有显著影响。同时，在实时环境反馈方面，优于之前的逐步指导。这项工作为未来LLM驱动的Web代理研究提供了宝贵的见解。

    

    大型语言模型（LLMs）已经成为用于Web导航任务的有前景的代理，它们解释目标并与Web页面进行交互。然而，这种任务中使用拼接提示的效果仍未得到充分探索。我们引入了AllTogether，一个标准化的提示模板，增强任务背景表示，从而提高LLMs在基于HTML的Web导航中的性能。我们通过来自开源Llama-2和可访问的GPT模型的提示学习和指令微调来评估这种方法的效果。我们的结果表明，像GPT-4这样的模型在Web导航任务中优于较小的模型。此外，我们发现HTML代码片段的长度和历史轨迹显著影响性能，并且之前的逐步指导比实时环境反馈更有效。总体而言，我们相信我们的工作为未来LLM驱动的Web代理研究提供了宝贵的见解。

    Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
    
[^53]: TarGEN: 基于大型语言模型的目标数据生成技术

    TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])

    [http://arxiv.org/abs/2310.17876](http://arxiv.org/abs/2310.17876)

    TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。

    

    大型语言模型（LLM）的快速发展引发了对数据合成技术的兴趣，旨在生成多样且高质量的合成数据集。然而，这些合成数据集往往缺乏多样性并且存在噪声。在本文中，我们提出了TarGEN，一种利用LLM生成高质量的合成数据集的多步提示策略。TarGEN的一个优点是无需种子；它不需要特定的任务实例，扩大了其适用性。我们还通过一种称为自我修正的方法，使LLM能够在创建数据集过程中纠正标记错误的实例，确保可靠的标签。为了评估我们技术的有效性，我们模拟了SuperGLUE基准测试中的8个任务，并在合成和原始训练集上微调了各种语言模型，包括仅编码器、编码器-解码器和仅解码器模型。在原始测试集上的评估结果显示，模型在合成数据集上训练的效果与原始数据集相当。

    The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine
    
[^54]: 使用RadGraph和少样本提示的风格感知放射学报告生成

    Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])

    [http://arxiv.org/abs/2310.17811](http://arxiv.org/abs/2310.17811)

    该论文提出了一种使用RadGraph和少样本提示的风格感知放射学报告生成的方法。通过将报告的内容和风格分开处理，可以避免生成临床不准确的报告。定量评估和人工评估结果均表明该方法表现出良好的性能，并生成与个体放射科医生风格完全相同的报告。

    

    自动从医学影像中生成报告有望改善放射科医生的工作流程。现有方法通过直接从图像生成完整的报告来考虑图像到报告的建模任务。然而，这样混淆了报告的内容（如发现和其属性）与其风格（如格式和词汇选择），可能导致临床不准确的报告。为了解决这个问题，我们提出了一种放射学报告生成的两步方法。首先，我们从图像中提取内容，然后将提取的内容转化为与特定放射科医生风格相匹配的报告。为此，我们利用RadGraph——一种报告的图表示——以及大型语言模型（LLM）。在定量评估中，我们发现我们的方法在性能方面具有益处。通过临床评估者进行的人工评估表明，AI生成的报告与个体放射科医生的风格完全相同，无法区别。

    Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
    
[^55]: CodeFusion: 一种用于代码生成的预训练扩散模型

    CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])

    [http://arxiv.org/abs/2310.17680](http://arxiv.org/abs/2310.17680)

    CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。

    

    假设一个开发者只能修改其最后一行代码，在正确之前，他们需要多少次从头开始编写函数呢？自然语言代码生成的自回归模型也有类似的限制：它们不容易重新考虑之前生成的标记。我们介绍了一种名为CodeFusion的预训练扩散代码生成模型，通过迭代地对以编码的自然语言为条件的完整程序进行去噪，以解决这个限制。我们针对Bash、Python和Microsoft Excel条件格式(CF)规则的自然语言到代码生成任务对CodeFusion进行评估。实验结果显示，CodeFusion（75M参数）在top-1准确率上表现与最先进的自回归系统（350M-175B参数）相当，并且在top-3和top-5准确率上表现优于它们，这是由于它在多样性与质量之间的平衡更好。

    Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
    
[^56]: FormaT5: 以自然语言生成条件表格格式化的抽样和示例

    FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])

    [http://arxiv.org/abs/2310.17306](http://arxiv.org/abs/2310.17306)

    FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。

    

    表格的格式化是可视化、展示和分析中的重要属性。电子表格软件允许用户通过编写数据相关的条件格式规则来自动格式化表格。但对用户来说，编写这样的规则通常是具有挑战性的，因为它要求他们理解和实现底层逻辑。我们提出了一个基于转换器的模型FormaT5，可以根据目标表格和期望的格式逻辑的自然语言描述生成一个条件格式规则。我们发现，用户为这些任务提供的描述通常是不明确或含糊的，这使得代码生成系统难以在一步中准确学习到所需的规则。为了解决这个规范不足的问题并减少参数错误，FormaT5通过放弃目标的方式学习预测占位符。这些占位符可以由第二个模型或者当可用的行示例时，由一个基于示例的编程系统填充。

    Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
    
[^57]: 通过回溯法纠正，减少摘要中的幻觉

    Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])

    [http://arxiv.org/abs/2310.16176](http://arxiv.org/abs/2310.16176)

    本文介绍了一种简单而有效的技术，CoBa，用于减少摘要中的幻觉。该方法通过测量条件词概率和上下文词距离的统计信息进行幻觉检测，并通过直观的回溯法进行减轻。实验证明，CoBa在减少摘要幻觉方面是有效且高效的。

    

    摘要生成旨在生成源文件的自然语言摘要，既简洁又保留重要元素。尽管最近取得了一些进展，但神经文本摘要模型容易产生幻觉（或更准确地说是混淆），即生成的摘要包含源文件中没有根据的细节。在本文中，我们引入了一种简单而有效的技术，CoBa，用于减少摘要中的幻觉。该方法基于两个步骤：幻觉检测和减轻。我们展示了通过测量有关条件词概率和上下文词距离的简单统计信息可以实现前者。此外，我们还证明了直观的回溯法在减轻幻觉方面的惊人效果。我们在三个文本摘要基准数据集上对所提出的方法进行了全面评估。结果表明，CoBa在减少摘要幻觉方面是有效且高效的。

    Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
    
[^58]: FANToM: 在交互中对机器心智理论进行压力测试的基准

    FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])

    [http://arxiv.org/abs/2310.15421](http://arxiv.org/abs/2310.15421)

    FANToM是一个新的基准，用于通过问答在信息不对称的对话环境中压力测试机器的心智理论。这个基准对最先进的大型语言模型来说具有挑战性，即使是具有思维链推理和微调的模型也比人类表现得差。

    

    目前关于心智理论（ToM）的评估主要集中在使用缺乏互动性的被动故事，我们介绍了FANToM，一个新的基准，通过问答在信息不对称的对话环境中进行心智理论的压力测试。我们的基准结合了心理学中的重要理论要求和对评估大型语言模型（LLM）时必要的经验考虑。特别地，我们制定了多种类型的问题，要求相同的基本推理来识别LLM中不存在或虚假的心智理论能力。我们展示了FANToM对最先进的LLM来说具有挑战性，即使是具有思维链推理和微调的LLM也表现比人类差得多。

    Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
    
[^59]: 用于越南社区基于COVID-19问答的生成前训练转换器

    Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering. (arXiv:2310.14602v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14602](http://arxiv.org/abs/2310.14602)

    本研究实现了用于越南社区的生成前训练转换器(GPT-2)，专注于COVID-19相关问答。实验结果表明，GPT-2模型在社区COVID-19问答数据集中表现出色，优于其他模型。

    

    最近的研究提供了生成前训练转换器（GPT），一种预训练的语言模型，在自然语言处理领域的广泛潜力的经验证据。 GPT已被有效地应用于最先进的问答系统中作为解码器，在各种任务中表现出色。然而，有关GPT在越南语中应用的当前研究现状仍然有限。 本文旨在通过在越南语中特别关注COVID-19相关查询的社区问答中实现GPT-2来填补这一差距。我们通过对社区COVID-19问答数据集中不同的转换器与SOTA模型进行比较分析，引入了一种新颖的方法。实验结果表明，GPT-2模型显示出非常有希望的结果，优于其他SOTA模型以及先前的社区COVID-19问答模型。

    Recent studies have provided empirical evidence of the wide-ranging potential of Generative Pre-trained Transformer (GPT), a pretrained language model, in the field of natural language processing. GPT has been effectively employed as a decoder within state-of-the-art (SOTA) question answering systems, yielding exceptional performance across various tasks. However, the current research landscape concerning GPT's application in Vietnamese remains limited. This paper aims to address this gap by presenting an implementation of GPT-2 for community-based question answering specifically focused on COVID-19 related queries in Vietnamese. We introduce a novel approach by conducting a comparative analysis of different Transformers vs SOTA models in the community-based COVID-19 question answering dataset. The experimental findings demonstrate that the GPT-2 models exhibit highly promising outcomes, outperforming other SOTA models as well as previous community-based COVID-19 question answering mod
    
[^60]: 立体地图：量化大型语言模型中人类化刻板印象的意识

    StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models. (arXiv:2310.13673v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.13673](http://arxiv.org/abs/2310.13673)

    StereoMap提出了一个理论基础的框架，通过使用刻板印象内容模型的维度来量化大型语言模型对社会各群体的感知。研究结果显示，大型语言模型对这些群体表现出多样化的感知，具有混合的特征。

    

    大型语言模型（LLMs）被观察到在训练数据中编码和传播有害的关联。我们提出了一个理论基础的框架，称为StereoMap，以了解它们对社会各群体的看法。该框架基于心理学中的刻板印象内容模型（SCM），该理论认为刻板印象并不相同，而是温暖和能力这两个维度的因素划分了刻板印象的性质。基于SCM理论，StereoMap使用温暖和能力这两个维度来描绘LLMs对社会群体（通过社会人口特征定义）的感知。此外，该框架还可以调查LLMs判断的关键词和推理的言语化，以揭示影响它们感知的潜在因素。我们的结果显示，LLMs对这些群体表现出多样化的感知，具有混合的特征。

    Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed
    
[^61]: AI反馈促进的质量-多样性算法

    Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])

    [http://arxiv.org/abs/2310.13032](http://arxiv.org/abs/2310.13032)

    基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。

    

    在许多文本生成问题中，用户可能不仅偏好单一回复，而是希望得到多样性的高质量输出以供选择。质量-多样性（QD）搜索算法旨在通过不断改进和多样化候选人群来实现这一目标。然而，QD在创作性写作等质性领域的应用受到算法指定质量和多样性度量的困难的限制。有趣的是，最近语言模型（LMs）的发展使得通过AI反馈指导搜索成为可能，其中LMs在自然语言中被提示来评估文本的质性方面。借助这一进展，我们引入了通过AI反馈实现的质量-多样性算法（QDAIF），其中进化算法应用LMs来生成变异并评估候选文本的质量和多样性。在创作性写作领域的评估中，与非QDAIF算法相比，QDAIF更广泛地覆盖高质量样本的指定搜索空间。

    In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
    
[^62]: 对于零样本神经机器翻译性能变化的更好理解

    Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance. (arXiv:2310.10385v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10385](http://arxiv.org/abs/2310.10385)

    该论文研究了零样本神经机器翻译性能变化的原因，发现目标语言的翻译质量、词汇重叠和语言特性是影响性能变化的关键因素。

    

    多语言神经机器翻译（MNMT）促进了知识分享，但往往在零样本（ZS）翻译质量上表现不佳。虽然之前的研究已经探讨了整体ZS性能低下的原因，但我们的工作引入了一个新的视角：ZS性能存在较大的变化。这表明MNMT的ZS能力并不是均匀地差，而是在特定的翻译方向上产生了合理的结果。通过系统性的实验，涵盖40种语言的1,560个语言方向，我们确定了三个影响ZS NMT性能变化的关键因素：1）目标语言的翻译能力，2）词汇重叠，3）语言特性。我们的研究结果表明，目标语言的翻译质量是最有影响力的因素，词汇重叠始终影响ZS性能。此外，语言特性，如语言家族和书写系统，尤其在较小的模型中起到了一定作用。

    Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low ZS performance, our work introduces a fresh perspective: the presence of high variations in ZS performance. This suggests that MNMT does not uniformly exhibit poor ZS capability; instead, certain translation directions yield reasonable results. Through systematic experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target side translation capability 2) vocabulary overlap 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furt
    
[^63]: Ziya-VL: 双语大规模视觉语言模型通过多任务指令调整

    Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning. (arXiv:2310.08166v1 [cs.CL])

    [http://arxiv.org/abs/2310.08166](http://arxiv.org/abs/2310.08166)

    本论文介绍了Ziya-VL系列，这是一组双语大规模视觉语言模型，旨在将视觉语义融入语言模型以进行多模态对话。模型采用了查询变换器和优化方案，如指令调整和多阶段训练，以实现视觉语言对齐。

    

    最近的进展扩大了大型语言模型（LLMs）在零射击图像到文本生成和理解方面的能力，通过整合多模输入。然而，这样的成功通常局限于英语场景，原因是缺乏大规模和高质量的非英语多模资源，使得在其他语言中建立竞争对手变得极其困难。在本文中，我们介绍了Ziya-VL系列，这是一组双语大规模视觉语言模型（LVLMs），旨在将视觉语义融入LLM以进行多模态对话。我们的模型由Ziya-VL-Base和Ziya-VL-Chat组成，采用BLIP-2中的查询变换器，并进一步探索指令调整、多阶段训练和低秩适应模块等优化方案的辅助作用，以实现视觉语言对齐。此外，我们刺激GPT-4在多模态场景中的理解能力，将我们收集的英文图像文本数据集翻译成...

    Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-VL series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into 
    
[^64]: 使用合成任务教授语言模型更少幻觉

    Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.06827](http://arxiv.org/abs/2310.06827)

    通过设计合成任务，我们的研究表明减少合成任务上的幻觉可以帮助减少现实世界的抽象概括任务上的幻觉。

    

    大型语言模型（LLMs）在抽象概括任务（如基于文档的问答、会议概述和临床报告生成）中经常产生幻觉，即使所有必要信息都在上下文中。然而，在这些任务上优化LLMs以减少幻觉是具有挑战性的，因为在每个优化步骤中有效评估幻觉是困难的。在这项工作中，我们展示了通过减少合成任务上的幻觉也可以减少现实世界下游任务上的幻觉。我们的方法，SynTra，首先设计了一个合成任务，其中易于诱发和衡量幻觉。然后，通过对合成任务进行前缀调优来优化LLM的系统消息，并最终将系统消息转移到现实中难以优化的任务中。通过对三个现实的抽象概括任务，使用仅合成检索任务进行监督，SynTra减少了两个具有13B参数的LLMs的幻觉。我们还发现通过优化合成任务上的系统消息，可以最大限度地减少现实任务上的幻觉。

    Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the sy
    
[^65]: 探索用于衡量文本相关性的嵌入方法: 揭示在线评论中的情感和关系

    Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments. (arXiv:2310.05964v1 [cs.CL])

    [http://arxiv.org/abs/2310.05964](http://arxiv.org/abs/2310.05964)

    本论文通过使用嵌入方法和词语间的语义关系，研究了不同社交媒体平台上评论之间的情感和语义关系，并通过分析用户评论中的文本，让研究人员、政治家和商业代表能够追踪全球用户之间共享情感的路径。

    

    在一场导致互联网使用量增长70%的大流行病之后，全世界的人们开始更多地使用社交媒体。Twitter、Meta Threads、YouTube和Reddit等应用程序变得越来越普及，几乎没有任何数字空间不表达公众意见。本文研究了不同社交媒体平台上评论之间的情感和语义关系，并讨论了这些不同媒体平台上共享意见的重要性，使用词嵌入分析句子和文档中的组成部分。它使研究人员、政治家和商业代表能够追踪全球用户之间共享情感的路径。本研究论文提出了多种方法来衡量从这些热门在线平台的用户评论中提取的文本的相关性。通过利用捕捉词语间语义关系并有助于分析网络情感的嵌入方法，我们可以进行情感分析。

    After a pandemic that caused internet usage to grow by 70%, there has been an increased number of people all across the world using social media. Applications like Twitter, Meta Threads, YouTube, and Reddit have become increasingly pervasive, leaving almost no digital space where public opinion is not expressed. This paper investigates sentiment and semantic relationships among comments across various social media platforms, as well as discusses the importance of shared opinions across these different media platforms, using word embeddings to analyze components in sentences and documents. It allows researchers, politicians, and business representatives to trace a path of shared sentiment among users across the world. This research paper presents multiple approaches that measure the relatedness of text extracted from user comments on these popular online platforms. By leveraging embeddings, which capture semantic relationships between words and help analyze sentiments across the web, we
    
[^66]: DeBERTinha: 一种用于巴西葡萄牙语自然语言处理任务的DebertaV3 XSmall的多步骤适应方法

    DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v1 [cs.CL])

    [http://arxiv.org/abs/2309.16844](http://arxiv.org/abs/2309.16844)

    DeBERTinha是一种通过多步骤训练和微调适应DebertaV3 XSmall模型的方法，用于巴西葡萄牙语自然语言处理任务。它在命名实体识别、情感分析和判断句子相关性等任务上表现出优越性能。

    

    本文提出了一种适应英语预训练DebertaV3 XSmall模型用于巴西葡萄牙语自然语言处理（NLP）任务的方法。该方法的关键在于多步训练过程，以确保模型在葡萄牙语上的有效调优。使用Carolina和BrWac的初始数据集进行预处理，解决表情符号、HTML标签和编码等问题。使用SentencePiece创建一个包含50,000个token的葡萄牙语特定词汇表。模型不是从头训练，而是使用预训练的英语模型的权重来初始化网络的大部分，仅包括随机嵌入，以识别从头训练的昂贵成本。采用替换标记检测任务以与DebertaV3训练的相同格式微调模型。适应的模型称为DeBERTinha，在命名实体识别、情感分析和判断句子相关性等下游任务上表现出优越性能，胜过BER。

    This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BER
    
[^67]: COCO-Counterfactuals:自动构建图像-文本对的反事实例

    COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])

    [http://arxiv.org/abs/2309.14356](http://arxiv.org/abs/2309.14356)

    COCO-Counterfactuals是一个自动构建图像-文本对的反事实例的框架，通过使用文本到图像扩散模型来自动生成多模态反事实例。通过人工评估，我们验证了COCO-Counterfactuals的质量，并展示了其对于改善域外泛化能力的实用性。

    

    反事实例在自然语言处理(NLP)领域中已证明对于评估和改进语言模型对数据集中的虚假相关性的鲁棒性非常有价值。尽管反事实例在NLP领域具有显著的效用，但由于创建最小反事实变化的图像-文本配对数据的难度，多模态反事实例的研究相对较少。为了解决这一挑战，我们引入了一个可扩展的框架，利用文本到图像扩散模型自动生成反事实例。我们使用这个框架来创建COCO-Counterfactuals，这是一个基于MS-COCO数据集的多模态反事实数据集，包括图像和文本标题的配对。我们通过人工评估验证了COCO-Counterfactuals的质量，并展示了现有的多模态模型在我们的反事实图像-文本配对中面临的挑战。此外，我们展示了COCO-Counterfactuals在改善域外泛化能力方面的实用性。

    Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
    
[^68]: 因果自我解释中的D-分离

    D-Separation for Causal Self-Explanation. (arXiv:2309.13391v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.13391](http://arxiv.org/abs/2309.13391)

    本研究提出了一种新的准则，最小条件依赖（MCD）准则，来揭示因果解释。通过最小化选择理由候选项上未选择部分与目标标签的依赖，强制选择所有的标签原因。

    

    理性化是一种用于自然语言处理模型的自我解释框架。传统方法通常使用最大互信息（MMI）准则来找到最能说明目标标签的理由。然而，这个准则可能受到与因果解释或目标标签相关的虚假特征的影响。我们提出了一种新颖的准则来揭示因果解释，称为最小条件依赖（MCD）准则，该准则建立在我们发现的非因果特征与目标标签通过因果解释被“分离”之上。通过在选择的理由候选项上给出未选择部分与目标标签之间的依赖最小化，强制选择所有的标签原因。在这项研究中，我们使用一种简单而实用的依赖度量，具体是KL散度，来验证我们提出的MCD准则。

    Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are \emph{d-separated} by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empir
    
[^69]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^70]: 超越文档页分类：设计、数据集和挑战

    Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])

    [http://arxiv.org/abs/2308.12896](http://arxiv.org/abs/2308.12896)

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。

    

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，即在测试数据的性质上（$X$：多通道、多页、多行业；$Y$：类别分布和标签集的多样性）和考虑的分类任务上（$f$：多页文档、页面流和文档捆绑分类，...）。我们确定了公共的多页文档分类数据集的缺乏，并规范了应用场景中产生的不同分类任务，并激发了以高效的多页文档表示为目标的价值。对提出的多页文档分类数据集进行的实验研究表明，当前的基准测试已经变得无关紧要，并需要更新以评估实际中自然发生的完整文档。这个现实情况检查也呼吁更成熟的评估方法，涵盖校准评估、推理复杂性（时间-内存）和一系列现实分散情况。

    This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
    
[^71]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^72]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^73]: 通过蒸馏将知识更新传播到语言模型中

    Propagating Knowledge Updates to LMs Through Distillation. (arXiv:2306.09306v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.09306](http://arxiv.org/abs/2306.09306)

    本研究通过上下文蒸馏的方法成功将知识更新传播到语言模型中，实现了更广泛的推理能力。

    

    现代语言模型可以存储和使用大量关于现实世界实体的知识，但如何更新存储在模型参数中的知识尚不清楚。尽管先前的语言模型知识更新方法成功地注入了原子事实，但更新后的语言模型无法根据注入的事实进行推理。在这项工作中，我们证明了基于上下文蒸馏的方法可以在提供关于实体的知识的同时传播该知识以实现更广泛的推理。我们的方法由两个阶段组成：传输集生成和传输集上的蒸馏。我们首先通过提示语言模型从实体定义中生成延续来生成一个传输集。然后，我们更新模型参数，使得语言模型的分布（学生）与在传输集上给定定义条件下的语言模型的分布（教师）相匹配。我们的实验表明，这种方法在传播知识方面更有效。

    Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowled
    
[^74]: TrojPrompt：基于黑盒方式的预训练语言模型木马攻击

    TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2306.06815](http://arxiv.org/abs/2306.06815)

    本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。

    

    Prompt学习被证明在提高预训练语言模型（PLM）适应性方面非常有效，超越了传统的微调范式，并在专为少样本学习场景量身定制的应用程序和API中展现了杰出的前景。但是，尽管prompt学习的API越来越受欢迎，但它们的安全问题仍未得到充分探索。本文在prompt学习的PLM API的特洛伊易感性方面进行了开创性研究。我们发现，离散提示，少样本和黑盒设置是几个关键挑战，限制了现有后门攻击的适用性。为了解决这些挑战，我们提出了TrojPrompt，这是一种自动的黑盒框架，可有效生成通用的和隐秘的触发器，并将特洛伊木马插入硬提示。具体而言，我们提出了一种API驱动的通用触发器发现算法，通过查询受害者PLM API，为各种输入生成通用触发器。

    Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
    
[^75]: FACTIFY3M: 通过5W问答解释的多模式事实验证基准

    FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])

    [http://arxiv.org/abs/2306.05523](http://arxiv.org/abs/2306.05523)

    FACTIFY3M是一个以多模式虚假信息验证为目标的数据集。虚假信息如今已成为当下重大的社会问题，这一数据集旨在通过多模式验证来及时识别和缓解虚假信息。

    

    打击虚假信息是当前亟待解决的社会危机之一——大约67%的美国人认为虚假信息会产生大量的不确定性，其中有10%的人有意识地传播虚假信息。证据表明，虚假信息可以操纵民主进程和公众舆论，并在危机期间引起股市动荡、社会恐慌甚至死亡。因此，应及时识别并尽可能缓解虚假信息。然而，由于社交媒体平台每天分享大约32亿张图像和720,000 小时的视频，因此对于多模式虚假信息的可扩展性检测需要高效的事实验证。尽管在文本模式下自动事实验证取得了进展(例如，FEVER, LIAR)，但学术界在多模式事实验证方面缺乏实质性的努力。为了填补这一空白，我们引入了FACTIFY3M数据集，该数据集包含300万个样本，通过多种模式和5W问答提高了事实验证领域的极限。

    Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multi
    
[^76]: 骆驼能走多远？探索开放资源中指令调优的现状。

    How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])

    [http://arxiv.org/abs/2306.04751](http://arxiv.org/abs/2306.04751)

    本文探究了指令调优语言模型在一系列开放指令跟随数据集上的最新进展，提供了一组大型指令调优模型，并进行了系统评估。实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。

    

    本研究探索了指令调优语言模型在一系列开放指令跟随数据集上的最新进展。尽管最近声称开放模型可以与最先进的专有模型相媲美，但这些声称常常伴随着有限的评估，使得难以全面比较模型并确定各种资源的效用。我们提供了一组大型指令调优模型，大小为6.7B到65B个参数，在12个指令数据集上进行训练，包括手动策划的（例如OpenAssistant）和综合的指令数据集（例如Alpaca），并通过一系列自动、基于模型和基于人的指标对其在事实知识、推理、多语言、编码和开放式指令跟随能力方面进行系统评估。我们进一步介绍了T\"ulu，我们在高质量开放资源组合上微调的表现最佳的指令调优模型组合。我们的实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。

    In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.  Our experiments show that different instru
    
[^77]: 使用大型语言模型注释进行社会科学中的有效下游统计推断: 基于设计的半监督学习

    Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])

    [http://arxiv.org/abs/2306.04746](http://arxiv.org/abs/2306.04746)

    该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。

    

    在计算社会科学（CSS）中，研究人员通过分析文档来解释社会和政治现象。在大多数情况下，CSS研究人员首先获取文档的标签，然后使用可解释的回归分析来解释标签。大型语言模型（LLMs）的最近进展可以通过在规模上便宜地注释文档来降低CSS研究成本，但这些替代标签通常是不完美和有偏的。我们提出了一种新算法，用于使用LLMs的输出进行下游统计分析，同时保证与CSS研究基本相关的统计属性-如渐近无偏性和正确的不确定性量化。我们表明，直接在下游统计分析中使用LLM预测的替代标签会导致实质性偏差和无效置信区间，即使替代准确性高达80-90％。为了解决这个问题，我们基于无偏机器学习提出了基于设计的半监督学习（D-SSL）算法，该算法将LLM注释与有针对性的采样相结合，以实现有效的下游统计推断。我们的方法可以将标签获取的CSS研究成本降低80％，而不影响统计分析的有效性。模拟研究和实际数据示例表明，与直接使用LLM预测标签相比，D-SSL可以将回归估计的准确性提高多达40％。

    In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
    
[^78]: 多模态融合交互: 人类和自动量化研究

    Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])

    [http://arxiv.org/abs/2306.04125](http://arxiv.org/abs/2306.04125)

    本文比较研究了两种人类注释者可以用于注释多模态交互的分类，并提出了一种基于信息分解的分类学。

    

    在几乎所有多模态问题和应用中，多模态融合多种异构和互联的信号是一个基本挑战。为了进行多模态融合，我们需要理解模态可以展现的交互类型：每种模态如何单独提供对任务有用的信息，以及当存在其他模态时这些信息如何变化。在本文中，我们对人类注释者如何被利用来注释多模态交互的两种分类进行了比较研究：(1) 部分标签，其中不同随机分配的注释者注释给定第一个、第二个和两个模态的标签，以及(2) 反事实标签，其中同一注释者被要求在给出第一个模态之前注释标签，然后给出第二个模态，并要求他们明确地推理他们的答案如何改变，然后提出基于信息分解的另一种分类学。

    Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotator
    
[^79]: 语言模型和神经响应测量之间的结构相似性

    Structural Similarities Between Language Models and Neural Response Measurements. (arXiv:2306.01930v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.01930](http://arxiv.org/abs/2306.01930)

    本研究研究了语言模型和神经响应测量之间的结构相似性，发现神经语言模型越大，其表示越相似于脑成像的神经响应测量。

    

    大型语言模型具有复杂的内部动态，但可以研究其词汇和短语的表示的几何结构。人类语言处理也很难理解，但神经响应测量可以提供在听或读时激活的（嘈杂的）记录，我们可以从中提取相似的词汇和短语表示。本研究在脑解码的背景下研究了这些表示所引发的几何结构之间的相似性。我们发现，神经语言模型越大，其表示与脑成像的神经响应测量越相似。

    Large language models (LLMs) have complicated internal dynamics, but induce representations of words and phrases whose geometry we can study. Human language processing is also opaque, but neural response measurements can provide (noisy) recordings of activation during listening or reading, from which we can extract similar representations of words and phrases. Here we study the extent to which the geometries induced by these representations, share similarities in the context of brain decoding. We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging. Code is available at \url{https://github.com/coastalcph/brainlm}.
    
[^80]: 信仰与命运：Transformer在组合性方面的局限性。

    Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18654](http://arxiv.org/abs/2305.18654)

    研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。

    

    Transformer大型语言模型在需要复杂多步推理的任务上表现卓越，但同时在一些简单问题上也会出现失败。这引发了疑问：这些错误是偶然的，还是它们表明了更实质性的限制？为了揭示Transformer的神秘面纱，我们研究了这些模型在三个代表性的组合型任务中的极限 - 多位数乘法、逻辑网格谜题和一个经典的动态规划问题。 这些任务需要将问题分解为子步骤，并将这些步骤综合成精确的答案。我们将组合型任务转化为计算图，以系统地量化其复杂性，并将推理步骤分解为中间子程序。我们的实证结果表明，Transformer通过将多步组合推理转化为线性子图匹配来解决组合型任务。

    Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
    
[^81]: 理解情绪价值是一项联合深度学习任务

    Understanding Emotion Valence is a Joint Deep Learning Task. (arXiv:2305.17422v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17422](http://arxiv.org/abs/2305.17422)

    研究通过多任务学习方法，探索情绪价值与情绪载体之间的相互依赖关系，并在联合预测设置中使用判别性模型取得了最佳平衡。

    

    对说话人的话语或写作中情绪价值分析有助于理解对话中情绪状态的激活和变化。最近，引入了情绪载体（EC）的概念来解释说话人所感受到的情绪及其表现。在这项工作中，我们通过多任务学习方法研究了情绪价值和EC之间的自然相互依赖关系。我们尝试了预训练语言模型（PLM）在情绪价值和EC预测任务中的单任务、两步法和联合设置。我们比较和评估了生成性（GPT-2）和判别性（BERT）架构在每种设置中的性能。结果显示，在一个任务中提供了另一个任务的真实标签可以提高模型在另一个任务中的预测性能。我们进一步观察到，判别性模型在联合预测设置中取得了情绪价值和EC预测任务的最佳平衡。因此，我们获得了一个能在这两个任务中表现出色的单一模型。

    The valence analysis of speakers' utterances or written posts helps to understand the activation and variations of the emotional state throughout the conversation. More recently, the concept of Emotion Carriers (EC) has been introduced to explain the emotion felt by the speaker and its manifestations. In this work, we investigate the natural inter-dependency of valence and ECs via a multi-task learning approach. We experiment with Pre-trained Language Models (PLM) for single-task, two-step, and joint settings for the valence and EC prediction tasks. We compare and evaluate the performance of generative (GPT-2) and discriminative (BERT) architectures in each setting. We observed that providing the ground truth label of one task improves the prediction performance of the models in the other task. We further observed that the discriminative model achieves the best trade-off of valence and EC prediction tasks in the joint prediction setting. As a result, we attain a single model that perfo
    
[^82]: 只使用前向传递微调语言模型

    Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])

    [http://arxiv.org/abs/2305.17333](http://arxiv.org/abs/2305.17333)

    本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。

    

    微调语言模型已经在各种下游任务中取得了成功，但随着语言模型的增大，反向传播需要的存储空间数量变得过高。零阶（ZO）方法理论上仅使用两次前向传递就可以估计梯度，但通常情况下对大型模型进行优化的速度非常慢。在本文中，我们提出了一种内存高效的零阶优化器（MeZO），将经典的ZO-SGD方法适应于原地操作，从而使用与推理相同的存储空间微调语言模型。例如，只使用一张A100 80GB GPU，MeZO就可以训练一个300亿参数的模型，而使用反向传播可以在相同的预算下仅训练一个27亿个参数的语言模型。我们在各种模型类型（掩码和自回归语言模型）、模型规模（高达66B）和下游任务（分类、多项选择和生成）进行了全面的实验。我们的结果表明，（1）MeZO明显优于上下文学习和线性PR模型。

    Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
    
[^83]: 扩散模型是否是视觉语言推理器？

    Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])

    [http://arxiv.org/abs/2305.16397](http://arxiv.org/abs/2305.16397)

    本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。

    

    近期，使用去噪扩散过程的文本-图像生成模型已取得了巨大的定性成功。然而，与鉴别式视觉-语言模型不同，将基于扩散的生成模型置于自动细粒度定量评估高级现象（如组合性）的任务中是一项非常棘手的任务。为此，我们开展了两项创新。首先，我们使用一种称为DiffusionITM的新方法将基于扩散的模型（在我们的情况下，是稳定扩散）转换为任何图像文本匹配(ITM)任务。其次，我们引入了7个复杂的视觉语言任务、偏差评估和详细分析的生成-鉴别评估基准(GDBench)。我们发现，Stable Diffusion + DiffusionITM在许多任务上具有竞争力，并在组合性任务（如CLEVR和Winoground等）上优于CLIP。我们通过在MS-COCO上微调保持图像特征的转移设置进一步提高其组合性能。

    Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
    
[^84]: 在真实世界的信息查找场景中研究LLMs的表格生成能力

    Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios. (arXiv:2305.14987v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14987](http://arxiv.org/abs/2305.14987)

    本文研究了在真实世界的信息查找场景中，LLMs的表格生成能力。通过使用四个数据集，在包括数据洞察生成和基于查询生成的情境下进行了评估。实验结果表明，当前高性能的LLMs在表格生成方面表现良好。

    

    表格数据在各个行业中普遍存在，用户为了实现其信息查找目的需要花费大量的时间和精力来理解和操作这些数据。大型语言模型（LLMs）的进展已经显示出改善用户效率的巨大潜力。然而，在真实世界的表格信息查找应用中采用LLMs的状况尚未得到充分探索。本文通过在两种真实世界的信息查找场景下使用四个数据集，研究了不同LLMs的表格生成能力。其中包括LogicNLG和我们新构建的LoTNLG数据集，用于数据洞察生成；以及FeTaQA和我们新构建的F2WTQ数据集，用于基于查询的生成。我们的研究围绕三个研究问题展开，分别评估LLMs在表格生成、自动评估和反馈生成方面的性能。实验结果表明，当前的高性能LLMs，特别是...

    Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specificall
    
[^85]: 大型语言模型能力的可预测性如何？对BIG-bench的案例研究

    How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14947](http://arxiv.org/abs/2305.14947)

    本研究通过对BIG-bench实验记录的研究，发现大型语言模型（LLM）的能力具有可预测性，并提出了寻找信息丰富的子集以最大程度恢复整个集合性能的问题。

    

    我们研究了大型语言模型（LLM）能力的可预测性：在使用不同模型家族、参数数量、任务数量和上下文示例数量的过去实验记录的基础上，我们能否准确预测LLM在新实验配置上的性能？回答这个问题对LLM用户（例如，决定尝试哪些模型）、开发者（例如，优先评估代表性任务）和研究社区（例如，识别需要进一步调查的难以预测的能力）具有实际意义。我们在BIG-bench的实验记录上研究了性能预测问题。在随机的训练-测试分离中，基于多层感知器（MLP）的预测器的R^2得分超过95%，表明实验记录中存在可学习的模式。然后，我们提出了寻找“small-bench”的问题，即从BIG-bench任务中寻找信息丰富的子集，可以从中最大程度地恢复整个集合的性能。

    We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation).  We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an $R^2$ score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for "small-bench," an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We
    
[^86]: 迈向可靠的假新闻缓解：泛化，不确定性和GPT-4

    Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14928](http://arxiv.org/abs/2305.14928)

    本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。

    

    假新闻构成了一个重要的社会挑战，目前的方法尚未找到有效的解决方案。我们提出关注泛化，不确定性以及如何利用最新的大型语言模型，以便在无法完美分类的情况下创建更实用的工具来评估信息真实性。我们首先证明了GPT-4在多个设定和语言中可以胜过之前的方法。接下来，我们探索泛化，揭示了GPT-4和RoBERTa-large在失效模式上的差异。第三，我们提出了处理不确定性的技术，可以检测到不可能的例子并显著改进结果。我们还讨论了其他语言模型，温度，提示，版本控制，可解释性和网络检索的结果，每个结果都提供了实际的见解和未来研究的方向。最后，我们发布了具有新颖的英法配对假新闻数据和可行性标签的LIAR-New数据集。

    Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
    
[^87]: 实现大型语言模型生成带引文的文本

    Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])

    [http://arxiv.org/abs/2305.14627](http://arxiv.org/abs/2305.14627)

    本文提出ALCE，是首个自动LLMs引文评估基准，实现大型语言模型生成带引文的文本，提高其事实正确性和可验证性；提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。

    

    大型语言模型（LLMs）已成为广泛使用的信息寻找工具，但生成的输出容易出现幻觉。本文旨在实现LLMs生成带引文的文本，提高其事实正确性和可验证性。我们提出了ALCE，这是首个自动LLMs引文评估基准。ALCE收集了各种问题和检索语料库，并要求建立端到端系统以检索支持证据并生成带有引文的答案。我们沿着流畅性、正确性和引文质量三个维度构建自动指标，并展示了它们与人类判断的强相关性。我们使用最先进的LLMs和新的提示策略进行实验，结果表明当前系统仍有相当大的提升空间--例如，提示LLMs特定的关键词或利用外部知识源可以显著提高其引文准确性。我们的工作为未来研究发展能够生成可验证和可信赖输出的LLMs提供了坚实基础。

    Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
    
[^88]: 为比较推理预训练语言模型

    Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])

    [http://arxiv.org/abs/2305.14457](http://arxiv.org/abs/2305.14457)

    本文提出一种预训练语言模型的新框架，旨在增强其在比较推理方面的能力。通过使用可扩展的基于文本实体比较数据的方法和新的预训练任务，该框架得到了显著的结果。

    

    本文提出了一种新框架，用于预训练语言模型以增强其在文本比较推理方面的能力。我们的方法涉及可扩展的用于收集基于文本实体比较数据的方法，并设计了三个新的预训练任务。在多个下游任务，包括比较问答、问句生成和摘要生成方面的评估表明，我们的预训练框架大大提高了语言模型的比较推理能力，尤其是在资源匮乏的情况下。此外，本工作还发布了第一个比较推理综合基准。

    In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While recent research has developed models for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach involves a scalable method for collecting data for text-based entity comparison, which leverages both structured and unstructured data, and the design of three novel pre-training tasks. Evaluation on a range of downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning over texts.
    
[^89]: 对于表示偏见下的多模态学习的反事实增强

    Counterfactual Augmentation for Multimodal Learning Under Presentation Bias. (arXiv:2305.14083v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14083](http://arxiv.org/abs/2305.14083)

    本文提出了一种用于纠正表示偏见的新颖方法，即反事实增强。实证评估表明，反事实增强相比于未修正的模型和现有的偏见校正方法，可以获得更好的下游性能。模型分析进一步指出，在理想情况下，生成的反事实与真实反事实密切相关。

    

    在现实世界的机器学习系统中，标签通常是从系统希望鼓励的用户行为中得出的。随着时间的推移，随着新的训练样本和特征的提供，需要训练新模型。然而，用户和模型之间的反馈循环可能导致未来用户行为的偏见，进而导致标签中的表示偏见，这损害了训练新模型的能力。在本文中，我们提出了一种新颖的因果方法，即反事实增强，通过生成的反事实标签来纠正表示偏见。我们的实证评估表明，相比未修正的模型和现有的偏见校正方法，反事实增强可以产生更好的下游性能。模型分析进一步表明，在理想情况下，生成的反事实与真实反事实密切相关。

    In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.
    
[^90]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^91]: ToolkenGPT：通过工具嵌入扩充冻结语言模型

    ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])

    [http://arxiv.org/abs/2305.11554](http://arxiv.org/abs/2305.11554)

    本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。

    

    将大型语言模型与外部工具结合起来解决复杂问题已成为一种有前途的方法。然而，传统方法需要用工具演示数据对LLM进行微调，既费时又受限于预定义的工具集。最近的上下文学习范例缓解了这些问题，但是有限的上下文长度只允许演示几次，导致对工具的理解不够充分。此外，当有大量工具可供选择时，上下文学习可能完全无法正常工作。在本文中，我们提出了一种$\textbf{ToolkenGPT}$的替代方法，将两种方法的优点结合起来。我们的方法将每个$\underline{工具}$表示为一个$\underline{token}$（$\textit{toolken}$），并为其学习一个嵌入，使得工具调用与生成常规单词标记的方式相同。一旦触发了toolken，LLM被提示完成工具执行所需的参数。ToolkenGPT提供了以下贡献：1）引入了toolken的概念，以扩充LLM与外部工具的交互，2）提出了一种新的学习范例，利用tool embeddings实现无缝交互，3）在各种下游任务上展示了我们方法的有效性。

    Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
    
[^92]: 自适应宽松优化用于强韧问答系统

    Adaptive loose optimization for robust question answering. (arXiv:2305.03971v1 [cs.CL])

    [http://arxiv.org/abs/2305.03971](http://arxiv.org/abs/2305.03971)

    本论文提出了一种简单而有效的自适应宽松优化损失函数，用于为问答系统综合内外分布的最佳表现，并显示了对对抗攻击的强韧性。

    

    问答方法以利用数据偏差为特点，如视觉问答中的语言先验和机器阅读理解（抽取式问答）中的位置偏差。目前的去偏方法往往以在分布内表现不佳为代价获得有利的分布外泛化能力，而不去偏方法则在获得高分布内表现的同时牺牲了相当数量的分布外表现。因此，它们难以应对复杂变化的现实世界情况。本文提出了一种简单而有效的新型自适应宽松优化损失函数，为问答系统综合两者最佳表现而努力。我们的主要技术贡献是根据小批量训练数据上先前和当前优化状态之间的比率自适应地减少损失。这种宽松优化可以用来防止非凸优化陷入局部最小值，并帮助模型学习更好的表示。实验证明，我们的方法在各种基准测试中与最先进的方法具有竞争性能，同时表现出对对抗性攻击的强韧性。

    Question answering methods are well-known for leveraging data bias, such as the language prior in visual question answering and the position bias in machine reading comprehension (extractive question answering). Current debiasing methods often come at the cost of significant in-distribution performance to achieve favorable out-of-distribution generalizability, while non-debiasing methods sacrifice a considerable amount of out-of-distribution performance in order to obtain high in-distribution performance. Therefore, it is challenging for them to deal with the complicated changing real-world situations. In this paper, we propose a simple yet effective novel loss function with adaptive loose optimization, which seeks to make the best of both worlds for question answering. Our main technical contribution is to reduce the loss adaptively according to the ratio between the previous and current optimization state on mini-batch training data. This loose optimization can be used to prevent non
    
[^93]: 无限长度输入的长距离Transformer-Unlimiformer

    Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])

    [http://arxiv.org/abs/2305.01625](http://arxiv.org/abs/2305.01625)

    Unlimiformer是一种Transformer模型的通用方法，可以将所有层的注意计算卸载到单个k近邻索引上，从而可处理无限长度的输入，而不增加额外的学习负担。

    

    基于Transformer的模型通常对输入长度有预定义的限制，因为它们可能需要参考输入中的每个标记。本文提出了一种通用方法-Unlimiformer，可以包装任何现有的预训练编码器-解码器Transformer，并将所有层的注意计算卸载到单个k近邻索引上。我们在几个长文档和多文档摘要基准测试中证明了Unlimiformer的有效性，展示了它可以总结350k令牌长的输入而不进行测试时的截断。

    Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned
    
[^94]: ChatGPT生成的代码真的正确吗？对大型语言模型在代码生成方面的严格评估

    Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])

    [http://arxiv.org/abs/2305.01210](http://arxiv.org/abs/2305.01210)

    本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。

    

    程序综合一直以来都是被长期研究的领域，最近的方法集中于直接利用大型语言模型(LLMs)根据自然语言中用户的意图生成代码。代码评估数据集，包含策划好的综合问题和各种输入/输出测试用例，被用来衡量各种LLMs在代码综合上的性能。然而，这些数据集中的测试用例在完全评估生成代码的功能正确性方面，数量和质量都可能有所限制。这种现有基准中的限制引出了以下问题：在LLMs时代，生成的代码真的正确吗？为了回答这个问题，我们提出了EvalPlus——一个评估LLM-synthesized代码功能正确性的严格基准评估框架。EvalPlus接受基础评估数据集，并利用自动输入生成步骤，使用LLM-based和基于变异的方法生成和多样化大量新的测试输入。

    Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
    
[^95]: 学习使用自注记进行推理和记忆

    Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.00833](http://arxiv.org/abs/2305.00833)

    该论文提出了一种学习使用自注记进行推理和记忆的方法，通过允许模型明确思考、记录自己的想法，并整合先前的推理步骤，从而提高了多步推理的能力。

    

    大型语言模型在多步推理方面表现不佳，且不能保留以供将来使用的先前推理步骤。我们提出了一种解决这两个问题的简单方法，即允许模型进行自注记。与最近的思维链或草稿本方法不同，该模型可以随时偏离输入上下文来明确思考和记录自己的想法。这使得模型可以在阅读上下文时即时推理，并整合先前的推理步骤，从而增强其记忆并进行多步推理。广泛的实验表明，通过使用交织输入文本的自注记方法，我们的方法可以胜过思维链和草稿本方法。

    Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.
    
[^96]: 控制文本生成中的关键词和位置

    Controlling keywords and their positions in text generation. (arXiv:2304.09516v1 [cs.CL])

    [http://arxiv.org/abs/2304.09516](http://arxiv.org/abs/2304.09516)

    本文研究了一种控制文本生成中关键词及其位置的新方法，通过使用特殊标记控制关键词相对位置，可以生成更符合用户意图的文本。

    

    文本生成中的挑战之一是按照用户意图控制生成结果。之前的研究提出了指定应包含在生成文本中的关键词的方法。然而，这还不足以生成反映用户意图的文本。例如，将重要关键词放在文本开头有助于吸引读者的注意力，但现有方法不允许这种灵活的控制。在本文中，我们解决了控制文本生成中关键词和其位置的新任务。为此，我们展示了一种使用特殊标记的方法，可以控制关键词的相对位置。摘要生成和故事生成实验结果表明，所提出的方法可以控制关键词及其位置。我们还证明，控制关键词位置可以生成比基线更符合用户意图的摘要文本。我们公开了我们的代码。

    One of the challenges in text generation is to control generation as intended by a user. Previous studies have proposed to specify the keywords that should be included in the generated text. However, this is insufficient to generate text which reflect the user intent. For example, placing the important keyword beginning of the text would helps attract the reader's attention, but existing methods do not enable such flexible control. In this paper, we tackle a novel task of controlling not only keywords but also the position of each keyword in the text generation. To this end, we show that a method using special tokens can control the relative position of keywords. Experimental results on summarization and story generation tasks show that the proposed method can control keywords and their positions. We also demonstrate that controlling the keyword positions can generate summary texts that are closer to the user's intent than baseline. We release our code.
    
[^97]: OpenAssistant Conversations -- 民主化大型语言模型的对齐方法

    OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])

    [http://arxiv.org/abs/2304.07327](http://arxiv.org/abs/2304.07327)

    释放了OpenAssistant Conversations，这是一个由全球超过1,000名参与者进行人工生成和人工注释的助手风格对话语料库，可以通过SFT和RLHF有效地用于LLM对齐，提高模型性能和可用性。

    

    对齐大型语言模型（LLM）与人类偏好的技术已被证明可以显著提高可用性并推动其快速应用，如ChatGPT所示。 监督微调（SFT）和根据人类反馈进行的强化学习（RLHF）等对齐技术大大降低了有效发挥LLM能力所需的技能和领域知识，提高了它们在各个领域的可访问性和实用性。 然而，像RLHF这样的最先进的对齐技术依赖于高质量的人类反馈数据，这些数据往往昂贵且保密。 为了民主化大规模对齐的研究，我们发布了OpenAssistant Conversations，这是一个由全球超过1,000名参与者进行人工生成和人工注释的助手风格对话语料库，包含161,443条消息，分布在66,497个对话树中，并在35种不同的语言中用461,292个质量评分进行注释。我们的实验表明，OpenAssistant Conversations可以通过SFT和RLHF有效地用于LLM对齐，从而提高模型性能和可用性。我们发布语料库，使更广泛的研究社区能够进一步研究民主化LLM的能力，从而改善人类交互。

    Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 1
    
[^98]: 从检索到生成：高效且有效的实体集扩展方法

    From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])

    [http://arxiv.org/abs/2304.03531](http://arxiv.org/abs/2304.03531)

    本文提出了GenExpan，一种基于生成式预训练语言模型的实体集扩展框架，利用前缀树保证实体生成的有效性，采用自动生成的类名来引导模型生成同一类实体，从而提高了效率和可扩展性。

    

    实体集扩展（ESE）是一项至关重要的任务，旨在扩展由小的种子实体集描述的目标语义类的实体。大多数现有的ESE方法是基于检索的框架，需要提取实体的上下文特征，并计算种子实体和候选实体之间的相似性。为了实现这两个目的，它们必须迭代地遍历语料库和数据集中提供的实体词汇，导致效率和可扩展性较差。实验结果表明，基于检索的ESE方法消耗的时间与实体词汇和语料库的大小成线性增长。本文首先提出了一种生成式ESE框架，Generative Entity Set Expansion (GenExpan)，它利用生成式预训练语言模型来完成ESE任务。具体而言，采用前缀树来保证实体生成的有效性，并采用自动生成的类名来引导模型生成同一类实体。

    Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
    
[^99]: 关于多语言神经机器翻译的Pareto前沿研究

    On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])

    [http://arxiv.org/abs/2304.03216](http://arxiv.org/abs/2304.03216)

    本研究针对多语言神经机器翻译的数据不平衡问题，提出双重幂律方法用于预测独特的性能权衡前沿，并建立基于该方法的样本比例选择优化问题，取得更好的结果。

    

    本研究探讨了在多语言神经机器翻译中，给定方向的泛化性能如何随其采样比例的变化而变化。通过训练200多个具有不同模型大小、方向和总任务数量的多语言模型，我们发现在训练语料库存在数据不平衡时，标量化导致了一个多任务权衡前沿，该前沿偏离了传统的Pareto前沿。基于我们的观察，我们提出了双重幂律来预测MNMT中独特的性能权衡前沿，该方法在各种语言、数据充足性和任务数量方面都很鲁棒。最后，我们将MNMT中的样本比例选择问题建模为基于双重幂律的优化问题，取得了更好的结果。

    In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
    
[^100]: 有效地对齐跨语言会话任务的提示调整跨语言转移学习

    Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])

    [http://arxiv.org/abs/2304.01295](http://arxiv.org/abs/2304.01295)

    本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。

    

    针对自然语言处理任务，跨语言转移的语言模型已被广泛研究，但是对于会话任务的研究相对较少。本文提出了XSGD，这是一个由Schema-Guided Dialogue（SGD）翻译成105种其他语言的平行大规模多语种会话数据集。为了实现对齐的跨语言表示方法，我们开发了一种有效的基于提示调整的方法来学习对齐提示。我们还研究了两种不同的分类器：NLI-based和vanilla分类器，并测试了对齐提示所实现的跨语言能力。我们在两个对话任务（插槽填充和意图分类）上评估了我们模型的跨语言泛化能力。

    Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
    
[^101]: GLEN：面向数千种类型的通用事件检测

    GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])

    [http://arxiv.org/abs/2303.09093](http://arxiv.org/abs/2303.09093)

    研究者建立了一个通用事件检测数据集GLEN，涵盖了超过3,465种不同的事件类型，利用现有的标注，他们提出了一种新的多阶段事件检测模型，展示了在大本体大小和部分标签的情况下，该模型具有优越的性能。

    

    事件抽取系统的发展一直受限于缺乏广泛覆盖、大规模数据集。为了使事件抽取系统更易于使用，我们建立了一个通用事件检测数据集GLEN，涵盖了3,465种不同的事件类型，本体比任何当前数据集都大20倍以上。GLEN利用DWD叠加技术创建，通过提供维基百科Qnode和PropBank角色集之间的映射，使用PropBank的现有标注作为间接监督来完成创建。此外，我们还提出了一种新的多阶段事件检测模型，专门设计用于处理GLEN的大本体大小和部分标签。我们展示了我们的模型表现出优越的性能（F1分数提高了约10%），与传统的分类基线和较新的基于定义的模型相比。最后，我们进行了错误分析，并显示标签噪声仍然是提高性能的最大挑战。

    The development of event extraction systems has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 3,465 different event types, making it over 20x larger in ontology than any current dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size and partial labels in GLEN. We show that our model exhibits superior performance (~10% F1 gain) compared to both conventional classification baselines and newer definition-based models. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance.
    
[^102]: 学习用于排名的列表级别领域不变表示

    Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.10764](http://arxiv.org/abs/2212.10764)

    本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。

    

    领域适应旨在将在（数据丰富）源领域学到的知识转移到（资源有限）目标领域，一种常用的方法是不变表示学习，它匹配并对齐特征空间上的数据分布。尽管这种方法在分类和回归问题上得到了广泛研究和应用，但在排名问题上的应用却是零散的，并且现有的几种实现缺乏理论上的证明。本文重新审视了用于排名的不变表示学习。在审查之前的工作时，我们发现他们实施了我们称之为项目级别对齐的方法，该方法在聚合的所有列表中对进行排名的项目分布进行对齐，但忽略了列表的结构。然而，列表的结构应该被利用，因为它是排名问题的固有特性，其中数据和度量是在列表上定义和计算的，而不是在项目本身上。为了解决这一不一致，我们提出了列表级别对齐的学习

    Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
    
[^103]: 无视觉基线的多模式语法归纳

    A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10564](http://arxiv.org/abs/2212.10564)

    本论文研究了在多模式设置下，只使用文本进行训练的大型语言模型（LLMs）是否能够提供强大的辅助来进行语法归纳。结果显示，基于LLM的纯文本方法在多种多模式数据集上优于先前的方法，并且在性能、参数数量和训练速度方面取得了最先进的结果。

    

    过去的研究表明，配对的视觉与语言信号能够显著改善多模式数据集（如MSCOCO）中的语法归纳。我们研究了只使用文本进行训练的大型语言模型（LLMs）在多模式设置下是否能够提供强大的辅助来进行语法归纳。我们发现，我们的纯文本方法，即基于LLM的C-PCFG（LC-PCFG），在各种多模式数据集上优于先前的多模式方法，并且获得了最先进的语法归纳性能。与带图像的语法归纳相比，LC-PCFG在语料库F1得分上超过了先前的最先进方法7.9个点，参数数量减少了85％，训练速度加快了1.7倍。在三个辅助视频的语法归纳基准中，LC-PCFG在语料库F1上优于先前的最先进方法最多7.7个点，训练速度加快了8.8倍。

    Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
    
[^104]: 基于概念器辅助的大型语言模型去偏见

    Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11087](http://arxiv.org/abs/2211.11087)

    本论文提出一种基于概念器的大型语言模型去偏见方法。我们通过后处理和一种新架构CI-BERT将概念器投影纳入所有层中。概念器后处理方法取得了最先进的去偏见结果，同时保持或改善了模型的性能。

    

    预训练的大型语言模型(LLMs)反映了它们训练语料库中固有的社会偏见。许多方法已被提出来减轻这个问题，但它们通常未能去偏见或者会牺牲模型的准确性。我们使用概念器——一种软投影方法——来识别和去除如BERT和GPT等LLMs中的偏见子空间。我们提出了两种应用概念器的方法：（1）通过后处理进行偏见子空间投影；（2）一种新的架构——概念器介入BERT(CI-BERT)，它在训练期间明确地将概念器投影纳入所有层中。我们发现，概念器后处理在保持或提高LLMs在GLUE基准测试中的性能的同时，实现了最先进的去偏见结果。此外，它在各种情况下都很稳健，并且可以通过对现有偏见子空间的逻辑操作来有效地减轻交集偏见。虽然CI-BERT的训练考虑了所有层的偏见，并且在某些任务上表现更好，但它的训练成本更高。

    Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
    
[^105]: EffEval:一种全面评估机器翻译评价指标效率的方法

    EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics. (arXiv:2209.09593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.09593](http://arxiv.org/abs/2209.09593)

    EffEval是一种对机器翻译评价指标效率进行全面评估的方法，其中TinyBERT在质量和效率之间提供了最佳平衡，CPU加速比GPU更显著，WMD近似没有提高效率但降低了质量，适配器提高了训练效率并在某些情况下提高了指标的质量。

    

    效率是促进包容性和减少环境成本的关键特性，特别是在LLM时代。在这项工作中，我们对机器翻译评价指标的效率进行了全面评估。我们的方法是用轻量级替代计算密集型的transformers，并在LLM表示之上采用线性和二次近似的对齐算法。我们评估了六个（无参考和有参考）指标在三个机器翻译数据集上，并检查了16个轻量级transformers。此外，我们通过使用适配器来研究COMET等指标的训练效率。我们的结果表明：（a）TinyBERT在质量和效率之间提供了最佳平衡，（b）CPU加速比GPU更显著，（c）WMD近似没有提高效率，但降低了质量，（d）适配器提高了训练效率（关于反向传播速度和内存要求），在某些情况下提高了指标的质量。

    Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and examine 16 lightweight transformers. In addition, we look into the training efficiency of metrics like COMET by utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal balance between quality and efficiency, (b) CPU speed-ups are more substantial than those on GPU; (c) WMD approximations yield no efficiency gains while reducing quality and (d) adapters enhance training efficiency (regarding backward pass speed and memory requirements) as well as, in some cases, metric qualit
    
[^106]: 词语和图像之间的联系有多直接？

    How direct is the link between words and images?. (arXiv:2206.15381v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.15381](http://arxiv.org/abs/2206.15381)

    该研究调查了词语和图像之间的联系，实验结果表明存在直接的联系。研究探讨了除了图像模拟外，受试者可能采用的其他策略，并分析了任务解决是否依赖于视觉信息。

    

    尽管当前的词嵌入模型取得了成功，但它们仍然缺乏与现实世界的联系。在这一研究中，Gunther等人提出了一项行为实验来探究词语和图像之间的关系。在实验中，参与者将被呈现一个目标名词和一对图像，其中一个由模型选择，另一个随机选择。参与者被要求选择与目标名词最匹配的图像。在大多数情况下，参与者更喜欢模型选择的图像。因此，Gunther等人得出了词语和具体体验之间存在直接联系的可能性。我们以他们的实验为出发点，探讨了以下问题。1.除了利用给定图像的视觉化模拟外，受试者可能还使用了哪些策略来解决这个任务？这个设置在多大程度上依赖于来自图像的视觉信息？是否可以使用纯文本表示来解决它？

    Current word embedding models despite their success, still suffer from their lack of grounding in the real world. In this line of research, Gunther et al. 2022 proposed a behavioral experiment to investigate the relationship between words and images. In their setup, participants were presented with a target noun and a pair of images, one chosen by their model and another chosen randomly. Participants were asked to select the image that best matched the target noun. In most cases, participants preferred the image selected by the model. Gunther et al., therefore, concluded the possibility of a direct link between words and embodied experience. We took their experiment as a point of departure and addressed the following questions. 1. Apart from utilizing visually embodied simulation of given images, what other strategies might subjects have used to solve this task? To what extent does this setup rely on visual information from images? Can it be solved using purely textual representations?
    
[^107]: 带有视觉的语言: 对基于感知的词语和句子嵌入的研究

    Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.08823](http://arxiv.org/abs/2206.08823)

    本研究通过提出一个简单但非常有效的计算基于预训练词嵌入的模型，有效平衡了文本表示和视觉感知之间的相互作用。

    

    将语言与视觉联系起来是一个积极研究的领域，旨在通过将来自视觉的感知知识融入到基于文本的表示中，构建符合认知的词语和句子表示。尽管在语言感知方面已经尝试了许多方法，但在文本表示和人类经验之间实现最优平衡仍然是一个开放的领域。一些常见问题是：视觉感知是否对抽象词语有优势，还是仅限于具体词语？在文本和视觉之间建立联系的最佳方法是什么？图像的感知知识对于获取高质量的嵌入是否有优势？借助当前机器学习和自然语言处理的进展，本研究提出了一个简单且非常有效的计算基于预训练词嵌入的模型，有效平衡了相互作用。

    Grounding language in vision is an active field of research seeking to construct cognitively plausible word and sentence representations by incorporating perceptual knowledge from vision into text-based representations. Despite many attempts at language grounding, achieving an optimal equilibrium between textual representations of the language and our embodied experiences remains an open field. Some common concerns are the following. Is visual grounding advantageous for abstract words, or is its effectiveness restricted to concrete words? What is the optimal way of bridging the gap between text and vision? To what extent is perceptual knowledge from images advantageous for acquiring high-quality embeddings? Leveraging the current advances in machine learning and natural language processing, the present study addresses these questions by proposing a simple yet very effective computational grounding model for pre-trained word embeddings. Our model effectively balances the interplay betwe
    
[^108]: 为自动标签而生：新目标函数的更快更好。

    Born for Auto-Tagging: Faster and better with new objective functions. (arXiv:2206.07264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07264](http://arxiv.org/abs/2206.07264)

    本论文提出了一种用于自动标签的新目标函数BAT，通过改进模型结构和学习率策略，在50个epochs内实现了更快、更好的收敛速度和F得分，为自动标记提供了更准确和高效的解决方案。

    

    关键词提取是文本挖掘的任务，应用于增加搜索引擎优化和广告的搜索量。在自动标记中实施，可以高效准确地对在线文章和照片进行大规模标记。BAT是为自动标记而发明的，它作为awoo的AI营销平台（AMP）提供服务。awoo AMP不仅提供定制的推荐系统服务，还提高了电子商务的转化率。BAT的优势在于其4层结构在50个epochs达到了最佳的F得分，收敛速度更快，效果更好。换句话说，它比其他模型表现更好，其他模型在100个epochs需要更深的层数。为了生成丰富和清晰的标签，awoo创建了新的目标函数，同时在保持与交叉熵相似的F1得分的同时增强了F2得分。为了保证F得分的更好表现，awoo改进了Transformer提出的学习率策略，增加了F1得分。

    Keyword extraction is a task of text mining. It is applied to increase search volume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass scale of online articles and photos efficiently and accurately. BAT is invented for auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP not only provides service as a customized recommender system but also increases the converting rate in E-commerce. The strength of BAT converges faster and better than other SOTA models, as its 4-layer structure achieves the best F scores at 50 epochs. In other words, it performs better than other models which require deeper layers at 100 epochs. To generate rich and clean tags, awoo creates new objective functions to maintain similar ${\rm F_1}$ scores with cross-entropy while enhancing ${\rm F_2}$ scores simultaneously. To assure the even better performance of F scores awoo revamps the learning rate strategy proposed by Transformer \cite{Transformer} to increase ${\rm F_1}$ 
    
[^109]: 跨语言调整上下文词表示对零翻译转移的影响

    The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.06457](http://arxiv.org/abs/2204.06457)

    本文研究了跨语言调整上下文词表示在多种语言和任务中的影响，结论是该方法对多语言NLI有益，同时对NER、XSR和跨语言QA也有改进，尤其对某些语言更为明显。而单语QA性能没有改善，有时甚至下降。

    

    大型多语言语言模型如mBERT或XLM-R可以在各种IR和NLP任务中实现零翻译跨语言转移。Cao等人提出了一种数据和计算高效的mBERT跨语言调整方法，利用小型平行语料库使不同语言中相关单词的嵌入相似。他们在五种欧洲语言的NLI中证明了其有效性。与此相反，我们在包含西班牙语、俄语、越南语和印地语在内的语言集上进行了实验，并扩展了他们的原始实现来适应新的任务（XSR、NER和QA）和额外的训练机制（持续学习）。我们的研究为四种语言重现了NLI的增益，在三种语言上显示出了改进的NER、XSR和跨语言QA结果（尽管某些跨语言QA的增益在统计上不显著），而单语QA性能从未提高，有时甚至下降。对相关和不相关上下文词嵌入的距离进行了分析。

    Large multilingual language models such as mBERT or XLM-R enable zero-shot cross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed a data- and compute-efficient method for cross-lingual adjustment of mBERT that uses a small parallel corpus to make embeddings of related words across languages similar to each other. They showed it to be effective in NLI for five European languages. In contrast we experiment with a typologically diverse set of languages (Spanish, Russian, Vietnamese, and Hindi) and extend their original implementations to new tasks (XSR, NER, and QA) and an additional training regime (continual learning). Our study reproduced gains in NLI for four languages, showed improved NER, XSR, and cross-lingual QA results in three languages (though some cross-lingual QA gains were not statistically significant), while mono-lingual QA performance never improved and sometimes degraded. Analysis of distances between contextualized embeddings of related and unrel
    
[^110]: BERT WEAVER：使用加权平均使生物医学领域基于Transformer的模型实现终身学习

    BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.10101](http://arxiv.org/abs/2202.10101)

    研究提出了WEAVER方法，它可以将旧知识融入到新模型中，以有效降低灾难性遗忘，并实现生物医学领域基于Transformer的模型的终身学习。

    

    最近在转移学习方面的发展推动了自然语言处理任务的进展。然而，性能取决于高质量的手动标注训练数据。尤其是在生物医学领域，已经表明一种训练语料库不足以学习能够在新数据上高效预测的通用模型。因此，最先进的模型需要具备终身学习的能力，以便在新数据可用时提高性能 - 而无需从头开始重新训练整个模型。我们提出WEAVER，一种简单但高效的后处理方法，将旧知识融入到新模型中，从而减少灾难性遗忘。我们展示了顺序应用WEAVER会产生类似于一次性使用所有数据进行联合训练的词嵌入分布，同时计算效率更高。由于没有数据共享的必要，因此所介绍的方法在数据隐私是一项关注的情况下也可以轻松应用。

    Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of re-training the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily app
    

