# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning](https://arxiv.org/abs/2402.15506) | AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。 |
| [^2] | [API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs](https://arxiv.org/abs/2402.15491) | 本文介绍了API-BLEND，一个用于训练和系统测试工具增强型LLMs的大型语料库，旨在解决获取涉及调用工具/API的训练和测试数据的挑战，并模拟真实场景的API任务。 |
| [^3] | [Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models](https://arxiv.org/abs/2402.15481) | 提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。 |
| [^4] | [Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization](https://arxiv.org/abs/2402.15473) | 提出了一种方法，在RLHF中利用领域知识来降低训练奖励模型所需的大量人类偏好注释数量。 |
| [^5] | [Repetition Improves Language Model Embeddings](https://arxiv.org/abs/2402.15449) | 回声嵌入方法通过重复输入来提取信息，解决了自回归模型无法包含后续令牌信息的限制，实验结果表明其能够最大程度充分利用高质量的语言模型进行嵌入。 |
| [^6] | [Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2402.15444) | 提出了自适应多模态融合和模态对抗训练（AdaMF-MAT）方法，以解决多模态知识图完成中存在的模态信息不平衡问题，发挥不平衡模态信息的力量。 |
| [^7] | [A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models](https://arxiv.org/abs/2402.15422) | 本研究探讨了使用大型语言模型基于医生笔记生成患者总结的潜力，通过严格的标记协议和医学专家标记实验发现，在无幻觉数据上进行微调能有效减少幻觉的生成，并保留相关信息。 |
| [^8] | [PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning](https://arxiv.org/abs/2402.15420) | 本文提出了一种在强化学习中利用零样本语言推理来划分偏好的方法，通过扩展查询信息并重新定义奖励学习目标，提高了样本效率。 |
| [^9] | [Faithful Temporal Question Answering over Heterogeneous Sources](https://arxiv.org/abs/2402.15400) | 提出了一个能跨异构来源进行操作的时态问答系统，通过强制执行时间约束以确保忠实回答，正确处理隐含问题，并以统一方式覆盖知识库、文本和网络表格。 |
| [^10] | [Explorations of Self-Repair in Language Models](https://arxiv.org/abs/2402.15390) | 自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。 |
| [^11] | [Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction](https://arxiv.org/abs/2402.15370) | 提出了一种双编码器模型（D2E2S），结合了BERT通道和增强型LSTM通道来最大化单词间的句法和语义关系，引入了异构特征交互模块用于捕获复杂互动和动态选择重要节点。 |
| [^12] | [NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data](https://arxiv.org/abs/2402.15343) | 利用LLM注释数据进行实体识别编码器预训练，创建了NuNER，一种专门用于命名实体识别任务的紧凑语言表示模型，可以在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。 |
| [^13] | [Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies](https://arxiv.org/abs/2402.15337) | 本研究通过使用LLMs探索概念空间维度，提出了一种新颖的实体排名方法，并分析其在感知和主观特征上的转移能力。 |
| [^14] | [GPTVQ: The Blessing of Dimensionality for LLM Quantization](https://arxiv.org/abs/2402.15319) | 通过增加量化维度，GPTVQ方法在大型语言模型的量化中取得了新的最优结果，不仅显著改善了大小与准确性的权衡，还提高了处理效率。 |
| [^15] | [ArabianGPT: Native Arabic GPT-based Large Language](https://arxiv.org/abs/2402.15313) | 提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。 |
| [^16] | [Counterfactual Generation with Identifiability Guarantees](https://arxiv.org/abs/2402.15309) | 反事实生成面临着配对数据稀缺和标注信息有限等挑战，现有方法依赖过度简化的假设，但复杂数据分布下这些假设可能不成立。 |
| [^17] | [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302) | 本研究探讨了大型语言模型（LLMs）对指令中心响应的容忍度，并提出了一个包含复杂查询的数据集，旨在揭示触发不道德响应的方法。 |
| [^18] | [Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models](https://arxiv.org/abs/2402.15301) | 利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。 |
| [^19] | [Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300) | CLIP相似性作为更强大和更稳健的幻觉指标，研究提出了CLIP引导解码（CGD）方法，在大型视觉-语言模型中有效减少对象幻觉。 |
| [^20] | [MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models](https://arxiv.org/abs/2402.15268) | MemoryPrompt方法通过引入辅助循环网络，将信息传递给语言模型，从而改进了预训练语言模型在上下文跟踪方面的性能，避免了灾难性遗忘现象。 |
| [^21] | [CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models](https://arxiv.org/abs/2402.15265) | 研究调查了用户如何定制代理人设以及其对互动质量、多样性和动态的影响，开发了CloChat界面以支持在LLMs中轻松准确定制代理人设 |
| [^22] | [DEEM: Dynamic Experienced Expert Modeling for Stance Detection](https://arxiv.org/abs/2402.15264) | 本文提出了一种Dynamic Experienced Expert Modeling（DEEM）方法，利用生成的经验专家使LLMs能够以半参数化方式进行推理，提高了在立场检测任务中的性能。 |
| [^23] | [Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues](https://arxiv.org/abs/2402.15248) | 通过使用少样本提示和Llama-2-70B增强MultiWOZ数据集，引入用户背景故事，有效解决面向任务的对话中的闲聊干扰问题，并能够同时承认用户背景故事并推动任务的进行。 |
| [^24] | [GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?](https://arxiv.org/abs/2402.15238) | GPT-HateCheck提出了一个框架，通过指导大型语言模型从头开始生成更多样化和现实的功能测试，以解决现有测试案例过于通用简单的问题。 |
| [^25] | [ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition](https://arxiv.org/abs/2402.15220) | ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。 |
| [^26] | [BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators](https://arxiv.org/abs/2402.15218) | 提出了一种黑匣子隐蔽提示攻击（BSPA），采用检索器模拟攻击，以提高图像生成器的安全性。 |
| [^27] | [Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models](https://arxiv.org/abs/2402.15202) | 提出一种通过实例级前缀在注意力空间中进行细粒度比较，从而实现大型语言模型的细粒度脱毒的方法。 |
| [^28] | [DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators](https://arxiv.org/abs/2402.15200) | DeMPT提出了解码增强的多阶段提示优化，使得LLMs更好地模拟和利用句间和句内上下文，从而更有效地适应上下文感知NMT。 |
| [^29] | [Biomedical Entity Linking as Multiple Choice Question Answering](https://arxiv.org/abs/2402.15189) | 提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。 |
| [^30] | [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180) | 提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。 |
| [^31] | [Advancing Parameter Efficiency in Fine-tuning via Representation Editing](https://arxiv.org/abs/2402.15179) | RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果 |
| [^32] | [Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models](https://arxiv.org/abs/2402.15162) | 分析了基于微调的摘要模型在处理知识冲突时的实体级事实适应性，并提出了一种反事实数据增强方法，实验结果表明该方法增强了事实适应性，同时保持了事实一致性。 |
| [^33] | [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159) | 本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。 |
| [^34] | [Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings](https://arxiv.org/abs/2402.15153) | 提出了一种新颖的Self-Adaptive Reconstruction Contrastive Sentence Embeddings（SARCSE）框架，利用自动编码器重建句子中的所有令牌，以帮助模型保留更多细粒度语义，并提出了一种自适应重建损失来缓解对令牌频率的偏见。 |
| [^35] | [Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing](https://arxiv.org/abs/2402.15151) | 提出了一个新颖的VSP-LLM框架，用于最大化上下文建模能力，实现视觉语音识别和翻译的多任务执行。 |
| [^36] | [Improving Sentence Embeddings with an Automatically Generated NLI Dataset](https://arxiv.org/abs/2402.15132) | 通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。 |
| [^37] | [Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models](https://arxiv.org/abs/2402.15131) | 提出了一种互动式KBQA框架，通过直接与知识库互动生成逻辑形式，开发了用于KB交互的通用API，并设计了示例来指导大型语言模型进行推理。 |
| [^38] | [Large Multimodal Agents: A Survey](https://arxiv.org/abs/2402.15116) | 大型语言模型驱动的多模态代理（LMAs）的系统审查，涵盖了开发组件、研究类型分类以及集体效能增强的合作框架等内容。 |
| [^39] | [A First Look at GPT Apps: Landscape and Vulnerability](https://arxiv.org/abs/2402.15105) | 通过大规模监测和分析GPT商店，开发了自动化工具来研究GPT应用程序中的漏洞和抄袭情况。 |
| [^40] | [AttributionBench: How Hard is Automatic Attribution Evaluation?](https://arxiv.org/abs/2402.15089) | AttributionBench是一个综合基准，揭示了自动归因评估的挑战，即使对于最先进的语言模型也只能达到80%的准确率。 |
| [^41] | [Hands-Free VR](https://arxiv.org/abs/2402.15083) | Hands-Free VR 是一种无需手部操作的虚拟现实系统，通过语音命令实现，具有英语口音鲁棒性，通过深度学习模型和大型语言模型实现对文本的转换和执行。 |
| [^42] | [PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2402.15082) | PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。 |
| [^43] | [Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition](https://arxiv.org/abs/2402.15080) | 提出了一种基于提示调整的参数高效框架，通过层级标签细化方法深度整合层级指导，用于解决多层次隐式话语关系识别中的问题 |
| [^44] | [Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions](https://arxiv.org/abs/2402.15062) | 提出了一种自我调整方法，利用大型语言模型增强回答未知问题的能力，包括拒绝回答并解释未知问题无法回答的原因。 |
| [^45] | [Fine-tuning Large Language Models for Domain-specific Machine Translation](https://arxiv.org/abs/2402.15061) | 提出了一种名为LlamaIT的基于提示的微调方法，用于领域特定机器翻译任务，解决了大型语言模型在领域特定机器翻译中遇到的挑战。 |
| [^46] | [ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval](https://arxiv.org/abs/2402.15059) | ColBERT-XM模型通过从单一高资源语言的数据中学习，实现了零-shot转移到广泛的语言，从而消除了对特定语言标记数据的需求。 |
| [^47] | [On the Multi-turn Instruction Following for Conversational Web Agents](https://arxiv.org/abs/2402.15057) | 提出了一个新任务——对话式网络导航，引入了一个名为MT-Mind2Web的特殊数据集，并提出了一个名为Self-MAP的框架，旨在解决大型语言模型在多轮指令跟踪中的长度和上下文依赖性问题。 |
| [^48] | [Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions](https://arxiv.org/abs/2402.15055) | 该研究探究了Transformer中注意力头和MLP之间的相互作用，并揭示了特定上下文下激活特定token预测的机制，从而阐明在LLMs中注意力如何促成依赖上下文的专门化处理。 |
| [^49] | [ToMBench: Benchmarking Theory of Mind in Large Language Models](https://arxiv.org/abs/2402.15052) | 提出了ToMBench框架，在大型语言模型中进行心灵理论性能评估，发现最先进的模型仍然落后于人类表现超过10%。 |
| [^50] | [Unlocking the Power of Large Language Models for Entity Alignment](https://arxiv.org/abs/2402.15048) | ChatEA是一个创新性框架，利用大型语言模型提高实体对齐准确性，通过引入KG-code翻译模块和两阶段EA策略来克服传统方法的局限性。 |
| [^51] | [CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean](https://arxiv.org/abs/2402.15046) | 提出了一个用于韩文方面级别情感分类的CARBD-Ko基准数据集，引入了双标极性以区分特定方面和方面不可知情感分类，并提出了采用Siamese网络的新方法来解决双标方面极性问题。 |
| [^52] | [KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models](https://arxiv.org/abs/2402.15043) | 该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估 |
| [^53] | [CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models](https://arxiv.org/abs/2402.15021) | 本文提出了一个框架，显著提高现有模型编码组合性语言的能力，在组合性基准上取得超过10% 的绝对改进，同时保持或提高了在标准对象识别和检索基准上的性能。 |
| [^54] | [Probabilistically-sound beam search with masked language models](https://arxiv.org/abs/2402.15020) | 提出了在掩码语言模型上进行束搜索的概率健壮方法，表明其在多个领域中优于传统方法。 |
| [^55] | [Unintended Impacts of LLM Alignment on Global Representation](https://arxiv.org/abs/2402.15018) | 对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。 |
| [^56] | [Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning](https://arxiv.org/abs/2402.15017) | 多任务微调的方法通过在基础模型上对相关任务进行微调，然后适应限制标签数的目标任务，能够降低目标任务中的误差，并提出了一种实用的任务选择算法。 |
| [^57] | [Ar-Spider: Text-to-SQL in Arabic](https://arxiv.org/abs/2402.15012) | 本文介绍了Ar-Spider，这是第一个阿拉伯跨领域文本到SQL数据集，为解决阿拉伯语言的独特性质所带来的模式语言和SQL结构挑战，引入了两个基线模型并测试了两个跨语言模型，取得了不错的性能。 |
| [^58] | [How Important Is Tokenization in French Medical Masked Language Models?](https://arxiv.org/abs/2402.15010) | 子词标记化成为自然语言处理领域的主流标准，但其成功因素，如不同任务和语言的最佳分割粒度、数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍不明确。 |
| [^59] | [CommVQA: Situating Visual Question Answering in Communicative Contexts](https://arxiv.org/abs/2402.15002) | CommVQA数据集将图像置于自然环境中，挑战了当前的VQA模型，结果表明为模型提供上下文信息能够提高性能。 |
| [^60] | [Divide-or-Conquer? Which Part Should You Distill Your LLM?](https://arxiv.org/abs/2402.15000) | 本文提出了一种将推理任务分解为问题分解阶段和问题解决阶段的策略，发现问题分解阶段相比问题解决更容易提炼为较小模型，并证实该策略胜过单阶段解决方案。 |
| [^61] | [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992) | 本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。 |
| [^62] | [Optimizing Language Models for Human Preferences is a Causal Inference Problem](https://arxiv.org/abs/2402.14979) | 本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。 |
| [^63] | [GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data](https://arxiv.org/abs/2402.14973) | 提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。 |
| [^64] | [MultiLS: A Multi-task Lexical Simplification Framework](https://arxiv.org/abs/2402.14972) | MultiLS是第一个允许创建多任务LS数据集的框架，提出了MultiLS-PT作为第一个使用该框架创建的数据集，展示了其在词汇简化相关任务中的潜力。 |
| [^65] | [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968) | 提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。 |
| [^66] | [Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning](https://arxiv.org/abs/2402.14963) | Mirror 提出了一种多视角自我反思方法，通过导航者和推理者之间的启发式交互，促进多样性而具有可靠性的推理轨迹发展，解决了大型语言模型在处理知识丰富问题上的困难。 |
| [^67] | [In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization](https://arxiv.org/abs/2402.14951) | 该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。 |
| [^68] | [Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach](https://arxiv.org/abs/2402.14948) | 提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法 |
| [^69] | [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905) | MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法 |
| [^70] | [Watermarking Makes Language Models Radioactive](https://arxiv.org/abs/2402.14904) | 本文研究了LLM生成文本的放射性，表明使用数字水印训练数据能更容易检测到，同时也展示了即使只有很少比例的水印训练文本，仍可以高置信度地检测出使用数字水印进行微调的情况。 |
| [^71] | [Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/abs/2402.14903) | 本研究探讨了在大型语言模型中对输入文本进行tokenization对数值推理的影响，发现采用从右到左的tokenization方式可显著提高算术任务的性能表现。 |
| [^72] | [A Usage-centric Take on Intent Understanding in E-Commerce](https://arxiv.org/abs/2402.14901) | 该论文提出了电子商务中意图理解的一个新视角，不依赖于产品本体，通过引入产品恢复基准验证了当前意图知识图的弱点。 |
| [^73] | [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897) | 了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。 |
| [^74] | [Data Augmentation is Dead, Long Live Data Augmentation](https://arxiv.org/abs/2402.14895) | 数据增强不过是更好地微调模型，零唁态和少样本数据生成可提高性能 |
| [^75] | [LLMBind: A Unified Modality-Task Integration Framework](https://arxiv.org/abs/2402.14891) | 提出了LLMBind，一种统一的模态任务集成框架，通过将大型语言模型和预训练任务模型绑定在一起，实现了多种模态任务的灵活输入和输出组合。 |
| [^76] | [Vygotsky Distance: Measure for Benchmark Task Similarity](https://arxiv.org/abs/2402.14890) | 论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。 |
| [^77] | [COBIAS: Contextual Reliability in Bias Assessment](https://arxiv.org/abs/2402.14889) | 我们提出了COBIAS，旨在通过考虑多样情境的用户输入内容，衡量语句的情境可靠性，从而培养偏见意识。 |
| [^78] | [Efficient data selection employing Semantic Similarity-based Graph Structures for model training](https://arxiv.org/abs/2402.14888) | 提出了一种基于语义相似性的图结构的高效数据选择机制，可在不经过计算密集型模型或其他密集的预处理转换的情况下，用于模型训练。 |
| [^79] | [A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating](https://arxiv.org/abs/2402.14881) | 研究揭示了基于ChatGPT的作弊对测试题的漏洞，并开发了一个工具来辨别测试题中对ChatGPT最容易回答错误的类型。 |
| [^80] | [Automatic Histograms: Leveraging Language Models for Text Dataset Exploration](https://arxiv.org/abs/2402.14880) | 该论文提出了一种利用语言模型的自动直方图可视化工具AutoHistograms，能够自动识别相关特征、以直方图形式展示并允许用户交互式地查询数据集，帮助数据工作者快速探索文本数据集。 |
| [^81] | [Driving Generative Agents With Their Personality](https://arxiv.org/abs/2402.14879) | 大型语言模型（LLMs）利用心理测量值，在视频游戏角色开发中代表给定的人格特征，增强游戏角色的类人特性。 |
| [^82] | [What's in a Name? Auditing Large Language Models for Race and Gender Bias](https://arxiv.org/abs/2402.14875) | 调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。 |
| [^83] | [Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation](https://arxiv.org/abs/2402.14874) | 该研究提出了一种叫做蒸馏对比解码（DCD）的方法，通过结合对比提示与蒸馏技术，有效提升了大型语言模型（LLM）在推理任务上的性能表现，超过了传统的对比解码方法，并在多个基准数据集上取得了显著成果。 |
| [^84] | [Technical Report on the Checkfor.ai AI-Generated Text Classifier](https://arxiv.org/abs/2402.14873) | Checkfor.ai AI生成文本分类器在区分大型语言模型生成文本和人类编写文本方面表现优异，提出了硬负挖掘与合成镜像训练算法，具有高准确性和泛化能力。 |
| [^85] | [Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs](https://arxiv.org/abs/2402.14872) | 本文提出了一种语义镜像越狱（SMJ）方法，通过生成在语义上类似于原始问题的越狱提示来绕过LLMs。 |
| [^86] | [LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain](https://arxiv.org/abs/2402.14871) | 该论文创新之处在于将LLMs与提示工程和多Agent系统相结合，以生成符合特定结构的新文档。 |
| [^87] | [Effects of term weighting approach with and without stop words removing on Arabic text classification](https://arxiv.org/abs/2402.14867) | 本研究比较不同的加权特征方法（二元和词频加权）在文本分类中使用和不使用停用词时的影响，通过评估准确性、召回率、精确度和F-度量值，结果表明停用词的处理方式对文本分类结果具有重要影响。 |
| [^88] | [APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2402.14866) | APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率 |
| [^89] | [DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents](https://arxiv.org/abs/2402.14865) | 本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。 |
| [^90] | [Evaluation of a semi-autonomous attentive listening system with takeover prompting](https://arxiv.org/abs/2402.14863) | 一种半自主系统，允许远程操作员在实时控制下接管自主专注听取系统，并通过自动检测低兴趣和参与度来为操作员提供明确的接管提示，相较于完全自主系统，该系统被普遍认为更积极。 |
| [^91] | [Ranking Large Language Models without Ground Truth](https://arxiv.org/abs/2402.14860) | 不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。 |
| [^92] | [ChatEL: Entity Linking with Chatbots](https://arxiv.org/abs/2402.14858) | ChatEL框架通过三步框架改进了实体链接任务的性能，使得平均F1性能提高超过2％ |
| [^93] | [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857) | 系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。 |
| [^94] | [Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning](https://arxiv.org/abs/2402.14856) | 该研究通过对大型语言模型对命题逻辑问题的响应进行评估，揭示出它们展现出与人类类似的推理模式和策略。 |
| [^95] | [An LLM Maturity Model for Reliable and Transparent Text-to-Query](https://arxiv.org/abs/2402.14855) | 这项工作提出了一种适用于文本到查询应用的LLM成熟度模型，不仅关注准确性，还扩展到更多维度。同时，展示了一个用于执法领域的实际案例，介绍了域特定文本到查询助手QueryIQ。 |
| [^96] | [A Dual-Prompting for Interpretable Mental Health Language Models](https://arxiv.org/abs/2402.14854) | 提出了一种双提示方法，结合专家身份和自杀词典与心理健康特定LLM相结合，有效提升了在心理健康分析中的解释性和帮助临床医生评估心理状态进展。 |
| [^97] | [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853) | 提出了NL2Formula任务，旨在通过自然语言查询生成基于电子表格表格的可执行公式，并提供了一个名为fCoder的基准实现。 |
| [^98] | [HumanEval on Latest GPT Models -- 2024](https://arxiv.org/abs/2402.14852) | 使用最新的GPT-4模型在程序合成方面取得显著进展，通过在HumanEval任务中展示了在零样本Python代码生成中的竞争性性能和更多多步骤范式综合。 |
| [^99] | [SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning](https://arxiv.org/abs/2402.14851) | 提出了SQL-CRAFT框架，通过交互式改进和增强推理，提升了大语言模型在文本到SQL转换任务中的性能，实验结果显示性能提升高达5.7%，并在Spider榜单上超越了当前最先进技术。 |
| [^100] | [CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management](https://arxiv.org/abs/2402.14850) | 本研究探讨了如何将大型语言模型应用于非安全关键的战略交通流量管理环境，提出了一个名为CHATATC的模型，通过训练大量历史数据集实现对话系统，并测试了其查询和响应能力。 |
| [^101] | [Asynchronous and Segmented Bidirectional Encoding for NMT](https://arxiv.org/abs/2402.14849) | 本文介绍了一种基于Transformer的改进模型，引入了异步和分段的双向编码策略，以提高神经机器翻译的效率和准确性。 |
| [^102] | [Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models](https://arxiv.org/abs/2402.14848) | 输入长度对大型语言模型的推理性能有显著影响，降级趋势出现在比技术最大值短得多的输入长度下。 |
| [^103] | [Stick to your Role! Stability of Personal Values Expressed in Large Language Models](https://arxiv.org/abs/2402.14846) | 本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。 |
| [^104] | [Purifying Large Language Models by Ensembling a Small Language Model](https://arxiv.org/abs/2402.14845) | 通过将大型语言模型与小型语言模型集成，可以有效净化大型语言模型，保持其性能并减轻版权侵权、数据污染和隐私侵犯等问题 |
| [^105] | [Text Diffusion with Reinforced Conditioning](https://arxiv.org/abs/2402.14843) | 提出了一种名为TREC的文本扩散模型，通过强化调节和时间感知方差缩放解决了现有文本扩散模型在训练过程中自我调节的退化和训练与采样不一致的问题，展示了其在不同序列生成任务中的竞争力。 |
| [^106] | [RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning](https://arxiv.org/abs/2402.14840) | 介绍了RJUA-MedDQA，一个医学专业领域的全面基准，具有挑战性的要求，涉及解释图像内容、数值推理和临床推理能力。 |
| [^107] | [RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts](https://arxiv.org/abs/2402.14838) | 该研究探究了语义和句法两个方面用于区分AI生成文本和人类撰写文本的问题，并提出了一个高准确度的AI模型，在M4数据集上表现出较好的性能。 |
| [^108] | [An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide](https://arxiv.org/abs/2402.14837) | 编制了一个全面的大型语言模型提示技术清单，并建立了一个跨学科的分类框架，以帮助从业者更有效地利用这些技术。 |
| [^109] | [Stealthy Attack on Large Language Model based Recommendation](https://arxiv.org/abs/2402.14836) | 大型语言模型推荐系统容易受到隐秘攻击，攻击者可以通过微调文本内容在不干预模型训练的情况下显著提高物品的曝光度，而这种攻击对整体推荐性能无影响且难以被检测到。 |
| [^110] | [MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing](https://arxiv.org/abs/2402.14835) | MIKE是一个针对细粒度多模态实体知识编辑的全面基准和数据集，突破了现有基准主要侧重于粗粒度知识的局限性，引入了新的知识编辑形式以评估编辑效率。 |
| [^111] | [MSynFD: Multi-hop Syntax aware Fake News Detection](https://arxiv.org/abs/2402.14834) | 提出一种新的多跳语法感知假新闻检测方法，通过引入补充的语法信息来处理假新闻中的微妙转折 |
| [^112] | [CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness](https://arxiv.org/abs/2402.14833) | CliqueParcel提出了一种通过提示批处理来提高LLM效率的方法，旨在在推理过程中同时确保准确性和最小化与原始输出的偏差，解决了折价输出问题。 |
| [^113] | [Orca-Math: Unlocking the potential of SLMs in Grade School Math](https://arxiv.org/abs/2402.14830) | 提出了一个基于Mistral-7B的70亿参数的Orca-Math小语言模型，旨在在小学数学中实现更高的准确度。 |
| [^114] | [An LLM-Enhanced Adversarial Editing System for Lexical Simplification](https://arxiv.org/abs/2402.14704) | 该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。 |
| [^115] | [ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models](https://arxiv.org/abs/2402.14660) | 介绍了ConceptMath，一种双语的细粒度基准测试，用于评估大型语言模型的概念性数学推理能力，并发现现有模型在不同数学概念上存在显著性能差异，甚至可能在最基本的概念上出现失败。 |
| [^116] | [OmniPred: Language Models as Universal Regressors](https://arxiv.org/abs/2402.14547) | 本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。 |
| [^117] | [Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis](https://arxiv.org/abs/2402.14484) | 本研究全面评估了ChatGPT的因果文本挖掘能力，发现ChatGPT在各种数据集上表现良好，但当有足够多的训练数据时，以往的模型仍然超越了它。 |
| [^118] | [Novi jezi\v{c}ki modeli za srpski jezik](https://arxiv.org/abs/2402.14379) | Rad predstavlja novi jezički model za srpski jezik zasnovan na transformerima, obučen na resursima Društva za jezičke resurse i tehnologije, koji će biti upoređen sa deset odabranih modela vektorizacije na četiri zadatka obrade prirodnog jezika. |
| [^119] | [Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News](https://arxiv.org/abs/2402.14224) | 本文提出了一个计算框架，旨在分析主流媒体在报道经济消息时的编辑选择，通过对经济指标的报道进行框架分析，我们可以理解出版物选择和构架的方式。 |
| [^120] | [Content Conditional Debiasing for Fair Text Embedding](https://arxiv.org/abs/2402.14208) | 通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。 |
| [^121] | [Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2402.13950) | 本文研究了大型语言模型的推理过程中的忠实性问题，引入了FRODO框架来改进生成推理步骤和坚固推理的方法 |
| [^122] | [CriticBench: Evaluating Large Language Models as Critic](https://arxiv.org/abs/2402.13764) | CriticBench是一个旨在全面和可靠地评估大型语言模型的评论能力的新型基准，展示了评论能力与任务、响应质量和模型规模之间的关系。 |
| [^123] | [Benchmarking Retrieval-Augmented Generation for Medicine](https://arxiv.org/abs/2402.13178) | 通过提出首个医学信息检索增强生成评估(MIRAGE)基准测试，并使用MedRAG工具包进行大规模实验，实现了对多个大型语言模型的准确性改进。 |
| [^124] | [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2402.13116) | 本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。 |
| [^125] | [Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models](https://arxiv.org/abs/2402.13035) | 通过精心设计训练数据和构建检查-校正数据集，本研究增强了大型语言模型的自我校正能力，提高了自我校正的准确性。 |
| [^126] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^127] | [How Susceptible are Large Language Models to Ideological Manipulation?](https://arxiv.org/abs/2402.11725) | 大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。 |
| [^128] | [Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection](https://arxiv.org/abs/2402.11621) | 通过研究GPT-3.5 Turbo、GPT-4和Flan-T5模型在识别新闻标题中框架偏见的性能，发现可解释提示能够显著提高这些模型的可靠性，GPT-4在少射场景中表现较好，而FLAN-T5的表现较差，指出较小模型可能需要更多任务特定微调。 |
| [^129] | [Chain of Logic: Rule-Based Reasoning with Large Language Models](https://arxiv.org/abs/2402.10400) | 介绍了一种新的提示方法，逻辑链，通过分解和重新组合来促进基于规则的推理，受到律师使用的序贯推理方法的启发。 |
| [^130] | [A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts](https://arxiv.org/abs/2402.09727) | ReadAgent是一个具有长期上下文概要记忆的阅读代理系统，通过实现一个简单的提示系统，它能够处理长输入并提高有效上下文长度。在评估中表现良好。 |
| [^131] | [Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications](https://arxiv.org/abs/2402.09015) | 本研究引入了AgentEval框架，用于评估LLM驱动应用的任务效用。该框架通过自动提出一套针对特定应用的评估标准，简化了效用验证过程，并对应用的效用进行了全面量化分析。 |
| [^132] | [Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style](https://arxiv.org/abs/2402.08498) | 这项研究提出了一个新的数据集，用于生成具有证据和风格的反驳，该数据集基于Reddit ChangeMyView数据集中的帖子，并可用于论证的改进和评估。评估结果显示，GPT-3.5 turbo模型在论证质量方面表现出色，并且具有很高的风格融合能力。互惠式反驳的效果最佳。 |
| [^133] | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。 |
| [^134] | [QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners](https://arxiv.org/abs/2402.07913) | 为解决编程智能教育系统中数据稀缺问题，本文提出了一个新的针对Python学习者的中文问答数据集，通过收集与分类真实学生问题，提高在线编程教育的效果和质量。 |
| [^135] | [Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations](https://arxiv.org/abs/2402.05629) | 该论文研究了长文生成中的事实性问题，发现大型语言模型在生成段落时可能会由于实体模糊而将可验证的事实组合成非事实的段落。现有的事实准确度评估方法无法正确评估这些非事实段落，作者提出了一种增强的度量指标来应对这个问题。 |
| [^136] | [KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion](https://arxiv.org/abs/2402.02389) | 本文提出了KICGPT，它是一个集成了大型语言模型和基于三元组的知识图谱补全检索器的框架。它通过知识提示的上下文学习策略，缓解了长尾问题，并且无需额外的训练开销。实验证明了其有效性。 |
| [^137] | [Evaluating Large Language Models in Analysing Classroom Dialogue](https://arxiv.org/abs/2402.02380) | 本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。 |
| [^138] | [Towards Efficient and Exact Optimization of Language Model Alignment](https://arxiv.org/abs/2402.00856) | 本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。 |
| [^139] | [PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM](https://arxiv.org/abs/2401.03855) | PythonSaga提出了一种新的基准，针对Python代码生成进行评估,弥补了现有基准存在的编程概念偏见和简单任务普遍性的问题 |
| [^140] | [Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person Name Recognition](https://arxiv.org/abs/2312.15304) | 本论文探索了ChatGPT在古代汉语翻译和人名识别方面的能力，并发现其在翻译方面的表现仍有待提高，最佳表现是在输入三个上下文句子时实现的。 |
| [^141] | [Structured Probabilistic Coding](https://arxiv.org/abs/2312.13933) | 结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。 |
| [^142] | [Mathematical Language Models: A Survey](https://arxiv.org/abs/2312.07622) | 该调查论文系统地概述了近年来在数学领域中利用语言模型取得的显著进展，包括对数学LLMs的分类和对超过60个数学数据集的编制，为数学LM领域未来的发展指明了方向。 |
| [^143] | [Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification](https://arxiv.org/abs/2312.06522) | 通过在文本情感分类中进行深入分析，发现标签平滑可以加速深度模型的收敛，并使不同标签的样本更容易区分 |
| [^144] | [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127) | 该研究引入了一种新颖的自动越狱方法RADIAL，通过放大LLMs生成肯定响应的潜力来绕过安全机制，实现了对英语恶意指令的优秀攻击性能。 |
| [^145] | [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective](https://arxiv.org/abs/2312.01957) | 本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。 |
| [^146] | [Disinformation Capabilities of Large Language Models](https://arxiv.org/abs/2311.08838) | 本文研究了当前一代大型语言模型在生成英语虚假新闻文章方面的能力，发现它们能够生成令人信服的支持危险虚假信息的新闻文章。 |
| [^147] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^148] | [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397) | 提出了一种无需LLM的多维基准AMBER，可用于评估MLLM的幻象，设计了低成本高效的评估流程，并对主流MLLMs进行了全面评估和分析 |
| [^149] | [Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback](https://arxiv.org/abs/2311.07215) | 开源代码LLMs难以生成正确指导的反馈，本研究提出了Coffee框架，旨在利用Coffee数据集构建CoffeePots，通过优化调整和选择，实现自动生成带有正确指导的反馈以用于代码修复。 |
| [^150] | [Exploring Memorization in Fine-tuned Language Models](https://arxiv.org/abs/2310.06714) | 在微调语言模型过程中，该研究首次全面分析了不同任务中模型的记忆现象，发现了记忆在各种微调任务中表现出显著的差异，并通过稀疏编码理论解释了这种任务差异性。 |
| [^151] | [Who Wrote this Code? Watermarking for Code Generation](https://arxiv.org/abs/2305.15060) | 基于代码独特的句法和语义特征，提出了一种新的水印方法SWEET，通过在具有高熵的位置仅放置“绿色”令牌来确保生成代码的正确性，并通过统计测试和Z分数进行检测。 |
| [^152] | [Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation](https://arxiv.org/abs/2305.14828) | 本文提出了一种针对文档图像中的少样本实体识别问题的图神经网络方法，通过引入标记之间的拓扑相邻关系，强调相对位置信息，实现对图像操作具有鲁棒性。 |
| [^153] | [Words that Matter: The Impact of Negative Words on News Sentiment and Stock Market Index](https://arxiv.org/abs/2304.00468) | 负面词汇对新闻情绪评分的负面性产生显著影响，增强的情绪词典更有效地捕捉新闻情绪对股市指数的影响。 |
| [^154] | [Can large language models build causal graphs?](https://arxiv.org/abs/2303.05279) | 大型语言模型被证明对于探测词、上下文和提示敏感，但可以作为一种工具辅助因果图的发展。 |
| [^155] | [Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study](https://arxiv.org/abs/2212.10233) | 通过深入的实证研究，本文研究了不依赖预训练的神经模型与整合预训练语言模型（PLMs）在关键词生成任务中的性能比较，揭示了PLMs具有具有竞争力的高资源性能和最先进的低资源性能，并探讨了不同设计选择对基于PLM模型性能的影响。 |
| [^156] | [Medical Knowledge Graph QA for Drug-Drug Interaction Prediction based on Multi-hop Machine Reading Comprehension](https://arxiv.org/abs/2212.09400) | 该论文提出了一种医学知识图问答模型MedKGQA，利用机器阅读理解和开放域文档构建药物-蛋白质三元组知识图，通过向量化靶点属性和建立有向连接预测药物相互作用，取得4.5%的预测准确性改进。 |
| [^157] | [Generating Unsupervised Abstractive Explanations for Rumour Verification.](http://arxiv.org/abs/2401.12713) | 该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。 |
| [^158] | [Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation.](http://arxiv.org/abs/2401.06477) | Kun是一种使用指令反向翻译和答案优化的方法，用于创建高质量的指导调整数据集，该方法不依赖于手动注释，通过自我筛选过程来改善和选择最有效的指令-输出对。它的主要创新在于通过算法改进提高数据的保留和清晰度，并通过创新的数据生成方法减少了手动注释的依赖。 |
| [^159] | [Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation.](http://arxiv.org/abs/2401.06310) | 本论文提出了一种多方面的方法，利用现有的文本资源，基于135个全球范围内的身份群体对文本到图像生成（T2I）模型生成的图像中的地理文化刻板印象进行评估。研究结果表明，刻板属性在图像中的存在可能性是刻板属性的三倍。 |
| [^160] | [EpiK-Eval: Evaluation for Language Models as Epistemic Models.](http://arxiv.org/abs/2310.15372) | 这项研究介绍了一种新的评估方法EpiK-Eval，旨在评估大型语言模型（LLMs）在从分割的叙述中构建连贯和一致的知识表示方面的能力。研究发现当前的训练目标存在固有的缺陷，因此提出了改进知识整合方法的建议，以大幅提高LLMs的整体效果和性能。 |
| [^161] | [MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use.](http://arxiv.org/abs/2310.03128) | 本文提出了一个名为MetaTool的基准，旨在评估大型语言模型（LLMs）是否具有工具使用意识并且能够正确选择工具。基准中包含一个名为ToolE的数据集，其中包含各种类型的用户查询，用于触发LLMs使用工具。 |
| [^162] | [The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature.](http://arxiv.org/abs/2309.04198) | 该研究介绍了CALLA数据集，用于探索LLMs从中文医学文献中获取交互式知识。通过自由对话事实核查任务，评估了LLMs掌握医学知识的能力，并发现了一种称为“事实跟随响应”的现象。为了提供更准确的评估方法，人工构建了两种角度的测试数据：一种与事实一致，一种与事实不一致。 |
| [^163] | [ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer.](http://arxiv.org/abs/2308.15459) | ParaGuide是一种用于通用风格转移的引导性扩散改写器，可以灵活适应任意目标风格，通过梯度引导和改写条件的扩散模型实现文本的风格转变，同时保留语义信息。 |
| [^164] | [The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection.](http://arxiv.org/abs/2308.12215) | 本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。 |
| [^165] | [ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.04695) | 本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。 |
| [^166] | [Large Language Models as Counterfactual Generator: Strengths and Weaknesses.](http://arxiv.org/abs/2305.14791) | 本文研究了大型语言模型（LLMs）作为反事实生成器的能力，通过数据增强实验发现它们在各个任务中表现优异，但仍存在自我限制和缺乏逻辑指导等问题。 |
| [^167] | [Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation.](http://arxiv.org/abs/2305.09651) | 本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。 |
| [^168] | [Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts.](http://arxiv.org/abs/2305.03237) | 本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。 |
| [^169] | [Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.](http://arxiv.org/abs/2304.08177) | 这篇论文提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以改善LLaMA对中文的理解和生成能力。 |
| [^170] | [Training Language Models with Language Feedback at Scale.](http://arxiv.org/abs/2303.16755) | 本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。 |
| [^171] | [Improving Code Generation by Training with Natural Language Feedback.](http://arxiv.org/abs/2303.16749) | 该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。 |
| [^172] | [On the Inconsistencies of Conditionals Learned by Masked Language Models.](http://arxiv.org/abs/2301.00068) | 本论文研究发现，遮蔽语言模型学习的条件句往往存在着不一致性，无法从一个连贯的联合分布中推导出来。我们通过实证发现这种不一致性普遍存在于不同尺寸和配置的遮蔽语言模型中。为了解决这个问题，我们提出了条件句集合方法来在推断阶段处理不一致性。 |

# 详细

[^1]: AgentOhana：为有效智能体学习设计统一数据和训练流水线

    AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning

    [https://arxiv.org/abs/2402.15506](https://arxiv.org/abs/2402.15506)

    AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。

    

    由大型语言模型（LLMs）提供支持的自主智能体引起了重大研究关注。然而，充分利用LLMs的潜力进行基于智能体的任务面临困难，这是由于具有多轮轨迹的多样化数据源的异构性。在本文中，我们介绍AgentOhana作为解决这些挑战的综合解决方案。AgentOhana从不同环境中聚合智能体轨迹，涵盖了各种情景。它精心地将这些轨迹标准化和统一到一致的格式中，简化了为智能体训练优化的通用数据加载器的创建。通过数据统一，我们的训练流水线在不同数据源之间保持平衡，并在数据集划分和模型训练过程中保持设备之间的独立随机性。此外，我们还介绍了xLAM-v0.1，一个大动作模式

    arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
    
[^2]: API-BLEND：用于训练和基准测试API LLM的综合语料库

    API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs

    [https://arxiv.org/abs/2402.15491](https://arxiv.org/abs/2402.15491)

    本文介绍了API-BLEND，一个用于训练和系统测试工具增强型LLMs的大型语料库，旨在解决获取涉及调用工具/API的训练和测试数据的挑战，并模拟真实场景的API任务。

    

    随着对大型语言模型（LLMs）有效使用工具和外部应用程序编程接口（APIs）来规划和完成任务的需求日益增长，对可以获取涉及调用工具/API的足够数量的训练和测试数据的方法引起了极大关注。本文关注识别、整理和转化现有数据集，并介绍API-BLEND，一个用于训练和系统测试工具增强型LLMs的大型语料库。这些数据集模拟涉及API任务的真实场景，如API/工具检测、槽填充以及检测到的API的排序。

    arXiv:2402.15491v1 Announce Type: cross  Abstract: There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrat
    
[^3]: 偏见和反复无常：衡量大型语言模型社会歧视的统计框架

    Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models

    [https://arxiv.org/abs/2402.15481](https://arxiv.org/abs/2402.15481)

    提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。

    

    arXiv:2402.15481v1 公告类型: 新的 摘要: 大型语言模型（LLMs）在社会运营中的日益融合加剧了它们对经济、法律、教育和医疗等重要领域决策的影响，引发了公众对这些模型涉及歧视安全和可靠性的担忧。然而，先前的歧视测量框架仅评估LLMs的平均歧视行为，往往由于忽视了一个额外的导致歧视的因素，即LLMs在不同上下文中的预测变化而变得不足。在这项工作中，我们提出了Prejudice-Caprice Framework（PCF），通过考虑LLMs的一贯偏见偏好和在多样上

    arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
    
[^4]: 利用领域知识在RLHF中高效建模奖励：电子商务意见摘要的案例研究

    Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization

    [https://arxiv.org/abs/2402.15473](https://arxiv.org/abs/2402.15473)

    提出了一种方法，在RLHF中利用领域知识来降低训练奖励模型所需的大量人类偏好注释数量。

    

    从人类反馈中进行强化学习（RLHF）已成为引导语言模型（LMs）朝向人类价值/目标的主导策略。该策略的关键在于使用一个能够反映与人类相关的潜在奖励模型的奖励模型（{$\varphi$}）。虽然这一策略已被证明是有效的，但训练方法需要大量人类偏好注释（通常数量级为数万）来训练{$\varphi$}。如果奖励模型可以被普遍使用，这种大规模偏好注释是可以实现的。然而，人类价值/目标是主观的，并且取决于任务的性质。这对于收集下游应用程序的多样化偏好构成挑战。为了解决这个问题，我们提出了一种新颖的方法，将领域知识融入{$\varphi$}中，从而减少所需注释的大小。我们在电子商务意见摘要中验证了我们的方法，具有显著的

    arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant
    
[^5]: 重复改善语言模型嵌入

    Repetition Improves Language Model Embeddings

    [https://arxiv.org/abs/2402.15449](https://arxiv.org/abs/2402.15449)

    回声嵌入方法通过重复输入来提取信息，解决了自回归模型无法包含后续令牌信息的限制，实验结果表明其能够最大程度充分利用高质量的语言模型进行嵌入。

    

    最近改进从自回归大型语言模型（LLMs）中提取文本嵌入的方法主要集中在改进数据、骨干预训练语言模型或通过指令改进任务差异化上。在这项工作中，我们解决了自回归模型的一个架构限制：令牌嵌入不能包含来自输入中后续令牌的信息。为了解决这一限制，我们提出了一种简单的方法，“回声嵌入”，其中我们在上下文中将输入重复两次，并从第二次出现中提取嵌入。我们展示了早期令牌的回声嵌入可以编码关于后续令牌的信息，从而使我们能够最大程度地利用高质量的LLMs进行嵌入。在MTEB排行榜上，回声嵌入在零射击中比经典嵌入提高了超过9%，在微调时提高了约0.7%。使用Mistral-7B模型的回声嵌入实现了与当前最先进模型的比较。

    arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared
    
[^6]: 发挥不平衡模态信息在多模态知识图完成中的力量

    Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion

    [https://arxiv.org/abs/2402.15444](https://arxiv.org/abs/2402.15444)

    提出了自适应多模态融合和模态对抗训练（AdaMF-MAT）方法，以解决多模态知识图完成中存在的模态信息不平衡问题，发挥不平衡模态信息的力量。

    

    多模态知识图完成（MMKGC）旨在通过将实体的结构、视觉和文本信息纳入判别模型来预测多模态知识图中缺失的三元组。来自不同模态的信息将共同工作以衡量三元组的可能性。现有的MMKGC方法忽视了实体之间模态信息不平衡的问题，导致模态融合不足以及对原始模态信息的低效利用。为解决上述问题，我们提出了自适应多模态融合和模态对抗训练（AdaMF-MAT），以发挥不平衡模态信息在MMKGC中的力量。AdaMF-MAT通过自适应模态权重实现多模态融合，并通过模态对抗训练生成对抗样本，以增强不平衡模态信息。我们的方法是MMKGC模型和训练的协同设计。

    arXiv:2402.15444v1 Announce Type: new  Abstract: Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training s
    
[^7]: 用大型语言模型生成忠实且高质量的病人总结的数据中心方法

    A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models

    [https://arxiv.org/abs/2402.15422](https://arxiv.org/abs/2402.15422)

    本研究探讨了使用大型语言模型基于医生笔记生成患者总结的潜力，通过严格的标记协议和医学专家标记实验发现，在无幻觉数据上进行微调能有效减少幻觉的生成，并保留相关信息。

    

    患者经常面临难以理解其住院情况的困难，而医护人员资源有限以提供解释。在这项工作中，我们研究了大型语言模型基于医生笔记生成患者总结的潜力，并研究了训练数据对生成总结的忠实性和质量的影响。为此，我们开发了严格的标记协议用于幻觉，让两位医学专家标记了100个真实总结和100个生成的总结。我们展示了在无幻觉数据进行微调可以有效地减少Llama 2每个总结的幻觉从2.60降低到1.55，同时保留相关信息。虽然效果仍然存在，但当使用五个例子提示GPT-4时，该效果要小得多（0.70降至0.40）。我们还对无幻觉和改进的训练数据进行了定性评估。即使在幻觉自由数据下，GPT-4也展现出非常好的结果。

    arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
    
[^8]: PREDILECT：在强化学习中利用零样本语言推理划分偏好

    PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning

    [https://arxiv.org/abs/2402.15420](https://arxiv.org/abs/2402.15420)

    本文提出了一种在强化学习中利用零样本语言推理来划分偏好的方法，通过扩展查询信息并重新定义奖励学习目标，提高了样本效率。

    

    基于偏好的强化学习已经成为机器人学习中的一个新领域，在这个领域中，人类通过对不同状态-动作序列表达偏好来塑造机器人行为。然而，为机器人制定现实政策需要人类对大量查询的响应。本工作通过扩展每个查询收集的信息，包含偏好和可选文本提示，来解决样本效率挑战。为了实现这一目标，我们利用大型语言模型(LLM)的零样本能力来从人类提供的文本中进行推理。为了适应额外的查询信息，我们重新定义了奖励学习目标，包含灵活的重点 —— 包含相对高信息量且与零射样本传递的特征相关的状态-动作对。在仿真场景和实际场景中，我们展示了我们方法的有效性。

    arXiv:2402.15420v1 Announce Type: cross  Abstract: Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a u
    
[^9]: 跨异构来源的忠实时态问答

    Faithful Temporal Question Answering over Heterogeneous Sources

    [https://arxiv.org/abs/2402.15400](https://arxiv.org/abs/2402.15400)

    提出了一个能跨异构来源进行操作的时态问答系统，通过强制执行时间约束以确保忠实回答，正确处理隐含问题，并以统一方式覆盖知识库、文本和网络表格。

    

    时态问答涉及时间约束，例如“...在2019年”或“...在COVID之前”。在前者中，时间是一个明确的条件，在后者中，它是隐含的。现有技术方法在三个方面存在局限性。首先，使用神经推理时，时间约束仅被软匹配，容易导致无效或无法解释的答案。其次，对于涉及隐含时间的问题支持不足。第三，答案只来自单一来源：知识库（KB）或文本语料库。我们提出了一个时态问答系统来解决这些缺陷。首先，它通过具体证据强制执行时间约束以确保忠实回答。其次，它正确处理隐含问题。第三，它以统一方式覆盖知识库、文本和网络表格，跨异构来源进行操作。该方法分为三个阶段：（i）理解问题及其时间条件，（ii）从中检索证据

    arXiv:2402.15400v1 Announce Type: cross  Abstract: Temporal question answering (QA) involves time constraints, with phrases such as "... in 2019" or "... before COVID". In the former, time is an explicit condition, in the latter it is implicit. State-of-the-art methods have limitations along three dimensions. First, with neural inference, time constraints are merely soft-matched, giving room to invalid or inexplicable answers. Second, questions with implicit time are poorly supported. Third, answers come from a single source: either a knowledge base (KB) or a text corpus. We propose a temporal QA system that addresses these shortcomings. First, it enforces temporal constraints for faithful answering with tangible evidence. Second, it properly handles implicit questions. Third, it operates over heterogeneous sources, covering KB, text and web tables in a unified manner. The method has three stages: (i) understanding the question and its temporal conditions, (ii) retrieving evidence from
    
[^10]: 在语言模型中自修复的探索

    Explorations of Self-Repair in Language Models

    [https://arxiv.org/abs/2402.15390](https://arxiv.org/abs/2402.15390)

    自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。

    

    先前研究狭窄分布的可解释性发现了自修复现象，即如果剥离大型语言模型中的组件，后续组件会改变其行为以进行补偿。我们的工作基于这些过去的文献，展示了当在完整的训练分布上剥离单个注意力头时，自修复存在于各种模型家族和尺寸上。我们进一步表明，在完整的训练分布上，自修复是不完美的，因为头部的原始直接效果并未完全恢复，并且是嘈杂的，因为自修复程度在不同提示之间显著变化（有时超过原始效果）。我们强调了促成自修复的两种不同机制，包括最终LayerNorm缩放因子的变化（可修复直接效果的30%）以及实现反擦除的稀疏神经元集。

    arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
    
[^11]: 双编码器：利用句法和语义潜力进行方面情感三元组提取

    Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction

    [https://arxiv.org/abs/2402.15370](https://arxiv.org/abs/2402.15370)

    提出了一种双编码器模型（D2E2S），结合了BERT通道和增强型LSTM通道来最大化单词间的句法和语义关系，引入了异构特征交互模块用于捕获复杂互动和动态选择重要节点。

    

    方面情感三元组提取（ASTE）是精细情感分析中的一个新兴任务。最近的研究使用图神经网络（GNN）来建模三元组元素固有的句法-语义关系。然而，他们尚未充分发挥ASTE任务中句法和语义信息的巨大潜力。在这项工作中，我们提出了一种\emph{双编码器：利用句法和语义潜力}模型（D2E2S），最大化单词间的句法和语义关系。具体而言，我们的模型利用双通道编码器，其中包括一个BERT通道来捕捉语义信息，以及一个增强型LSTM通道用于全面捕捉句法信息。随后，我们介绍了异构特征交互模块，以捕获依赖句法与注意力语义之间的复杂互动，并动态选择重要节点。我们利用这些模块的协同作用

    arXiv:2402.15370v1 Announce Type: cross  Abstract: Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these m
    
[^12]: NuNER: 利用LLM注释数据进行实体识别编码器预训练

    NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data

    [https://arxiv.org/abs/2402.15343](https://arxiv.org/abs/2402.15343)

    利用LLM注释数据进行实体识别编码器预训练，创建了NuNER，一种专门用于命名实体识别任务的紧凑语言表示模型，可以在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。

    

    大型语言模型（LLMs）展现出在数据标注方面令人印象深刻的能力，为解决经典的自然语言处理问题提供了新的途径。本文展示了如何利用LLMs创建NuNER，这是一个专门针对命名实体识别（NER）任务的紧凑语言表示模型。NuNER可以被微调以以高效的方式解决下游的NER问题，在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。我们发现预训练数据集的大小和实体类型的多样性是取得良好性能的关键。我们将NuNER视为最近被LLMs解锁的更广泛的特定任务基础模型家族的一员。

    arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.
    
[^13]: 使用LLMs沿着概念空间维度对实体进行排名：微调策略分析

    Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies

    [https://arxiv.org/abs/2402.15337](https://arxiv.org/abs/2402.15337)

    本研究通过使用LLMs探索概念空间维度，提出了一种新颖的实体排名方法，并分析其在感知和主观特征上的转移能力。

    

    概念空间以实体的原始语义特征表示。这种表示非常有价值，但学习起来非常困难，特别是在建模感知和主观特征时。从大型语言模型（LLMs）中提炼概念空间最近出现为一种有前途的策略。然而，现有工作仅限于使用相对简单的零样本策略探查预训练的LLMs。我们特别关注根据给定的概念空间维度对实体进行排名的任务。不幸的是，由于概念空间维度的真实排名很少见，我们无法直接在这个任务上微调LLMs。因此，我们使用更容易获得的特征作为训练数据，并分析由此产生的模型的排名能力是否能转移到感知和主观特征。我们发现在某种程度上确实是这种情况，但是未完成的句子。

    arXiv:2402.15337v1 Announce Type: new  Abstract: Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but havi
    
[^14]: GPTVQ：LLM量化中维度的福音

    GPTVQ: The Blessing of Dimensionality for LLM Quantization

    [https://arxiv.org/abs/2402.15319](https://arxiv.org/abs/2402.15319)

    通过增加量化维度，GPTVQ方法在大型语言模型的量化中取得了新的最优结果，不仅显著改善了大小与准确性的权衡，还提高了处理效率。

    

    在这项工作中，我们展示了通过增加量化维度可以显著改善神经网络量化的大小与准确性权衡。我们提出了GPTVQ方法，这是一种新的快速后训练向量量化（VQ）方法，适用于大型语言模型（LLMs）。我们的方法交替进行一个或多个列的量化，并使用来自每层输出重建MSE的Hessian信息来更新其余未量化的权重。量化码书使用一种高效的数据感知版本的EM算法进行初始化。然后，通过使用整数量化和基于SVD的压缩进一步压缩码书。GPTVQ在诸如Llama-v2和Mistral等各种LLMs上建立了新的最新技术，大小与准确性之间的权衡。此外，我们的方法高效：在单个H100上，处理一个Llamav2-70B需要3至11小时。

    arXiv:2402.15319v1 Announce Type: cross  Abstract: In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B
    
[^15]: ArabianGPT：基于原生阿拉伯语的大型语言模型

    ArabianGPT: Native Arabic GPT-based Large Language

    [https://arxiv.org/abs/2402.15313](https://arxiv.org/abs/2402.15313)

    提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。

    

    英语和拉丁语为主导的大型语言模型（LLMs）的主导地位导致了本土阿拉伯语LLMs的显著不足。本文提出ArabianGPT，这是一系列基于Transformer的模型，专门为阿拉伯语设计而成。这些模型包括ArabianGPT-0.1B和ArabianGPT-0.3B，大小和复杂性不同，与阿拉伯语的微妙语言特征相契合。

    arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
    
[^16]: 具有可识别性保证的反事实生成

    Counterfactual Generation with Identifiability Guarantees

    [https://arxiv.org/abs/2402.15309](https://arxiv.org/abs/2402.15309)

    反事实生成面临着配对数据稀缺和标注信息有限等挑战，现有方法依赖过度简化的假设，但复杂数据分布下这些假设可能不成立。

    

    反事实生成是各种机器学习任务的核心，包括图像转换和可控文本生成。这一生成过程通常需要识别潜在的分解表征，如内容和风格，这些表征潜在地支撑着观察到的数据。然而，当面临配对数据和标注信息的稀缺时，情况变得更具挑战性。现有的分解方法关键依赖于过度简化的假设，比如假设内容和风格变量独立，以识别潜在变量，尽管这样的假设可能并不适用于复杂的数据分布。例如，食物评论往往涉及“美味”等词语，而电影评论通常含有“惊心动魄”等词语表示同样的积极情感。当数据从多个领域采样时，由于内容和风格之间的相互关系可能会显著变化，这个问题会变得更加严重。

    arXiv:2402.15309v1 Announce Type: cross  Abstract: Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation. This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data. However, it becomes more challenging when faced with a scarcity of paired data and labeling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly
    
[^17]: 有关LLMs指令中心响应的（不道德）程度有多高？揭示安全防护栏对有害查询的漏洞

    How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries

    [https://arxiv.org/abs/2402.15302](https://arxiv.org/abs/2402.15302)

    本研究探讨了大型语言模型（LLMs）对指令中心响应的容忍度，并提出了一个包含复杂查询的数据集，旨在揭示触发不道德响应的方法。

    

    在这项研究中，我们解决了一个围绕大型语言模型（LLMs）安全和道德使用日益关注的问题。尽管这些模型具有潜力，但它们可能会被各种复杂的方法欺骗，产生有害或不道德内容，包括“越狱”技术和有针对性的操纵。我们的工作集中在一个特定问题上：LLMs在要求它们生成以伪代码、程序或软件片段为中心的响应时，有多大程度上可能会被误导，而不是生成普通文本。为了调查这个问题，我们引入了TechHazardQA，一个数据集，其中包含应以文本和以指令为中心格式（例如伪代码）回答的复杂查询，旨在识别不道德响应的触发器。我们查询了一系列LLMs-- Llama-2-13b，Llama-2-7b，Mistral-V2和Mistral 8X7B--并要求它们生成文本和指令为中心的响应。为了评估我们的方法，

    arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
    
[^18]: 基于大型语言模型的检索增强生成的因果图发现

    Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models

    [https://arxiv.org/abs/2402.15301](https://arxiv.org/abs/2402.15301)

    利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。

    

    因果图恢复在因果推断领域至关重要。传统方法通常是基于知识或统计估计，受数据收集偏见和个体关于影响变量之间关系的知识的限制。大型语言模型（LLMs）的进步为解决这些问题提供了机会。我们提出了一种利用大量科学文献中所包含的知识推导一般因果图恢复任务中的因果关系的新方法。该方法利用基于检索增强生成（RAG）的LLMs系统地分析和提取来自广泛研究论文集的相关信息。我们的方法首先从汇总的文献中检索相关文本片段。然后，LLM被用来识别和标记因素之间的潜在关联。最后，我们给出了一个...

    arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
    
[^19]: 见证为信：通过CLIP引导解码缓解大型视觉-语言模型中的幻觉

    Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding

    [https://arxiv.org/abs/2402.15300](https://arxiv.org/abs/2402.15300)

    CLIP相似性作为更强大和更稳健的幻觉指标，研究提出了CLIP引导解码（CGD）方法，在大型视觉-语言模型中有效减少对象幻觉。

    

    大型视觉-语言模型(LVLMs)容易出现对象幻觉，即生成的文本包含不存在的对象，严重限制了它们的可靠性和实用性。我们首先对句子级LVLM幻觉进行实证分析，发现与图像的CLIP相似性作为一个比单词可能性更强大、更稳健的幻觉指示器。基于这一发现，我们提出了CLIP引导解码（CGD）方法，这是一种简单但有效的无需训练的方法，用于减少解码时的对象幻觉。CGD利用CLIP来引导模型的解码过程，通过增强生成文本与图像的视觉联系。实验表明，CGD有效地减轻了对象幻觉。

    arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
    
[^20]: MemoryPrompt: 一种改进预训练语言模型上下文跟踪的轻量封装方法

    MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models

    [https://arxiv.org/abs/2402.15268](https://arxiv.org/abs/2402.15268)

    MemoryPrompt方法通过引入辅助循环网络，将信息传递给语言模型，从而改进了预训练语言模型在上下文跟踪方面的性能，避免了灾难性遗忘现象。

    

    基于Transformer的语言模型通过大型硬编码输入窗口跟踪上下文信息。我们引入MemoryPrompt，一种更精简的方法，其中语言模型由一个小的辅助循环网络补充，通过在其常规输入之前添加一系列向量（类似于软提示）将信息传递给语言模型，而无需需要对语言模型进行微调。在对一个旨在检测语言模型跟踪多个事实更新能力的任务上进行测试时，MemoryPrompt增强的语言模型优于那些可以访问完整输入历史记录的更大型语言模型。我们还在一个长距离对话数据集上测试了MemoryPrompt，在该数据集上，其性能与在整个对话历史记录上进行条件处理的模型相当。在这两个实验中，我们还观察到，与完全微调方法不同，MemoryPrompt在适应新任务时不会出现灾难性遗忘，因此不会破坏非专家能力。

    arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un
    
[^21]: CloChat：了解人们如何定制、互动和体验大型语言模型中的人设

    CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models

    [https://arxiv.org/abs/2402.15265](https://arxiv.org/abs/2402.15265)

    研究调查了用户如何定制代理人设以及其对互动质量、多样性和动态的影响，开发了CloChat界面以支持在LLMs中轻松准确定制代理人设

    

    大型语言模型(LLMs)已经在生成对话代理方面取得了重大进展，实现了在不同主题上进行无缝、具有上下文相关性的对话。然而，现有的LLM驱动的对话代理具有固定的个性和功能，限制了它们对个体用户需求的适应性。创建具有不同专业知识或特点的个性化代理人设可以解决这个问题。尽管如此，我们缺乏关于人们如何定制和互动代理人设的知识。在这项研究中，我们调查了用户如何定制代理人设以及它们对互动质量、多样性和动态的影响。为此，我们开发了CloChat，这是一种支持在LLMs中轻松准确定制代理人设的界面。我们进行了一项研究，比较参与者如何与CloChat和ChatGPT进行互动。结果表明，参与者与定制代理人设建立了情感联系，参与了...

    arXiv:2402.15265v1 Announce Type: cross  Abstract: Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged
    
[^22]: DEEM：面向立场检测的动态体验专家建模

    DEEM: Dynamic Experienced Expert Modeling for Stance Detection

    [https://arxiv.org/abs/2402.15264](https://arxiv.org/abs/2402.15264)

    本文提出了一种Dynamic Experienced Expert Modeling（DEEM）方法，利用生成的经验专家使LLMs能够以半参数化方式进行推理，提高了在立场检测任务中的性能。

    

    最近的研究初步尝试使用大型语言模型（LLMs）来解决立场检测任务，展现了有希望的结果。然而，考虑到立场检测通常需要详细的背景知识，传统的推理方法可能会忽视领域知识，以进行专业和准确的分析。因此，LLMs的推理仍有改进空间，尤其在利用LLMs的生成能力模拟特定专家（即多智能体）来检测立场方面。与现有需要详细描述并使用固定专家的多智能体作品不同，本文提出了一种Dynamic Experienced Expert Modeling（DEEM）方法，可以利用生成的经验专家，并让LLMs以半参数化方式进行推理，使专家更具普适性和可靠性。实验结果表明，DEEM在三个场景上一直达到最佳结果。

    arXiv:2402.15264v1 Announce Type: new  Abstract: Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on thre
    
[^23]: Chitchat作为干扰：向面向任务的对话添加用户背景故事

    Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues

    [https://arxiv.org/abs/2402.15248](https://arxiv.org/abs/2402.15248)

    通过使用少样本提示和Llama-2-70B增强MultiWOZ数据集，引入用户背景故事，有效解决面向任务的对话中的闲聊干扰问题，并能够同时承认用户背景故事并推动任务的进行。

    

    在面向任务的对话（TOD）中，人类用户自然会引入超出任务范围的闲聊，干扰了对话的流程。为了解决这一问题，我们利用Llama-2-70B进行少样本提示，以增强MultiWOZ数据集，其中包括用户背景故事，这是TOD中典型的闲聊干扰的一个例子。我们通过测试两个模型来评估此添加的影响：一个仅在TOD上进行训练，另一个在TOD上进行初步闲聊交互的训练。我们的分析表明，我们丰富的数据集对这些系统构成了重要挑战。此外，我们证明我们的数据集可以有效用于训练，使系统能够在同一轮中持续承认用户背景故事并成功推动任务的进行，这得到了人类评估的确认。这些发现突显了引入用户背景故事的好处。

    arXiv:2402.15248v1 Announce Type: new  Abstract: During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of ge
    
[^24]: GPT-HateCheck: LLMs能否为仇恨言论检测编写更好的功能测试？

    GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?

    [https://arxiv.org/abs/2402.15238](https://arxiv.org/abs/2402.15238)

    GPT-HateCheck提出了一个框架，通过指导大型语言模型从头开始生成更多样化和现实的功能测试，以解决现有测试案例过于通用简单的问题。

    

    在线仇恨检测受到数据采样、注释和模型预训练中引入的偏见的影响。因此，仅测量在留存测试数据中所有示例上的平均性能是不足够的。相反，我们必须识别特定模型的弱点，并在其更有可能失败时获得信息。本文提出了一个最近的方向，即HateCheck，这是一个用于在使用模板生成的合成数据上测试精细粒度模型功能的套件，模板的形式为“你只是一个[骂人的词]对我来说”。然而，尽管HateCheck允许获得更详细的诊断见解，但其测试用例通常是通用的，并且具有简单的句子结构，不符合真实世界数据。为解决这一局限性，我们提出了GPT-HateCheck，一个框架，通过指导大型语言模型（LLMs）从头开始生成更多样化和现实的功能测试。我们采用额外的自然语言推理（NLI）模型来验证生成。

    arXiv:2402.15238v1 Announce Type: new  Abstract: Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind "You are just a [slur] to me." However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations
    
[^25]: ChunkAttention: 具有前缀感知KV缓存和两阶段分区的高效自注意力

    ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition

    [https://arxiv.org/abs/2402.15220](https://arxiv.org/abs/2402.15220)

    ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。

    

    自注意力是大型语言模型（LLMs）的重要组成部分，但对于长序列来说是推理延迟的一个显著来源。在多租户LLMs服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。本文介绍了ChunkAttention，一种具有前缀感知的自注意力模块，可以在运行时检测多个请求之间匹配的提示前缀，并共享它们的键/值张量以改进KV缓存的内存利用率。这是通过将整体键/值张量分解为较小的块，并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力内核，其中实现了两阶段分区算法，以改善自注意力计算中的数据局部性。

    arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
    
[^26]: 探索图像生成器的黑匣子隐蔽提示攻击

    BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators

    [https://arxiv.org/abs/2402.15218](https://arxiv.org/abs/2402.15218)

    提出了一种黑匣子隐蔽提示攻击（BSPA），采用检索器模拟攻击，以提高图像生成器的安全性。

    

    极其大型的图像生成器在各个领域提供了重大的变革潜力，使用户能够设计特定提示来通过一些黑匣子API生成逼真的图像。然而，一些研究表明图像生成器容易受到攻击，通过人工设计的毒素文本生成不适宜工作内容（NSFW），尤其是对人类观察者极难察觉。我们迫切需要大量通用且可迁移的提示，以提高图像生成器的安全性，尤其是黑盒发布的API。然而，它们受限于劳动密集型的设计流程，并且严重依赖于给定指令的质量。为了实现这一目标，我们引入了一种黑匣子隐蔽提示攻击（BSPA），采用检索器模拟API用户的攻击。它能有效利用过滤器评分来调整敏感词汇的检索空间，以匹配输入提示。

    arXiv:2402.15218v1 Announce Type: cross  Abstract: Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, t
    
[^27]: 通过实例级前缀实现大型语言模型的细粒度脱毒

    Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models

    [https://arxiv.org/abs/2402.15202](https://arxiv.org/abs/2402.15202)

    提出一种通过实例级前缀在注意力空间中进行细粒度比较，从而实现大型语言模型的细粒度脱毒的方法。

    

    通过训练大型语言模型（LLMs），在自然语言处理（NLP）任务中取得了令人印象深刻的结果。然而，这些模型偶尔会对某些提示生成毒性内容，如侮辱、威胁和粗话，从而限制了它们的实际效用。为了解决这一问题，利用各种基于微调和基于解码的方法来减轻毒性。然而，这些方法通常需要额外的成本，如高质量的训练数据或辅助模型。在本文中，我们提出了通过实例级前缀进行细粒度脱毒（FGDILP），以减轻毒性文本而无需额外费用。具体来说，FGDILP通过在实例级别使用一个带有正前缀的提示来对比注意力空间中的上下文表示，而多个带有负前缀的提示。这允许构建细粒度的次毒性向量，使文本被识别为次毒性变得更加精细。

    arXiv:2402.15202v1 Announce Type: new  Abstract: Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, whic
    
[^28]: DeMPT：解码增强的多阶段提示优化，使LLMs成为更好的上下文感知翻译器

    DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators

    [https://arxiv.org/abs/2402.15200](https://arxiv.org/abs/2402.15200)

    DeMPT提出了解码增强的多阶段提示优化，使得LLMs更好地模拟和利用句间和句内上下文，从而更有效地适应上下文感知NMT。

    

    通常，仅具有解码器的大型语言模型（LLMs）通过连接的方式适应上下文感知神经机器翻译（NMT），其中LLMs将源句（即句内上下文）和句间上下文的连接作为输入，然后顺序生成目标标记。本文提出了一种名为Decoding-enhanced Multi-phase Prompt Tuning（DeMPT）的替代适应方法，以使LLMs能够歧视性地对模组和利用句间和句内上下文，并更有效地将LLMs调整到上下文感知NMT。首先，DeMPT将上下文感知NMT过程分为三个单独阶段。在每个阶段，引入不同的连续提示，使LLMs能够区分地模型。

    arXiv:2402.15200v1 Announce Type: new  Abstract: Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminatel
    
[^29]: 将生物医学实体链接视为多项选择问答

    Biomedical Entity Linking as Multiple Choice Question Answering

    [https://arxiv.org/abs/2402.15189](https://arxiv.org/abs/2402.15189)

    提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。

    

    尽管预训练语言模型在生物医学实体链接（BioEL）方面取得了显著进展，但对于细粒度和长尾实体仍然存在挑战。为了解决这些挑战，我们提出了BioELQA，这是一种将生物医学实体链接视为多项选择问答的新颖模型。BioELQA首先利用快速检索器获得候选实体，将提及和候选实体共同呈现给生成器，然后输出与其选定实体相关的预测符号。这种公式使得不同候选实体之间的明确比较成为可能，从而捕捉了提及和实体之间以及实体之间的精细交互。为了改善长尾实体的泛化能力，我们检索相似的已标记训练实例作为线索，并将输入与检索实例连接到生成器。广泛的实验结果表明，BioELQA的表现优于统计结果。

    arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
    
[^30]: 打破Breakout: 用自我完善重新定义LM对抗越狱攻击的防御

    Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement

    [https://arxiv.org/abs/2402.15180](https://arxiv.org/abs/2402.15180)

    提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。

    

    警告：本文包含可能引起不快的冒犯性词语。语言模型（LMs）容易被利用进行恶意滥用。对LM进行安全对齐的训练非常复杂，使得难以立即应对快速发展的攻击，如越狱攻击。我们提出了一种通过格式自我完善的方法，即使在非安全对齐的LMs中也能实现出色的安全性，并将我们的方法与几种防御基线进行评估，表明这是针对越狱攻击最安全的无训练方法。此外，我们提出了一种改进自我完善过程效率的格式化方法，同时在较少迭代中降低攻击成功率。我们还观察到非安全对齐的LM在安全任务中表现优于安全对齐的LM，因为它们给出更有用且更安全的回复。总之，我们的发现能够在较少的计算成本下实现更少的安全风险。

    arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
    
[^31]: 通过表示编辑推进微调中的参数效率

    Advancing Parameter Efficiency in Fine-tuning via Representation Editing

    [https://arxiv.org/abs/2402.15179](https://arxiv.org/abs/2402.15179)

    RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果

    

    参数有效微调（PEFT）因其能够在仅更新可训练参数的一个小子集时达到竞争性结果而受到了重视。在解决这些挑战问题中，我们提出了一种新颖的微调神经模型的方法，称为表示编辑（RED），其扩放和偏置每一层产生的表示。与完全参数微调相比，RED将可训练参数数量降低了$25,700$倍，并与LoRA相比降低了32倍。值得注意的是，RED实现了与完全参数微调和其他PEFT方法相当或更好的结果。对不同架构和规模的模型进行了大量实验。

    arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
    
[^32]: 基于微调的抽象式摘要模型的实体级事实适应性

    Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models

    [https://arxiv.org/abs/2402.15162](https://arxiv.org/abs/2402.15162)

    分析了基于微调的摘要模型在处理知识冲突时的实体级事实适应性，并提出了一种反事实数据增强方法，实验结果表明该方法增强了事实适应性，同时保持了事实一致性。

    

    抽象式摘要模型在处理参数化知识与输入文档中的知识冲突时，往往生成事实不一致的内容。本文分析了基于微调的摘要模型对知识冲突的鲁棒性，即我们称之为事实适应性。我们利用预训练语言模型构建评估集，并发现事实适应性与原始数据集上的事实一致性并非强相关。此外，我们引入了一种可控的反事实数据增强方法，其中增强数据中的知识冲突程度是可调节的。我们在两个预训练语言模型（PEGASUS 和 BART）和两个微调数据集（XSum 和 CNN/DailyMail）上的实验结果表明，我们的方法增强了事实适应性，同时在原始数据集上实现了与对比方法相当的事实一致性。

    arXiv:2402.15162v1 Announce Type: cross  Abstract: Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastiv
    
[^33]: 面向预训练大型语言模型的机器遗忘

    Machine Unlearning of Pre-trained Large Language Models

    [https://arxiv.org/abs/2402.15159](https://arxiv.org/abs/2402.15159)

    本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。

    

    本研究探讨了大型语言模型（LLMs）背景下“被遗忘权”的概念。我们以机器遗忘作为一个关键解决方案，重点关注预训练模型——一个明显缺乏研究的领域。我们在预训练LLMs中勾勒了一个全面的机器遗忘框架，包括对七种不同遗忘方法的批判性分析。通过使用来自arXiv、书籍和GitHub的策划数据集进行严格评估，我们建立了一个有力的机器遗忘性能基准，表明这些方法的计算效率比重新训练高出 $10^5$ 倍以上。我们的结果表明，在分布数据上将梯度上升与梯度下降结合可以改善超参数的鲁棒性。我们还提供了关于在遗忘过程中进行高效超参数调整的详细指南。我们的研究推动了有关伦理人工智能实践的讨论，提供了

    arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
    
[^34]: 自适应对比学习的无监督句子嵌入自适应重建

    Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings

    [https://arxiv.org/abs/2402.15153](https://arxiv.org/abs/2402.15153)

    提出了一种新颖的Self-Adaptive Reconstruction Contrastive Sentence Embeddings（SARCSE）框架，利用自动编码器重建句子中的所有令牌，以帮助模型保留更多细粒度语义，并提出了一种自适应重建损失来缓解对令牌频率的偏见。

    

    无监督句子嵌入任务旨在将句子转换为语义向量表示。大多数先前的工作直接使用从预训练语言模型派生的句子表示。然而，由于预训练语言模型中的令牌偏差，模型无法捕捉句子中的细粒度语义，导致预测能力不佳。为解决这一问题，我们提出了一种新颖的自适应重建对比句子嵌入（SARCSE）框架，该框架利用自动编码器重建句子中的所有令牌，帮助模型在聚合令牌过程中保留更多细粒度语义。此外，我们提出了一种自适应重建损失来缓解对令牌频率的偏见。实验结果表明，与强基准SimCSE相比，SARCSE在7个STS任务上取得了显著的改进。

    arXiv:2402.15153v1 Announce Type: new  Abstract: Unsupervised sentence embeddings task aims to convert sentences to semantic vector representations. Most previous works directly use the sentence representations derived from pretrained language models. However, due to the token bias in pretrained language models, the models can not capture the fine-grained semantics in sentences, which leads to poor predictions. To address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in sentences with an AutoEncoder to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive reconstruction loss to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.
    
[^35]: 视觉语音遇见语言：VSP-LLM框架用于高效和上下文感知的视觉语音处理

    Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing

    [https://arxiv.org/abs/2402.15151](https://arxiv.org/abs/2402.15151)

    提出了一个新颖的VSP-LLM框架，用于最大化上下文建模能力，实现视觉语音识别和翻译的多任务执行。

    

    在视觉语音处理中，由于唇部运动的模糊性质，上下文建模能力是最重要的要求之一。例如，同音异义词，即具有相同唇部运动但产生不同声音的单词，可以通过考虑上下文来区分。本文提出了一种新颖的框架，称为集成LLM的视觉语音处理（VSP-LLM），通过引入LLM的强大能力来最大化上下文建模能力。具体来说，VSP-LLM旨在执行视觉语音识别和翻译的多任务，其中给定的指令控制任务类型。通过利用自监督视觉语音模型，将输入视频映射到LLM的输入潜在空间。针对输入帧存在冗余信息的事实，我们提出了一种新颖的去重方法，通过使用视觉语音单元减少嵌入的视觉特征。

    arXiv:2402.15151v1 Announce Type: cross  Abstract: In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Thr
    
[^36]: 通过自动生成的NLI数据集改进句子嵌入

    Improving Sentence Embeddings with an Automatically Generated NLI Dataset

    [https://arxiv.org/abs/2402.15132](https://arxiv.org/abs/2402.15132)

    通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。

    

    基于解码器的大型语言模型在自然语言处理的许多任务中表现出了很高的性能。这在句子嵌入学习中同样成立，其中基于解码器的模型PromptEOL 在语义文本相似性（STS）任务中取得了最佳表现。然而，PromptEOL 在很大程度上利用了对自然语言推理（NLI）数据集的手动标注进行微调。我们旨在通过使用LLM自动生成的NLI数据集来改进在无监督设置下学习的句子嵌入，并将其用于微调PromptEOL。在STS任务的实验中，提出的方法在人类评估方面达到了82.21的平均Spearman等级相关系数，从而优于现有方法而无需使用大规模手动注释的数据集。

    arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.
    
[^37]: 互动式知识库问答：基于大型语言模型的多轮交互式知识库问答

    Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models

    [https://arxiv.org/abs/2402.15131](https://arxiv.org/abs/2402.15131)

    提出了一种互动式KBQA框架，通过直接与知识库互动生成逻辑形式，开发了用于KB交互的通用API，并设计了示例来指导大型语言模型进行推理。

    

    本研究探讨了知识库问答（KBQA）的领域。KBQA被认为是一项具有挑战性的任务，特别是在将复杂问题解析为可执行逻辑形式方面。传统的基于语义解析（SP）的方法需要大量的数据注释，这导致了显著的成本。最近，由大型语言模型（LLM）推动的少样本上下文学习的出现展示了很好的能力。然而，在低资源情景下充分利用LLMs将问题解析为逻辑形式是一个重大挑战。为了应对这些障碍，我们引入了互动式知识库问答（Interactive-KBQA），这是一个旨在通过与知识库（KBs）直接互动来生成逻辑形式的框架。在这个框架内，我们开发了三个用于KB交互的通用API。对于每种复杂问题类别，我们设计了示例来指导LLMs完成推理过程。我们的方法取得了具有竞争力的结果。

    arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
    
[^38]: 大型多模态代理：一项调查

    Large Multimodal Agents: A Survey

    [https://arxiv.org/abs/2402.15116](https://arxiv.org/abs/2402.15116)

    大型语言模型驱动的多模态代理（LMAs）的系统审查，涵盖了开发组件、研究类型分类以及集体效能增强的合作框架等内容。

    

    大型语言模型（LLMs）在推动基于文本的人工智能代理时取得了卓越表现，赋予它们类似于人类的决策和推理能力。同时，有一个新兴的研究趋势专注于将这些LLM驱动的人工智能代理扩展到多模态领域。本文系统地审查了LLM驱动的多模态代理，我们将其称为大型多模态代理（LMAs简称）。首先，我们介绍了发展LMAs所涉及的基本组成部分，并将当前的研究范畴分为四种不同类型。随后，我们审查了集成多个LMAs以增强集体效能的合作框架。这一领域面临的一个关键挑战是现有研究中使用的多样化评估方法。

    arXiv:2402.15116v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studie
    
[^39]: GPT应用的初探：格局与脆弱性

    A First Look at GPT Apps: Landscape and Vulnerability

    [https://arxiv.org/abs/2402.15105](https://arxiv.org/abs/2402.15105)

    通过大规模监测和分析GPT商店，开发了自动化工具来研究GPT应用程序中的漏洞和抄袭情况。

    

    随着大型语言模型（LLMs）的进步，越来越复杂和强大的GPT进入市场。尽管它们很受欢迎，但LLM生态系统仍然尚未被探索。此外，LLMs对攻击的敏感性引发了对安全性和抄袭的担忧。因此，在这项工作中，我们对GPT商店进行了开创性的探索，旨在研究GPT应用程序中的漏洞和抄袭。首先，我们进行了据我们所知的第一次大规模监测和分析，分别是一个非官方的GPTStore.AI和一个官方的OpenAI GPT Store。然后，我们提出了一种TriLevel GPT Reversing（T-GR）策略，用于提取GPT内部信息。为了有效地完成这两项任务，我们开发了两个自动化工具：一个用于网络抓取，另一个设计用于与GPT进行程序化交互。我们的发现揭示了用户和开发者对GPT交互和创建的巨大热情，

    arXiv:2402.15105v1 Announce Type: cross  Abstract: With the advancement of Large Language Models (LLMs), increasingly sophisticated and powerful GPTs are entering the market. Despite their popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs' susceptibility to attacks raises concerns over safety and plagiarism. Thus, in this work, we conduct a pioneering exploration of GPT stores, aiming to study vulnerabilities and plagiarism within GPT applications. To begin with, we conduct, to our knowledge, the first large-scale monitoring and analysis of two stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals. To complete these two tasks efficiently, we develop two automated tools: one for web scraping and another designed for programmatically interacting with GPTs. Our findings reveal a significant enthusiasm among users and developers for GPT interaction and creation, as
    
[^40]: AttributionBench：自动归因评估有多难？

    AttributionBench: How Hard is Automatic Attribution Evaluation?

    [https://arxiv.org/abs/2402.15089](https://arxiv.org/abs/2402.15089)

    AttributionBench是一个综合基准，揭示了自动归因评估的挑战，即使对于最先进的语言模型也只能达到80%的准确率。

    

    现代生成式搜索引擎通过提供引用证据增强了大型语言模型（LLM）响应的可靠性。然而，评估答案的归因，即生成响应中的每个声明是否都得到其引用证据的充分支持，仍然是一个未解决的问题。传统上依赖于昂贵的人工评估的这种验证强调了对自动归因评估方法的迫切需求。为了填补这种方法缺乏标准化基准的差距，我们提出了AttributionBench，这是一个综合性基准，由各种现有的归因数据集编制而成。我们在AttributionBench上的大量实验揭示了自动归因评估面临的挑战，即使对于最先进的LLM也是如此。具体而言，我们的发现表明，即使是经过优化的GPT-3.5在二元分类公式下也只能达到约80%的宏F1分数。更 than 300 error c

    arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c
    
[^41]: 无需手部操作的虚拟现实系统

    Hands-Free VR

    [https://arxiv.org/abs/2402.15083](https://arxiv.org/abs/2402.15083)

    Hands-Free VR 是一种无需手部操作的虚拟现实系统，通过语音命令实现，具有英语口音鲁棒性，通过深度学习模型和大型语言模型实现对文本的转换和执行。

    

    本文介绍了一种名为Hands-Free VR的基于语音的自然语言虚拟现实界面。用户可以通过语音发出命令，其语音音频数据经过一个针对单词音素相似性和英语口音的鲁棒性进行微调的语音识别深度学习模型转换为文本，然后利用一个对自然语言多样性具有鲁棒性的大型语言模型将文本映射为可执行的虚拟现实命令。Hands-Free VR在一个受控的被试研究中（N = 22）进行了评估，要求参与者找到特定物体并以各种配置放置它们。在对照条件下，参与者使用传统的虚拟现实用户界面通过手持控制器抓取、搬运和定位物体。在实验条件下，参与者使用Hands-Free VR。结果表明：（1）Hands-Free VR对英语口音具有鲁棒性，因为在我们的20名参与者中，英语不是他们的首选语言。

    arXiv:2402.15083v1 Announce Type: cross  Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their f
    
[^42]: PEMT: 多任务相关性引导的专家混合模型实现了参数高效的迁移学习

    PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning

    [https://arxiv.org/abs/2402.15082](https://arxiv.org/abs/2402.15082)

    PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。

    

    针对参数高效微调（PEFT）作为将预训练语言模型有效地适应各种任务的方法已经崛起。最近，人们对从一个或多个任务转移知识到下游目标任务以实现性能提升产生了越来越浓厚的兴趣。然而，当前方法通常要么在各个任务上训练适配器，要么从源任务中提取共享知识，未能充分利用任务特定知识和源任务与目标任务之间的相关性。为了克服这些限制，我们提出了PEMT，这是一种基于多任务迁移学习的创新参数高效微调框架。PEMT将专家混合（MoE）框架扩展为源任务上训练的适配器的加权组合以捕获可转移知识。这些权重由一个门控单元确定，利用任务之间的相关性来测量目标任务和每个源任务之间的相关性。

    arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
    
[^43]: 将层级指导融入提示调整：一种用于多层次隐式话语关系识别的参数高效框架

    Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition

    [https://arxiv.org/abs/2402.15080](https://arxiv.org/abs/2402.15080)

    提出了一种基于提示调整的参数高效框架，通过层级标签细化方法深度整合层级指导，用于解决多层次隐式话语关系识别中的问题

    

    多层次隐式话语关系识别(MIDRR)旨在识别论点之间的层次话语关系。先前的方法通过微调PLM来实现推广。然而，由于数据稀缺和任务差距，预训练特征空间无法准确调整到特定任务空间，甚至加剧了基础空间的崩溃。此外，对于MIDRR的层次语义理解使得转换更加困难。在本文中，我们提出了一种基于提示的参数高效多层次IDRR（PEMI）框架来解决以上问题。首先，我们利用参数高效的提示调整将输入的论点驱动到匹配预训练空间并利用少量参数实现近似。此外，我们提出了一种层级标签细化（HLR）方法，用于让提示表达器深度整合层级指导到提示调整中。

    arXiv:2402.15080v1 Announce Type: new  Abstract: Multi-level implicit discourse relation recognition (MIDRR) aims at identifying hierarchical discourse relations among arguments. Previous methods achieve the promotion through fine-tuning PLMs. However, due to the data scarcity and the task gap, the pre-trained feature space cannot be accurately tuned to the task-specific space, which even aggravates the collapse of the vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR makes the conversion much harder. In this paper, we propose a prompt-based Parameter-Efficient Multi-level IDRR (PEMI) framework to solve the above problems. First, we leverage parameter-efficient prompt tuning to drive the inputted arguments to match the pre-trained space and realize the approximation with few parameters. Furthermore, we propose a hierarchical label refining (HLR) method for the prompt verbalizer to deeply integrate hierarchical guidance into the prompt tuning. Finally, our mo
    
[^44]: 别耍花招！大型语言模型自我调整以回答未知问题

    Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions

    [https://arxiv.org/abs/2402.15062](https://arxiv.org/abs/2402.15062)

    提出了一种自我调整方法，利用大型语言模型增强回答未知问题的能力，包括拒绝回答并解释未知问题无法回答的原因。

    

    尽管大型语言模型（LLMs）具有出色的回答问题能力，但它们在问题没有明确答案时往往表现出相当程度的自信过度。为了避免向这些未知问题提供虚构答案，现有研究通常探讨拒绝回答这些问题的方法。在这项工作中，我们提出了一种新颖且可扩展的自我调整方法，利用LLM本身来增强其对不同类型未知问题的回应能力，不仅能够拒绝回答，还能够解释未知问题无法回答的原因。具体来说，Self-Align方法首先采用两阶段类感知自我增强方法生成大量未知问题-回应数据。然后，我们进行差异驱动的自我整理，选择合格数据对LLM本身进行微调，以调整对未知问题的响应。

    arXiv:2402.15062v1 Announce Type: new  Abstract: Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown q
    
[^45]: 针对领域特定机器翻译的大型语言模型微调

    Fine-tuning Large Language Models for Domain-specific Machine Translation

    [https://arxiv.org/abs/2402.15061](https://arxiv.org/abs/2402.15061)

    提出了一种名为LlamaIT的基于提示的微调方法，用于领域特定机器翻译任务，解决了大型语言模型在领域特定机器翻译中遇到的挑战。

    

    大型语言模型（LLMs）在机器翻译（MT）领域取得了重要进展。然而，它们在领域特定MT中的潜力尚未得到充分探索。当前基于LLMs的MT系统仍然面临一些挑战。为了解决这些挑战，本文提出了一种名为LlamaIT的基于提示的微调方法，以有效高效地为领域特定MT任务微调通用LLM。

    arXiv:2402.15061v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we constru
    
[^46]: ColBERT-XM：一种用于零-shot 多语信息检索的模块化多向量表示模型

    ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval

    [https://arxiv.org/abs/2402.15059](https://arxiv.org/abs/2402.15059)

    ColBERT-XM模型通过从单一高资源语言的数据中学习，实现了零-shot转移到广泛的语言，从而消除了对特定语言标记数据的需求。

    

    最先进的神经检索器主要集中在英语等高资源语言上，这阻碍了它们在涉及其他语言的检索场景中的应用。当前方法通过利用能够进行跨语言转移的多语言预训练语言模型来规避非英语语言中缺乏高质量标记数据的问题。然而，这些模型需要在多种语言上进行大量特定任务的微调，在预训练语料库中表示极少的语言中通常表现不佳，并且在预训练阶段之后难以融合新语言。在这项工作中，我们提出了一种新颖的模块化稠密检索模型，它从单一高资源语言的丰富数据中学习，并有效地零-shot 转移到广泛的语言，从而消除了对特定语言标记数据的需求。我们的模型 ColBERT-XM 在现有模型上展现出有竞争力的性能。

    arXiv:2402.15059v1 Announce Type: new  Abstract: State-of-the-art neural retrievers predominantly focus on high-resource languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual pretrained language models capable of cross-lingual transfer. However, these models require substantial task-specific fine-tuning across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular dense retrieval model that learns from the rich data of a single high-resource language and effectively zero-shot transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing st
    
[^47]: 关于面向对话式网络代理的多轮指令跟踪

    On the Multi-turn Instruction Following for Conversational Web Agents

    [https://arxiv.org/abs/2402.15057](https://arxiv.org/abs/2402.15057)

    提出了一个新任务——对话式网络导航，引入了一个名为MT-Mind2Web的特殊数据集，并提出了一个名为Self-MAP的框架，旨在解决大型语言模型在多轮指令跟踪中的长度和上下文依赖性问题。

    

    由大型语言模型（LLMs）驱动的网络代理在规划和执行复杂基于网络的多步交互方面展示了出色的能力，完成了各种网络导航任务。然而，尽管取得了这些进展，以LLM为动力的代理在真实场景中有效与顺序用户指令进行交互的潜力尚未得到充分探索。本研究介绍了一个名为对话式网络导航的新任务，该任务需要与用户和环境进行跨多轮的复杂交互，支持使用一个名为多轮Mind2Web（MT-Mind2Web）的特别开发的数据集。为了解决LLMs的有限上下文长度和对话任务的上下文依赖性问题，我们进一步提出了一种名为自反映记忆增强规划（Self-MAP）的新框架，采用了记忆利用和自我反思技术。

    arXiv:2402.15057v1 Announce Type: cross  Abstract: Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive
    
[^48]: 在Transformer中解释上下文查找：探究注意力-MLP交互

    Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions

    [https://arxiv.org/abs/2402.15055](https://arxiv.org/abs/2402.15055)

    该研究探究了Transformer中注意力头和MLP之间的相互作用，并揭示了特定上下文下激活特定token预测的机制，从而阐明在LLMs中注意力如何促成依赖上下文的专门化处理。

    

    在本文中，我们研究了注意力头和Multilayer Perceptron中专门预测特定token的"next-token"神经元之间的相互作用。通过促使像GPT-4这样的LLM解释这些模型内部，我们可以阐明激活某些next-token神经元的注意力机制。我们的分析确定了识别与预测特定token相关的上下文的attention heads，通过残差连接激活相关联的神经元。我们专注于在较早的层中始终激活相同next-token神经元的attention heads。探索这些不同的激活模式揭示了为不同语言上下文专门化的头与生成某些tokens相关联。总体而言，我们的方法结合了神经解释和探测孤立的组件，以阐明注意力如何使LLMs中的依赖上下文的专门处理成为可能。

    arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
    
[^49]: 在大型语言模型中基准测试心灵理论

    ToMBench: Benchmarking Theory of Mind in Large Language Models

    [https://arxiv.org/abs/2402.15052](https://arxiv.org/abs/2402.15052)

    提出了ToMBench框架，在大型语言模型中进行心灵理论性能评估，发现最先进的模型仍然落后于人类表现超过10%。

    

    心灵理论（ToM）是指感知和归因自己以及他人的心理状态的认知能力。最近的研究引发了关于大型语言模型（LLMs）是否表现出一种形式的心灵理论的争论。然而，现有的心灵理论评估受到诸如受限范围、主观判断和意外污染等挑战的制约，导致评估不足。为了填补这一空白，我们引入了ToMBench，具有三个关键特征：系统评估框架涵盖社会认知中的8项任务和31项能力，多项选择题格式以支持自动化和无偏见的评估，以及基于双语清单的从头构建，严格避免数据泄漏。基于ToMBench，我们进行了大量实验，评估了10个流行LLMs在任务和能力方面的心灵理论表现。我们发现，即使像GPT-4这样的最先进的LLMs也比人类表现落后超过10个百分点。

    arXiv:2402.15052v1 Announce Type: cross  Abstract: Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicati
    
[^50]: 发挥大型语言模型在实体对齐中的力量

    Unlocking the Power of Large Language Models for Entity Alignment

    [https://arxiv.org/abs/2402.15048](https://arxiv.org/abs/2402.15048)

    ChatEA是一个创新性框架，利用大型语言模型提高实体对齐准确性，通过引入KG-code翻译模块和两阶段EA策略来克服传统方法的局限性。

    

    实体对齐（EA）对于整合不同知识图（KG）数据至关重要，在数据驱动的人工智能应用中发挥着关键作用。传统的EA方法主要依赖于比较实体嵌入，但受限于有限的输入KG数据和表示学习技术的能力，它们的有效性受到约束。在这一背景下，我们介绍了ChatEA，这是一个创新性框架，它将大型语言模型（LLMs）融入以改善EA。为了解决有限的输入KG数据的限制，ChatEA引入了一个KG-code翻译模块，将KG结构翻译成LLMs可理解的格式，从而使LLMs能够利用其广泛的背景知识提高EA的准确性。为了克服对实体嵌入比较的过度依赖，ChatEA实现了一个两阶段EA策略，利用LLMs在对话格式中的多步推理能力，从而提高准确性。

    arXiv:2402.15048v1 Announce Type: cross  Abstract: Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy wh
    
[^51]: CARBD-Ko: 一个用于韩文方面级别情感分类的情境注释评论基准数据集

    CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean

    [https://arxiv.org/abs/2402.15046](https://arxiv.org/abs/2402.15046)

    提出了一个用于韩文方面级别情感分类的CARBD-Ko基准数据集，引入了双标极性以区分特定方面和方面不可知情感分类，并提出了采用Siamese网络的新方法来解决双标方面极性问题。

    

    这篇论文探讨了在预训练语言模型（PLMs）中面向方面的情感分类（ABSC）所面临的挑战，特别关注了上下文化和幻觉问题。为了解决这些挑战，我们引入了CARBD-Ko（用于韩文方面级别情感分类的情境注释评论基准数据集），这是一个基准数据集，它结合了方面和双标极性，以区分特定方面和方面不可知情感分类。该数据集包含了用特定方面、方面极性、方面不可知极性和方面强度进行注释的句子。为了解决双标方面极性的问题，我们提出了一种采用孪生网络的新方法。我们的实验结果突出了准确预测双极性的固有困难性，并强调了情境化情感分析模型的重要性。

    arXiv:2402.15046v1 Announce Type: new  Abstract: This paper explores the challenges posed by aspect-based sentiment classification (ABSC) within pretrained language models (PLMs), with a particular focus on contextualization and hallucination issues. In order to tackle these challenges, we introduce CARBD-Ko (a Contextually Annotated Review Benchmark Dataset for Aspect-Based Sentiment Classification in Korean), a benchmark dataset that incorporates aspects and dual-tagged polarities to distinguish between aspect-specific and aspect-agnostic sentiment classification. The dataset consists of sentences annotated with specific aspects, aspect polarity, aspect-agnostic polarity, and the intensity of aspects. To address the issue of dual-tagged aspect polarities, we propose a novel approach employing a Siamese Network. Our experimental findings highlight the inherent difficulties in accurately predicting dual-polarities and underscore the significance of contextualized sentiment analysis mod
    
[^52]: KIEval：面向大型语言模型的知识引导式交互评估框架

    KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models

    [https://arxiv.org/abs/2402.15043](https://arxiv.org/abs/2402.15043)

    该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估

    

    大型语言模型（LLMs）的自动评估方法受到数据污染的影响，导致对其有效性的评估被夸大。现有的策略旨在检测受污染的文本，但侧重于量化污染程度而非准确衡量模型性能。本文介绍了KIEval，这是一种知识引导式交互评估框架，首次引入了LLM驱动的“交互者”角色，实现了动态抗污染评估。从涉及特定领域知识的常规LLM基准问题开始，KIEval利用动态生成的、多轮、以知识为重点的对话，以确定模型的响应是否仅是基准答案的回忆，还是表明了深入理解并能在更复杂的对话中应用知识。在五个数据集上对七个领先的LLM进行了大量实验证实了KI

    arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
    
[^53]: CLoVe: 在对比视觉-语言模型中编码组合性语言

    CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models

    [https://arxiv.org/abs/2402.15021](https://arxiv.org/abs/2402.15021)

    本文提出了一个框架，显著提高现有模型编码组合性语言的能力，在组合性基准上取得超过10% 的绝对改进，同时保持或提高了在标准对象识别和检索基准上的性能。

    

    近年来，视觉和语言任务的表现显著提升。基础视觉-语言模型（VLMs）如CLIP已在多个设置中得到应用，并展示了在几个任务上的显著性能。这些模型擅长于对象中心识别，但学习的文本表示似乎对词序不变，未能以新颖方式组成已知概念。然而，没有证据表明任何VLM，包括大规模单流模型如GPT-4V，成功识别组合。本文介绍了一个框架，显著提高了现有模型编码组合性语言的能力，在组合性基准上取得了超过10% 的绝对改进，同时在标准对象识别和检索基准上保持或提高了性能。我们的代码和预训练模型可在https://github.com/netflix/处公开获取。

    arXiv:2402.15021v1 Announce Type: cross  Abstract: Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/
    
[^54]: 具有掩码语言模型的概率健壮束搜索

    Probabilistically-sound beam search with masked language models

    [https://arxiv.org/abs/2402.15020](https://arxiv.org/abs/2402.15020)

    提出了在掩码语言模型上进行束搜索的概率健壮方法，表明其在多个领域中优于传统方法。

    

    具有掩码语言模型（MLMs）的束搜索存在挑战，部分原因是由于序列的联合概率分布不像自回归模型那样readily available。然而，估算这样的分布在许多领域中具有应用，包括蛋白工程和古代文本恢复。我们提出了一种具有概率健壮性的使用MLMs进行束搜索的方法。首先，我们阐明了在哪些条件下使用标准束搜索对MLMs执行文本填充在理论上是可靠的。当这些条件失败时，我们提供了一种具有概率健壮性的修改，而且无需额外的计算复杂性，并且证明在预期条件下它优于前述的束搜索。然后，我们提出了比较多个领域中几种使用MLMs进行填充的方法的经验结果。

    arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
    
[^55]: LLM对全球表示的意外影响

    Unintended Impacts of LLM Alignment on Global Representation

    [https://arxiv.org/abs/2402.15018](https://arxiv.org/abs/2402.15018)

    对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。

    

    在为面向用户的应用程序部署之前，开发人员通过各种程序（如从人类反馈中学习强化学习（RLHF）和直接偏好优化（DPO））将大型语言模型（LLMs）与用户偏好进行对齐。目前对这些程序的评估侧重于遵循指导、推理和真实性的基准。然而，人类偏好并非普遍，对特定偏好集进行对齐可能会产生意外影响。我们探讨了对三个全球表示维度：英语方言、多语言能力和全球各国意见的影响。我们的结果显示，当前的对齐程序在英语方言和全球意见之间产生差异。我们发现对齐提高了多种语言的能力。最后，我们讨论了导致这些意外影响的设计决策，并为更公平的建议。

    arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
    
[^56]: 通过多任务微调实现基础模型的少样本适应

    Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning

    [https://arxiv.org/abs/2402.15017](https://arxiv.org/abs/2402.15017)

    多任务微调的方法通过在基础模型上对相关任务进行微调，然后适应限制标签数的目标任务，能够降低目标任务中的误差，并提出了一种实用的任务选择算法。

    

    基础模型已经成为许多人工智能问题的有力工具。尽管基础模型取得了巨大成功，但有效地适应新任务，特别是那些数据标签有限的任务，仍然是一个开放问题，并且缺乏理论理解。最近在视觉和自然语言处理领域取得成功的一种新兴解决方案是，在基础模型上对一系列相关任务进行微调，然后再适应具有有限标记样本的目标任务。本文研究了这种多任务微调方法的理论验证。我们的理论分析表明，通过一个多样化的相关任务集，这种多任务微调可以降低目标任务中的误差，与直接适应相同预训练模型相比。我们通过多样性和一致性指标量化了微调任务和目标任务之间的关系，并进一步提出了一个实用的任务选择算法。

    arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our 
    
[^57]: Ar-Spider：阿拉伯语中的文本转SQL

    Ar-Spider: Text-to-SQL in Arabic

    [https://arxiv.org/abs/2402.15012](https://arxiv.org/abs/2402.15012)

    本文介绍了Ar-Spider，这是第一个阿拉伯跨领域文本到SQL数据集，为解决阿拉伯语言的独特性质所带来的模式语言和SQL结构挑战，引入了两个基线模型并测试了两个跨语言模型，取得了不错的性能。

    

    在自然语言处理（NLP）中，文本到SQL语义解析是最重要的任务之一，它旨在使用户以更自然的方式与数据库进行交互。近年来，文本到SQL已取得重大进展，但大多数是以英语为中心的。本文介绍了第一个阿拉伯跨领域文本到SQL数据集Ar-Spider。由于该语言的独特性质，我们遇到了两个主要挑战，即模式语言和SQL 结构挑战。为了处理这些问题并进行实验，我们采用了两个基线模型LGESQL和S2SQL，两者均与两个跨语言模型进行了测试，以减轻模式语言和SQL结构链接挑战的影响。基线模型在我们的阿拉伯文本到SQL数据集Ar-Spider上表现出不错的单语言性能，其中S2SQL实现了62.48%，LGESQL实现了65.57%，仅低于8.79%。

    arXiv:2402.15012v1 Announce Type: cross  Abstract: In Natural Language Processing (NLP), one of the most important tasks is text-to-SQL semantic parsing, which focuses on enabling users to interact with the database in a more natural manner. In recent years, text-to-SQL has made significant progress, but most were English-centric. In this paper, we introduce Ar-Spider 1, the first Arabic cross-domain text-to-SQL dataset. Due to the unique nature of the language, two major challenges have been encountered, namely schema linguistic and SQL structural challenges. In order to handle these issues and conduct the experiments, we adopt two baseline models LGESQL [4] and S2SQL [12], both of which are tested with two cross-lingual models to alleviate the effects of schema linguistic and SQL structure linking challenges. The baselines demonstrate decent single-language performance on our Arabic text-to-SQL dataset, Ar-Spider, achieving 62.48% for S2SQL and 65.57% for LGESQL, only 8.79% below the
    
[^58]: 法语医用口罩语言模型中的标记化有多重要？

    How Important Is Tokenization in French Medical Masked Language Models?

    [https://arxiv.org/abs/2402.15010](https://arxiv.org/abs/2402.15010)

    子词标记化成为自然语言处理领域的主流标准，但其成功因素，如不同任务和语言的最佳分割粒度、数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍不明确。

    

    近年来，基于子词的标记化已成为自然语言处理（NLP）领域中的主流标准，主要是由于预训练语言模型的广泛应用。然而，导致其成功的确切因素，如不同任务和语言的最佳分割粒度，数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍然不够清楚。这在生物医学术语方面尤为重要，其特点是具有管理形态素组合的特定规则。

    arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
    
[^59]: 将视觉问答置于交际背景中的CommVQA

    CommVQA: Situating Visual Question Answering in Communicative Contexts

    [https://arxiv.org/abs/2402.15002](https://arxiv.org/abs/2402.15002)

    CommVQA数据集将图像置于自然环境中，挑战了当前的VQA模型，结果表明为模型提供上下文信息能够提高性能。

    

    当前的视觉问答（VQA）模型往往在孤立的图像-问题对上进行训练和评估。然而，人们提出的问题取决于他们的信息需求和对图像内容的先前了解。为了评估将图像置于自然环境中如何塑造视觉问题，我们引入了CommVQA，这是一个包含图像、图像描述、图像可能出现的真实交际场景（例如旅行网站）以及依赖于场景的后续问题和答案的VQA数据集。我们展示了CommVQA对当前模型提出了挑战。为VQA模型提供上下文信息可广泛提高性能，突显将系统置于交际场景中的相关性。

    arXiv:2402.15002v1 Announce Type: new  Abstract: Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes visual questions, we introduce CommVQA, a VQA dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and follow-up questions and answers conditioned on the scenario. We show that CommVQA poses a challenge for current models. Providing contextual information to VQA models improves performance broadly, highlighting the relevance of situating systems within a communicative scenario.
    
[^60]: 划分还是征服？你应该提炼LLM的哪一部分？

    Divide-or-Conquer? Which Part Should You Distill Your LLM?

    [https://arxiv.org/abs/2402.15000](https://arxiv.org/abs/2402.15000)

    本文提出了一种将推理任务分解为问题分解阶段和问题解决阶段的策略，发现问题分解阶段相比问题解决更容易提炼为较小模型，并证实该策略胜过单阶段解决方案。

    

    最近的研究表明，大型语言模型（LLMs）在被鼓励先解决主要任务的子任务时可以更好地解决推理任务。本文设计了一种类似的策略，将推理任务分解为问题分解阶段和问题解决阶段，并展示该策略能够胜过单阶段解决方案。此外，我们假设与解决问题相比，分解阶段更容易被提炼为较小的模型，因为后者需要大量的领域知识，而前者只需要学习一般的问题解决策略。我们提出了提炼这两种能力的方法，并评估了它们对推理结果和推理成本的影响。我们发现我们可以提炼问题分解阶段，并同时在任务、数据集和模型之间实现良好的泛化。然而，要提炼问题解决阶段就更困难了。

    arXiv:2402.15000v1 Announce Type: new  Abstract: Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the pr
    
[^61]: 小型基准测试：用更少的示例评估LLM

    tinyBenchmarks: evaluating LLMs with fewer examples

    [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)

    本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。

    

    大型语言模型（LLMs）的多功能性导致创建了多种基准测试，彻底测试各种语言模型的能力。这些基准测试包含成千上万个示例，使得评估LLMs非常昂贵。本文研究了减少评估LLMs性能所需的评估次数的策略。例如，我们展示了要准确估计LLMs在MMLU上的性能（一个包含14K个示例的流行多选问答基准测试），只需要在100个精心挑选的示例上评估这个LLMs。我们发布了评估工具和流行基准测试的微型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和微型基准测试足以可靠且高效地重现原始评估结果。

    arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
    
[^62]: 优化语言模型以符合人类偏好是一个因果推断问题

    Optimizing Language Models for Human Preferences is a Causal Inference Problem

    [https://arxiv.org/abs/2402.14979](https://arxiv.org/abs/2402.14979)

    本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。

    

    随着大型语言模型(LLMs)在学术和商业领域的广泛应用，越来越多的人对允许语言模型生成符合人类偏好文本的方法产生了兴趣。本文首次探索了从直接结果数据集中针对人类偏好进行语言模型优化，其中每个样本由一段文本和一个衡量读者响应的相关数值结果组成。我们首次提出应将语言模型优化视为一个因果问题，以确保模型正确学习文本与结果之间的关系。我们正式化了这个因果语言优化问题，并开发了一种方法--因果偏好优化(CPO)--来解决该问题的无偏替代目标。我们进一步使用双重稳健的CPO(DR-CPO)扩展CPO，降低了替代目标的方差，同时保留了明显强有力的保证。

    arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
    
[^63]: GenCeption：使用未标记的单模态数据评估多模态LLM

    GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data

    [https://arxiv.org/abs/2402.14973](https://arxiv.org/abs/2402.14973)

    提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。

    

    多模态大型语言模型（MLLMs）通常使用昂贵的带标注的多模态基准进行评估。然而，这些基准通常难以跟上MLLM评估的快速发展要求。我们提出了GenCeption，这是一个新颖的无需注释的MLLM评估框架，仅需要单模态数据来评估跨模态语义一致性，并反映出模型产生幻觉的倾向。类似于流行的DrawCeption游戏，GenCeption从一个非文本样本开始，并经历一系列迭代的描述和生成步骤。迭代之间的语义漂移使用GC@T指标进行量化。我们的实证发现验证了GenCeption的有效性，并显示出与流行的MLLM基准结果的强相关性。GenCeption可以通过利用普遍存在且以前未见的单模态数据来扩展，以减轻训练数据的污染。

    arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
    
[^64]: MultiLS: 一个多任务词汇简化框架

    MultiLS: A Multi-task Lexical Simplification Framework

    [https://arxiv.org/abs/2402.14972](https://arxiv.org/abs/2402.14972)

    MultiLS是第一个允许创建多任务LS数据集的框架，提出了MultiLS-PT作为第一个使用该框架创建的数据集，展示了其在词汇简化相关任务中的潜力。

    

    词汇简化（LS）自动替换难以理解的单词为更易读的替代词，同时保留句子的原始含义。LS是文本简化的前身，旨在改善文本对各种目标人群的可访问性，包括儿童、第二语言学习者、阅读障碍或低识字率的人群。存在一些专门用于LS的数据集，这些数据集专注于LS流程中的一个或两个子任务。然而，目前尚未开发出一个覆盖所有LS子任务的单个LS数据集。我们提出了MultiLS，这是第一个允许创建多任务LS数据集的LS框架。我们还提出了MultiLS-PT，这是第一个使用MultiLS框架创建的数据集。我们通过执行所有LS子任务，包括（1）词汇复杂性预测（LCP）、（2）替代词生成和（3）替代词排名，展示了MultiLS-PT的潜力。

    arXiv:2402.14972v1 Announce Type: cross  Abstract: Lexical Simplification (LS) automatically replaces difficult to read words for easier alternatives while preserving a sentence's original meaning. LS is a precursor to Text Simplification with the aim of improving text accessibility to various target demographics, including children, second language learners, individuals with reading disabilities or low literacy. Several datasets exist for LS. These LS datasets specialize on one or two sub-tasks within the LS pipeline. However, as of this moment, no single LS dataset has been developed that covers all LS sub-tasks. We present MultiLS, the first LS framework that allows for the creation of a multi-task LS dataset. We also present MultiLS-PT, the first dataset to be created using the MultiLS framework. We demonstrate the potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical complexity prediction (LCP), (2). substitute generation, and (3). substitute ranking for Portugu
    
[^65]: 使用后门增强对齐来缓解微调越狱攻击

    Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment

    [https://arxiv.org/abs/2402.14968](https://arxiv.org/abs/2402.14968)

    提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。

    

    尽管大型语言模型（LLMs）如GPT-4和Llama-2具有一般能力，但在满足特定业务需求和定制用例的复杂性时，仍然需要对其进行微调或自适应以满足需求。然而，这个过程不可避免地引入了新的安全威胁，特别是针对基于微调的越狱攻击（FJAttack），在这种情况下，将仅几个有害示例纳入微调数据集就可能显着地损害模型的安全性。虽然已经提出了一些潜在的防御方法，例如将安全示例纳入微调数据集以减少安全问题，但这些方法需要纳入大量的安全示例，效率低下。为了有效地针对FJAttack进行防御并只使用有限的安全示例，我们提出了一种灵感来自后门攻击概念的后门增强安全对齐方法。

    arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
    
[^66]: 镜像：一种适用于知识丰富推理的多视角自我反思方法

    Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning

    [https://arxiv.org/abs/2402.14963](https://arxiv.org/abs/2402.14963)

    Mirror 提出了一种多视角自我反思方法，通过导航者和推理者之间的启发式交互，促进多样性而具有可靠性的推理轨迹发展，解决了大型语言模型在处理知识丰富问题上的困难。

    

    虽然大型语言模型（LLMs）有能力反复反思自己的输出，但最近的研究观察到它们在没有外部资源的情况下处理知识丰富问题时存在困难。除了LLMs在自我评估方面的低效率外，我们还观察到尽管受到明确负面反馈，LLMs仍然难以重新审视其预测。因此，我们提出了Mirror，一种适用于知识丰富推理的多角度自我反思方法，以避免在特定反思迭代中卡住。Mirror使LLMs能够通过导航者和推理者之间的启发式交互获得多视角线索的反思，引导代理向多样性而具有可靠性的推理轨迹发展，而无需访问地面真相，通过鼓励（1）导航者生成的方向的多样性与（2）策略性引发的扰动在产生的回应中的一致性。

    arXiv:2402.14963v1 Announce Type: cross  Abstract: While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by 
    
[^67]: 一个线性Transformer块的上下文学习：MLP组件和一步GD初始化的优势

    In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization

    [https://arxiv.org/abs/2402.14951](https://arxiv.org/abs/2402.14951)

    该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。

    

    我们研究了结合线性注意力组件和线性多层感知器（MLP）组件的线性Transformer块（LTB）的上下文学习（ICL）能力。对于具有高斯先验和非零均值的线性回归的ICL，我们表明LTB可以实现几乎贝叶斯最优的ICL风险。相比之下，仅使用线性注意力必须产生不可避免的附加近似误差。此外，我们建立了LTB与具有可学习初始化的一步梯度下降估计器（$\mathsf{GD}-\mathbf{\beta}$）之间的对应关系，从每个$\mathsf{GD}-\mathbf{\beta}$估计器可以通过LTB估计器实现，到最小化类内ICL风险的每个最优LTB估计器实际上是一个$\mathsf{GD}-\mathbf{\beta}$估计器。最后，我们表明$\mathsf{GD}-\mathbf{\beta}$估计器可以通过梯度优化高效地优化。

    arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator. Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient f
    
[^68]: 重新审视远程监督命名实体识别：一个新的基准和简单方法

    Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach

    [https://arxiv.org/abs/2402.14948](https://arxiv.org/abs/2402.14948)

    提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法

    

    本文深入探讨了在远程监督（DS-NER）框架下的命名实体识别（NER），主要挑战在于标签质量受到误差的影响，如假阳性、假阴性和正向类型错误。我们批判性地评估了当前DS-NER方法的有效性，使用了一个名为QTL的真实世界基准数据集，揭示它们的性能往往不符合预期。为了解决标签噪声普遍问题，我们引入了一种简单而有效的方法，基于课程的正无标记学习（CuPUL），在训练过程中策略性地从“易”和更清洁的样本开始，以增强模型对嘈杂样本的韧性。我们的实证结果突出了CuPUL减少嘈杂标签影响并胜过现有方法的能力。

    arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
    
[^69]: MobileLLM：优化亚十亿参数语言模型以用于设备端应用

    MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

    [https://arxiv.org/abs/2402.14905](https://arxiv.org/abs/2402.14905)

    MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法

    

    本文解决了移动设备上高效的大型语言模型(LLMs)的迫切需求问题，这是由于云成本和延迟问题不断增加所导致的。我们专注于设计具有不到十亿参数的顶级LLMs，这是移动部署的实际选择。与普遍的观点相反，强调数据和参数数量在确定模型质量方面的关键作用，我们的研究强调了亚十亿规模LLMs的模型架构的重要性。利用深度和瘦身结构，再加上嵌入共享和分组查询注意机制，我们建立了一个强大的基准网络，称为MobileLLM，其在将近125M/350M先进模型上分别获得了惊人的2.7%/4.3%的准确率提升。此外，我们提出了一种立即的块状权重共享方法，不增加模型大小，且仅具有极小的延迟开销。由此产生的模型被命名为MobileLLM-L

    arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
    
[^70]: 数字水印使语言模型具有放射性

    Watermarking Makes Language Models Radioactive

    [https://arxiv.org/abs/2402.14904](https://arxiv.org/abs/2402.14904)

    本文研究了LLM生成文本的放射性，表明使用数字水印训练数据能更容易检测到，同时也展示了即使只有很少比例的水印训练文本，仍可以高置信度地检测出使用数字水印进行微调的情况。

    

    本文研究了LLM生成的文本的放射性，即是否可以检测到这种输入被用作训练数据。传统方法如成员推断可以以一定水平的准确性进行这种检测。我们表明，带有数字水印的训练数据留下的痕迹比成员推断更容易检测且更可靠。我们将污染水平与水印的鲁棒性、在训练集中的比例和微调过程联系起来。特别是我们展示，即使只有5％的训练文本被数字水印标记，训练在带有数字水印的合成指令上仍然可以具有高置信度（p值<1e-5）被检测到。因此，原本设计用于检测机器生成文本的LLM水印技术，使我们能够轻松确定带有数字水印的LLM的输出是否被用来对另一个LLM进行微调。

    arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value < 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
    
[^71]: Tokenization计数：Tokenization对前沿LLMs中算术的影响

    Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs

    [https://arxiv.org/abs/2402.14903](https://arxiv.org/abs/2402.14903)

    本研究探讨了在大型语言模型中对输入文本进行tokenization对数值推理的影响，发现采用从右到左的tokenization方式可显著提高算术任务的性能表现。

    

    Tokenization，即将输入文本分成输入token的过程，是大型语言模型（LLM）管道中经常被忽视的一个方面，可能是有用的或有害的归纳偏差的来源。在历史上，LLMs倾向于使用字节对编码，而没有考虑特定的输入领域。随着LLMs用于推理的增加，各种特定于数字的tokenization方案得到了采用，像LLaMa和PaLM这样的流行模型选择了单个数字tokenization，而GPT-3.5和GPT-4为每个1位、2位和3位数字都有单独的token。在这项工作中，我们通过算术任务研究这种选择对数值推理的影响。我们考虑了GPT-3.5和-4的从左到右和从右到左的tokenization，发现从右到左的tokenization（在推理时通过逗号分离数字）导致了大幅提高的性能。此外，我们发现在使用标准的从左到右tokenization时模型存在误差

    arXiv:2402.14903v1 Announce Type: new  Abstract: Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-r
    
[^72]: 电子商务中意图理解的使用中心视角

    A Usage-centric Take on Intent Understanding in E-Commerce

    [https://arxiv.org/abs/2402.14901](https://arxiv.org/abs/2402.14901)

    该论文提出了电子商务中意图理解的一个新视角，不依赖于产品本体，通过引入产品恢复基准验证了当前意图知识图的弱点。

    

    识别和理解用户意图是电子商务中至关重要的任务。尽管意图理解很受欢迎，但其定义并不一致，且缺乏准确的基准。本文关注将用户意图定义为"顾客如何使用产品"的预测性用户意图，并将意图理解视为一项自然语言推理任务，独立于产品本体。我们发现了FolkScope的两个弱点，这是目前最先进的电子商务意图知识图，限制了其推理用户意图和推荐多样有用产品的能力。基于这些观察，我们引入了一个产品恢复基准，包括一个新颖的评估框架和一个示例数据集。我们在这个基准上进一步验证了上述FolkScope的弱点。

    arXiv:2402.14901v1 Announce Type: cross  Abstract: Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as "how a customer uses a product", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery Benchmark including a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark.
    
[^73]: Chain-of-Thought不忠诚作为伪装的准确性

    Chain-of-Thought Unfaithfulness as Disguised Accuracy

    [https://arxiv.org/abs/2402.14897](https://arxiv.org/abs/2402.14897)

    了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。

    

    了解Chain-of-Thought (CoT)生成与大语言模型(LLM)内部计算的一致程度对于决定是否信任LLM的输出至关重要。作为CoT忠实度的代理，arXiv:2307.13702提出了一个度量模型依赖其CoT生成答案的指标。在一个专有模型系列中，他们发现LLM表现出模型大小与其忠实度测量之间的缩放-反向缩放关系，并且130亿参数模型相比于尺寸介于8.1亿到1750亿参数之间的模型表现出增加的忠实度。我们评估这些结果是否作为所有LLM的特性泛化。我们使用三种不同系列的模型复制他们的实验设置，并在特定条件下，成功复制了他们报告的CoT忠实度的缩放趋势。然而，我们发现简单的改变设定会导致这些模式在多大程度上重复。

    arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
    
[^74]: 数据增强已死，数据增强万岁

    Data Augmentation is Dead, Long Live Data Augmentation

    [https://arxiv.org/abs/2402.14895](https://arxiv.org/abs/2402.14895)

    数据增强不过是更好地微调模型，零唁态和少样本数据生成可提高性能

    

    文本数据增强（DA）是一个繁荣的研究领域，不断提出新颖的技术来创建人工数据，已经在小数据环境中表现出很高的效率，至少对于文本分类任务而言。在本文中，我们质疑这些结果，表明经典的数据增强只是一种更好地进行微调的方式，并且在应用数据增强之前花更多时间进行微调会抵消其效果。这是一个重要的贡献，因为它回答了最近几年留下的几个问题，即：哪种DA技术表现最佳（只要它们生成的数据与训练集足够接近，不会损害训练），为什么DA表现出积极的结果（简化网络训练）。此外，我们还展示了通过对话代理（如ChatGPT或LLama2）零唁态和少样本数据生成可以提高性能，从而得出了结论，此法可以提高模型性能。

    arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
    
[^75]: LLMBind: 一种统一的模态任务集成框架

    LLMBind: A Unified Modality-Task Integration Framework

    [https://arxiv.org/abs/2402.14891](https://arxiv.org/abs/2402.14891)

    提出了LLMBind，一种统一的模态任务集成框架，通过将大型语言模型和预训练任务模型绑定在一起，实现了多种模态任务的灵活输入和输出组合。

    

    最近对于多模态大型语言模型在处理各种模态任务方面取得了进展，但它们对于复杂的多模态任务的集成能力有限，从而限制了该领域的发展。在这项工作中，我们带头探索并提出了LLMBind，一种用于模态任务集成的统一框架，该框架将大型语言模型和相应的预训练任务模型与任务特定的标记绑定在一起。因此，LLMBind可以以多种图像、文本、视频和音频的组合解释输入并生成输出。具体来说，我们引入了一种专家混合技术，通过不同专家之间的协作实现不同多模态任务的有效学习。此外，我们创建了一个包含40万条指令数据的多任务数据集，解锁了交互式视觉生成和编辑任务的能力。大量实验证明了我们的方法的有效性。

    arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
    
[^76]: Vygotsky Distance: 用于基准任务相似性的度量方法

    Vygotsky Distance: Measure for Benchmark Task Similarity

    [https://arxiv.org/abs/2402.14890](https://arxiv.org/abs/2402.14890)

    论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。

    

    论文介绍了一种理论工具和实践算法来计算基准任务之间的相似性，称之为"维果茨基距离"。这种相似性度量的核心思想是基于“学生”在给定任务上的相对表现，而不是基于任务本身的属性。如果两个任务在维果茨基距离上彼此接近，模型在这些任务上 tend to have similar relative performance。因此，通过了解任务之间的维果茨基距离，可以显著减少评估任务数量，同时保持高验证质量。

    arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
    
[^77]: COBIAS：偏见评估中的情境可靠性

    COBIAS: Contextual Reliability in Bias Assessment

    [https://arxiv.org/abs/2402.14889](https://arxiv.org/abs/2402.14889)

    我们提出了COBIAS，旨在通过考虑多样情境的用户输入内容，衡量语句的情境可靠性，从而培养偏见意识。

    

    大型语言模型（LLMs）是基于固有偏见数据训练的。以往的去偏见模型研究依赖基准数据集来衡量模型性能。然而，这些数据集由于对偏见的极其主观理解而存在多个缺陷，凸显出对情境探索的迫切需求。我们提出考虑输入用户内容的情境，考虑到输入语句可能存在的多种情况。这种方法将允许培养偏见意识的框架，而不是伤害用户参与的防护设施。我们的贡献有两个方面：(i) 我们创建了一个包含2287个陈词滥调语句以及添加情境要点的数据集；(ii) 我们开发了面向情境的偏见指标和评估分数（COBIAS）来评估语句在衡量偏见方面的情境可靠性。我们的度量是衡量偏见基准数据集情境可靠性的重要预测因子。

    arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
    
[^78]: 利用基于语义相似性的图结构进行高效数据选择用于模型训练

    Efficient data selection employing Semantic Similarity-based Graph Structures for model training

    [https://arxiv.org/abs/2402.14888](https://arxiv.org/abs/2402.14888)

    提出了一种基于语义相似性的图结构的高效数据选择机制，可在不经过计算密集型模型或其他密集的预处理转换的情况下，用于模型训练。

    

    自然语言处理（NLP）领域的最新发展凸显了模型准确捕捉文本信息所需大量数据的必要性。这引发了关于训练此类模型所需的计算资源和时间的担忧。本文介绍了一种称为“SeSaME”的数据选择机制，它仅基于文本信息进行高效的数据采样，无需通过计算密集型模型或其他密集的预处理转换。

    arXiv:2402.14888v1 Announce Type: cross  Abstract: Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated speech recognition (ASR) models, which excessively rely on text-to-speech (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilou
    
[^79]: 对抗基于ChatGPT作弊的测试题漏洞研究

    A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating

    [https://arxiv.org/abs/2402.14881](https://arxiv.org/abs/2402.14881)

    研究揭示了基于ChatGPT的作弊对测试题的漏洞，并开发了一个工具来辨别测试题中对ChatGPT最容易回答错误的类型。

    

    ChatGPT是一种聊天机器人，可以相当准确地回答文本提示，甚至在研究生级别的问题上表现出色。许多教育工作者发现他们的课业或远程测试和考试容易受到基于ChatGPT的作弊的影响，因为学生可能直接使用ChatGPT等工具提供的答案。在本文中，我们试图回答一个重要问题：ChatGPT能多好回答测试题，以及我们如何检测测试题是否能被ChatGPT正确回答。我们生成了ChatGPT对MedMCQA数据集的响应，该数据集包含超过10,000个医学院入学考试问题。我们分析了这些回答，并揭示了ChatGPT在某些问题上的回答比其他问题更不准确。此外，我们创建了一个基本的自然语言处理模型，可以在一组问题或样本考试中筛选出对ChatGPT最易受攻击的问题。我们的工具可以被使用。

    arXiv:2402.14881v1 Announce Type: cross  Abstract: ChatGPT is a chatbot that can answer text prompts fairly accurately, even performing very well on postgraduate-level questions. Many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating because students may directly use answers provided by tools like ChatGPT. In this paper, we try to provide an answer to an important question: how well ChatGPT can answer test questions and how we can detect whether the questions of a test can be answered correctly by ChatGPT. We generated ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical school entrance exam questions. We analyzed the responses and uncovered certain types of questions ChatGPT answers more inaccurately than others. In addition, we have created a basic natural language processing model to single out the most vulnerable questions to ChatGPT in a collection of questions or a sample exam. Our tool can be us
    
[^80]: 自动直方图：利用语言模型进行文本数据集探索

    Automatic Histograms: Leveraging Language Models for Text Dataset Exploration

    [https://arxiv.org/abs/2402.14880](https://arxiv.org/abs/2402.14880)

    该论文提出了一种利用语言模型的自动直方图可视化工具AutoHistograms，能够自动识别相关特征、以直方图形式展示并允许用户交互式地查询数据集，帮助数据工作者快速探索文本数据集。

    

    理解非结构化文本数据集一直是困难的，但随着大型语言模型的出现变得越来越重要。数据工作人员常常依赖于数据集摘要，特别是各种派生特征的分布。一些特征，如毒性或主题，对许多数据集都有影响，但许多有趣的特征是特定于领域的：音乐数据集的乐器和流派，或医学数据集的疾病和症状。因此，数据工作者经常为每个数据集运行自定义分析，这既繁琐又困难。我们提出了AutoHistograms，这是一个利用LLM的可视化工具。AutoHistograms自动识别相关特征，用直方图形式展示它们，并允许用户交互式地查询数据集的实体类别并创建新的直方图。在与10名数据工作者（n=10）进行的用户研究中，我们发现参与者可以快速利用AutoHistograms识别见解并探索数据。

    arXiv:2402.14880v1 Announce Type: cross  Abstract: Making sense of unstructured text datasets is perennially difficult, yet increasingly relevant with Large Language Models. Data workers often rely on dataset summaries, especially distributions of various derived features. Some features, like toxicity or topics, are relevant to many datasets, but many interesting features are domain specific: instruments and genres for a music dataset, or diseases and symptoms for a medical dataset. Accordingly, data workers often run custom analyses for each dataset, which is cumbersome and difficult. We present AutoHistograms, a visualization tool leveragingLLMs. AutoHistograms automatically identifies relevant features, visualizes them with histograms, and allows the user to interactively query the dataset for categories of entities and create new histograms. In a user study with 10 data workers (n=10), we observe that participants can quickly identify insights and explore the data using AutoHistogr
    
[^81]: 以人格驱动生成式智能体

    Driving Generative Agents With Their Personality

    [https://arxiv.org/abs/2402.14879](https://arxiv.org/abs/2402.14879)

    大型语言模型（LLMs）利用心理测量值，在视频游戏角色开发中代表给定的人格特征，增强游戏角色的类人特性。

    

    本研究探讨了大型语言模型（LLMs）利用心理测量值，特别是人格信息，在视频游戏角色开发背景下的潜力。情感计算（AC）系统量化了非玩家角色（NPC）的心理，LLM可以利用该系统的信息，使用值进行提示生成。研究表明，LLM可以始终代表给定的人格特征，从而增强游戏角色的类人特性。将人类检查重新用于评估LLM的国际人格项目池（IPIP）问卷表明，该模型能够准确生成与所提供人格相关的内容。结果显示，LLM的改进，如最新的GPT-4模型，可以始终利用和解释人格以代表行为。

    arXiv:2402.14879v1 Announce Type: cross  Abstract: This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.
    
[^82]: 名字的含义是什么？审计大型语言模型中的种族和性别偏见

    What's in a Name? Auditing Large Language Models for Race and Gender Bias

    [https://arxiv.org/abs/2402.14875](https://arxiv.org/abs/2402.14875)

    调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。

    

    我们采用审计设计来调查最先进的大型语言模型中的偏见，包括GPT-4。在我们的研究中，我们引发模型在各种情景下为个人提供建议，比如在购车谈判或选举结果预测过程中。我们发现该建议系统性地对与种族少数群体和女性常见相关的名字产生不利影响。与黑人女性相关的名字得到的结果最不利。这些偏见在42个提示模板和多个模型中都是一致的，表明这是一个系统性问题，而不是孤立事件。在提示中提供数值、与决策相关的锚点可以成功抵消偏见，而定性细节的影响并不一致，甚至可能会加剧差异。我们的研究结果强调了在语言模型部署和实施时进行审计的重要性，以减轻其潜在影响。

    arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
    
[^83]: 蒸馏对比解码：利用对比解码和蒸馏提升LLM的推理能力

    Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation

    [https://arxiv.org/abs/2402.14874](https://arxiv.org/abs/2402.14874)

    该研究提出了一种叫做蒸馏对比解码（DCD）的方法，通过结合对比提示与蒸馏技术，有效提升了大型语言模型（LLM）在推理任务上的性能表现，超过了传统的对比解码方法，并在多个基准数据集上取得了显著成果。

    

    我们提出了一种称为蒸馏对比解码（DCD）的简单方法，以增强大型语言模型（LLMs）在推理过程中的推理能力。与先前依赖于较小的业余模型或隐藏状态差异分析的方法不同，DCD采用了对比式思维引导和先进的蒸馏技术，包括Dropout和量化。这种方法有效地解决了对比解码（CD）的局限性，后者通常需要专家和业余模型，从而增加计算资源需求。通过将对比提示与蒸馏相结合，DCD消除了对业余模型的需求并减少了内存使用。我们的评估表明，DCD显著增强了LLM在各种推理基准测试中的性能，在GSM8K和StrategyQA数据集中均超过了CD和现有方法。

    arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
    
[^84]: Checkfor.ai AI生成文本分类器技术报告

    Technical Report on the Checkfor.ai AI-Generated Text Classifier

    [https://arxiv.org/abs/2402.14873](https://arxiv.org/abs/2402.14873)

    Checkfor.ai AI生成文本分类器在区分大型语言模型生成文本和人类编写文本方面表现优异，提出了硬负挖掘与合成镜像训练算法，具有高准确性和泛化能力。

    

    我们提出了Checkfor.ai文本分类器，这是一个基于Transformer的神经网络，经过训练可以区分由大型语言模型编写的文本和由人类编写的文本。Checkfor.ai在由十种文本领域（学生写作、创意写作、科学写作、书籍、百科全书、新闻、电子邮件、科学论文、简答问答）和8个开源闭源大型语言模型组成的综合基准测试中，表现优于零冲击方法如DetectGPT以及主流商业AI检测工具，误差率降低了9倍以上。我们提出了一种训练算法，即硬负挖掘与合成镜像，使我们的分类器能够在评论等高数据领域实现几个数量级的更低误报率。最后，我们展示了Checkfor.ai不对非母语英语人士产生偏见，并推广到训练过程中未见的领域和模型。

    arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
    
[^85]: 语义镜像越狱:基于遗传算法的针对开源LLM的越狱提示

    Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs

    [https://arxiv.org/abs/2402.14872](https://arxiv.org/abs/2402.14872)

    本文提出了一种语义镜像越狱（SMJ）方法，通过生成在语义上类似于原始问题的越狱提示来绕过LLMs。

    

    大型语言模型（LLMs）通常用于创意写作、代码生成和翻译，根据输入序列生成文本，但容易受到越狱攻击的影响，其中精心设计的提示会导致有害输出。大多数越狱提示方法使用一组越狱模板，然后跟随提出问题，创建越狱提示。然而，现有的越狱提示设计通常存在过多的语义差异，导致无法抵御使用简单语义度量作为阈值的防御。越狱提示在语义上比用于查询的原始问题更加多样化。在本文中，我们介绍了一种称为语义镜像越狱（SMJ）的方法，通过生成在语义上类似于原始问题的越狱提示来绕过LLMs。我们将寻找既满足语义相似性又具有越狱有效性的越狱提示建模为一个多目标优化问题。

    arXiv:2402.14872v1 Announce Type: cross  Abstract: Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization proble
    
[^86]: 基于LLM的多Agent生成公共行政领域语义模板中的半结构文档

    LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain

    [https://arxiv.org/abs/2402.14871](https://arxiv.org/abs/2402.14871)

    该论文创新之处在于将LLMs与提示工程和多Agent系统相结合，以生成符合特定结构的新文档。

    

    在过去几年的数字化过程中，各个领域，特别是公共行政领域中文档的创建和管理变得越来越复杂和多样化。半结构文档需要处理一系列特定数据但没有固定格式，因此不能使用基于模板的解决方案。本文提出了一种新方法，将LLMs与提示工程和多Agent系统相结合，生成符合期望结构的新文档。

    arXiv:2402.14871v1 Announce Type: cross  Abstract: In the last years' digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual prompting with a task descr
    
[^87]: 使用和不使用停用词对阿拉伯文本分类的加权方法的影响

    Effects of term weighting approach with and without stop words removing on Arabic text classification

    [https://arxiv.org/abs/2402.14867](https://arxiv.org/abs/2402.14867)

    本研究比较不同的加权特征方法（二元和词频加权）在文本分类中使用和不使用停用词时的影响，通过评估准确性、召回率、精确度和F-度量值，结果表明停用词的处理方式对文本分类结果具有重要影响。

    

    分类文本是一种将文档分类为预先建立的群组的方法。在分类之前，文本文档必须以适合数据挖掘所使用的算法的方式进行准备和表示。因此，文献中已经创建了许多术语加权策略来增强文本分类算法的功能性。本研究比较了二元和词频加权特征方法对文本分类方法的影响，一次删除停用词和不删除停用词。为了评估先前特征加权方法对分类结果的影响，我们使用了一个包含322份文档的阿拉伯数据集，分为六个主题（农业、经济、健康、政治、科学和体育），每个主题包含50份文档，唯独健康类别除外。

    arXiv:2402.14867v1 Announce Type: cross  Abstract: Classifying text is a method for categorizing documents into pre-established groups. Text documents must be prepared and represented in a way that is appropriate for the algorithms used for data mining prior to classification. As a result, a number of term weighting strategies have been created in the literature to enhance text categorization algorithms' functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on the text's classification method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health categ
    
[^88]: APTQ: 针对大型语言模型的注意力感知后训练混合精度量化

    APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models

    [https://arxiv.org/abs/2402.14866](https://arxiv.org/abs/2402.14866)

    APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率

    

    大型语言模型（LLMs）极大地推动了自然语言处理范式。然而，高计算负载和巨大的模型尺寸对在边缘设备上部署构成了巨大挑战。为此，我们提出了针对LLMs的APTQ（Attention-aware Post-Training Mixed-Precision Quantization），该方法不仅考虑了每层权重的二阶信息，而且首次考虑了注意力输出对整个模型的非线性影响。我们利用Hessian迹作为混合精度量化的敏感度度量，确保经过理性的精度降低能保持模型性能。实验表明，APTQ超越了先前的量化方法，在C4数据集中以平均4位宽度获得5.22困惑度，几乎等效于全精度。此外，APTQ在LLaMa-7B和LLaMa-1中以平均3.8位宽度达到了68.24％和70.48％的最先进零-shot准确率。

    arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
    
[^89]: DyVal 2: 元探测代理动态评估大型语言模型

    DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents

    [https://arxiv.org/abs/2402.14865](https://arxiv.org/abs/2402.14865)

    本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。

    

    大型语言模型（LLMs）的评估引起了社区的极大关注，因为存在数据污染问题。现有工作设计了使用针对特定任务的明确定义算法的评估协议，这些协议无法轻松扩展到不同的场景。此外，当前的评估基准只能提供整体基准结果，不能支持对LLMs能力进行细粒度和多方面的分析。在本文中，我们提出了元探测代理（MPA），这是一种受心理测量学启发的通用动态评估协议，用于评估LLMs。 MPA 是 DyVal 2 的关键组件，自然地扩展了先前的 DyVal。 MPA 设计了探测和评判代理，以自动将原始评估问题转化为一个新问题，遵循心理测量理论在三个基本认知能力上的应用: 语言理解、问题解决和领域知识。

    arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
    
[^90]: 评估一种半自主注意力听系统与接管提示

    Evaluation of a semi-autonomous attentive listening system with takeover prompting

    [https://arxiv.org/abs/2402.14863](https://arxiv.org/abs/2402.14863)

    一种半自主系统，允许远程操作员在实时控制下接管自主专注听取系统，并通过自动检测低兴趣和参与度来为操作员提供明确的接管提示，相较于完全自主系统，该系统被普遍认为更积极。

    

    沟通中断和失去参与度的处理是口语对话系统的重要方面，尤其是对于像专注听取这样的聊天系统，用户主要在说话时。我们认为最适合处理这项任务并挽救对话流程的是人类。为此，我们提出了一个半自主系统，其中远程操作员可以实时控制自主注意力听取系统。为了使人类干预变得简单且一致，我们引入了对低兴趣和参与度的自动检测，以向远程操作员提供明确的接管提示。我们实现了这个半自主系统，该系统检测操作员的接管点，并将其与完全远程操作和完全自主的专注听取系统进行了比较。我们发现半自主系统通常比自主系统更受肯定。结果表明，

    arXiv:2402.14863v1 Announce Type: new  Abstract: The handling of communication breakdowns and loss of engagement is an important aspect of spoken dialogue systems, particularly for chatting systems such as attentive listening, where the user is mostly speaking. We presume that a human is best equipped to handle this task and rescue the flow of conversation. To this end, we propose a semi-autonomous system, where a remote operator can take control of an autonomous attentive listening system in real-time. In order to make human intervention easy and consistent, we introduce automatic detection of low interest and engagement to provide explicit takeover prompts to the remote operator. We implement this semi-autonomous system which detects takeover points for the operator and compare it to fully tele-operated and fully autonomous attentive listening systems. We find that the semi-autonomous system is generally perceived more positively than the autonomous system. The results suggest that i
    
[^91]: 在没有基准实况的情况下对大型语言模型进行排名

    Ranking Large Language Models without Ground Truth

    [https://arxiv.org/abs/2402.14860](https://arxiv.org/abs/2402.14860)

    不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。

    

    随着大型语言模型（LLMs）的普及和影响力的增强，评估和排名LLMs已成为一个重要问题。现有的评估方法要么需要获取昂贵的人类响应，要么使用LLMs成对地互相评估，这可能不够可靠。本文提供了一个新的视角，在给定一组提示数据集（比如问题、说明等）和一组LLMs的情况下，我们在没有任何基准实况或参考响应的情况下对它们进行排名。受到现实生活的启发，其中专家和有知识的人都能识别一个新手，我们的主要思路是考虑模型的三元组，其中每个模型评估其他两个模型，能够以很高的概率正确识别最差的模型。我们还分析了我们的想法并提供了成功的充分条件。通过反复应用这一想法，我们提出了两种对LLMs进行排名的方法。

    arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
    
[^92]: ChatEL: 与聊天机器人一起进行实体链接

    ChatEL: Entity Linking with Chatbots

    [https://arxiv.org/abs/2402.14858](https://arxiv.org/abs/2402.14858)

    ChatEL框架通过三步框架改进了实体链接任务的性能，使得平均F1性能提高超过2％

    

    实体链接（EL）是自然语言处理中一个重要且具有挑战性的任务，旨在将文档或句子中表示实体的一些文本与字典或知识库中相应的条目进行链接。现有的大部分方法都专注于创建复杂的上下文模型，以寻找周围单词的线索来帮助解决链接问题。尽管这些经过调整的语言模型往往有效，但它们可能难以处理，难以训练，并且在其他领域转移效果不佳。幸运的是，像GPT这样的大型语言模型（LLMs）为EL模型中固有问题提供了高度先进的解决方案，但对LLM进行简单的提示并不奏效。在本研究中，我们定义了ChatEL，这是一个三步框架，用于提示LLM返回准确结果。总体而言，ChatEL框架将10个数据集的平均F1性能提高了超过2％。

    arXiv:2402.14858v1 Announce Type: cross  Abstract: Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough
    
[^93]: 大型语言模型中的系统消息对越狱是否真的很重要？

    Is the System Message Really Important to Jailbreaks in Large Language Models?

    [https://arxiv.org/abs/2402.14857](https://arxiv.org/abs/2402.14857)

    系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。

    

    大型语言模型（LLMs）的快速发展使它们在现代社会中不可或缺。尽管通常会采取安全措施在发布前将LLMs与人类价值观保持一致，但最近的研究揭示了一个令人担忧的现象，被称为"越狱"。这个术语指的是当LLMs受到恶意问题提示时产生意外且可能有害的响应。现有研究侧重于生成越狱提示，但我们的研究旨在回答一个不同的问题：系统消息对LLMs中的越狱是否真的很重要？为了回答这个问题，我们在一个稳定的GPT版本gpt-3.5-turbo-0613中进行了实验，生成了具有不同系统消息的越狱提示：短，长和无消息。我们发现不同的系统消息通过实验具有不同的抵抗越狱的能力。此外，我们还探讨了越狱在LLMs之间的可转移性。这一发现强调了系统消息在防止LLMs越狱中的重要性。

    arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
    
[^94]: 在推理思维中比较人类和大型语言模型的推理策略

    Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning

    [https://arxiv.org/abs/2402.14856](https://arxiv.org/abs/2402.14856)

    该研究通过对大型语言模型对命题逻辑问题的响应进行评估，揭示出它们展现出与人类类似的推理模式和策略。

    

    推理思维在制定健全和连贯论点方面扮演了关键角色。它允许个体根据所提供信息的真值得出逻辑上的结论。在大型语言模型（LLMs）领域的最新进展展示了它们在执行演绎推理任务方面的能力。然而，大部分研究主要评估LLMs在解决此类任务中的准确性，往往忽视了对其推理行为进行更深入的分析。在本研究中，我们借鉴认知心理学原理，通过对它们对命题逻辑问题的响应进行详细评估，来研究LLMs采用的推理策略。我们的研究结果表明，LLMs展现出类似于人类观察到的推理模式，包括诸如“假定跟随”或“链构建”等策略。此外，我们的研究证明了arXiv:2402.14856v1 Announce Type: cross

    arXiv:2402.14856v1 Announce Type: cross  Abstract: Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the ar
    
[^95]: 一种适用于可靠透明文本到查询的LLM成熟度模型

    An LLM Maturity Model for Reliable and Transparent Text-to-Query

    [https://arxiv.org/abs/2402.14855](https://arxiv.org/abs/2402.14855)

    这项工作提出了一种适用于文本到查询应用的LLM成熟度模型，不仅关注准确性，还扩展到更多维度。同时，展示了一个用于执法领域的实际案例，介绍了域特定文本到查询助手QueryIQ。

    

    认识到解决大语言模型（LLM）的可靠性和透明性问题的必要性，本研究提出了一种针对文本到查询应用的LLM成熟度模型。该成熟度模型旨在填补在评估LLM在此类应用中的不足，同时纳入了超越纯粹正确性或准确性的维度。此外，该工作引入了执法领域的一个真实用例，并展示了QueryIQ，一个基于LLM的领域特定文本到查询助手，以加速用户工作流程并揭示数据中隐藏的关系。

    arXiv:2402.14855v1 Announce Type: cross  Abstract: Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications. This maturity model seeks to fill the existing void in evaluating LLMs in such applications by incorporating dimensions beyond mere correctness or accuracy. Moreover, this work introduces a real-world use case from the law enforcement domain and showcases QueryIQ, an LLM-powered, domain-specific text-to-query assistant to expedite user workflows and reveal hidden relationship in data.
    
[^96]: 一种可解释的心理健康语言模型的双提示方法

    A Dual-Prompting for Interpretable Mental Health Language Models

    [https://arxiv.org/abs/2402.14854](https://arxiv.org/abs/2402.14854)

    提出了一种双提示方法，结合专家身份和自杀词典与心理健康特定LLM相结合，有效提升了在心理健康分析中的解释性和帮助临床医生评估心理状态进展。

    

    尽管越来越多的人工智能心理健康监测工具的需求增加，但由于缺乏可解释性，它们对临床医生的实际效用有限。CLPsych 2024共享任务旨在通过提供自杀意识的证据来增强大型语言模型（LLM）的可解释性，特别是在心理健康分析领域。我们提出了一种双提示方法：（i）通过利用专家身份和自杀词典与心理健康特定LLM相结合，进行知识感知证据提取；以及（ii）通过使用基于LLM的一致性评估器来进行证据总结。全面的实验证明了结合领域特定信息的有效性，揭示了性能的提升和该方法在帮助临床医生评估心理状态进展方面的潜力。

    arXiv:2402.14854v1 Announce Type: cross  Abstract: Despite the increasing demand for AI-based mental health monitoring tools, their practical utility for clinicians is limited by the lack of interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to enhance the interpretability of Large Language Models (LLMs), particularly in mental health analysis, by providing evidence of suicidality through linguistic content. We propose a dual-prompting approach: (i) Knowledge-aware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental health-specific LLM; and (ii) Evidence summarization by employing an LLM-based consistency evaluator. Comprehensive experiments demonstrate the effectiveness of combining domain-specific information, revealing performance improvements and the approach's potential to aid clinicians in assessing mental state progression.
    
[^97]: 从自然语言查询生成电子表格公式的NL2Formula

    NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries

    [https://arxiv.org/abs/2402.14853](https://arxiv.org/abs/2402.14853)

    提出了NL2Formula任务，旨在通过自然语言查询生成基于电子表格表格的可执行公式，并提供了一个名为fCoder的基准实现。

    

    在电子表格上编写公式，如Microsoft Excel和Google Sheets，是许多进行数据分析的用户广泛使用的做法。然而，对于许多最终用户来说，制作电子表格公式仍然是一项繁琐且容易出错的任务，特别是在处理复杂操作时。为了减轻编写电子表格公式所带来的负担，本文介绍了一个称为NL2Formula的新型基准任务，旨在根据输入的自然语言（NL）查询生成基于电子表格的可执行公式。为此，我们构建了一个包含70,799个配对NL查询和相应电子表格公式的全面数据集，涵盖21,670个表格和37种公式函数类型。我们通过提供一个称为fCoder的基于序列到序列的基准实现来实现NL2Formula任务。实验结果验证了fCoder的有效性，证明了其出色的性能。

    arXiv:2402.14853v1 Announce Type: cross  Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior per
    
[^98]: 最新GPT模型上的HumanEval -- 2024

    HumanEval on Latest GPT Models -- 2024

    [https://arxiv.org/abs/2402.14852](https://arxiv.org/abs/2402.14852)

    使用最新的GPT-4模型在程序合成方面取得显著进展，通过在HumanEval任务中展示了在零样本Python代码生成中的竞争性性能和更多多步骤范式综合。

    

    在2023年，我们正在使用最新的GPT-4模型来推进程序合成。这些大型语言模型显著改进了这一目的的最新技术。为了使这些进展更易于访问，我们创建了一个将这些模型连接到Human Eval的存储库。该数据集最初是为与名为CODEGEN的语言模型在自然语言和编程语言数据上使用而开发的。通过展示这些经过训练的模型在与以前的最先进解决方案相比在HumanEval任务上零样本Python代码生成中的竞争性性能，展示了这些训练模型的效用。此外，这为开发更多的多步骤范式综合创造了可能。这一基准测试包含160个多样化的问题集，这些问题集被分解成多步提示，我们的分析表明这显著改进了单轮输入上的程序综合。所有代码均以开源方式发布在https://github.com/daniel442li/gpt-human-eval。

    arXiv:2402.14852v1 Announce Type: cross  Abstract: In 2023, we are using the latest models of GPT-4 to advance program synthesis. The large language models have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called CODEGEN on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in zero-shot Python code generation on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This benchmark features 160 diverse problem sets factorized into multistep prompts that our analysis shows significantly improves program synthesis over single-turn inputs. All code is open source at https://github.com/daniel442li/gpt-human-eval .
    
[^99]: SQL-CRAFT: 通过交互式改进和增强推理实现文本到SQL转换

    SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning

    [https://arxiv.org/abs/2402.14851](https://arxiv.org/abs/2402.14851)

    提出了SQL-CRAFT框架，通过交互式改进和增强推理，提升了大语言模型在文本到SQL转换任务中的性能，实验结果显示性能提升高达5.7%，并在Spider榜单上超越了当前最先进技术。

    

    现代大语言模型已经变得越来越强大，但在专门任务（如文本到SQL）方面仍面临挑战。我们提出了SQL-CRAFT，一个通过交互式改进和增强推理来提升大语言模型SQL生成能力的框架。我们利用交互式纠错循环（IC-Loop）使大语言模型与数据库自动交互，同时采用增强推理的方法。我们在两个文本到SQL数据集Spider和Bird上进行实验，性能比朴素提示方法提高了高达5.7%。此外，我们的方法在Spider榜单上超过了当前最先进技术，展示了我们框架的有效性。

    arXiv:2402.14851v1 Announce Type: cross  Abstract: Modern LLMs have become increasingly powerful, but they are still facing challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a framework to advance LLMs' SQL generation Capabilities through inteRActive reFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop (IC-Loop) for LLMs to interact with databases automatically, as well as Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets, Spider and Bird, with performance improvements of up to 5.7% compared to the naive prompting method. Moreover, our method surpasses the current state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of our framework.
    
[^100]: CHATATC：用于支持战略空中交通流量管理的大型语言模型驱动的对话系统

    CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management

    [https://arxiv.org/abs/2402.14850](https://arxiv.org/abs/2402.14850)

    本研究探讨了如何将大型语言模型应用于非安全关键的战略交通流量管理环境，提出了一个名为CHATATC的模型，通过训练大量历史数据集实现对话系统，并测试了其查询和响应能力。

    

    生成人工智能（AI）和大型语言模型（LLMs）已经通过诸如ChatGPT等公开可用工具快速走红。LLMs在个人和专业领域的应用得到推动，是由于人类用户与ChatGPT等计算机应用之间自然的互动，以及强大的摘要和文本生成能力。在这项工作中，我们调查了这些生成AI工具如何在非安全关键的战略交通流量管理环境中部署。具体来说，我们基于包含超过80,000个GDP实施、修订和取消的大量历史数据集，对CHATATC进行训练。我们测试了CHATATC的查询和响应能力，记录了成功之处（例如，提供正确的GDP率、持续时间和原因）以及不足之处（例如，最高水平）

    arXiv:2402.14850v1 Announce Type: cross  Abstract: Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative
    
[^101]: 异步和分段的双向编码对神经机器翻译的影响

    Asynchronous and Segmented Bidirectional Encoding for NMT

    [https://arxiv.org/abs/2402.14849](https://arxiv.org/abs/2402.14849)

    本文介绍了一种基于Transformer的改进模型，引入了异步和分段的双向编码策略，以提高神经机器翻译的效率和准确性。

    

    随着神经机器翻译(NMT)的迅速发展，提高翻译效率和质量已成为研究的焦点。本文提出了一种基于Transformer的改进模型，实施了异步和分段的双向解码策略，旨在提高翻译效率和准确性。与传统的从左到右或从右到左的单向翻译相比，我们的方法在处理长句时表现出更高的效率和更好的翻译质量。在IWSLT2017数据集上的实验结果验证了我们方法在加速翻译和提高准确性方面的有效性，尤其是超越了传统的单向翻译。

    arXiv:2402.14849v1 Announce Type: cross  Abstract: With the rapid advancement of Neural Machine Translation (NMT), enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer, implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirection
    
[^102]: 任务相同，令牌更多：输入长度对大型语言模型推理性能的影响

    Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models

    [https://arxiv.org/abs/2402.14848](https://arxiv.org/abs/2402.14848)

    输入长度对大型语言模型的推理性能有显著影响，降级趋势出现在比技术最大值短得多的输入长度下。

    

    本文探讨了扩展输入长度对大型语言模型（LLMs）能力的影响。尽管LLMs在最近取得了进展，但它们在不同输入长度下的性能一致性尚不明确。我们通过引入一种新颖的问答推理框架来研究此方面，该框架专门设计用于评估输入长度的影响。我们通过使用同一样本的多个版本，每个版本都通过不同长度、类型和位置的填充进行了扩展，从而分离了输入长度的影响。我们的研究结果显示，在比它们的技术最大值短得多的输入长度下，LLMs的推理性能明显降低。我们展示了降级趋势出现在我们数据集的每个版本中，尽管强度不同。此外，我们的研究揭示传统的困惑度度量与LLMs在长输入推理任务中的表现没有相关性。我们分析了我们的结果并识别了

    arXiv:2402.14848v1 Announce Type: cross  Abstract: This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs' in long input reasoning tasks. We analyse our results and identif
    
[^103]: 坚持你的角色！个人价值在大型语言模型中的稳定性

    Stick to your Role! Stability of Personal Values Expressed in Large Language Models

    [https://arxiv.org/abs/2402.14846](https://arxiv.org/abs/2402.14846)

    本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。

    

    通过基准测试或心理问卷的标准方式研究大型语言模型(LLMs)是提供许多来源于类似最小背景的不同查询（例如多项选择问题）。然而，由于LLM高度依赖于背景，因此从这种最小背景评估中得出的结论可能对模型在部署中的行为（在那里它将暴露于许多新背景）的说明很少。我们认为，依赖于背景的特性应该作为LLM比较的另一个维度来研究，而不是其他维度，如认知能力、知识或模型大小。在本文中，我们提出了一个关于在不同背景下（模拟对不同话题的对话）价值表达稳定性的案例研究，并使用标准心理学问卷（PVQ）和行为下游任务进行测量。我们考虑了来自五个家族的19个开源LLM。借鉴心理学方法，我们研究了等级稳定性。

    arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
    
[^104]: 通过集成小型语言模型来净化大型语言模型

    Purifying Large Language Models by Ensembling a Small Language Model

    [https://arxiv.org/abs/2402.14845](https://arxiv.org/abs/2402.14845)

    通过将大型语言模型与小型语言模型集成，可以有效净化大型语言模型，保持其性能并减轻版权侵权、数据污染和隐私侵犯等问题

    

    大型语言模型（LLMs）的成功很大程度上取决于从外部（不受信任）来源收集丰富的训练数据。尽管已经付出了大量努力进行数据清洗和精心策划，但已有报道显示构建良好的LLMs存在版权侵权、数据污染和/或隐私侵犯问题，这将阻碍LLMs的实际部署。在本研究中，我们提出了一种简单易行的方法，通过将LLMs与良性小语言模型（SLMs）集成来净化LLMs免受未经筛选数据带来的负面影响。除了理论保证外，我们进行了全面实验，从经验证实，LLMs与SLMs集成可以有效保持LLMs的性能，同时减轻版权侵权、数据污染和隐私侵犯等问题。

    arXiv:2402.14845v1 Announce Type: cross  Abstract: The emerging success of large language models (LLMs) heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs. In this study, we propose a simple and easily implementable method for purifying LLMs from the negative effects caused by uncurated data, namely, through ensembling LLMs with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling LLMs with SLMs, which can effectively preserve the performance of LLMs while mitigating issues such as copyright infringement, data poisoning, and privacy violations.
    
[^105]: 具有强化调节的文本扩散模型

    Text Diffusion with Reinforced Conditioning

    [https://arxiv.org/abs/2402.14843](https://arxiv.org/abs/2402.14843)

    提出了一种名为TREC的文本扩散模型，通过强化调节和时间感知方差缩放解决了现有文本扩散模型在训练过程中自我调节的退化和训练与采样不一致的问题，展示了其在不同序列生成任务中的竞争力。

    

    扩散模型在生成高质量图像、视频和音频方面表现出色，由于在迭代改进中的适应性，它们对实现更好的非自回归序列生成具有潜力。然而，由于处理语言的离散性的挑战，现有的文本扩散模型在性能方面仍然存在不足。本文对文本扩散模型进行了彻底分析，并揭示了两个重要限制：训练过程中自我调节的退化和训练与采样之间的不一致性。在我们的发现的启发下，我们提出了一个名为TREC的新型文本扩散模型，通过强化调节缓解了退化问题，通过时间感知方差缩放解决了不一致性。我们的大量实验证明了TREC在自回归、非自回归和扩散基线中的竞争力。此外，定性分析显示...

    arXiv:2402.14843v1 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis sh
    
[^106]: RJUA-MedDQA：医疗文档问答和临床推理的多模态基准

    RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning

    [https://arxiv.org/abs/2402.14840](https://arxiv.org/abs/2402.14840)

    介绍了RJUA-MedDQA，一个医学专业领域的全面基准，具有挑战性的要求，涉及解释图像内容、数值推理和临床推理能力。

    

    在大型语言模型（LLMs）和大型多模型模型（LMMs）的最新进展中显示出在各种医学应用中的潜力，比如智能医学诊断。尽管取得了令人印象深刻的成果，但我们发现现有的基准并未反映出真实医疗报告的复杂性和专业的深入推理能力。在这项工作中，我们引入了RJUA-MedDQA，这是医学专业领域的一个全面基准，提出了几个挑战：全面解释不同挑战性布局中的图像内容，具备识别异常指标的数值推理能力，并展示临床推理能力，根据医学背景提供疾病诊断、状态和建议的陈述。我们精心设计了数据生成流水线，并提出了旨在恢复文本和表格的高效结构恢复注释（ESRA）方法。

    arXiv:2402.14840v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we introduced RJUA-MedDQA, a comprehensive benchmark in the field of medical specialization, which poses several challenges: comprehensively interpreting imgage content across diverse challenging layouts, possessing numerical reasoning ability to identify abnormal indicators and demonstrating clinical reasoning ability to provide statements of disease diagnosis, status and advice based on medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabu
    
[^107]: RFBES在SemEval-2024任务8中的应用：探究用于区分AI生成和人类撰写文本的句法和语义特征

    RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts

    [https://arxiv.org/abs/2402.14838](https://arxiv.org/abs/2402.14838)

    该研究探究了语义和句法两个方面用于区分AI生成文本和人类撰写文本的问题，并提出了一个高准确度的AI模型，在M4数据集上表现出较好的性能。

    

    最近，大型语言模型（LLMs）的使用越来越广泛，并且LLMs已被用于在不同语言和不同任务中生成文本。此外，由于谷歌和OpenAI等知名公司的参与，LLMs现在更易获得，人们可以轻松使用它们。然而，一个重要问题是如何检测AI生成的文本与人类撰写的文本区别。本文从语义和句法两个方面探讨了AI生成文本检测问题。最终，我们提出了一个AI模型，可以在M4数据集上高准确度区分AI生成文本和人类撰写文本，无论是多语言还是单语任务。根据我们的结果，使用语义方法对于检测更有帮助。然而，在句法方法上还有很大改进空间，这将是未来工作的一个良好途径。

    arXiv:2402.14838v1 Announce Type: cross  Abstract: Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, LLMs are now more accessible, and people can easily use them. However, an important issue is how we can detect AI-generated texts from human-written ones. In this article, we have investigated the problem of AI-generated text detection from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.
    
[^108]: 大型语言模型提示技术的实证分类：从业者指南

    An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide

    [https://arxiv.org/abs/2402.14837](https://arxiv.org/abs/2402.14837)

    编制了一个全面的大型语言模型提示技术清单，并建立了一个跨学科的分类框架，以帮助从业者更有效地利用这些技术。

    

    由于大型语言模型（LLMs）的快速发展，最近用提示语来编程这些模型引起了人们的极大关注。然而，现有提示工程技术的数量庞大，对于希望利用这些工具的从业者来说，这构成了一个令人难以应对的挑战。为了最有效地利用LLMs，编制一个全面的提示技术清单并建立一个标准化的跨学科分类框架是很重要的。本调查研究了一些最知名的提示技术，从学术和实践角度对它们进行了分类，分为七个不同的类别。我们概述了每个类别，旨在澄清它们的独特贡献，并展示它们在真实世界示例中的实际应用，以为同行从业者提供一个结构化框架，帮助他们理解和归类提示技术。

    arXiv:2402.14837v1 Announce Type: cross  Abstract: Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniqu
    
[^109]: 大型语言模型推荐中的隐秘攻击

    Stealthy Attack on Large Language Model based Recommendation

    [https://arxiv.org/abs/2402.14836](https://arxiv.org/abs/2402.14836)

    大型语言模型推荐系统容易受到隐秘攻击，攻击者可以通过微调文本内容在不干预模型训练的情况下显著提高物品的曝光度，而这种攻击对整体推荐性能无影响且难以被检测到。

    

    最近，强大的大型语言模型(LLMs)在推动推荐系统(RS)的进展方面发挥了重要作用。然而，尽管这些系统蓬勃发展，但它们对安全威胁的敏感性却被大多忽视了。在这项工作中，我们揭示了LLMs引入推荐模型中产生新安全漏洞的情况，这是由于它们注重物品的文本内容。我们证明了攻击者可以在测试阶段仅通过改变物品的文本内容显著增加其曝光度，而无需直接干预模型的训练过程。此外，该攻击具有显著的隐秘性，因为它不会影响整体推荐性能，对文本的修改微妙，使用户和平台难以检测到。我们在四个主流的LLM-based推荐模型上进行了全面的实验。

    arXiv:2402.14836v1 Announce Type: cross  Abstract: Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior
    
[^110]: MIKE：细粒度多模态实体知识编辑的新基准

    MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing

    [https://arxiv.org/abs/2402.14835](https://arxiv.org/abs/2402.14835)

    MIKE是一个针对细粒度多模态实体知识编辑的全面基准和数据集，突破了现有基准主要侧重于粗粒度知识的局限性，引入了新的知识编辑形式以评估编辑效率。

    

    多模态知识编辑是增强多模态大语言模型（MLLMs）功能的重要进展。尽管其潜力巨大，但当前的基准主要集中在粗粒度知识上，细粒度多模态实体知识的复杂性大多未被探索。为了弥补这一差距，我们引入了MIKE，这是一个专门为细粒度多模态实体知识编辑设计的全面基准和数据集。

    arXiv:2402.14835v1 Announce Type: cross  Abstract: Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations
    
[^111]: MSynFD: 多跳语法感知假新闻检测

    MSynFD: Multi-hop Syntax aware Fake News Detection

    [https://arxiv.org/abs/2402.14834](https://arxiv.org/abs/2402.14834)

    提出一种新的多跳语法感知假新闻检测方法，通过引入补充的语法信息来处理假新闻中的微妙转折

    

    社交媒体平台的广泛传播助长了假新闻的快速传播，对我们的现实社会构成威胁。现有方法使用多模态数据或上下文信息来增强对假新闻的检测，通过分析新闻内容和/或其社会背景。然而，这些方法常常忽视了基本的文本新闻内容（文章），并且过分依赖序列建模和全局注意力来提取语义信息。这些现有方法无法处理新闻文章中的复杂、微妙的转折，比如句法-语义不匹配和先验偏差，导致性能较低，并在缺失模态或社会背景时可能失败。为了弥合这些重要差距，我们提出了一种新颖的多跳语法感知假新闻检测（MSynFD）方法，该方法融合了补充的语法信息，以处理假新闻中的微妙转折。

    arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
    
[^112]: CliqueParcel：一种同时优化效率和忠实度的批处理LLM提示的方法

    CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness

    [https://arxiv.org/abs/2402.14833](https://arxiv.org/abs/2402.14833)

    CliqueParcel提出了一种通过提示批处理来提高LLM效率的方法，旨在在推理过程中同时确保准确性和最小化与原始输出的偏差，解决了折价输出问题。

    

    大型语言模型（LLM）在最近的研究中变得至关重要。然而，在推理过程中，LLM仍然需要大量资源。本文提出了CliqueParcel，一种旨在通过提示批处理来提高LLM效率的方法。现有的优化推理效率的策略通常会对输出质量进行妥协，导致折价输出问题。这个问题可能导致准确性降低或输出缺乏细节。CliqueParcel是我们对这一挑战的回应。在确保准确性和最小化与原始输出的偏差（即忠实度）的情况下，我们的方法在推理过程中显著提高了效率。为了奠定基础，我们首先通过排除由于长度缩短而导致的运行时间减少来重新定义效率测量标准。然后，我们提供了效率和忠实度之间的全面权衡，以阐明“折价输出”问题的本质。

    arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. 
    
[^113]: Orca-Math：释放小语言模型在小学数学中的潜力

    Orca-Math: Unlocking the potential of SLMs in Grade School Math

    [https://arxiv.org/abs/2402.14830](https://arxiv.org/abs/2402.14830)

    提出了一个基于Mistral-7B的70亿参数的Orca-Math小语言模型，旨在在小学数学中实现更高的准确度。

    

    数学单词问题解决长期以来一直被认为是小语言模型（SLMs）面临的复杂任务。最近的一项研究假设，为了在GSM8K基准测试上实现超过80%的准确度，最小的模型大小需要为340亿个参数。为了用更小的模型达到这一性能水平，研究人员经常训练SLMs生成Python代码或使用工具来帮助避免计算错误。此外，他们使用集成，将多达100次模型运行的输出组合在一起，得到更准确的结果。结果选择是通过共识、多数投票或与SLM一起使用的单独的验证器模型来进行的。集成大大提高了准确性，但随之而来的是对模型的多次调用造成的显著成本增加（例如，Phi-GSM使用前48个来将性能从68.2提升到81.5）。在这项研究中，我们提出了Orca-Math，一个基于Mistral-7B的70亿参数SLM。

    arXiv:2402.14830v1 Announce Type: cross  Abstract: Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achie
    
[^114]: 一种LLM增强的词汇简化对抗编辑系统

    An LLM-Enhanced Adversarial Editing System for Lexical Simplification

    [https://arxiv.org/abs/2402.14704](https://arxiv.org/abs/2402.14704)

    该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。

    

    词汇简化（LS）旨在在词汇级别简化文本。现有方法严重依赖标注数据，这使得在资源匮乏的情况下难以应用。在本文中，我们提出了一种新颖的LS方法，不需要平行语料库。该方法采用对抗编辑系统，并结合混淆损失和不变性损失来预测原始句子中的词汇修改。同时，我们引入了一种创新的LLM增强损失，以将大型语言模型（LLMs）的知识提炼成小型LS系统。通过这种方式，句子中的复杂词被屏蔽，制作了一个基于难度感知的填充模块，用更简单的词替换屏蔽位置。最后，对三个基准LS数据集进行了广泛的实验结果和分析，证明了我们提出方法的有效性。

    arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.
    
[^115]: ConceptMath：用于衡量大型语言模型数学推理能力的双语概念评测基准

    ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models

    [https://arxiv.org/abs/2402.14660](https://arxiv.org/abs/2402.14660)

    介绍了ConceptMath，一种双语的细粒度基准测试，用于评估大型语言模型的概念性数学推理能力，并发现现有模型在不同数学概念上存在显著性能差异，甚至可能在最基本的概念上出现失败。

    

    本文介绍了ConceptMath，这是一个双语（英语和中文），细粒度的基准测试，用来评估大型语言模型（LLMs）的概念性数学推理能力。与评估一般数学推理的传统基准不同，ConceptMath将数学问题系统地组织在数学概念的层次结构下，从而可以以概念为单位准确性评估数学推理。基于我们的ConceptMath，我们评估了广泛范围的LLMs，并观察到现有的LLMs尽管在传统基准上取得了高平均准确性，但在不同数学概念上表现出显著的性能差异，甚至可能在最基本的概念上出现严重失败。此外，我们还介绍了一种有效的微调策略来增强现有LLMs的弱点。最后，我们希望ConceptMath能够指导开发者理解细致的数学推理能力。

    arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai
    
[^116]: OmniPred：语言模型作为通用回归器

    OmniPred: Language Models as Universal Regressors

    [https://arxiv.org/abs/2402.14547](https://arxiv.org/abs/2402.14547)

    本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。

    

    在实验设计的广阔领域中，回归一直是一个强大的工具，可以准确预测系统或模型在给定一组参数的情况下的结果指标，但传统上只限于适用于特定任务的方法。在本文中，我们提出了OmniPred，这是一个用于训练语言模型作为通用端到端回归器的框架，使用来自多样真实世界实验的$(x,y)$评估数据。通过使用源自Google Vizier的数据，这是世界上最大的黑盒优化数据库之一，我们的大量实验表明，仅通过数学参数和值的文本表示，语言模型能够进行非常精确的数值回归，如果有机会训练多个任务，则可以显著优于传统的回归模型。

    arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
    
[^117]: ChatGPT是因果文本挖掘的未来吗？一项全面评估和分析

    Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis

    [https://arxiv.org/abs/2402.14484](https://arxiv.org/abs/2402.14484)

    本研究全面评估了ChatGPT的因果文本挖掘能力，发现ChatGPT在各种数据集上表现良好，但当有足够多的训练数据时，以往的模型仍然超越了它。

    

    因果性在人类认知中至关重要，并在各种研究领域引起关注。随着文本数据量的增加，识别文本数据中的因果关系至关重要，因果文本挖掘在提取有意义模式中发挥着关键作用。本研究对ChatGPT的因果文本挖掘能力进行了全面评估。首先，我们引入了一个超出一般英语数据集的基准，包括领域特定和非英语数据集。我们还提供了一个评估框架，以确保ChatGPT和之前方法之间的公平比较。最后，我们的分析概述了在应用ChatGPT进行因果文本挖掘时的局限性和未来挑战。具体而言，我们的分析表明，ChatGPT对于各种数据集来说都是一个良好的起点。然而，当配备足够数量的训练数据时，以往的模型仍然优于ChatGPT的性能。

    arXiv:2402.14484v1 Announce Type: new  Abstract: Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatG
    
[^118]: Novi jezički modeli za srpski jezik

    Novi jezi\v{c}ki modeli za srpski jezik

    [https://arxiv.org/abs/2402.14379](https://arxiv.org/abs/2402.14379)

    Rad predstavlja novi jezički model za srpski jezik zasnovan na transformerima, obučen na resursima Društva za jezičke resurse i tehnologije, koji će biti upoređen sa deset odabranih modela vektorizacije na četiri zadatka obrade prirodnog jezika.

    

    Rad će ukratko predstaviti istoriju razvoja modela jezika zasnovanih na transformatorima za srpski jezik. Takođe će biti predstavljeni novi modeli za generisanje teksta i vektorizaciju, obučeni na resursima Društva za jezičke resurse i tehnologije. Biće upoređeno deset izabranih modela vektorizacije za srpski jezik, uključujući dva nova, na četiri zadatka obrade prirodnog jezika. Rad će analizirati koji modeli su najbolji za svaki izabrani zadatak, kako njihova veličina i veličina skupova za obuku utiču na performanse na tim zadacima, i koji je optimalni skup za obuku najboljih jezičkih modela za srpski jezik.

    arXiv:2402.14379v1 Announce Type: new  Abstract: The paper will briefly present the development history of transformer-based language models for the Serbian language. Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.
    
[^119]: 在支持数据存在的情况下进行框架构建：以美国经济新闻为例

    Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News

    [https://arxiv.org/abs/2402.14224](https://arxiv.org/abs/2402.14224)

    本文提出了一个计算框架，旨在分析主流媒体在报道经济消息时的编辑选择，通过对经济指标的报道进行框架分析，我们可以理解出版物选择和构架的方式。

    

    主流媒体在选择何事物进行报道以及如何进行报道方面有很大的自由裁量权。这些选择会对人们所了解的信息和随后的行为产生真实世界的影响。然而，缺乏客观的评估编辑选择的度量使得这一领域的研究特别困难。本文认为在一些有支持数据存在的值得报道的话题中，可以提出一个计算框架来分析编辑选择。我们选择经济作为研究重点，因为经济指标的报道为我们提供了一个相对容易确定各种出版物选择和构架的方式。这些指标为我们提供了一个有关经济表现的真实情况，相对于出版物对其进行报道的方式。为了实现这一目标，我们将框架预测定义为一组相互依赖的任务。

    arXiv:2402.14224v1 Announce Type: new  Abstract: The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the gene
    
[^120]: 面向公平文本嵌入的内容条件去偏方法

    Content Conditional Debiasing for Fair Text Embedding

    [https://arxiv.org/abs/2402.14208](https://arxiv.org/abs/2402.14208)

    通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。

    

    在自然语言处理（NLP）中，减轻机器学习模型中的偏见引起了越来越多的关注。然而，只有少数研究集中在公平的文本嵌入上，这对实际应用至关重要且具有挑战性。本文提出了一种学习公平文本嵌入的新方法。我们通过确保在内容条件下敏感属性与文本嵌入之间的条件独立性来实现公平性，同时保持效用权衡。具体来说，我们强制要求具有不同敏感属性但相同内容的文本的嵌入与其对应中立文本的嵌入保持相同的距离。此外，我们通过使用大型语言模型（LLMs）将文本增强为不同的敏感组，来解决缺乏适当训练数据的问题。我们广泛的评估表明，我们的方法有效地提高了公平性同时保持了嵌入的效用。

    arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
    
[^121]: 使推理变得重要：衡量和提高链式思维推理的忠实性

    Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning

    [https://arxiv.org/abs/2402.13950](https://arxiv.org/abs/2402.13950)

    本文研究了大型语言模型的推理过程中的忠实性问题，引入了FRODO框架来改进生成推理步骤和坚固推理的方法

    

    大型语言模型(LLMs)在回答问题之前经过逐步推理已被证明表现更好。然而，模型最终答案与所述推理步骤的忠实程度尚不明确。本文对十二个LLMs进行因果中介分析，以检验LLM生成的中间推理步骤如何影响最终结果，并发现LLMs在生成答案时并不可靠地使用其中间推理步骤。为了解决这个问题，我们介绍了FRODO，一个旨在定制小型LM以生成正确推理步骤并在这些步骤上进行坚固推理的框架。FRODO包括一个推断模块，通过学习使用隐式因果奖励函数生成正确推理步骤，并且一个推理模块，通过学习使用反事实和因果偏好目标在这些中间推理上忠实推理。我们的实验证明F

    arXiv:2402.13950v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that F
    
[^122]: CriticBench: 将大型语言模型作为评论家进行评估

    CriticBench: Evaluating Large Language Models as Critic

    [https://arxiv.org/abs/2402.13764](https://arxiv.org/abs/2402.13764)

    CriticBench是一个旨在全面和可靠地评估大型语言模型的评论能力的新型基准，展示了评论能力与任务、响应质量和模型规模之间的关系。

    

    论文提出了 CriticBench，这是一个旨在全面和可靠地评估大型语言模型（LLMs）的四个关键评论能力维度（反馈、比较、改进和元反馈）的新型基准。CriticBench包含九个不同的任务，每个任务评估LLMs在不同质量细粒度水平上评论响应的能力。对开源和闭源LLMs进行的广泛评估揭示了评论能力与任务、响应质量和模型规模之间有趣的关系。CriticBench的数据集、资源和评估工具包将在https://github.com/gmftbyGMFTBY/Cri上公开发布。

    arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
    
[^123]: 用于医学领域的检索增强生成的基准测试

    Benchmarking Retrieval-Augmented Generation for Medicine

    [https://arxiv.org/abs/2402.13178](https://arxiv.org/abs/2402.13178)

    通过提出首个医学信息检索增强生成评估(MIRAGE)基准测试，并使用MedRAG工具包进行大规模实验，实现了对多个大型语言模型的准确性改进。

    

    大型语言模型(LLMs)在广泛的医学问答任务上取得了最先进的性能，但仍然面临幻觉和过时知识的挑战。检索增强生成(RAG)是一个有前途的解决方案，并得到了广泛采用。然而，RAG系统可能涉及多个灵活的组件，并且缺乏关于各种医学目的的最佳RAG设置的最佳实践。为了系统地评估这些系统，我们提出了医学信息检索增强生成评估(MIRAGE)，这是一个首创的基准测试，包括来自五个医学问答数据集的7,663个问题。利用MIRAGE，我们通过本文介绍的MedRAG工具包，在41种不同语料库、检索器和骨干LLMs的组合上进行了超过1.8万亿的提示标记的大规模实验。总体而言，MedRAG提高了六种不同LLMs的准确性。

    arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs 
    
[^124]: 对大型语言模型知识蒸馏的调查

    A Survey on Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2402.13116](https://arxiv.org/abs/2402.13116)

    本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。

    

    本调查对大型语言模型（LLMs）领域中知识蒸馏（KD）技术进行了深入探讨，重点关注KD在将诸如GPT-4之类的专有巨头的复杂能力转移到可访问的开源模型（如LLaMA和Mistral）中起着关键作用。在不断发展的人工智能领域，本项工作阐明了专有和开源LLMs之间的关键差异，展示了KD如何成为第二者赋予第一者先进功能和细致理解的重要媒介。我们的调查围绕算法、技能和垂直化这三个基础支柱精心构建，全面探讨了KD机制、特定认知能力的增强以及它们在不同领域的实际影响。重要的是，调查引导着数据增强（DA）和KD之间错综复杂的相互作用。

    arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
    
[^125]: 学习检查：释放大型语言模型自我校正的潜力

    Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models

    [https://arxiv.org/abs/2402.13035](https://arxiv.org/abs/2402.13035)

    通过精心设计训练数据和构建检查-校正数据集，本研究增强了大型语言模型的自我校正能力，提高了自我校正的准确性。

    

    大型语言模型（LLMs）在推理能力方面取得了显著进展，不断努力通过自我校正来完善推理。然而，最近的研究表明，没有外部准确知识的自我校正可能存在局限性甚至可能适得其反，这就引发了关于自我校正的限制和有效性的疑问。本文旨在通过精心设计训练数据来增强LLM的自检功能，从而提高自我校正的准确性。我们对数学推理中的错误类型进行了详细分析，并开发了一个量身定制的提示，称为“Step CoT Check”。然后我们构建了一个检查-校正数据集用于训练模型。在将原始CoT数据和检查校正数据整合后进行训练，我们观察到模型可以改善其自检能力，从而提高其自我校正能力并消除了需要

    arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
    
[^126]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^127]: 大型语言模型对意识形态操纵的易感性有多高？

    How Susceptible are Large Language Models to Ideological Manipulation?

    [https://arxiv.org/abs/2402.11725](https://arxiv.org/abs/2402.11725)

    大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。

    

    大型语言模型(LLMs)具有对公众观念和信息互动施加重要影响的潜力。这引发了关于如果这些模型内的意识形态易受操纵可能带来社会影响的担忧。在这项工作中，我们研究了LLMs在学习和泛化意识形态偏见方面的效果。我们的发现揭示了一个令人担忧的脆弱性：仅接触到少量意识形态驱动的样本就会显著改变LLMs的意识形态。值得注意的是，LLMs表现出惊人的能力，能够从一个主题吸收意识形态并将其泛化到甚至不相关的主题上。LLMs的意识形态容易被扭曲的事实强调了恶意行为者故意毒害训练数据或数据注释者无意引入偏见所带来的风险。这也强调了采取强有力措施以减轻这些威胁的迫切性。

    arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf
    
[^128]: 解码新闻叙事：对大型语言模型在框架偏见检测中的关键分析

    Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection

    [https://arxiv.org/abs/2402.11621](https://arxiv.org/abs/2402.11621)

    通过研究GPT-3.5 Turbo、GPT-4和Flan-T5模型在识别新闻标题中框架偏见的性能，发现可解释提示能够显著提高这些模型的可靠性，GPT-4在少射场景中表现较好，而FLAN-T5的表现较差，指出较小模型可能需要更多任务特定微调。

    

    这项工作通过检验GPT-3.5 Turbo、GPT-4和Flan-T5模型在通过零射、少射和可解释提示方法检测新闻标题中框架偏见的表现，为LLMs在社会科学中的适用性不断扩展的研究做出贡献。我们评估的一个关键洞察是，可解释提示在提升这些模型可靠性方面表现出显著效果，凸显了解释设置对于社会科学关于框架偏见的研究的重要性。特别是，GPT-4在提供一系列相关领域内例子时，表现出改进的少射情况下的性能。FLAN-T5的表现不佳表明较小的模型可能需要额外的任务特定微调以识别框架偏见检测。我们的研究还发现模型，特别是GPT-4，经常将情绪语言误解为框架偏见的指标，突显了区分的挑战。

    arXiv:2402.11621v1 Announce Type: new  Abstract: This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distingu
    
[^129]: 逻辑链：基于大型语言模型的基于规则的推理

    Chain of Logic: Rule-Based Reasoning with Large Language Models

    [https://arxiv.org/abs/2402.10400](https://arxiv.org/abs/2402.10400)

    介绍了一种新的提示方法，逻辑链，通过分解和重新组合来促进基于规则的推理，受到律师使用的序贯推理方法的启发。

    

    基于规则的推理是一种基本的法律推理类型，它使我们能够通过准确地将规则应用于一组事实来得出结论。我们探讨了因果语言模型作为基于规则的推理者，特别是关于组合规则 - 由多个元素组成形成复杂逻辑表达式的规则。推理组合规则具有挑战性，因为它需要多个推理步骤，并且需要关注元素之间的逻辑关系。我们引入了一种新的提示方法，逻辑链，通过分解（将元素作为独立的逻辑线索解决）和重新组合（重新组合这些子答案以解决潜在的逻辑表达式）。这种方法受到了IRAC（问题、规则、应用、结论）框架的启发，这是律师使用的一种序贯推理方法。我们在八个基于规则的推理任务中评估了逻辑链。

    arXiv:2402.10400v1 Announce Type: new  Abstract: Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks in
    
[^130]: 一种具有长期上下文概要记忆的人工智能阅读代理

    A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts

    [https://arxiv.org/abs/2402.09727](https://arxiv.org/abs/2402.09727)

    ReadAgent是一个具有长期上下文概要记忆的阅读代理系统，通过实现一个简单的提示系统，它能够处理长输入并提高有效上下文长度。在评估中表现良好。

    

    当前的大型语言模型不仅限制在某个最大上下文长度内，而且无法稳定地处理长输入。为了解决这些限制，我们提出了ReadAgent，一个增加了有效上下文长度的语言模型代理系统，在我们的实验中可以达到20倍。受到人类交互式阅读长文档的启发，我们将ReadAgent实现为一个简单的提示系统，利用LLM的高级语言能力来：（1）决定将哪些内容存储在一个记忆片段中，（2）将这些记忆片段压缩成为称为概要记忆的短时记忆，（3）在需要时通过原始文本查找段落来提醒自己相关细节以完成任务。我们使用检索方法、使用原始长上下文以及使用概要记忆来评估ReadAgent与基线的性能。这些评估是在三个长文档阅读理解任务上进行的。

    arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
    
[^131]: 朝着更好的人机对齐方向：评估LLM驱动应用中的任务效用

    Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications

    [https://arxiv.org/abs/2402.09015](https://arxiv.org/abs/2402.09015)

    本研究引入了AgentEval框架，用于评估LLM驱动应用的任务效用。该框架通过自动提出一套针对特定应用的评估标准，简化了效用验证过程，并对应用的效用进行了全面量化分析。

    

    大型语言模型（LLM）领域的快速发展导致了一系列应用的出现，这些应用通过协助多个代理人与人类合作，帮助人们完成日常任务。然而，目前仍存在一个重大问题，即如何评估LLM驱动应用是否真正提升用户体验和任务执行效率。这凸显了验证LLM驱动应用效用的方法的迫切需求，特别是要确保应用程序的功能与最终用户的需求相一致。我们引入了AgentEval，它提供了一个实施数学问题的估测模型，这是一个新的框架，旨在通过自动提出一套针对任何给定应用程序独特目标的评估标准，简化效用验证过程。这样可以对应用程序的效用进行全面评估，并量化其与建议标准相比的表现。我们对该框架的稳健性进行了全面的分析。

    arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
    
[^132]: 审计反火：评估具有证据和风格的先进反驳生成

    Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style

    [https://arxiv.org/abs/2402.08498](https://arxiv.org/abs/2402.08498)

    这项研究提出了一个新的数据集，用于生成具有证据和风格的反驳，该数据集基于Reddit ChangeMyView数据集中的帖子，并可用于论证的改进和评估。评估结果显示，GPT-3.5 turbo模型在论证质量方面表现出色，并且具有很高的风格融合能力。互惠式反驳的效果最佳。

    

    我们提出了一个新颖的数据集，用于控制性反驳的合成，旨在进一步应用于论证的改进、挖掘和评估。我们的数据集包含与Reddit ChangeMyView数据集中的帖子相结合的丰富的反驳，这些反驳融入了从高质量来源中检索到的证据，并根据用户偏好生成，调整了证据和论证风格的关键属性。由此产生的Counterfire语料库包括从GPT-3.5 turbo、Koala和PaLM 2模型以及它们的两个微调变体生成的论证（N = 32,000）。模型评估表明，在证据方面具有强大的改写能力，尽管词汇重叠有限，同时表现出高度的风格融合（对于“互惠”的得分为0.9682），显示了LLM融合多样风格的能力。在所有模型中，GPT-3.5 turbo在论证质量评估中显示出最高分数，表现出一致准确性（得分 >0.8）。在进一步的分析中，互惠式反驳证明效果最佳，能够产生更好的论证结果。

    We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterargument
    
[^133]: UFO: 一个专注于Windows操作系统交互的用户界面智能体

    UFO: A UI-Focused Agent for Windows OS Interaction

    [https://arxiv.org/abs/2402.07939](https://arxiv.org/abs/2402.07939)

    UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。

    

    我们介绍了UFO，一个创新的专注于Windows操作系统上应用程序的用户界面智能体，利用了GPT-Vision的能力来满足用户需求。UFO采用双智能体框架，精确观察和分析Windows应用程序的图形用户界面（GUI）和控制信息。这使得智能体可以无缝地在单个应用程序内以及跨应用程序进行导航和操作，以满足用户的需求，即使涉及多个应用程序。该框架包括一个控制交互模块，实现无需人工干预的动作连接，并实现完全自动化执行。因此，UFO将艰巨而耗时的过程转变为仅通过自然语言命令就可以完成的简单任务。我们在9个流行的Windows应用程序上对UFO进行了测试，涵盖了反映用户日常使用情景的各种情况。通过定量指标和真实案例研究得出的结果强调了UFO的效果。

    We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
    
[^134]: 为帮助中国Python编程学习者提供的一个带有注释的问答数据集

    QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners

    [https://arxiv.org/abs/2402.07913](https://arxiv.org/abs/2402.07913)

    为解决编程智能教育系统中数据稀缺问题，本文提出了一个新的针对Python学习者的中文问答数据集，通过收集与分类真实学生问题，提高在线编程教育的效果和质量。

    

    在在线学习平台中，特别是在快速增长的计算机编程课程中，解答成千上万学生的学习问题需要相当大的人力成本。为编程教育定制智能助手大型语言模型（LLMs）的创建需要独特的数据支持。然而，在实际应用场景中，用于训练此类LLMs的数据资源相对稀缺。因此，为了解决编程智能教育系统中的数据稀缺问题，本文提出了一个新的针对Python学习者的中文问答数据集。为确保问题的来源的真实性和可靠性，我们收集了实际学生提出的问题，并根据问题的类型和学习者的类型进行分类。这种注释原则旨在提高在线编程教育的效果和质量，为开发这方面的工作提供坚实的数据基础。

    In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing th
    
[^135]: 合并事实，塑造谬误：评估长文生成中聚合事实性主张的矛盾性质

    Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations

    [https://arxiv.org/abs/2402.05629](https://arxiv.org/abs/2402.05629)

    该论文研究了长文生成中的事实性问题，发现大型语言模型在生成段落时可能会由于实体模糊而将可验证的事实组合成非事实的段落。现有的事实准确度评估方法无法正确评估这些非事实段落，作者提出了一种增强的度量指标来应对这个问题。

    

    大型语言模型（LLMs）产生的长文生成物包含了一系列事实和非事实的主张，这使得评估事实性变得困难。为了以更精细的方式评估长文生成物的事实准确性，先前的研究提出将长文生成物分解为多个可验证的事实并独立验证这些事实。生成物的事实性是所有事实中可验证事实的比例。这些方法假设结合了事实主张形成了一个事实性段落。本文展示了这一假设可能因为实体模糊而被违反。我们展示了LLMs可以生成包含可验证事实的段落，但由于实体模糊，这些事实被结合形成了一个非事实的段落。我们进一步揭示了现有的事实准确度度量指标，包括FActScore和引用回顾，无法正确评估这些非事实段落的事实性。为了解决这个问题，我们引入了一种增强度量指标，D-FActScore，作为一个具体的解决方案。

    Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifi
    
[^136]: KICGPT: 具备上下文知识的大型语言模型用于知识图谱补全

    KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.02389](https://arxiv.org/abs/2402.02389)

    本文提出了KICGPT，它是一个集成了大型语言模型和基于三元组的知识图谱补全检索器的框架。它通过知识提示的上下文学习策略，缓解了长尾问题，并且无需额外的训练开销。实验证明了其有效性。

    

    知识图谱补全对于解决知识图谱不完整性和支持下游应用至关重要。已经提出了许多用于知识图谱补全的模型，它们可以分为基于三元组和基于文本的方法两类。基于三元组的方法由于结构信息有限和实体分布不均衡而困难重重。基于文本的方法可以缓解这个问题，但需要昂贵的语言模型训练和特定的知识图谱微调，从而限制了其效率。为了解决这些限制，本文提出了KICGPT，一种集成了大型语言模型(LLM)和基于三元组的知识图谱补全检索器的框架。它可以缓解长尾问题，而不会增加额外的训练开销。KICGPT使用了一种上下文学习策略，称为知识提示，它将结构知识编码为演示，以引导LLM的学习。在基准数据集上的实证结果证明了其有效性。

    Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven
    
[^137]: 在分析课堂对话时评估大型语言模型

    Evaluating Large Language Models in Analysing Classroom Dialogue

    [https://arxiv.org/abs/2402.02380](https://arxiv.org/abs/2402.02380)

    本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。

    

    本研究探讨了大型语言模型（LLM），特别是GPT-4，在分析课堂对话中的应用，这是教学诊断和质量改进的重要研究任务。鉴于传统教育研究中知识密集和劳动密集的定性方法，本研究调查了LLM在优化和增强分析过程方面的潜力。该研究涉及中学的数据集，包括数学和语文课堂上的对话。这些对话由教育专家手动编码，然后使用定制的GPT-4模型进行分析。本研究侧重于比较手动注释与GPT-4的输出，以评估其在分析教育对话方面的效果。评估时间效率、编码者间一致性和编码者间可靠性之间的差异。结果表明，使用GPT-4可以显著节省时间，并在编码一致性方面具有很高的一致性。

    This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
    
[^138]: 实现语言模型对齐的高效精确优化方法

    Towards Efficient and Exact Optimization of Language Model Alignment

    [https://arxiv.org/abs/2402.00856](https://arxiv.org/abs/2402.00856)

    本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。

    

    将语言模型与人类偏好进行对齐对于其在实际任务中的应用至关重要。该问题被建模为优化模型策略，以最大化反映人类偏好的预期奖励，并尽量减小与初始策略的偏差。尽管强化学习（RL）被认为是一种直接的解决方案，但其策略更新的方差很高，阻碍了高效的策略改进。最近，直接偏好优化（DPO）被提出以直接从偏好数据中优化策略。尽管实现简单，DPO是基于不一定能在实践中实现的最优策略导出的，这削弱了其收敛到预期解决方案的能力。本文提出了一种高效精确优化（EXO）的对齐目标方法。我们证明了对于策略的任意参数化，EXO保证渐近地与RL算法的优化方向一致，并且能够实现高效优化。

    The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op
    
[^139]: PythonSaga：重新定义评估代码生成LLM的基准

    PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM

    [https://arxiv.org/abs/2401.03855](https://arxiv.org/abs/2401.03855)

    PythonSaga提出了一种新的基准，针对Python代码生成进行评估,弥补了现有基准存在的编程概念偏见和简单任务普遍性的问题

    

    受到使用大型语言模型(LLMs)生成代码激增的推动，出现了许多基准用于评估这些LLMs的功能。我们对HumanEval和MBPP两个流行的Python代码生成基准进行了大规模人工评估，分析了它们的多样性和难度。我们的研究揭示了对一组有限的编程概念存在严重偏见，完全忽视了大多数其他概念。此外，我们发现了大量简单任务的普遍存在，可能夸大了模型性能的估计。为了解决这些限制，我们提出了一种新颖的基准，PythonSaga，包含了185个手工制作的提示，涵盖了38个不同难度级别的编程概念。

    arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
    
[^140]: 探索ChatGPT在古代汉语翻译和人名识别中的能力

    Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person Name Recognition

    [https://arxiv.org/abs/2312.15304](https://arxiv.org/abs/2312.15304)

    本论文探索了ChatGPT在古代汉语翻译和人名识别方面的能力，并发现其在翻译方面的表现仍有待提高，最佳表现是在输入三个上下文句子时实现的。

    

    ChatGPT在处理现代标准语言方面的熟练表明其具有潜力用于理解古代汉语。本文通过两项任务探讨了ChatGPT在古代汉语方面的能力：将古代汉语翻译为现代汉语和识别古代汉语人名。通过将ChatGPT的输出与人类翻译进行比较，评估其对古代汉语的理解。研究发现：（1.）ChatGPT对古代汉语的熟练程度尚未达到令人满意的水平；（2.）在输入三个上下文句子时，ChatGPT在古代汉语到现代汉语的翻译中表现最佳。为了帮助重现我们的工作，我们展示了本研究中使用的Python代码片段。

    arXiv:2312.15304v2 Announce Type: replace-cross  Abstract: ChatGPT's proficiency in handling modern standard languages suggests potential for its use in understanding ancient Chinese. This paper explores ChatGPT's capabilities on ancient Chinese via two tasks: translating ancient Chinese to modern Chinese and recognizing ancient Chinese names. A comparison of ChatGPT's output with human translations serves to evaluate its comprehension of ancient Chinese. The findings indicate that: (1.)the proficiency of ancient Chinese by ChatGPT is yet to reach a satisfactory level; (2.) ChatGPT performs the best on ancient-to-modern translation when feeding with three context sentences. To help reproduce our work, we display the python code snippets used in this study.
    
[^141]: 结构化概率编码

    Structured Probabilistic Coding

    [https://arxiv.org/abs/2312.13933](https://arxiv.org/abs/2312.13933)

    结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。

    

    本论文提出了一种新的监督式表示学习框架，即结构化概率编码（SPC），用于从与目标任务相关的输入中学习紧凑和信息丰富的表示。SPC是一种仅有编码器的概率编码技术，具有来自目标空间的结构化正则化。它可以提高预训练语言模型的泛化能力，以实现更好的语言理解。具体而言，我们的概率编码在一个模块中同时进行信息编码和任务预测，以更充分地利用输入数据中的有效信息。它使用输出空间的变分推断来减少随机性和不确定性。此外，为了更好地控制概率表示的学习过程，在潜在空间中提出了结构化正则化，以促进类别之间的均匀性。通过正则化项，SPC可以保持潜在编码的高斯结构，并实现更好的覆盖率。

    This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
    
[^142]: 数学语言模型: 一项调查

    Mathematical Language Models: A Survey

    [https://arxiv.org/abs/2312.07622](https://arxiv.org/abs/2312.07622)

    该调查论文系统地概述了近年来在数学领域中利用语言模型取得的显著进展，包括对数学LLMs的分类和对超过60个数学数据集的编制，为数学LM领域未来的发展指明了方向。

    

    近年来，在数学领域中利用语言模型（LMs），包括预训练语言模型（PLMs）和大规模语言模型（LLMs），取得了显著进展。本文对数学LMs进行了全面调查，系统地从两个不同的视角对重要的研究努力进行了分类：任务和方法论。调查结果显示出大量提出的数学LLMs，进一步划分为指令学习、基于工具的方法、基础CoT技术和高级CoT方法。此外，我们的调查包括编制了60多个数学数据集，包括训练数据集、基准数据集和增强数据集。解决主要挑战，并勾勒数学LM领域未来的发展轨迹，本调查被定位为一个有价值的资源，旨在促进并激励未来的创新。

    arXiv:2312.07622v3 Announce Type: replace  Abstract: In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics. This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies. The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, and advanced CoT methodologies. In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets. Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is positioned as a valuable resource, poised to facilitate and inspire future inn
    
[^143]: 重新审视标签平滑在增强文本情感分类中的作用

    Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification

    [https://arxiv.org/abs/2312.06522](https://arxiv.org/abs/2312.06522)

    通过在文本情感分类中进行深入分析，发现标签平滑可以加速深度模型的收敛，并使不同标签的样本更容易区分

    

    标签平滑是一种广泛应用于各个领域的技术，如文本分类、图像分类和语音识别，以有效对抗模型过拟合而闻名。然而，对于标签平滑如何增强文本情感分类的细致分析却很少。为了填补这一空白，本文在八个文本情感分类数据集和三种深度学习架构（TextCNN、BERT和RoBERTa）以及两种学习方案下进行了一系列深入分析。通过调整平滑参数，我们可以在每个模型架构的几乎所有数据集上实现性能提升。我们进一步研究了标签平滑的好处，发现标签平滑可以加速深度模型的收敛，并使不同标签的样本更容易区分。

    arXiv:2312.06522v2 Announce Type: replace-cross  Abstract: Label smoothing is a widely used technique in various domains, such as text classification, image classification and speech recognition, known for effectively combating model overfitting. However, there is little fine-grained analysis on how label smoothing enhances text sentiment classification. To fill in the gap, this article performs a set of in-depth analyses on eight datasets for text sentiment classification and three deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning. By tuning the smoothing parameters, we can achieve improved performance on almost all datasets for each model architecture. We further investigate the benefits of label smoothing, finding that label smoothing can accelerate the convergence of deep models and make samples of different labels easily distinguishable.
    
[^144]: 分析LLMs的固有响应倾向：真实世界指令驱动的越狱

    Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak

    [https://arxiv.org/abs/2312.04127](https://arxiv.org/abs/2312.04127)

    该研究引入了一种新颖的自动越狱方法RADIAL，通过放大LLMs生成肯定响应的潜力来绕过安全机制，实现了对英语恶意指令的优秀攻击性能。

    

    大量工作致力于改善大型语言模型（LLMs）的安全机制。然而，当面对恶意指令时，LLMs仍然倾向于生成有害响应，这种现象被称为“越狱攻击”。 在我们的研究中，我们介绍了一种新颖的自动越狱方法RADIAL，通过放大LLMs生成肯定响应的潜力来绕过安全机制。我们方法的越狱思想是“固有响应倾向分析”，它识别出那些在本质上可以导致LLMs生成肯定响应的真实世界指令，相应的越狱策略是“真实世界指令驱动的越狱”，它涉及通过在恶意指令周围策略性地拼接通过上述分析识别出的真实世界指令。我们的方法在五个开源先进的LLMs上对英语恶意指令取得了优秀的攻击性能。

    arXiv:2312.04127v2 Announce Type: replace  Abstract: Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as "Jailbreak Attack". In our research, we introduce a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses. The jailbreak idea of our method is "Inherent Response Tendency Analysis" which identifies real-world instructions that can inherently induce LLMs to generate affirmation responses and the corresponding jailbreak strategy is "Real-World Instructions-Driven Jailbreak" which involves strategically splicing real-world instructions identified through the above analysis around the malicious instruction. Our method achieves excellent attack performance on English malicious instructions with five open-source advanced LLMs wh
    
[^145]: 使用合成数据的经过精炼的LLMs自我批评：一种贝叶斯视角

    Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective

    [https://arxiv.org/abs/2312.01957](https://arxiv.org/abs/2312.01957)

    本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。

    

    本文提出了将RLAIF解释为贝叶斯推断的方法，通过引入经过精炼的自我批评(dSC)，该方法通过Gibbs采样器对LLM的输出进行精炼，然后将其蒸馏成一个微调模型。只需要合成数据，dSC在涉及安全性、情感和隐私控制的实验中得到了应用，表明它可以作为对齐LLMs的一种可行且廉价的替代方案。代码在\url{https://github.com/vicgalle/distilled-self-critique}上发布。

    arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
    
[^146]: 大型语言模型的虚假信息能力

    Disinformation Capabilities of Large Language Models

    [https://arxiv.org/abs/2311.08838](https://arxiv.org/abs/2311.08838)

    本文研究了当前一代大型语言模型在生成英语虚假新闻文章方面的能力，发现它们能够生成令人信服的支持危险虚假信息的新闻文章。

    

    自动虚假信息生成经常被列为大型语言模型（LLMs）相关的重要风险。在信息空间中充斥虚假信息内容的理论能力可能对全球社会产生重大影响。本文对当前一代LLMs的虚假信息能力进行了全面研究，生成了英语虚假新闻文章。在研究中，我们使用20个虚假信息叙事评估了10个LLMs的能力。我们评估了LLMs的几个方面：它们生成新闻文章的效果如何，它们倾向于支持或反对虚假叙事的程度如何，它们生成安全警告的频率等。我们还评估了检测模型检测这些文章是否为LLM生成。我们得出结论，LLMs能够生成令人信服的新闻文章，支持危险的虚假信息。

    arXiv:2311.08838v2 Announce Type: replace  Abstract: Automated disinformation generation is often listed as an important risk associated with large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in the English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how good they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation na
    
[^147]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^148]: AMBER：一种无需LLM的多维基准用于评估MLLM的幻象

    AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation

    [https://arxiv.org/abs/2311.07397](https://arxiv.org/abs/2311.07397)

    提出了一种无需LLM的多维基准AMBER，可用于评估MLLM的幻象，设计了低成本高效的评估流程，并对主流MLLMs进行了全面评估和分析

    

    尽管在多模态任务上取得了显著进展，但当前的多模态大型语言模型（MLLMs）面临幻觉的重大挑战，这可能导致有害后果。因此，评估MLLMs的幻觉对于模型改进和实际应用部署变得越来越重要。先前的工作在高评估成本（例如依赖人类或高级LLMs）和评估维度不足（例如任务和幻觉类型）方面存在局限性。在本文中，我们提出了一种无需LLM的多维基准AMBER，可用于评估生成任务和判别任务，包括存在、属性和关系幻觉。基于AMBER，我们设计了一个低成本高效的评估流程。此外，我们对主流MLLMs进行了全面评估和详细分析，包括GPT-4V(ision)，并提出了指导建议。

    arXiv:2311.07397v2 Announce Type: replace  Abstract: Despite making significant progress in multi-modal tasks, current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences. Therefore, evaluating MLLMs' hallucinations is becoming increasingly important in model improvement and practical application deployment. Previous works are limited in high evaluation costs (e.g., relying on humans or advanced LLMs) and insufficient evaluation dimensions (e.g., types of tasks and hallucinations). In this paper, we propose an LLM-free multi-dimensional benchmark AMBER, which can be used to evaluate both generative task and discriminative task including existence, attribute and relation hallucination. Based on AMBER, we design a low-cost and efficient evaluation pipeline. Additionally, we conduct a comprehensive evaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision), and also give guideline suggest
    
[^149]: 咖啡：通过反馈修复错误来提升代码LLMs

    Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback

    [https://arxiv.org/abs/2311.07215](https://arxiv.org/abs/2311.07215)

    开源代码LLMs难以生成正确指导的反馈，本研究提出了Coffee框架，旨在利用Coffee数据集构建CoffeePots，通过优化调整和选择，实现自动生成带有正确指导的反馈以用于代码修复。

    

    arXiv:2311.07215v2 公告类型：替换 摘要：代码编辑是确保程序综合的一个重要步骤，可以自动纠正代码LLMs生成的关键错误。最近的研究表明，闭源LLMs（如ChatGPT和GPT-4）能够生成纠正性反馈，用于编辑错误输入。然而，开源代码LLMs生成用于代码编辑的反馈仍然具有挑战性，因为这些模型倾向于遵循表面格式提供与误导信息相混淆的反馈。因此，我们的工作重点是利用开源代码LLMs生成具有正确指导的有用反馈用于代码编辑。为此引入了Coffee，一个专为带有反馈的代码修复而设计的数据集。利用该数据集，构建了CoffeePots，一个通过偏好优化调整和选择的COde Fixing with FEEdback框架。该框架旨在自动生成有用的反馈以帮助代码编辑。

    arXiv:2311.07215v2 Announce Type: replace  Abstract: Code editing is an essential step towards reliable program synthesis to automatically correct critical errors generated from code LLMs. Recent studies have demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable of generating corrective feedback to edit erroneous inputs. However, it remains challenging for open-source code LLMs to generate feedback for code editing, since these models tend to adhere to the superficial formats of feedback and provide feedback with misleading information. Hence, the focus of our work is to leverage open-source code LLMs to generate helpful feedback with correct guidance for code editing. To this end, we present Coffee, a collected dataset specifically designed for code fixing with feedback. Using this dataset, we construct CoffeePots, a framework for COde Fixing with FEEdback via Preference-Optimized Tuning and Selection. The proposed framework aims to automatically generate helpful 
    
[^150]: 探索微调语言模型中的记忆能力

    Exploring Memorization in Fine-tuned Language Models

    [https://arxiv.org/abs/2310.06714](https://arxiv.org/abs/2310.06714)

    在微调语言模型过程中，该研究首次全面分析了不同任务中模型的记忆现象，发现了记忆在各种微调任务中表现出显著的差异，并通过稀疏编码理论解释了这种任务差异性。

    

    大型语言模型（LLMs）展现出在各种任务中的巨大能力，但同时也表现出对训练数据的记忆，引起了巨大的隐私和版权担忧。 在此工作中，我们进行了首次全面分析，探讨了在各种任务中微调语言模型（LMs）时的记忆现象。 我们使用开源和我们自己的微调LMs进行了研究，结果表明在不同微调任务中，记忆呈现出较强的差异性。 我们通过稀疏编码理论提供了对这种任务差异性的直观解释，并揭示了记忆和注意力分数之间的强相关性。

    arXiv:2310.06714v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models' (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention scor
    
[^151]: 谁编写了这段代码？用于代码生成的水印技术

    Who Wrote this Code? Watermarking for Code Generation

    [https://arxiv.org/abs/2305.15060](https://arxiv.org/abs/2305.15060)

    基于代码独特的句法和语义特征，提出了一种新的水印方法SWEET，通过在具有高熵的位置仅放置“绿色”令牌来确保生成代码的正确性，并通过统计测试和Z分数进行检测。

    

    随着大型语言模型的出色生成性能，关于使用它们的道德和法律问题日益受到关注，如抄袭和版权问题。为了应对这些问题，最近提出了几种用于水印和检测LLM生成文本的方法。然而，我们发现先前的方法由于代码的句法和语义特征，无法有效地应用于代码生成任务。基于Kirchenbauer等人的研究，我们提出了一种新的水印方法，名为Selective WatErmarking via Entropy Thresholding（SWEET），该方法仅在生成期间将“绿色”令牌放置在具有高熵的令牌分布位置，从而保留生成代码的正确性。水印代码通过基于熵信息的统计测试和Z分数进行检测。我们在HumanEval和MBPP上的实验表明，SWEET显著改善了生成代码的质量。

    arXiv:2305.15060v3 Announce Type: replace  Abstract: With the remarkable generation performance of large language models, ethical and legal concerns about using them have been raised, such as plagiarism and copyright issues. For such concerns, several approaches to watermark and detect LLM-generated text have been proposed very recently. However, we discover that the previous methods fail to function appropriately with code generation tasks because of the syntactic and semantic characteristics of code. Based on \citet{Kirchenbauer2023watermark}, we propose a new watermarking method, Selective WatErmarking via Entropy Thresholding (SWEET), that promotes "green" tokens only at the position with high entropy of the token distribution during generation, thereby preserving the correctness of the generated code. The watermarked code is detected by the statistical test and Z-score based on the entropy information. Our experiments on HumanEval and MBPP show that SWEET significantly improves th
    
[^152]: 针对文档图像中的少样本实体识别：一种对图像操作鲁棒的图神经网络方法

    Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation

    [https://arxiv.org/abs/2305.14828](https://arxiv.org/abs/2305.14828)

    本文提出了一种针对文档图像中的少样本实体识别问题的图神经网络方法，通过引入标记之间的拓扑相邻关系，强调相对位置信息，实现对图像操作具有鲁棒性。

    

    最近，将布局信息（通常为边界框坐标）融入预训练语言模型，在文档图像中实现了实体识别取得了显著的性能。然而，使用坐标可以轻松地建模每个标记的绝对位置，但在文档图像（如平移、旋转或缩放等）的操纵下可能很敏感，尤其是在少样本设置下训练数据有限的情况下。为了解决这一问题，我们进一步引入了标记之间的拓扑相邻关系，强调它们的相对位置信息。具体而言，我们将文档中的标记视为节点，并根据最近邻边界框的拓扑启发式形成边。这些相邻图对仿射变换（包括平移、旋转和缩放）不变。我们通过将这些图结构结合到预训练语言模型中来实现这一目标。

    arXiv:2305.14828v2 Announce Type: replace  Abstract: Recent advances of incorporating layout information, typically bounding box coordinates, into pre-trained language models have achieved significant performance in entity recognition from document images. Using coordinates can easily model the absolute position of each token, but they might be sensitive to manipulations in document images (e.g., shifting, rotation or scaling), especially when the training data is limited in few-shot settings. In this paper, we propose to further introduce the topological adjacency relationship among the tokens, emphasizing their relative position information. Specifically, we consider the tokens in the documents as nodes and formulate the edges based on the topological heuristics from the k-nearest bounding boxes. Such adjacency graphs are invariant to affine transformations including shifting, rotations and scaling. We incorporate these graphs into the pre-trained language model by adding graph neura
    
[^153]: 有重要影响的词语：负面词汇对新闻情绪和股市指数的影响

    Words that Matter: The Impact of Negative Words on News Sentiment and Stock Market Index

    [https://arxiv.org/abs/2304.00468](https://arxiv.org/abs/2304.00468)

    负面词汇对新闻情绪评分的负面性产生显著影响，增强的情绪词典更有效地捕捉新闻情绪对股市指数的影响。

    

    本研究调查了负面词汇对情绪分析的影响以及其对韩国股市指数KOSPI200的影响。研究使用Word2Vec、余弦相似度和扩展词汇表分析了45,723篇韩国日常经济新闻文章的数据集。研究发现，将负面词汇纳入情绪评分显著增加了新闻标题中的负面性，这可能会影响股市指数。研究表明，一个增强的情绪词典（Sent1000），包括与“危机”具有高余弦相似度的前1,000个负面词汇，比原始情绪词典（Sent0）更有效地捕捉新闻情绪对股市指数的影响。结果强调了在分析新闻内容及其对市场动态和公众舆论的潜在影响时考虑负面细微差别和语境的重要性。

    arXiv:2304.00468v2 Announce Type: replace  Abstract: This study investigates the impact of negative words on sentiment analysis and its effect on the South Korean stock market index, KOSPI200. The research analyzes a dataset of 45,723 South Korean daily economic news articles using Word2Vec, cosine similarity, and an expanded lexicon. The findings suggest that incorporating negative words significantly increases sentiment scores' negativity in news titles, which can affect the stock market index. The study reveals that an augmented sentiment lexicon (Sent1000), including the top 1,000 negative words with high cosine similarity to 'Crisis,' more effectively captures the impact of news sentiment on the stock market index than the original sentiment lexicon (Sent0). The results underscore the importance of considering negative nuances and context when analyzing news content and its potential impact on market dynamics and public opinion.
    
[^154]: 大型语言模型能够构建因果图吗？

    Can large language models build causal graphs?

    [https://arxiv.org/abs/2303.05279](https://arxiv.org/abs/2303.05279)

    大型语言模型被证明对于探测词、上下文和提示敏感，但可以作为一种工具辅助因果图的发展。

    

    建立因果图可能是一个费时费力的过程。为了确保捕捉到所有相关的因果路径，研究人员通常不仅要与临床医生和专家讨论，还要审阅大量相关的医学文献。通过编码常见和医学知识，大型语言模型(LLMs)代表了一种机会，可以通过自动评分潜在图中的边缘（即两个变量之间的联系）来简化这一过程。然而，已经证明LLMs对用户选择的探测词、上下文和提示非常敏感。在这项工作中，我们评估了LLMs是否能够成为补充因果图发展的有用工具。

    arXiv:2303.05279v2 Announce Type: replace-cross  Abstract: Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.
    
[^155]: 面向关键词生成的预训练语言模型：一项深入的实证研究

    Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study

    [https://arxiv.org/abs/2212.10233](https://arxiv.org/abs/2212.10233)

    通过深入的实证研究，本文研究了不依赖预训练的神经模型与整合预训练语言模型（PLMs）在关键词生成任务中的性能比较，揭示了PLMs具有具有竞争力的高资源性能和最先进的低资源性能，并探讨了不同设计选择对基于PLM模型性能的影响。

    

    不依赖预训练的神经模型在拥有大量注释数据集的情况下在关键词生成任务中表现出色。与此同时，新方法已将预训练语言模型（PLMs）纳入其数据效率。然而，目前缺乏对比这两种方法以及不同设计选择如何影响基于PLM模型性能的系统研究。为填补这一知识空白，并促进更加明智地使用PLMs进行关键词提取和关键词生成，我们提出了一个深入的实证研究。我们将关键词提取构建为序列标注，关键词生成构建为序列到序列生成，并在三个领域进行了广泛实验。在展示PLMs具有具有竞争力的高资源性能和最先进的低资源性能后，我们研究了重要的设计选择，包括领域内PLMs，具有不同预训练目标的PLMs等。

    arXiv:2212.10233v2 Announce Type: replace  Abstract: Neural models that do not rely on pre-training have excelled in the keyphrase generation task with large annotated datasets. Meanwhile, new approaches have incorporated pre-trained language models (PLMs) for their data efficiency. However, there lacks a systematic study of how the two types of approaches compare and how different design choices can affect the performance of PLM-based models. To fill in this knowledge gap and facilitate a more informed use of PLMs for keyphrase extraction and keyphrase generation, we present an in-depth empirical study. Formulating keyphrase extraction as sequence labeling and keyphrase generation as sequence-to-sequence generation, we perform extensive experiments in three domains. After showing that PLMs have competitive high-resource performance and state-of-the-art low-resource performance, we investigate important design choices including in-domain PLMs, PLMs with different pre-training objective
    
[^156]: 基于多跳机器阅读理解的药物相互作用预测的医学知识图问答

    Medical Knowledge Graph QA for Drug-Drug Interaction Prediction based on Multi-hop Machine Reading Comprehension

    [https://arxiv.org/abs/2212.09400](https://arxiv.org/abs/2212.09400)

    该论文提出了一种医学知识图问答模型MedKGQA，利用机器阅读理解和开放域文档构建药物-蛋白质三元组知识图，通过向量化靶点属性和建立有向连接预测药物相互作用，取得4.5%的预测准确性改进。

    

    药物相互作用预测在分子生物学中是一个关键问题。传统方法通过医学实验观察药物相互作用需要大量资源和人力。本文提出了一种名为MedKGQA的医学知识图问答模型，通过从封闭域文献中进行机器阅读理解和构建开放域文档中的药物-蛋白质三元组知识图，来预测药物相互作用。该模型通过实体嵌入向量化图中的药物-蛋白质靶点属性，并根据人体内蛋白质靶点的代谢相互作用通路之间建立药物和蛋白质实体之间的有向连接。这将多个外部知识对齐并应用于学习图神经网络。在没有花哨技巧的情况下，所提出的模型在药物相互作用预测准确性方面取得了4.5%的改进。

    arXiv:2212.09400v3 Announce Type: replace  Abstract: Drug-drug interaction prediction is a crucial issue in molecular biology. Traditional methods of observing drug-drug interactions through medical experiments require significant resources and labor. This paper presents a medical knowledge graph question answering model, dubbed MedKGQA, that predicts drug-drug interaction by employing machine reading comprehension from closed-domain literature and constructing a knowledge graph of drug-protein triplets from open-domain documents. The model vectorizes the drug-protein target attributes in the graph using entity embeddings and establishes directed connections between drug and protein entities based on the metabolic interaction pathways of protein targets in the human body. This aligns multiple external knowledge and applies it to learn the graph neural network. Without bells and whistles, the proposed model achieved a 4.5% improvement in terms of drug-drug interaction prediction accurac
    
[^157]: 生成无监督的言辞解释用于谣言验证

    Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])

    [http://arxiv.org/abs/2401.12713](http://arxiv.org/abs/2401.12713)

    该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。

    

    在社交媒体上进行谣言验证的任务涉及根据由该谣言引起的对话线程评估其真实性的问题。尽管之前的工作已经专注于预测真实性标签，但我们在这里重新制定了任务，以生成与模型相关的自由文本解释谣言的真实性。我们采用无监督的方法，首先利用事后可解释性方法对线程中最重要的帖子进行评分，然后使用这些帖子通过使用模板引导总结生成信息丰富的解释性摘要。为了评估解释性摘要的信息量，我们利用了大型语言模型的少样本学习能力。我们的实验表明，语言模型在评估摘要时可以与人类达到类似的一致性。重要的是，我们证明了解释性的概括摘要比仅使用线程中排名最高的帖子更具信息量，并更好地反映了预测的谣言真实性。

    The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.
    
[^158]: Kun: 使用指令反向翻译的中国自对齐问题的答案优化方法

    Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])

    [http://arxiv.org/abs/2401.06477](http://arxiv.org/abs/2401.06477)

    Kun是一种使用指令反向翻译和答案优化的方法，用于创建高质量的指导调整数据集，该方法不依赖于手动注释，通过自我筛选过程来改善和选择最有效的指令-输出对。它的主要创新在于通过算法改进提高数据的保留和清晰度，并通过创新的数据生成方法减少了手动注释的依赖。

    

    在本文中，我们介绍了一种名为Kun的新方法，用于在不依赖手动注释的情况下为大型语言模型（LLMs）创建高质量的指导调整数据集。Kun利用来自吾道、完卷和SkyPile等多个来源的未标记数据，采用基于指令反向翻译和答案优化的自我训练算法，生成了一个超过一百万个中文指导数据点的大规模数据集。该方法通过使用自我筛选过程来完善和选择最有效的指令-输出对，显著偏离传统方法。我们在多个基准测试上对6B参数的Yi模型进行了实验，结果表明Kun具有鲁棒性和可扩展性。我们方法的核心贡献在于算法的改进，增强了数据的保留和清晰度，并且创新的数据生成方法极大地减少了对昂贵和耗时的手动注释的依赖。这种方法ological方法提出了一种解决中文自对齐问题的方法，并提高了数据的准确性和质量。

    In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
    
[^159]: 超越表面：文本到图像生成中视觉刻板印象的全球规模分析

    Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])

    [http://arxiv.org/abs/2401.06310](http://arxiv.org/abs/2401.06310)

    本论文提出了一种多方面的方法，利用现有的文本资源，基于135个全球范围内的身份群体对文本到图像生成（T2I）模型生成的图像中的地理文化刻板印象进行评估。研究结果表明，刻板属性在图像中的存在可能性是刻板属性的三倍。

    

    近期的研究已经强调了在文本到图像生成（T2I）模型生成的人物形象中存在的不同身份群体的刻板印象问题。然而，这些现有方法存在一些关键限制，包括在评估中对全球身份群体的覆盖率明显不足，以及相关刻板印象的范围。此外，它们通常缺乏对本质上是视觉刻板印象（如“瘦弱”或“墨西哥草帽”）和文化相关的刻板印象（如“吸引人”或“恐怖分子”）之间的重要区别。在本研究中，我们采用多方面的方法来解决这些限制，利用现有的文本资源来将我们对来自T2I模型生成的图像中与地理文化相关的刻板印象的评估进行基础绑定。我们使用现有的刻板印象基准来识别和评估全球范围内涉及135个基于国籍的身份群体的视觉刻板印象。我们证明，在图像中存在刻板印象的可能性是刻板属性的三倍。

    Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
    
[^160]: EpiK-Eval：将语言模型作为认识模型的评估

    EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])

    [http://arxiv.org/abs/2310.15372](http://arxiv.org/abs/2310.15372)

    这项研究介绍了一种新的评估方法EpiK-Eval，旨在评估大型语言模型（LLMs）在从分割的叙述中构建连贯和一致的知识表示方面的能力。研究发现当前的训练目标存在固有的缺陷，因此提出了改进知识整合方法的建议，以大幅提高LLMs的整体效果和性能。

    

    在人工智能时代，大型语言模型（LLMs）的作用越来越重要。尽管它们日益普及，但它们在从不同训练文档中整合知识的能力——在许多应用中都是关键能力——仍未得到探索。本文首次研究了LLMs在其参数空间内有效地结合这种信息的能力。我们引入了EpiK-Eval，一个新颖的问答基准，旨在评估LLMs在从分割的叙述中构建一种连贯和一致的知识表示方面的能力。对各种LLMs的评估揭示了在这一领域存在的显著弱点。我们认为这些缺点源于现有训练目标的固有性质。因此，我们主张改进知识整合的方法，因为这有潜力显著提高LLMs的整体效果和性能。

    In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from 
    
[^161]: MetaTool基准：决定是否使用工具和选择使用哪个工具。

    MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])

    [http://arxiv.org/abs/2310.03128](http://arxiv.org/abs/2310.03128)

    本文提出了一个名为MetaTool的基准，旨在评估大型语言模型（LLMs）是否具有工具使用意识并且能够正确选择工具。基准中包含一个名为ToolE的数据集，其中包含各种类型的用户查询，用于触发LLMs使用工具。

    

    大型语言模型（LLMs）由于其出色的自然语言处理（NLP）能力而受到了广泛关注。最近，许多研究关注LLMs的工具利用能力。它们主要研究了LLMs如何有效地与给定的特定工具合作。然而，在LLMs充当智能体的场景中，例如AutoGPT和MetaGPT应用中，LLMs被期望参与涉及是否使用工具以及从可用工具集中选择最合适的工具来满足用户请求的复杂决策过程。因此，在本文中，我们介绍了MetaTool，这是一个用于评估LLMs是否具有工具使用意识并且能够正确选择工具的基准。具体而言，我们在该基准中创建了一个名为ToolE的数据集。该数据集包含以触发LLMs使用工具的提示形式出现的各种类型的用户查询，包括单一工具和多种工具。

    Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
    
[^162]: CALLA数据集：从中文医学文献中探索LLMs的交互式知识获取

    The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])

    [http://arxiv.org/abs/2309.04198](http://arxiv.org/abs/2309.04198)

    该研究介绍了CALLA数据集，用于探索LLMs从中文医学文献中获取交互式知识。通过自由对话事实核查任务，评估了LLMs掌握医学知识的能力，并发现了一种称为“事实跟随响应”的现象。为了提供更准确的评估方法，人工构建了两种角度的测试数据：一种与事实一致，一种与事实不一致。

    

    大型语言模型（LLMs）在医学领域的应用引起了研究人员的兴趣。最近的研究集中于通过医学知识图构建指导微调（IFT）数据，以丰富LLMs的交互式医学知识。然而，作为丰富的医学知识来源的医学文献仍未被开发利用。我们的工作引入了CALLA数据集，以探索LLMs从中国医学文献中获取交互式知识。它通过自由对话事实核查任务评估LLMs掌握医学知识的能力。我们发现一种现象称为“事实跟随响应”，LLMs倾向于确认问题中提到的事实，并对挑战这些事实表现出不情愿。为消除这种现象导致的不准确评估，对于黄金事实，我们从两个角度人工构建测试数据：一个与事实一致，一个与事实不一致。根据这些测试数据，我们为LLMs评估其对医学知识的掌握能力提供了更准确的方法。

    The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the 
    
[^163]: ParaGuide: 用于即插即用文本风格转移的引导性扩散改写器

    ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])

    [http://arxiv.org/abs/2308.15459](http://arxiv.org/abs/2308.15459)

    ParaGuide是一种用于通用风格转移的引导性扩散改写器，可以灵活适应任意目标风格，通过梯度引导和改写条件的扩散模型实现文本的风格转变，同时保留语义信息。

    

    文本风格转移是在保留意义的同时转变文本的风格属性的任务。目标风格可以以多种方式定义，从单一属性（例如正式性）到作者（例如莎士比亚）。先前的无监督风格转移方法通常依赖于大量标记数据，仅适用于固定的风格集，或需要大型语言模型。相反，我们引入了一种新的基于扩散的通用风格转移框架，可以在推理时灵活适应任意目标风格。我们的参数高效方法ParaGuide利用了改写条件的扩散模型以及来自现成的分类器和强大的风格嵌入器的梯度引导，以转变文本的风格同时保留语义信息。我们在Enron邮件语料库上进行了验证，包括人工和自动评估，并发现其在正式性和... (内容太多，请参考英文摘要)

    Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
    
[^164]: 机器学习在信任与安全方面的挑战：一个针对虚假信息检测的案例研究

    The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])

    [http://arxiv.org/abs/2308.12215](http://arxiv.org/abs/2308.12215)

    本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。

    

    我们使用虚假信息检测作为案例研究，检查了在将机器学习应用于信任与安全问题上学术和实践之间的脱节。我们对该领域中270篇广受引用的论文进行了自动检测虚假信息的文献系统化，并对子集中的论文进行了数据和代码的可用性、设计失误、可复现性和泛化性等方面的研究。我们发现文献中存在严重的不足之处，这对所声称的性能和实用性提出了质疑。检测任务通常与在线服务真正面临的挑战有本质上的区别。数据集和模型评估通常不代表现实世界的情景，而且评估往往不独立于模型训练。数据和代码的可用性很差。模型在领域外的数据上泛化能力不强。基于这些结果，我们提出了评估机器学习应用于信任与安全问题的建议。

    We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
    
[^165]: ConceptBed: 评估文本到图像扩散模型的概念学习能力

    ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])

    [http://arxiv.org/abs/2306.04695](http://arxiv.org/abs/2306.04695)

    本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。

    

    理解视觉概念并从图像中复制和组合这些概念的能力是计算机视觉的一个核心目标。最近文本到图像（T2I）模型的进展使得通过学习大量图像及其描述来生成高清晰度和逼真的图像质量成为可能。然而，评估T2I模型的重点在于照片般的真实感和有限的视觉理解定性量度。为了量化T2I模型在学习和合成新的视觉概念方面的能力，我们引入了ConceptBed，一个包含284个独特视觉概念、5K个独特概念组合和33K个组合文本提示的大规模数据集。除了数据集，我们提出了一个评估指标Concept Confidence Deviation（CCD），它利用oracle概念分类器的置信度来衡量T2I生成器生成的概念与地面真实图像中包含的概念之间的对齐度。我们评估的视觉概念是对象或者...

    The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
    
[^166]: 大型语言模型作为反事实生成器: 优势和劣势

    Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])

    [http://arxiv.org/abs/2305.14791](http://arxiv.org/abs/2305.14791)

    本文研究了大型语言模型（LLMs）作为反事实生成器的能力，通过数据增强实验发现它们在各个任务中表现优异，但仍存在自我限制和缺乏逻辑指导等问题。

    

    大型语言模型（LLMs）在自然语言理解和生成任务中表现出卓越的性能。然而，它们生成反事实的能力，如用于数据增强的反事实，仍未得到充分的研究。本研究旨在调查LLMs的反事实生成能力，并分析影响这种能力的因素。首先，我们通过在情感分析、自然语言推理、命名实体识别和关系抽取等四个任务中进行数据增强实验来评估LLMs在反事实生成方面的效果。虽然LLMs在各种设置中都表现出有希望的改进，但由于其自我限制和缺乏与常识相符的逻辑指导来生成反事实，它们在复杂任务中仍然面临困难。其次，我们的分析揭示了为LLMs提供准确的任务定义和详细的逐步指导在生成有效的反事实中发挥了关键作用。

    Large language models (LLMs) have demonstrated remarkable performance in a range of natural language understanding and generation tasks. Yet, their ability to generate counterfactuals, which can be used for areas like data augmentation, remains under-explored. This study aims to investigate the counterfactual generation capabilities of LLMs and analysis factors that influence this ability. First, we evaluate how effective are LLMs in counterfactual generation through data augmentation experiments for small language models (SLMs) across four tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. While LLMs show promising enhancements in various settings, they struggle in complex tasks due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense. Second, our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in ge
    
[^167]: 个性化指导有助于知识蒸馏

    Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])

    [http://arxiv.org/abs/2305.09651](http://arxiv.org/abs/2305.09651)

    本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    

    先前研究表明，能力超群的教师模型并不一定能够让学生水平得到提升，这凸显了当前教师培训实践和有效知识传授之间的不一致性。为了提高教师培训过程的指导效果，本文引入了蒸馏效应的概念，以确定每个训练样本对学生泛化能力的影响。我们提出了一种名为学好教师很重要（LGTM）的有效训练技术，以将蒸馏效应纳入教师的学习过程中。通过优先选择可能提升学生泛化能力的样本，我们的LGTM在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
    
[^168]: 考虑多轮对话上下文的领域外意图检测

    Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])

    [http://arxiv.org/abs/2305.03237](http://arxiv.org/abs/2305.03237)

    本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。

    

    领域外（OOD）意图检测对于实用的对话系统非常重要，通常需要考虑多轮对话上下文。然而，大多数先前的OOD意图检测方法仅限于单轮对话。在本文中，我们介绍了一个上下文感知的OOD意图检测（Caro）框架，用于对OOD意图检测任务中的多轮上下文进行建模。具体地，我们遵循信息瓶颈原则从多轮对话上下文中提取稳健的表示。每个输入样本构建了两个不同的视角，使用多视图信息瓶颈损失删除与意图检测无关的多余信息。此外，我们还探索了在Caro中利用未标记的数据。引入了一个两阶段训练过程来从这些未标记的数据中挖掘OOD样本，并使用自举方法用这些OOD样本来训练生成的模型。全面的实验表明，Caro在OOD意图检测任务的几个基准数据集上建立了最先进的性能，并超越了仅考虑单轮上下文的先前方法。

    Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
    
[^169]: 中文LLaMA和Alpaca的高效有效的文本编码

    Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08177](http://arxiv.org/abs/2304.08177)

    这篇论文提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以改善LLaMA对中文的理解和生成能力。

    

    大型语言模型（LLM）已经彻底改变了自然语言处理研究，并显示出朝着人工通用智能（AGI）的有希望的进展。然而，训练和部署LLM的高成本对透明、可访问的学术研究构成了重大障碍。在这篇论文中，我们提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以便更好地理解和生成中文文本及其指令。

    Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
    
[^170]: 使用语言反馈规模化训练语言模型

    Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])

    [http://arxiv.org/abs/2303.16755](http://arxiv.org/abs/2303.16755)

    本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。

    

    预训练的语言模型经常生成不符合人类偏好的输出，例如有害的文本或事实不正确的摘要。最近的研究尝试通过学习一种简单的人类反馈形式（即模型生成输出之间的比较）来解决这些问题。但是，比较反馈只能传达有限的关于人类偏好的信息。在本文中，我们介绍了一种新的方法——使用语言反馈进行模仿学习（ILF），它利用了更丰富的语言反馈。ILF由三个迭代步骤组成：第一步，根据输入，初始LM输出和反馈对语言模型进行调节以生成改进。第二步，选择最多反馈的改进。第三步，微调语言模型，以最大化在给定输入的情况下选择的改进的可能性。我们在理论上证明了ILF可以被看作是贝叶斯推断，类似于从人类反馈中进行强化学习。我们还评估了ILF在各种基准测试中的性能。

    Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
    
[^171]: 利用自然语言反馈进行代码生成的改进

    Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])

    [http://arxiv.org/abs/2303.16749](http://arxiv.org/abs/2303.16749)

    该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。

    

    预先训练好的大型语言模型（LLM）在推理时使用自然语言反馈的潜力是最近的一个令人兴奋的发展。我们在此基础上提出一种名为Language Feedback（ILF）的算法，用于从自然语言反馈中进行学习。ILF在训练期间仅需要少量的人工编写反馈，并且在测试时不需要相同的反馈，因此使用起来既方便又高效。此外，我们进一步证明ILF可以被视为最小化与基准分布的KL散度的一种形式，并在神经程序合成任务上进行了概念验证。我们使用ILF在Mostly Basic Python Problems(MBPP)基准测试上将Codegen-Mono 6.1B模型的pass @ 1覆盖率相对提高了38%（绝对提高了10%），胜过了在MBPP上微调和在人类修复的程序上微调的模型。总的来说，我们的结果表明，即使只有少量反馈，从人类编写的自然语言反馈中进行学习也可以显著改进代码生成模型。

    The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
    
[^172]: 关于遮蔽语言模型学习条件句的不一致性

    On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00068](http://arxiv.org/abs/2301.00068)

    本论文研究发现，遮蔽语言模型学习的条件句往往存在着不一致性，无法从一个连贯的联合分布中推导出来。我们通过实证发现这种不一致性普遍存在于不同尺寸和配置的遮蔽语言模型中。为了解决这个问题，我们提出了条件句集合方法来在推断阶段处理不一致性。

    

    已经证明了在序列中学习预测遮蔽标记是一个对大型语言模型来说很有力的预训练目标。训练后，这些遮蔽语言模型可以提供基于双向上下文的标记分布。本论文展示了与常见假设相反，这种双向条件句经常表现出相当大的不一致性，即在考虑在一起时不能从一个连贯的联合分布导出它们。我们在遮蔽语言模型的两种常见风格（T5风格和BERT风格）的简单双字母词比较场景中通过实证量化了这种不一致性。例如，我们发现T5模型经常混淆自己对两个相似双字母词的偏好。我们还展示了不一致性在不同尺寸和配置的遮蔽语言模型中普遍存在，从RoBERTa-base到GLM-130B。作为解决这个问题的初始尝试，我们提出了条件句集合，在推断阶段处理这个问题。

    Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se
    

