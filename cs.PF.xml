<rss version="2.0"><channel><title>Chat Arxiv cs.PF</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.PF</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.05045</link><description>&lt;p&gt;
&#21033;&#29992;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20248;&#21270;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#24320;&#38144;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#35268;&#27169;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21098;&#26525;&#31639;&#27861;&#65292;&#33021;&#22815;&#21098;&#26525;&#65288;&#21363;&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65289;80-90&#65285;&#30340;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#19982;&#26410;&#21098;&#26525;&#29238;&#32593;&#32476;&#30456;&#31561;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;AxoNN&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#23618;&#38388;&#24182;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#26102;&#38388;&#21644;&#20869;&#23384;&#21033;&#29992;&#30340;&#20943;&#23569;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2110.06804</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of Binary Neural Network. (arXiv:2110.06804v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive overview of recent developments in Binary Neural Networks (BNN), with a focus on 1-bit activations and 1-bit convolution networks. These networks can be implemented and embedded on tiny restricted devices, saving significant storage, computation cost, and energy consumption.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26368;&#36817;&#25913;&#21464;&#20102;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#23613;&#31649;DL&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#21644;&#28508;&#21147;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#35745;&#31639;&#21463;&#38480;&#21644;&#33021;&#37327;&#21463;&#38480;&#35774;&#22791;&#20013;&#38656;&#35201;&#36827;&#34892;DL&#22788;&#29702;&#12290;&#30740;&#31350;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#31561;&#20855;&#26377;&#25913;&#21464;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#20197;&#22686;&#21152;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#26159;&#24456;&#33258;&#28982;&#30340;&#12290;&#26368;&#36817;&#22312;BNN&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;BNN&#34892;&#20026;&#37117;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;BNN&#26368;&#36817;&#21457;&#23637;&#30340;&#23436;&#25972;&#27010;&#36848;&#12290;&#26412;&#25991;&#19987;&#38376;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#19982;&#20197;&#21069;&#30340;&#35843;&#26597;&#28151;&#21512;&#20351;&#29992;&#20302;&#20301;&#20316;&#21697;&#30456;&#21453;&#12290;&#23427;&#23545;BNN&#30340;&#24320;&#21457;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has recently changed the development of intelligent systems and is widely adopted in many real-life applications. Despite their various benefits and potentials, there is a high demand for DL processing in different computationally limited and energy-constrained devices. It is natural to study game-changing technologies such as Binary Neural Networks (BNN) to increase deep learning capabilities. Recently remarkable progress has been made in BNN since they can be implemented and embedded on tiny restricted devices and save a significant amount of storage, computation cost, and energy consumption. However, nearly all BNN acts trade with extra memory, computation cost, and higher performance. This article provides a complete overview of recent developments in BNN. This article focuses exclusively on 1-bit activations and weights 1-bit convolution networks, contrary to previous surveys in which low-bit works are mixed in. It conducted a complete investigation of BNN's dev
&lt;/p&gt;</description></item></channel></rss>